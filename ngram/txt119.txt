Neural Network Modeling and Connectionism
and Brain Function: The Developing Interface
Stephen Jose Hanson and Carl R. Olson, editors
Neural Network Design and the Complexity of Learning
W. Thomas Miller, Richard S. Sutton, and Paul J. Werbos, editors
Edited by W. Thomas Miller, III, Richard S. Sutton, and Paul J.
part of this book may be reproduced in any form
by any electronic or mechanical means (including photocopying, recording,
or information storage and retrieval) without permission in writing from the
This book was set in Computer Modern by The MIT Press and printed and
Library of Congress Cataloging-in-Publication Data
edited by W. Thomas Miller, III, Richard S.
cm.-(Neural network modeling and connectionism)
workshop held at the University of New Hampshire in Octo­
ber, 1988, sponsored by the National Science Foundation, and entitled "The
application of neural networks to robotics and control."
Connectionist Learning for Control: An Overview
A Menu of Designs for Reinforcement Learning Over
Adaptive State Representation and Estimation Using
A Summary Comparison of CMAC Neural Network
Recent Advances in Numerical Techniques for Large
First Results with Dyna, An Integrated Architecture
Computational Schemes and Neural Network Mod­
els for Formation and Control of Mu lt ij o int Arm
Using Associative Content-Addressable Memories to
Christopher G. Atkeson and David J. Reinkensmeyer
An Adaptive Sensorimotor Network Inspired by the
James C. Houk, Satinder P. Singh, Charles Fisher, and
Some New Directions for Adaptive Control Theory
Judy A. Fran klin and Oliver G. Selfridge
Applications of Neural Networks in Robotics and
A Bioreactor Benchmark for Adaptive Networkbased Process Control
A Neural Network Baseline Problem for Control of
Intelligent Control for Multiple Autonomous Under427
Martin Herman, James S. Albus, and Tsai-Hong Hong
The goal of this series, Neural Network Modeling and Connectionism, is
to identify and bring to the public the best work in the exciting field of
neural network and connectionist modeling. The series includes mono­
graphs based on dissertations, extended reports of work by leaders in the
field, edited volumes and collections on topics of special interest, major
reference works, and undergraduate and graduate-level texts. The field
is highly interdisciplinary, and works published in the series will touch on
a wide variety of topics ranging from low-level vision to the philosophical
foundations of theories of representation.
Andrew Barto, University of Massachusetts, Amherst
Jerome Feldman, University of California, Berkeley
James McClelland, Carnegie-Mellon University
Domenico Parisi, Instituto di Psicologia del CNR
David Touretzky, Carnegie-Mellon University
David Zipser, University of California, San Diego
In this book, we have tried to bring together the different strands of
theory and application that are most relevant to understanding and ex­
tending the state of the art in the use of neural networks for control. By
neural networks we mean engineered computational systems modeled
after or inspired by the learning abilities and parallelism of biological
nervous systems. The past decade has seen a dramatic increase in in­
Today neural networks are actively explored
in artificial intelligence, psychology, engineering, and physics.
companies are producing commercial products based on the technology,
including speech recognizers, explosives detectors, airline seat allocation
being pursued as well. There seems little doubt but that neural network
technology will have a lasting impact on the engineering sciences.
By neural networks for control we mean neural networks that go be­
yond monitoring or classifying their input signals to actually influencing
them. Unlike most neural networks, these are explicitly designed to learn
from a closed-loop interaction with their environment, for example, from
a robot's interaction with the parts it must glasp and assemble. Closed­
loop control involves a very different set of requirements for learning
methods than those usually considered in neural-network research. For
example, in control it is much more important to learn online, incremen­
tally, and without an explicit supervisor specifying desired behavior. In
this book we focus on these and other special features of control appli­
This book is based on a workshop held at the University of New Hamp­
shire in October, 1988, sponsored by the National Science Foundation,
and entitled "The Application of Neural Networks to Robotics and Con­
The workshop brought together representatives from three key
neural network researchers from computer science and
psychology, 2) experts in control theory and optimization, and 3) spe­
cialists in areas of likely application of neural network controllers. These
three communities have been facing similar control challenges, but with
different objectives and from different perspectives.
workshop were to highlight key issues in learning control and to iden­
tify research directions that might lead to practical solutions in critical
issues in neural networks for control that are independent of applica­
tion domains, including recurrent networks, reinforcement learning, and
relationships with conventional and adaptive control theory.
discusses neural network learning with regard to motion planning and
control in robotics. This class of control problems has been the most fre­
quently studied in learning control research. The four chapters of part
III each treat an application domain well suited to the capabilities of
neural-network controllers: process control, manufacturing, flight con­
trol, and autonomous vehicles. Finally, the appendix presents complete
descriptions of seven benchmark control problems. These problems have
been selected to be easy to simulate yet to contain important aspects of
The varying emphases of the individual chapters reflects the diverse
scientific backgrounds of the contributing authors.
attempted to eliminate aspects which were judged to be unnecessarily
contradictory or redundant, but no attempt has been made to enforce a
The contents of this book are based on a workshop entitled "The Appli­
cation of Neural Networks to Robotics and Control," which was spon­
sored by the National Science Foundation, and held at the University
Although not all who attended the workshop are authors of chapters,
all contributed significantly through discussions with t he editors and au­
thors. The names and addresses of all participants are listed below. The
affiliations listed are those that applied at the time of the workshop.
National Institute of Standards and Technology
Computer and Intelligent Systems Laboratory
Department of Computer and Information Sciences
Division of Electrical, Communications and Systems Engineering
Computer and Intelligent Systems Laboratory
Department of Electrical and Computer Engineering
Department of Brain and Cognitive Sciences
ATR Auditory and Visual Perception Research Laboratories
Department of Electrical and Computer Engineering
Department of Electrical and Computer Engineering
Department of Electrical, Computer and Systems Engin eering
Computer and Intelligent Systems Laboratory
School of Engineering and Applied Sciences
Division of Emerging Engineering Technologies
Division of Emerging Engineering Technologies
Bioengineering and Research to Aid the Disabled
The editors would also like to acknowledge the efforts of Christopher
Aldrich and Janto Surwijo from the Department of Electrical and Com­
puter Engineering at the University of New Hampshire, who performed
much of the electronic formatting of the book manuscript.
The chapters in part I explore general issues in the application of neural
network learning to planning and control for dynamical systems. These
chapters do not present complete problem solutions.
scribe basic concepts and problems and suggest promising directions
for future development. Several critical research issues identified at the
workshop are addressed in these chapters:
Integration of neural network models in control architectures. Neu­
ral networks are well suited for nonlinear modeling in control.
Techniques for training and utilizing such models effectively as
components of the overall control system need to be investigated.
Neural network representations for dynamical systems. Dynamical
systems are in general history dependent and can not be modeled
training techniques for building dynamic models are required.
of performance over time. Many important control
problems are specified in terms of criteria to be optimized over
many control cycles, rather than in terms of instantaneous per­
Neural network architectures and training
techniques must be developed to handle such control problems for
Stability theory for closed loop learning controllers. The dynamics
of network adaptation in closed loop control structures with on­
line learning need to be investigated theoretically for simple plants,
and experimentally for complex plants. In such an environment,
open loop analyses alone are not sufficient.
Comparison of tools for adaptive/learning control. It is not useful
to approach neural networks as a blanket solution to all control
The practical solution of difficult problems in control
will require using a variety of tools in appropriate combinations.
To further this end, it is necessary to comparatively characterize
different neural network paradigms, and illustrate their relation­
ships to conventional and adaptive control techniques.
The first chapter, by Barto, provides an excellent overview of research
in artificial neural networks, focusing on relationships to more tradi­
tional research in control, signal processing and pattern classification.
Werbos extends the overview in the next chapter, comparing five pop­
also by Werbos, presents a very general perspective on control prob­
lems in which the goal is one of optimization over time, or reinforcement
Such problems are often encountered in practice and have
conventionally been approached with optimal control methods such as
dynamic programming. W illiams expands the discussion to include re­
current network architectures, such as Hopfield networks, in which closed
more complex dynamics than acyclic networks, and are less well under­
address the theme that runs throughout the part: the similarities and
differences between conventional and neural-network control methods.
Narendra relates the capabilities and dynamics of backpropagation and
Hopfield networks to conventional adaptive control methods, and Kraft
and Campagna compare and contrast adaptive control methods with a
neural network architecture developed by Miller and based on Albus's
Model Arithmetic Computer ) . In the seventh chap­
ter, Shanno discusses alternatives to steepest descent weight adjustment
techniques, with the goal of achieving faster convergence. Finally, in the
last chapter Sutton presents an approach to planning which integrates
learning in artificial neural networks with the classical optimization tech­
This chapter is an introductory overview of learning by artificial neural
networks with a focus on the ideas and methods most relevant to the
control of dynamical systems. The perspective taken emphasizes the
continuity of the current research on artificial neural networks, which I
call connectionist research, with more traditional research in control, sig­
nal processing, and pattern classification. Tying connectionist learning
methods to existing theory is important for those of us interested in con­
nectionist approaches to adaptive and learning control because control
theory is a well-developed field with a large literature, and many of the
learning methods being described by connectionists are closely related to
methods that already have been intensively studied by adaptive control
theorists. On the other hand, the directions that connectionists are tak­
ing these methods have characteristics that are absent in the traditional
engineering approaches. In this chapter, I describe these characteristics
and discuss their positive and negative aspects.
This chapter is not intended to be a review of all research on connec­
tionist learning, and no attempt is made to provide exhaustive references
to the connectionist literature. The review by Hinton ( 1 989) covers a
wider range of learning methods ( although it does not focus on control ) ,
and there are many publications that carefully elaborate many of the ob­
servations I make. Here attention is restricted to basic issues concerning
supervised and reinforcement learning, and I assume that readers already
know about the most popular connectionist learning algorithms. In the
other chapters of the overview part of this book, Werbos provides more
detailed analysis of reinforcement learning, and Williams provides an
overview of learning in recurrently connected networks. Unsupervised
learning is not discussed in these overview chapters even though it is
an important form of learning that has relevance for control problems.
We also do not discuss the hardware side of connectionist research­
taking for granted that the suitability of network methods for parallel
implementation is one of the attractive aspects of this approach; and
we do not discuss any of the interesting issues that arise when relating
connectionist research £�tIh/IHlcNlaterial
Figure 1.1 is a sketch of my mental model of the recent history of cop.nec­
tionist research in relation to engineering and artificial intelligence (AI).
Although it strongly reflects my own history and biases,l I think it ac­
curately represents several important features of connectionist research.
The first event shown in the sketch is the divergence of research into the
lines labeled "engineering methods" and "sequential symbol manipula­
tion methods," although these separate lines of research did not emerge
as suddenly as suggested by figure 1.1, and there were other offshoots
marking the end of what Minsky and Papert (1988) called the "romantic
period" during which biologically inspired computational schemes were
first studied (see Arbib 1987). I associate this divergence with Minsky's
paper, "Steps Toward Artificial Intelligence" (Minsky 1961), in which
he argued that symbolic "articulated" representations have powerful ad­
vantages over the feature-vector representations to which connectionist
methods were, and arguably still are, restricted. In fact, one might la­
bel these two main lines of research "feature vectors" on the left and
"articulated representations" on the right.
A feature vector is a list of measurements, having numerical or logical
values, made of some situation, state, object, etc. An articulated rep­
resentation, on the other hand, can have a recursive structure allowing
"wholes" to be represented in the same way that "parts" are represented.
This allows descriptions of situations to be recursively embedded within
the representations of more complex situations in a way that does not
have an obvious counterpart for feature vectors. Despite considerable
research effort directed toward bringing feature vectors and articulated,
or symbolic, representions closer together, e.g. , Touretzky and Hinton
(1985), Touretzky (1986) , Derthick (1988), Hinton (1988), and Pollack
(1988) , there is still a fundamental gulf between these representational
styles. The engineering tradition shown by the left line in Figure 1 . 1
has not addressed these representational issues because feature-vector
representations are natural for the kinds of prediction and control prob­
lems with which engineers are concerned. Consequently, discussions of
the relevance of connectionist methods to control tend not to address
the questions about connectionist analogs of symbolic representations
that are occupying much of the attention of AI and cognitive science
1 In this chapter, when I refer to "engineering" and "AI" I am often using these
terms in an admittedly stereotyped way that does not reflect the diversity of these
fields. I ask the reader not 1...0 take. tjli,s u!l�e too. literally and conclude that r am
personal view of the history of connectionist research.
researchers. Although symbolic processing will undoubtedly play an im­
portant role in intelligent control systems-perhaps with connectionist
approaches to these problems playing key roles-there seems to be am­
ple opportunity for progress in control before one needs to consider the
kinds of articulated representations that dominate symbolic AI. Perhaps
it is also significant that at this workshop we will not be hearing a great
deal about the profound problems in scaling-up network techniques to
large problems (Judd 1987a, 1987b, 1988) . This is certainly relevant for
control, but considerable progress in control seems possible before these
barriers are approachedc�pyrighted Material
Another major feature of my sketch of history shown in figure
i s that the line representing current connectionist research branches off
from the sequential symbol manipulation line-not the engineering line.
The significance of this is that while connectionist techniques generally
are closely related to engineering methods (numerical feature vectors,
gradients, mean-square-error, etc. ) , much connectionist research con­
tinues the tradition in AI of addressing problems that are not formu­
lated with a high degree of mathematical structure (such as linearity).
Partly because of this, connectionist research is more experimental and
heuristic than my impression of modern engineering methodology. For
example, many connectionist researchers are currently experimenting
with the back propagation method for training layered networks (inde­
framework that guides its application and provides guarantees about
its results. Additionally, connectionist researchers routinely exercise a
kind of representational freedom that is not commonplace in more or­
thodox engineering applications. For example, a feature vector may be
a distributed representation of a kind of symbolic representation (e.g.,
of measurements of "physical" quantities more natural in engineering
applications (position, velocity, etc.). Connectionist methods often em­
ploy "expansive representations" (about which I say more in section
having dimensionality much higher than the apparent intrinsic dimen­
I do not mean to imply that experiment and heuristics are absent
from engineering, or that connectionist researchers can lay sole claim to
all of the methods they use. Indeed, in the field of control engineering,
there have been numerous efforts to develop more heuristic approaches
research was explicitly influenced by the methodology of AI and in­
cludes some of the features of the connectionist approach discussed in
Studies such as these represent early attempts in con­
engineering and AI for applications to control. There are undoubtedly
many reasons that research of this kind did not lead to major develop­
ment within control engineering. Although some of these reasons may
involve real deficiencies of these methods for certain kinds of problems, I
would like to suggest that two additional factors may have been impor­
based methods-the same developments in AI that isolated the connec­
tionists of the 1960s-may have also isolated heuristically minded con­
trol theorists. Second, many of the control methods proposed by these
researchers had memory requirements that were excessive in terms of
the available technology. Now, however, not only is AI somewhat more
hospitable to work of this kind, but technological advances have made
memory-intensive approaches to control quite feasible. A perspective
taken in this chapter is that connectionist approaches to control are
instances of memory-instensive approaches, and that it is now techno­
logically possible to explore the utility of these methods.
One of the strengths of connectionist research is that it lies in this
middle ground between engineering and AI-utilizing engineering-like
mathematical techniques while simultaneously incorporating expansive
representations and an experimental methodology more like that of AI.
It is obviously desirable to maintain high standards of mathematical
rigor, but an experimental, heuristic approach seems essential both for
developing applications involving complex nonlinear systems and for de­
tecting the regularities over classes of problems that can guide rigorous
mathematical development. However, as for any approach to complex
control problems in which the hypotheses of convergence and stability
theorems are not likely to be satisfied, it is important to recognize that
some problems are not well suited for the application of connectionist
learning methods. These are problems for which control failure is unac­
ceptably costly or for which it is not possible to use proven control prin­
ciples to safeguard against unacceptable performance. Although connec­
tionist learning methods have the potential for extending the range of
control applications, it may turn out that the applications most suitable
for these methods will be those related to the control capabilities of ani­
mal nervous systems; although this is a very broad class of applications,
Connectionist research is providing a bridge between lines of research
that diverged more than twenty years ago. The current high level of
activity in connectionist research can be compared with the excitement
that would result if scientists from different but similar planets suddenly
found themselves able to communicate. There would be a sudden flow of
information as concepts on each side were recast within the other sides'
frameworks. Just as it would not be surprising that methods well-known
for years on one side were being reformulated using a different language,
it would not be surprising that some of the ideas on each side were new
to the other. These s�tER1 I1M\im1a), factors are as important
as the technical factors in understanding what has happened in the last
several years as interest in connectionist research has exploded.
In the rest of this chapter, I discuss connectionist learning methods
using the framework of parameter estimation as it is understood by
engineers, emphasizing the ideas and techniques that I
contributions from the connectionist research community.
Most of the learning methods studied by connectionists are examples of
Methods for parameter estimation are central to the fields of pattern
classification (e.g., Sklansky and Wassel 1981), adaptive signal process­
ing (e.g., Widrow and Stearns 1985), and adaptive control (e.g., Good­
win and Sin 1984). For example, one way to construct a pattern classifier
is to assume that the classification rule, or decision rule, is a member
of a specific class of rules, and that each rule in the class is specified
by selecting values for a set of parameters: Learning is the process of
adjusting parameter values based on how well the decision rule performs
on a set of training patterns whose correct classifications are supplied
In signal processing and control, one is often seeking a model capable
of producing numerical values that match values measured from some
physical system. This application of parameter estimation is known as
system identification. One assumes that the model has a specific math­
ematical form expressed in terms of a set of unknown parameter values
and then uses a parameter estimation method to find good parameter
values. The estimation method requires data in the form of examples of
how the system being modeled behaves for a collection of inputs. The
role of the teacher is played by the system being modeled. In a similar
manner, parameter estimation methods can be applied to the problem
of a system (Widrow and Stearns 1985). I dis­
cuss system identification, inverse system identification, and other types
of problems relevant to control in more detail below.
Another way to apply parameter estimation techniques is distinctively
attributable to the connectionist approach: the formation of
memory networks. Although the parameters (the connection weights)
of associative memory networks are updated using parameter estimation
techniques, connectionists think about associative memory networks dif­
models. Instead of regarding the parameter estimation process as one
of searching for the best-fitting model in a class of models, one thinks
of it as storing information in a memory structure.
usual lookup table form of memory, associative memory networks have
the desirable properties of some degree of noise resistance and content
addressability (see Kohonen 1977 and Hinton and Anderson
discussions of associative memory networks). Viewed as system identifi­
cation procedures, on the other hand, associative memory networks can
be criticised because they often implement transformations that are not
specialized enough to provide good models of the process generating the
The idea of an associative memory network is a �ood example of an
idea linking engineering methods with the style of thinking more like that
of computer science and AI. It is not surprising that the modern wave of
connectionist research began in the mid-1970s with the popularization
of associative memory networks, although Widrow and Hoff discussed
the associative memory idea in 1960 (Widrow and Hoff 1960). Recog­
nizing that associative memory and system, or rule, identification are
closely related to one another and yet are evaluated by different criteria
is important for understanding the relationship between connectionist
and traditional applications of parameter estimation. These points are
The essential elements of a parameter estimation problem are as fol­
Problem representation-This is an aspect of a parameter-estima­
tion problem that is guided the least by useful theory.
aspects of objects, inputs, states etc. should be measured to pro­
vide data for classification or modeling? If basic physical variables
are obvious candidates, should they be represented simply as real­
valued measurements, or should they be coded in some other way?
One characteristic of connectionist research is that representations
are often chosen that are suggested by what we know, or think we
know, about how nervous systems represent information.
representations often differ from those an engineer might choose.
In using connectionist networks, choosing a representation corre­
sponds to selecting how the signals reaching the network's input
units are related to the problem under consideration, as well as
how the network's output codes the relevant problem variables .
rameter estimatiQDo�"'e�tf8tle by theory.
select an appropriate class of decision rules or model structures­
a choice that includes deciding how many parameters to use and
how they define the decision rule or model. Generally, one must
make a compromise between rule or model complexity and its ad­
corresponds to choosing the types of units, the structure of the
network, and the constraints among connection weights to enforce
Performance cri te ri a -How does one compare the performance of
the different decision rules or models in the delineated class? An
important problem here is that one would like to measure the per­
formance of a system without having to completely implement it.
The best measure of performance might evaluate the overall, final
peformance of the system that makes use of the decision rule or
in the commercial world, "How well does the
device sell?" j in the biological world, "How well does the organ­
ism reproduce?"). However, in practice one has to devise simpler
criteria that are closely correlated with overall performance and
tion it is common to measure performance by the average of the
squared error between the output of the decision rule or model
and some target values. The problem of finding easily-measured
evaluation criteria that can act as surrogates for more basic but
hard-to-measure criteria is a central issue in studies of learning
Estimation methods-Two major types of parameter estimation
line methods operate when the training data are available at one
time. These data are collected before performing the analysis that
determines the parameter values. On-line methods, on the other
hand, process the data as it becomes available to the learning
They must use efficient methods to be able to keep up
with the temporal flow of events. Here, I restrict attention to on­
line methods because connectionist networks generally implement
Use of a priori knowledge-The parameter estimation process can
be improved by the use of a priori knowledge, not only as it is
the class of decision rules or models, but also in the form of initial
parameter values and constraints among parameter values.
Connectionist learning methods are often studied under the
sumption that there is little prior information about the task at
hand, but this assumption is not a defining characteristic of con­
whether embodied in connectionist networks or not, by no means
entails a tabula rasa view of learning and memory.
trary, the incorporation of prior knowiedge can greatly accelerate,
and otherwise improve, the learning process.
Feature Vectors, Decision Rules, and Models
The framework in which parameter estimation techniques are applied is
essentially the same for classification and system identification tasks.2
x denote any one of the patterns one wishes
of data that will constitute input to the relationship being modeled. In
call all such collections of data patterns, even though
this is not the usual terminology for system-identification tasks. Let
denote the set of all patterns relevant to a given task and assume there
of the functions <pi-they can be linear or nonlinear functions of X
( assuming X has enough mathematical structure for linearity to make
sense ) . They are given concrete interpretation when a method is applied
to a specific real-world problem, and this interpretation constitutes the
basic problem representation discussed above.
Of particular importance are linear decision rules and linear models.
For a two-class classification task, the set of linear decision rules is pa­
21 do not discuss identification methods using recursive, or autoregressive, models
that are widely studied in engineering. These are models whose past outputs can
form part of their input. Identification methods using these models are related to the
training of recurrently connected networks as discussed by Williams in the present
publication. Identification using recursive models should not be confused with what
is sometimes called recursive parameter estimation or recursive system identification,
This is the linear threshold rule often used in connectionist networks. In
an identification task, linear models take the form
models with vector output in the obvious way). For both pattern clas­
sification and system identification, one would estimate the value of the
that produces the best match with the data sup­
plied during training (where best is defined in terms of some specific
Two conventions are usually followed when decision rules or models
can write the decision rule given by Equation
The second convention is to refer to the time at which a pattern appears
for classification, or a pattern is input to the system being identified. Let
Yt respectively denote the value of the feature vector, the
parameter vector, and the classifier or model output at time
Notice that even in the case of a linear model, the model output is
a nonlinear function of the underlying patterns when the
are nonlinear functions of the patterns. However, because the
tions are not parameterized, the model (or the decision rule if we ignore
the threshold operation in equation 1.3) is a linear function of the pa­
¢ functions' structures do not directly enter into the
derivation of a parameter estimation method.3 Despite the flexibility
linear functions of the parameters are of considerable interest because
3The <P functions do influence the parameter estimation process, however, because
they determine important characteristics of the the training instances as seen by the
linear model. Connectionist approaches make extensive use of these possibilities as
they provide wider latitude for adjusting behavior. For example, a lay­
ered network of the appropriate kinds of nonlinear units defines a class
of nonlinear models whose parameters can be adjusted by the backprop­
I discuss nonlinear models in more detail in section
A typical method for on-line parameter estimation operates as follows.
At time step t, there is an estimate, Wt , for the unknown parameter vec­
tor that specifies the best model (according to some specified measure)
within t he class of models under consideration. There is also available
a feature vector, <Pt, describing the current input pattern. The output
of the decision rule or model specified by Wt is determined using the
feature vector <Pt as input. This output is compared with the target
output of the 'decision rule or model, which is assumed to be supplied by
a "teacher." For example, assuming the linear model given by
Y; denote the target output at time t, a signed error,
used methods, a new parameter estimate, Wt+l,
number of algorithms follow the form of equation 1.6
iterative manner based on training data (see,
connectionists tend to restrict attention to the simplest case in which
requires nonlocal computation that is costly to implement when there is
the simplest case of a constant scalar gain, one has the following update
parameterized linear model given by equation 1.4 is
1.7 can be expanded via equation 1.5 to yield:
This is the estimation method presented by Widrow and Hoff (1960),
widely known as the LMS (least mean square) rule. If one replaces the
linear model of equation 1.4 with the linear threshold decision rule given
by equation 1.3, one obtains the Perceptron learning rule of Rosenblatt
(1961) , which is identical to equation 1.8 except that the weighted sum
wi cPt is replaced by the threshold of the weighted sum as defined by
On-line estimation methods can be understood as incremental meth­
ods for attempting to minimize a measure of model error as a function
of the parameters. According to this view, the task is to search for the
parameter vector that minimizes a measure of classifier or model error
over all the training data. The LMS rule (equation 1.8), for example,
produces a step in parameter space whose expected direction is down the
surface giving this overall error, and hence the on-line application of the
LMS rule tends to minimize the mean square error over the data. The
backpropagation method is derived by computing this error gradient for
the particular class of models specified by layered networks.
The theory of the LMS rule, and other parameter estimation methods,
is very well developed. Under what conditions does a method converge
to a fixed estimate? What criterion of best-fit is satisfied by the final
estimate? In most cases, there is a requirement that the data set pro­
cessed by the algorithm satisfies certain properties for good parameter
estimates to result. For identification tasks, the system being identified
has to be driven with a set of inputs that reveals all of its modes of activ­
ity to the parameter estimator. A sufficiently rich set of input signals is
said to be persistently exciting. Exactly what this means depends on the
particular class of systems one is considering. Analogous conditions for
classification require a sufficiently varied set of training instances and a
training regime that repeats them all sufficiently often. For theoretical
treatments of on-line parameter estimation methods see, for example,
Duda and Hart (1973), Goodwin and Sin (1984), Ljung and Soderstrom
(1983), Sklansky and Wassel (1981), and Widrow and Stearns (1985).
Within the framework of parameter estimation just sketched, the two
major factors influencing how the framework is applied are the choice of
the basic problem representation embodied in the cP functions and the
choice of the parameterized class of models. It is in terms of these factors
that one can see how connectionist researchers tend to part company
with more traditional users of parameter estimation techniques.
Because the 4> functions determining the basic problem representation do
not have to belong to a specific restricted class of functions in order for
parameter estimation methods to apply, one can exercise wide latitude
in selecting these functions. Although some choices are more satisfactory
on characteristics of the task and the class of
models chosen) , and "feature selection" has been widely studied as a
problem in itself, it is considered largely an art to select suitable 4>
functions. So-called unsupervised learning methods4 can be regarded as
procedures for selecting good features, but these methods are beyond
the scope of this overview ( and, of course, one also has to select a basic
representation for an unsupervised method ) .
the reader of some of the consequences of selecting different types of
lookup ta bles , on one end, to purely computational schemes, on the
For example, if the set, X, of patterns is finite, then one can
Given this representation, a linear model ( equation
table, where the 4> functions provide the indexing scheme and the param­
eters give the values stored in the table.
tion is stored in distinct "locations" when this representation is used­
because there is a distinct parameter for each item stored-a sequential
search of memory locations is not required to retrieve information if one
assumes that each 4> is implemented by hardware. The collection of 4>
functions acts as a hashing-function; in this case, with no possibility
An infinite X can be quantized by a suitable selection of 4> functions
to achieve a lookup table based on the resulting finite set of disjoint
of systems that use this basic approach: Michie and Chambers's
"Boxes" system, on which Barto, Sutton, and Anderson
41 say "so-called" because I think it is less misleading to view a method for unsu­
pervised learning as a supervised method with a fixed, built-in teacher. Unsupervised
methods recode pattern info�atiol) a.c�ofllip.S tp cgding principles (such as principal
component analysis) that aklPR.J«l�elHfilil,/rule.
Feldman and Ballard's (1982) "parameter networks." In connectionist
terms, each of these </> functions represents the function computed by a
"value unit" (Feldman and Ballard 1982). If these kinds of </> functions
are used with nonlinear models, one obtains lookup tables where each
entry is more complicated than just a value. For example, a hierarchical
signature table of the kind used by Samuel (1967), in which lookup
tables contain parts of the addresses into other lookup tables, might be'
thought of as a combination of localized </> functions with a particular
kind of multilinear model. Raibert's (1978) lookup table in which each
entry is itself a linear model can be regarded as the result of using this
type of </> function with a class of bilinear models.
A more general type of lookup table results from using </> functions
that aggregate the values of underlying variables into overlapping regions
instead of disjoint regions. Albus's CMAC (1979) is a good example of
this approach. Information is stored by spreading each item to be stored
over neighboring table entries according to weighting profiles given by
the </> functions. Alternatively, one can locally store individual items and
use a spread function on recall, for example, averaging over neighboring
entries as discussed by Atkeson and Reinkensmeyer ( 1 988, see chapter
9) . In either case, connectionists think in terms of neuronlike units, one
for each </> function, with overlapping receptive fields and refer to this
way of coding information as a type of "coarse coding" (Hinton 1984).
For example, referring to figure 1.2 (after Hinton 1984; cf. Waltz and Fu
1965), each point on the x-y plane is encoded as a pattern of activity
over many units (Le., many </> functions take nonzero values), and each
unit participates in representing many points.
A more traditional method that amounts to an interpolating lookup
table is the Method of Potential Functions introduced by Aizerman,
Braverman, and Rozonoer (1964) and discussed by Duda and Hart
(1973) and Niranjan and Fallside (1988). In connectionist terms, this
method makes use of units each having a possibly global receptive field
with a sensitivity curve that changes as a function of the distance from a
particular point in the input space (cf. the decrease of electrical potential
with distance from a charged particle). Methods relying on representa­
tions similar to this are gaining notice as researchers seek learning meth­
ods that are faster than backpropagation (e.g., Moody and Darken 1 988;
Niranjan and Fallside 1988). The use of </> functions with overlapping
receptive fields clearly provides a kind of interpolation and extrapolation
that is absent from a simple lookup table.
Other choices of </> functions lead to more "algebraic" representations.
lSqPYlJiglJtdtJMaBmiIJ/the following five </> func=
of coarse coding: Overlapping recept ive fields of <P functions
Then with a linear model and a parameter estima­
tion method suitable for a linear model (such
can learn quadratic functions of X. Networks employing these kinds of
nonlinear pre-processing functions have been called "higher-order net­
ing this approach to high-order polynomials encounters a combinatorial
explosion,5 considerable improvement over linear models using linear
the workshop discussion, Ivakhnenko's Group
1971) is a method that helps avoid the combi­
natorial ex plosion by pruning each set of nth-degree terms before using the remaining
terms to generate the (n + l)st-degree terms. Alt.hough this is an off-line method,
Anderson (1986) relates it £O�e fbMaiBliiitl in layered networks.
For example, exclusive-or, and many other
functions exhibiting the same kind of context sensitivity ( where the sig­
nals on some input pathways radically alter how the information on
other input pathways is processed ) , are essentially quadratic functions
that can be learned quickly using second-order networks. Alternatively,
the algebraic structure of <I> functions can be determined from knowl­
edge about the function to be learned, an approach illustrated by the
the algebraic form of the dynamical equations of the 'robot arm to be
One characteristic of many connectionist applications of parameter
estimation that differs from more orthodox applications is that more
freedom is exercised in selecting <I> functions. For example, in conven­
tional system identification for adaptive control, strong assumptions are
made about the nature of the system to be controlled, and <I> functions
are often identity functions of basic system variables and their delayed
values. Connectionist researchers, on the other hand, often apply param­
eter estimation methods under weak assumptions about problems, and
they often use expansive representations of very high dimension based on
a multitude of nonlinear <I> functions. Although sometimes representa­
tions of this kind are motivated by a desire to model the kinds of coding
principles that might be employed in nervous systems
granular layer of the cerebellum being a good example, as described by
in other instances the ill-structured nature of the problem
calls for a flexible representation. This kind of representational freedom
is one of the most salient characteristics of connectionist research.
Another salient characteristic of connectionist research is its use of novel
classes of nonlinear models. The focus of the preceding section was the
use of nonlinear <I> functions with linear models, a situation in which one
can apply well-behaved estimation methods for linear models.
case, because the <I> functions are not parameterized, they do not com­
plicate the computation of the gradient of the model error as a function
of the parameters. For example, with a linear model, the mean square
error is a quadratic function of the parameters with a unique minimum
no matter what fixed representation is used. Nonlinear models, on the
other hand, can generate error surfaces with many local minima so that
the final parameter esti�"aieAWd on initial estimates and
the vagaries of training experiences. Despite these complexities, which
make it impossible to prove global convergence results analogous to those
existing for linear models, the on-line estimation of parameters for non­
linear models is being taken up by connectionists with great enthusiasm.
Largely responsible for this enthusiasm is the backpJ:opagation algo­
based on the class of nonlinear models consisting of layered networks of
units that apply a differentiable nonlinear "squashing function" to the
weighted sum of their inputs. Due to the layered structure of these net­
works, this class of models consists of functions that can be constructed
by recursively composing "squashed" weighted sums with other com­
positions of squashed weighted sums, etc. Extending gradient-descent
analysis to this class of models yields the parameter estimation method
Exactly what advantages this class of models has over others is not yet
clear, but a number of attractive properties are evident. First, because
such models correspond to networks, it is clearly feasible to construct
fast, parallel devices to implement these models for real-time applica­
tions. Second, the gradient calculations required to update parameter
estimates can be executed efficiently via the recursive backpropagation
method, which can itself be implemented in parallel hardware. Third,
the class of models is universal in the sense that essentially
can be implemented to any desired degree of accuracy by a sufficiently
large network (Hornik, Stinchcombe, and White
the negative side, there is no guarantee that the parameter estimates
converge to globally optimal parameters; convergence of any kind can
take a considerable amount of training; and, as I discuss in more detail
in the next section, there is no a priori reason to assume that the kind
of. generalizatiop. produced by this method is appropriate for a given
Despite the current popularity of the backpropagation method, it is a
mistake to identify the field of connectionism, or artificial neural network
research, with the use of this method, as sometimes seems to be done.
It is a technique whose capabilities are currently being explored, but it
is neither the linchpin of connectionist research nor the only technique
being studied for nonlinear parameter estimation. However, aspects of
backpropagation is being used are characteristic of connectionist re­
search. Its utility is being explored in a highly experimental fashion: one
6Le Cun (1988) writes that "Among all the supervised learning algorithms, back­
tries several different network architectures to see which produces the
best performance; one tinkers with the networks; etc. Less emphasis is
placed on the detailed justification of a specific cl!l8s of nonlinear models
than seems characteristic of more traditional applications of nonlinear
neer to use a parameter estimation method for a class of models that is
rather arbitrary in the context of a specific problem, that has thousands
of parameters, and for which there is no guarantee of convergence to a
It is easy to find fault with this use of parameter estimation, which is
undisciplined from the perspective of mathematical and engineering or­
thodoxy, and one might argue, in response, that this freewheeling nature
of current connectionist research will disappear as the field matures. But
I think the experimental nature of this kind of research is desirable-as
long as its limitations are properly understood.
many ways, one of which is to focus its attention on more and more
restricted classes of problems. One of the reasons that connectionist re­
search is generating so much activity is that it has made it acceptable,
at least among its adherents, to think about applying certain kinds of
methods to wider classes of problems than is acceptable according to en­
gineering orthodoxy. However, the negative side of this is that there are,
in fact, good reasons-theoretical and practical-for the more circum­
spect nature of orthodox engineering methodology, and connectionists
The parameter estimation methods discussed above have been called
desired model is a member of a specific parameterized class of models.
refers to the process of determining what
class of models is appropriate for a particular set of applications.
applying error backpropagation, for example, structural learning in­
volves how many layers, how many hidden units, and what kinds of
constraints should be used for a particular problem.
researcher tends to be an essential part of the structural-learning loop
not too many, hidden units and the right kinds of constraints among the
weights. More generally, the structural-learning loop would address the
It is tempting to regard structural learning as a special case of parametric learning. This is a view Anderson and I took in a
Since one can always regard structures as being parameter­
ized, so that adjusting structures amounts to adjusting more
parameters, this distinction [between parametric and struc­
tural learning] is not completely straightforward. However,
what we mean by structural learning generally involves a
space of parameters that is so large, and a performance eval­
uation surface that is so complex, that the usual algorithms
for parametric adaptation do not work . . . One can view the
adjustment of a connection weight in a complex network as
a structural adjustment since it affects the roles of other
weights in generating network behavior. (Barto and Ander­
It is indeed true that a class of models with a very large number of
parameters can include within it many different kinds of structures.
However, estimating such a large number of parameters does not au­
tomatically qualify as a structural learning technique because there is
no guarantee that a structure that is best in any sense, or even good,
will be selected by the parameter estimation process, nor that general­
ization, i.e., extrapolation to novel instances, will be effective. Adjusting
parameters to maximize a performance criterion defined only in terms of
matching the training data has no automatic implications for selecting
There are several approaches that address these shortcomings which
I mention below, but for the moment consider using an unadorned pa­
rameter estimation method to estimate a large number of parameters
from training examples, such as error back-propagation applied to a
large network. The problem has recently been discussed by Denker et
one in pattern classification. Duda and Hart
as follows: "In general, reliable interpolation or extrapolation cannot be
obtained unless the solution is overdetermined" (69) . As the number of
parameters increases, the amount of data necessary to reliably estimate
lem clear. It shows five data points that were generated by adding mean
zero noise to a quadratic function. Fitting a line to the data (i.e . , esti­
mating the parameters for the class of linear models), does reasonably
well; fitting a quadratic does better; and fitting a tenth-degree polyno­
mial produces zero erroC�fEtclI.QWat8f:liMls implies a different mode
Fitting curves to a s e t of data points. Figure 3.3
and Scene Analysis, R. O. Duda and P. E. Hart , Copyright 1973, by John Wiley
and Sons, Inc. Reprinted by permission of John Wiley and Sons, Inc.
of generalization, and it is clear that small mean square error on the
training set does not imply that generalization will be successful. This
is exactly analogous to what one observes in some applications of back­
pointed out by Denker et al. (1987). We cannot say that
the method necessarily discovers the "rule" by which the data set was
Reliable generalization requires that either the number of
training examples is many times larger than the number of parameters, 7
or the class of parameterized models is already matched in some way to
7 B aum and H aussler (1989) provide a theoretical analysis of t he relationship be­
tween the number of weights, the number of training examples, and the accuracy of
There is a trade-off between the generality of a class of models and
the adequacy of the kind of generalization it will produce. This has
been analyzed in terms of the entropy of a class of models by Denker et
al. ( 1987) and Solla ( 1 988) . I called this the generality/generalization
trade-off (Barto 1987) : It is because of representational restrictions built
into a class of models that correct generalization can occur if the class of
models happens to be appropriate to a task. To the extent that a class
of models can represent any structure, it cannot be expected to produce
meaningful extrapolations beyond the data on which it is trained. Ar­
tificial intelligence researchers studying machine learning speak of the
representational bias of an induction technique. One does not expect
an unbiased form of representation to produce meaningful generalizSr
tions. This is the reason that using parameter estimation for very gen­
eral classes of models, that is, classes of models lacking a high degree of
bias, is more closely related to storing information into a memory struc­
ture than it is to identifying the structure of the process generating the
data. That the class of layered networks trained by backpropagation is
a "universally approximating" class is not, in itself, a particularly useful
Although even large connectionist networks are not without bias,S ad­
ditional bias can be introduced into a connectionist learning method in
several ways in order to facilitate the appropriate kind of extrapolation.
In addition to introducing bias by the choice of the t/> functions, one
can enforce constraints among the parameter values. For example, one
can restrict parameter changes so that each model estimate has certain
symmetries, such as translational invariance. Numerous connectionist
researchers have illustrated the utility of this method of biasing learn­
ing (see, for example, Rumelhart, Hinton, and Williams 1986; Giles and
Maxwell 1987; Denker et al. 1987; Solla 1988) . Another method is to al­
ter the performance criterion by adding conditions other than a measure
of model error on the training instances, a point made by Sanderson in
the workshop discussion. Rumelhart (to appear), for example, has ex­
perimented with backpropagation with additional conditions designed
to force a network to favor representations utilizing few units. This is
a method for introducing a form of bias while attempting to maintain
8 Backpropagation experts emphasize that the number of degrees of freedom in a
network does not necessarily equal the number of adjustable weights. For example,
a network with m input lines and n output lines, consisting of linear units (Le. , no
squashing functions) has mn degrees of freedom no matter how many adjustable
weights there are. This is true because any such network is equivalent to a one-layer
network with mn weights. Something analogous to this can happen in non linear
in which networks are pruned through the use
of a relevance measure. Other methods for controlling the bias of net­
work learning involve a "developmental" approach in which a small, and
thereby highly biased , network is trained to some criterion, its weights
are frozen , and it is then embedded into a larger network as training
continues until a desired level of performance is achieved.
tally expanding the network during learning in this manner may result
in properties analogous to those obtained by representing a function
a linear combination of orthogonal functions as in Fourier analysis:
The expansion can be continued as far as necessary, and ' the most accu­
rate simple models form parts of more complex models of course, there
Finally, it is important not to overlook the fact that j ust representing
models in the form of networks of interconnected components offers some
possibilities for introducing bias that are absent in other kinds of repre­
sentation. Knowledge of the problem domain may dictate that certain
variables should influence certain other variables, and that connections
between other variables should be absent. In other words, selecting the
architecture of a network can be a good way to specify a class of models
is based on some understanding of the relationship to
an application of backpropagation to a judiciously designed layered net­
work that genuinely extracts structure from the training process . The
key point is that such rule extraction is not an automatic consequence
of applying a connectionist network; it is rather a consequence of intro­
ducing the right kind of bias into the network on the basis of a priori
knowledge. Consequently, one of the attractive features of the connec­
tionist approach to nonlinear parameter estimation is that it facilitates
the introduction of certain kinds of bias.
I suggested above that the idea of an associative memory network pro­
vides a perspective on parameter estimation that is a distinct contribu­
tion of the connectionist approach. Given the discussion above of repre­
sentations, nonlinear models, and parametric-versus-structural learning,
parameter estimation for forming models, or rules, and for implementing
memory structures. It was pointed out in section 1.4 that a lookup ta­
ble, which perhaps is the prototype memory structure, can be viewed as
a special case of parameter estimation applied to a linear model with a
particular expansive representation ( equation 1.9) . The parameters are
the contents of the memory locations, and the estimation process just
amounts to storing information in specific locations. In the simplest case
of noise-free training data, the learning rate, or gain, of the estimation
method can be set so as to accomplish one-trial learning because there
is never any interference between the stored items. Admittedly, this is a
trivial example of parameter estimation, but it is parameter estimation
nonetheless. However, in this case, it is not appropriate to regard the
estimation process as identifying a system or rule. The parameterized
class · of models is so general-purpose that it does not bias the induc­
tive process at all, and no generalization occurs. Moreover, assuming
noise-free data, the one-to-one relationship between the data used to
determine the "model" and the parameters is clear: each possible input
pattern has to be seen once to store the entire function.
The point is not that the parameter estimation framework provides
new insight about lookup tables, but rather that lookup tables repre­
sent one extreme of a continuum of possible applications of parameter
estimation-the other endpoint being the estimation of the relatively
few parameters specifying a model with a high degree of specialized
structure. One extreme is clearly characterized as pure memory storage;
the other as model-based rule identification. The criteria for evaluat­
ing the theoretical soundness of applications of parameter estimation
are markedly different for the two ends of this continuum: In the case
of pure memory, the relationship between data and parameters is rela­
tively simple, and "learning" should be fast; in the case of model-based
rule identification, on the other hand, slower learning is acceptable, but
the utility of extrapolation and interpolation critically depends on the
bias in the class of models and the degree to which parameter values
are constrained by the training data. Because connectionist uses of pa­
rameter estimation range over this entire spectrum from memory-based
to model-based applications-with the most controversial applications
falling in the intermediate range-it is not clear whether it is appropriate
to evaluate network performance as memory storage or as model-based
rule identification. This exposure of connectionist applications to both
sets of criteria accounts to some degree for the controversial nature of
connectionist methods. However, I believe that one of the strengths
of connectionist applic6�t&dI6faMJja#stimation is that they do,
in fact, freely range over this ambiguous region of the memory jmodel
continuum. Suitably designed adaptive networks can achieve a balance
between the relevant trade-offs that is not possible for methods at either
Where Does the Training Information Come From?
The parameter estimation methods that I have discussed all require a
training procedure in which desired, or target , outputs of the decision
rule, model, etc. are available for a sufficiently varied set of cases; that
is, they are methods for supervised learning. Where does this training
pose one wants a connectionist network to learn to control a plant that
is too complex, of. about which too little is known, to permit the use
of conventional techniques for designing controllers. In a typical con­
trol problem, one may have target plant outputs but not target network
are the control signals. How can training
signals be provided to the network unless some agency already knows a
great deal about how to control the plant?
Notice that a difficulty analogous to this is present in the task of
training the hidden units of a layered network when target responses
the portion of the network interposed between that hidden unit and the
output units, Le. , the portion of the network that determines how the
activity of the given hidden unit influences the output units, is analogous
The problem of determining target responses
for the hidden units is therefore analogous to the problem of training a
controller on the basis of target plant behavior, and methods that apply
in one case, such as backpropagation and reinforcement learning, also
In the remainder of this section, I describe in highly schematic form
the basic ways in which the kind of training information required for su­
pervised learning can be obtained in tasks relevant to control. In section
I discuss methods that do not require training signals as informa­
tive as those required by supervised learning methods.
made to describe in detail the types of plants and control problems with
9 Figure 1 .4 and those tha.t follow show la.yered networks, but these should be
interpreted merely as icons repr�I!-�i� �y meth� d for learning complex functions
via supervised learning, inc1GO�Jtl8 � ces in a lookup table.
The problem o f obtaining training information i n a control problem: Target plant
outputs may be known, but not t he control signals that produce them .
which control theorists are concerned, nor is justice done to the highly
developed methods that exist for prediction, system identification, and
adaptive control. Thinking of the plant simply as an input-output map­
ping and the controller as a feedforward controller suffices for most of the
observations I make, but these observations, suitably modified, also ap­
ply to more complex types of plants and can be integrated with feedback
control methods. It is not my intention to suggest that connectionist net­
works are suitable only for feedforward control, or that networks must
be used in the absence of more conventional feedback control loops.
Copying an Existing Controller-If there exists a controller capable of
controlling the plant, then the information required to train a connec­
tionist network can be obtained from this controller as shown in figure
1.6. The target network output for a given input is the output of the
existing controller for that input. The network learns to copy the ex­
isting controller. Widrow and Smith (1964) applied this method to a
version of the pole-balancing problem, and Widrow refers to this as a
method for constructing:;qoyPiglfted� acquiring knowledge from
training a controller: Target outputs are not directly available for a hidden unit.
The problem o f training hidden units i n a
an existing expert. One might question the utility of this method on the
grounds that if there already exists an effective controller, why would it
be useful to have another one in the form of a network? Two answers
are apparent: first, the existing controller may be a device that is im­
practical to use (such as a person); second, the adaptive network may
be able to form an effective control rule on the basis of a representation
of the system state that is easier to measure than the reperesentation
required by the existing controller (Widrow and Smith 1964) .
Adaptive Prediction-Figure 1.7 illustrates the basic idea in training a
network to be a predictor. Assume that the lines across the top of the
figure carry signals whose values one wishes to predict. For simplicity,
also assume that these same signals provide the information from which
the predictions will be made. For training purposes, the input to the
network consists of delayed values of the signals, and the target output
consists of the current values of the signals. The network therefore tries
to match the current signal values by adjusting a function of their past
values. Then when the input to the network bypasses the delay units,
the output of the network is a prediction of the values the signals will
have in the future. Assuming each delay unit shown in figure 1 . 7 delays
its input for T time st�j¥J§jhtNtMitena.work requires a negligi ble
Copying an existing controller with a connectionist network.
amount of time to compute its output from its input, then the trained
network provides estimates for the values of the signals T steps in the
future. This approach to adaptive prediction rests on the assumption of
a parameterized class of models for the functional relationship between
the current and past values of the signals and their later values, or
equivalently between earlier values of the signals and their current values.
A parameter estimation method is applied with training information
supplied by observations of the signal values over time.
The scheme illustrated in figure 1 . 7 is only the simplest special case of
more general prediction schemes where cascades of delay units (tapped
delay lines) are used to provide input to the model, or where recursive,
or autoregressive, models are used in which past values of model outputs
form part of the input (see, for example, Goodwin and Sin 1984, Widrow
and Stearns 1985). La�fi)ttJetef:fllS 7 ) provide an example of
Using a connectionist network for adaptive prediction.
nonlinear connectionist prediction. Sutton discusses another approach
to adaptive prediction elsewhere in the present publication.
System Identification-Training information
ing the input-output behavior of a plant , as suggested
where t h e network receives t h e same input as t h e plant, and t h e plant
where the model is a mapping from plant inputs to outputs, but more
complex types of models are commonly employed. For example, the in­
put to the model may consist of various delayed values of plant inputs
(so that adaptive prediction as discussed above is a special case of sys­
tem identification ) , and the model may be a recursive model (see, for
System identification i s a n essential part of some methods for adaptive
control. In one approach to adaptive control , one selects a class of mod­
els for identification purposes such that each model
tem for which one can design an effective controller using proven design
methods for known plants. One can therefore express a control law
terms of the parameter estimates produced by
procedure. This will be an effective control law if the plant can be ade­
Using a connectionist network for system identification .
that have been studied extensively by adaptive control theorists, some
arising when the control law is adjusted on-line at the same time the
Using a nonlinear connectionist network instead of a more conven­
tional system-identification procedure does not immediately extend the
applicability of adaptive control to the systems that nonlinear networks
are able to identify. That a system can be identified does not imply that
it can be controlled. It would be necessary to have controller design
techniques that apply to the classes of nonlinear systems identifiable by
networks. It may be possible to develop control theory in this direction,
but because networks can represent essentially any system, there is no
compelling reason to hope that the invocation of connectionist ideas is
likely, by itself, to shed any light on the theory of nonlinear control.
However, the method I describe below under "Differentiating a Model"
does illustrate the utility of having a model of a nonlinear plant in the
form of a network, but this model is used differently than models are
Identification of System Inverse -Figure 1.9a, shows how an adaptive
network can be used to identify the inverse of a plant. In contrast to the
situation shown in figure 1 .8, here the input to the network is the output
of the plant , and the target output of the network is the plant input. If
the network can be trained to match these targets, it will implement a
mapping that is a plant inverse. Once one has such an inverse, it can
be used for control purposes as shown in Figure 1 .9b. The desired plant
output is provided as input to the network; the resulting network output
is then used as input to the plant. If the network is a plant inve.rse, then
this plant input causes this desired plant output ( obviously, this simple
description overlooks many subleties of a real control problem).
Widrow, McCool, and Medoff (1978) proposed this method for the
adaptive control of linear systems ( also see Widrow 1986; and Widrow
and Steams 1 985) , and it can be applied to the control of nonlinear
systems as illustrated by Psaltis, Sideris, and Yamamura (1988) and
Srinivasan, Barto, and Ydstie ( 1988) , who use backpropagation to iden­
tify a nonlinear plant inverse. This basic method has also been used in
models of motor control and robot control systems for learning inverse
kinematics (e.g. , Grossberg and Kuperstein 1986; Kuperstein 1987) and
inverse dynamics (e.g. , Kawato, Thrukawa, and Suzuki 1987; Kawato,
Uno, Isobe and Suzuki 1 988; and Miller 1 987; see the chapter by Kawato
in the present pUblication ) . The method of Kawato, Furukawa, and
Suzuki ( 1987) , which they call feedback error learning, differs from the
scheme shown in figure 1 .9 in that the input to the network is the de­
sired plant output instead of its actual output, and a feedback controller
is used during training. Kawato describes this method and contrasts it
with basic inverse identification in his chapter in the present publication.
A major problem with inverse identification arises when many plant
inputs produce the same output, i.e. , when the plant's inverse is not
well defined. In this case, the network will attempt to map the same
network input to many different target responses. Most parameter es­
timation methods tend to average over the various targets and thereby
produce a mapping that is not necessarily an inverse ( as pointed out
by Jordan 1988). Nevertheless, the use of nonlinear networks for identi­
fying nonlinear plant inverses is a promising avenue for futUre research
because an inverse model of a nonlinear plant, unlike a forward model,
Differentiating a Model -This method for training a controller relies
more on backpropagati��work methods, but it is a
Using a connectionist network for identifying
way of using networks t h at may have promise for adaptive control. This
technique was described by Werbos ( 1974, 1987, 1 988) and Jordan and
Rumelhart (to app ear ) , and variants of it have been applied to prob­
lems in motor control by Jordan ( 1 988, 1990) , Kawato (see chapter 7) ,
and Widrow( 1986) . The method is illustrat ed in figure 1. 10. The back­
propagation algorithm is used to ident ify the plant as shown above in
figure 1 .8 ( t he training signals for this identification stage are not shown
in figure 1 . 10) , resulting in a forward model of the plant in the form of a
l ayere d network. The utility of a forward model having this form is that
one can efficiently compute the derivative of the model 's output with
respect to its input by means of the backpropagation process, which
as Jordan ( 1 988) pointttCJfJYl;i��el:ieJ transpose of the network
Backpropagating through a forward model of the plant to determine controller
Jacobian at the network's current input vector.
gating errors between actual and desired plant outputs back through
the forward model produces the error in the control signal, which can
be used to train another network, or other kind of supervised-learning
system, to be a controller. In Figure 1. 10 this backpropagation process
is illustrated by the dashed line passing back through the forward model
and continuing back through a second layered network that uses it to
vantages over the direct identification of a plant inverse when a plant
inverse is not well defined. Of course, to apply this basic idea one only
needs a model in a form that can be differentiated; it need not be a lay­
ered network. However, using a layered network that can be trained by
backpropagation is a nice idea because the backpropagation mechanism
can be used both to adjust the network's parameters and to differentiate
This method for obtaining training information for an adaptive con­
troller illustrates a general principle that can be exploited in other ways
Note , however, that the error which is back propagated differs in the parameter­
adjustment and model-differentiation stages. In the first case, it is the difference
between the network output and the plant output; whereas i n the second case, it is
the difference between the plant ou�put (or the net�ork output if the network is well
in control problems. This principle is to choose the class of models for
system identification partly on the basis of the existence of specialized
techniques applicable to the models in the class. For example, in con­
ventional approaches to adaptive control, a class of models is selected
such that techniques for controller synthesis can be applied to any sys­
tem representable within the class ( i.e. , time-invariant linear systems ) .
When a model is specified by the parameter estimation process, one can
design a controller under the assumption that this model is an accurate
model of the plant. In the same way, determining a plant model in the
form of a layered network with differentiable squashing functions allows
one to apply the backpropagation process to estimate how inputs to the
plant should be altered to change the plant's output in desired ways.
Le Cun ( 1988) has pointed out that the backpropagation method can
be derived using a Lagrangian formulation like the one used in optimal
control. It turns out that algorithms similar to backpropagation are
used in optimal control-although not for adjusting the parameters of
a system model. It is interesting that the method described here for
using backpropagation to differentiate a model for control purposes is
more closely related to optimal control methods than is its application to
parameter estimation for system identification ( see, for example, Bryson
The preceding section focuses on how the training information required
for supervised learning can be obtained in tasks relevant to control. It
is clear that there are many possibilities. However, some learning tasks
arising in control require methods that are not accurately characterized
as supervised-learning methods. Suppose one wishes to adjust a control
rule in order to improve the performance of a plant as measured by a
performance measure that in some way evaluates the overall behavior
of the plant; for example, one might wish to maximize a measure of the
plant's energy efficiency over time. Optimal control methods apply if
there are models of the plant and performance measure that are suffi­
ciently accurate and tractable. However, in less-s tructured situations it
may be possible to improve plant performance over time by means of
on-line learning methods performing what we call reinforcement learn­
ing , after Mendel and McLaren ( 1970) , who discuss this approach to
control. Whereas the performance measure for a supervised system is
defined in terms of a seCop�aIeMlI of a known error criterion
(e.g. , mean square error) , reinforcement learning addresses the problem
of improving performance as evaluated by any measure whose values can
be supplied to the learning system. Consequently, in tasks of this kind
there exist desired control signals-namely those that lead to optimal
plant performance-but the learning system is not told what they are
because there is no agency knowledgeable enough to act as this kind of
teacher. The problem is to find these optimal control signals, not simply
Viewed this way, reinforcement learning involves many of the issues
discussed above involved in obtaining the training information required
for supervised learning when it is not directly available (figure 1 .4). In
the case of reinforcement learning, however, the situation is more gen­
eral than that shown in figure 1 .4 in that instead of trying to determine
target controller outputs from target plant responses, one tries to deter­
mine target controller outputs, or desired changes in controller outputs,
that would lead to increases in a measure of plant performance-a mea­
sure not necessarily defined in terms of target plant responses. This is
illustrated in figure 1 . 1 1 which shows a reinforcement-learning controller
interacting with a plant and receiving an "evaluation signal" generated
by a "critic" capable of evaluating plant performance.
Reinforcement learning essentially involves two problems. The first
problem is to construct a critic capable of evaluating plant performance
in a way that is both appropriate to the actual control objective and in­
formative enough to allow learning. The second problem is to determine
how to alter controller outputs to improve performance as measured by
the critic. Assuming the first problem is solved-a highly nontrivial
assumption-the second problem, discussed next, is relatively straight­
forward given what has already been said about obtaining training in­
If one assumes that the critic is capable of providing an immediate eval­
uation of plant performance that is appropriate to the actual control
objective, then it is necessary to determine the gradient of the critic's
evaluation as a function of control signals. One approach is to adapt the
method of differentiating a model described above (figure l o 1O) . The
idea is to learn a model-in a form that can be differentiated easily,
such as a layered network-of the process by which control signals lead
to evaluations. Referring to figure 1 . 1 1 , this would be a model of the
plant together with th�hWM�..adient of evaluation as a
function of the controller's actions can be estimated by differentiating
the output of this model with respect to its input, for example, by back­
propagating through the network representing it, thus producing the
information required to adjust the control rule. This approach to rein­
forcement learning has been suggested by Werbos ( 1974, 1988) , and the
case where there is a critic but no explicit plant model has been studied
A more direct approach to adjusting control actions in order to im­
prove plant performance is to actively explore the space of controller out­
puts by modifying control actions and observing how the performance
evaluation changes as a result. Beneficial changes in control actions are
incorporated into the control rule. Learning in this way is related to the
trial-and-error learning G�fJhRYd�f;ists in which behavior is
selected according to its consequences in producing reinforcement­
hence the term "reinforcement learning." Active exploration is nec­
essary when the evaluation signal is available but not its gradient as a
function of control actions. In supervised learning, the appropriate gra­
dient is directly available (at least for each training example) because
the performance measure has a known form (e.g. , mean square error)
whose gradient can be precomputed and expressed in terms of target
responses. Similarly, the method for reinforcement learning described
above, where a model of the evaluation process is constructed, provides
a means for directly providing gradient estimates without the need for
active exploration, except for the exploration needed to construct the
Direct methods for reinforcement learning, on other hand, perform
gradient ascent (or descent) on an evaluation surface whose values (usu­
ally sc alars) are directly available to the learning system as evaluation
signals, but whose gradient as a function of control actions is not. This
gradient has to be estimated by comparing the evaluations obtained dur­
ing exploration and relating these changes to the actions taken, or to the
action changes made, during exploration. One way of understanding the
distinct ion between gradient descent on the basis of receiving evaluation
sign als instead of gradient vectors is to contemplate the difference be­
tween the sign of a scalar evaluation signal and the sign of an error signal
A negative error means that the scalar action was too high (assuming
error equals target minus actual) , whereas the sign of a negative evalua­
tion intrinsically means nothing (perhaps minus 10 is the best evaluation
possible) . One signal is a derivative with respect to an action; the other
is not. Confusing this view, however, is the situation that arises when
performance is evaluated with respect to some performance standard. In
this case, a negative evaluation may mean that system performance is
not as good as could be expected based on past experience with similar
situations. In this case, the signal is a kind of derivative, but it is not
a derivative with respect to actions, and its being negative does not in
itself indicate how the action should be altered (indeed, the action will
generally be a vector) . A single evaluation of an action does not contain
information about how to beneficially change that action. There are,
however, many ways to estimate the required gradient information on
the basis of evaluation signals, each of which produces a different type
A direct approach to reinforcement learning that is highly devel­
oped is the theory of learning automata. This theory originated in the
Soviet Union with the �glft�lilD 73 ) and the independent
research in the West of mathematical psychologists ( for example, Estes
1 950, and Bush and Mosteller 1 955) . It has been extensively devel­
oped since then within engineering (Narendra and Thathachar, 1974) .
A class of reinforcement-learning algorithms known as stochastic learning
automata has been widely studied. These algorithms probabilistically
select actions from a countable set of possible actions and update action
probabilities on the basis of evaluative feedback. They are applicable
when the evaluation process is stochastic and the task is to maximize
the expected value of the evaluation received. Although stochastic learn­
ing automata have been studied mostly in nonassociative form, that is,
where they search for a single optimal action, they can be extended
to learn mappings, such as control rules, that associate input patterns
with actions. It is straightforward to combine nonassociative learning
automata with lookup table representations of mappings. Each table en­
try effectively has its own nonassociative learning automaton whose job
is to discover-without being told-wha� is best to store there, that is,
what control action is best when that entry is accessed. This approach is
illustrated by the pole-balancing system of Barto, Sutton, and Anderson
( 1 983) and Franklin's ( 1 987, 1988) method for learning the feedforward
component of a control signal to cancel unmodeled disturbances. An­
other way to use learning automata to learn mappings is illustrated by
the SNARC ( stochastic neural-analog reinforcement calculator) system
of Minsky ( 1 954) , which used a nonassociative learning automaton to
control the transmission probability of each link in a network.
It is also possible to combine stochastic learning automata with the
framework of parameter estimation by parameterizing mappings from
pattern input to action probabilities. As these parameters are adjusted
under the influence of evaluative feedback, action probabilities are ad­
justed to increase expected evaluation. Farley and Clark ( 1 954) exper­
imented with such a method taking the form of a neuronlike unit, and
Barto, Sutton, and Brouwer ( 198 1 ) studied a related method in the con­
text of associative memory networks, following the neural hypothesis of
Klopf ( 1 972, 1 982) that neurons are attempting to extremize a physiolog­
ical quantity. Barto, Sutton, and Brouwer ( 198 1 ) called their network
an "Associative Search Network" because it actively searched for the
optimal output pattern to associate with each input pattern. Sutton
( 1 984) , Anderson ( 1 986, 1987) , and Gullapalli ( 1 988) have studied re­
lated methods and have combined stochastic reinforcement learning with
the back-propagation method. Barto and Anandan ( 1 985) presented an
algorithm of this type, called the associative reward-penalty , or AR - P ,
algorithm and proved ���M�m. The AR - P algorithm
has been used as the basis for learning in layered networks (Barto and
an approach analyzed and extended by Williams
The AR-p algorithm is closely related to an earlier method for para­
metric reinforcement learning described by Widrow, Gupta, and Maitra
which they called the "selective bootstrap algorithm."
now turn to the problem of constructing a critic capable of evaluat­
ing plant performance in a way that is both appropriate to the actual
control objective and informative enough to all ow efficient learning. If
the objective is relatively straightforward, such as maintaining plant
output near a reference level, then it is not difficult to provide useful
performance feedback continuously over time in the form of error sig­
nals, and it would not be necessary to resort to reinforcement learning.
On the other hand, if the objective is to achieve a terminal state with
desired properties, or a trajectory that maximizes a measure of process
yield , efficiency, etc. , then the problem of providing useful performance
information continuously over time is much more difficult. How is
performance to be evaluated if the objective concerns properties
trajectories? Although the theory of optimal control concerns
performance criteria of this kind , optimal control techniques require an
accurate plant model and either an enormous amount of computation
or sufficient mathematical tractability of the plant model and objective
subgoal performance measure (e.g. , Mendel and McLaren 1970;
and Gilbert 1970; Waltz and Fu 1965) that is consistent with the
actual control objective but more easily accessible, where consistency
means that the control objective can be achieved by maximizing the
subgoal performance measure . The idea of a critic, specifically, the idea
represents an adaptive approach to optimal con­
trol by means of an on-line procedure for learning a subgoal performance
measure that is consistent with control objectives.
The perspective most familiar to me comes from the early AI
discusses the basic problem as one of cred.it­
for reinforcement-learning systems. How does one correctly
assign credit or blame to an action when its consequences unfold over
time and interact with the consequences of other actions? Samuel
described a program capable of learning how to play a respect­
learning methods used in the program was one that adjusted a parame­
terized evaluation function so that the value it gave to a current board
position came to reflect the utility of board positions that were likely
to arise later in the game. Using this method, it was possible to as­
sign credit to moves that were instrumental in setting the stage for later
moves that directly captured opponent pieces. Because Sutton and Wer­
bos discuss this kind of algorithm elsewhere in the present publication,
I restrict my remarks to some general observations.
In his Ph. D. dissertation, Sutton ( 1 984) developed an algorithm, call­
ing it the "Adaptive Heuristic Critic" algorithm, that is closely related
to Samuel's method but extended, improved, and abstracted away from
the game-playing domain. This work, which began with Sutton's earlier
interest in classical conditioning and our exploration of Klopf's ( 1972,
1982) idea of "generalized reinforcement," led to the use of the algorithm
in the reinforcement-learning pole balancer of Barto, Sutton, and An­
derson ( 1983), where it was incorporated into a connectionist unit called
the adaptive critic element, or ACE. This system was studied further by
Selfridge, Sutton, and Barto ( 1 985) . Anderson ( 1 986, 1987) continued to
explore the method, combining it with error backpropagation in order to
learn nonlinear evaluation functions. Since then, Sutton ( 1988) has ex­
tended the theory and has proved a number of results for a general class
of algorithms he calls temporal difference, or TD, algorithms. Related
credit-assignment methods have been studied by Gaines and Andreae
( 1966) . Holland ( 1 986; the "bucket-brigade" algorithm ) . Booker ( 1982),
and Hampson ( 1 983) . Witten ( 1 977) discussed a similar method
in the context of Markov decision problems. Werbos ( 1 974, 1 977)
independently proposed a class of methods that includes TD-like algo­
rithms and related these algorithms to the framework of dynamic pro­
gramming, calling them "heuristic dynamic programming" methods. A
similar connection was made recently by Watkins ( 1 989) , who uses the
term "incremental dynamic programming." The relationship between
TD algorithms and dynamic programming is extensively discussed by
Barto. Sutton, and Watkins ( to appear ) .
This connection to dynamic programming provides an essential step
in understanding TD algorithms and their relevance to control. Com­
putational methods for optimal control rely on dynamic programming
to determine an optimal control rule by determining an ideal subgoal
performance measure that encompasses all future consequences of con­
trol decisions. However, conventional dynamic programming requires
an accurate model of the plant and is computationally intensive. A TD
method is an on-line �teUiMatR/method for approximating
an ideal subgoal performance measure while interacting with the plant.
This estimation process does not require a model of the plant, although
there are natural ways to use TD methods in conjunction with a plant
model (as is the case in Samuel's checker player; see chapter 3. Fi­
nally, I want to mention another aspect of TD methods that I find
especially interesting. In addition to providing methods for addressing
credit-assignment problems with strong ties to a large bo dy of mathe­
matical theory, TD methods provide good models of a wide range of ani­
mal behavior in classical, or Pavlovian, conditioning experiments. There
are a number of conditioning models using TD-type algorithms, includ­
ing those of Sutton and Barto ( 1981) and Klopf ( 1 988) . The more recent
conditioning model of Sutton and Barto ( 1 987, to appear), called the TD
model, most closely fits into the framework of dynamic programmin g.
That this simple model accounts for a wide range of animal conditioning
behavior suggests that the theoretical principles it embodies are relevant
to natural as well as synthetic learning systems.
The Role o f Reinforcement Learning i n Control
Reinforcement learning is a very general approach to learning that can
be applied when the knowledge required to apply supervised learning is
not available. The generality of reinforcement learning is obtained at
the cost of efficiency when compared to the more specialized supervised
methods when these methods are applicable. Consequently, although any
control task requiring learning can be formulated as a problem of rein­
forcement learning (where the objective is to maximize a performance
measure) , it is better to use more specialized methods whenever possible
in order to take full advantage of the available knowledge. For example,
if the performance measure is an error based on known targets, then
a reinforcement-learning method can solve the same problem a super­
vised method would solve, but it would do so without making use of the
fact that the performance measure has a specific form; consequently, it
would take longer than a more specialized supervised method. For even
small problems, these differences in efficiency can be very significant,
as illustrated by the simulations by Barto and Jordan ( 1 987) in which
comparisons are made between using reinforcement learning and error
backpropagation to train the hidden units of layered networks (which is
a kind of control problem as pointed out above and illustrated in F igure
It is clear, then, that the use of reinforcement learning is most eas­
ily justified when there CiiDfJ� �the knowledge required for
applying more specialized learning methods. However, it is always possi­
ble to approach these problems by applying methods designed to acquire
the requisite knowledge, either before or during the application of the
methods that make use of it. This approach is familiar from adap­
tive control, where some methods implement a control law whose design
is based on a model under the assumption that the model provides a
complete and accurate characterization of the plant being controlled.
This approach also can be applied to tasks requiring the attainment
of a future goal while optimizing plant performance over time: a plant
model can be formed and then used as the basis for a conventional
dynamic-programming approach to optimal control; or perhaps a kind
of space / time model in the form of a layered network can be obtained so
that the appropriate control signals can be computed by "backpropagat­
ing through time" (see chapter 7; Widrow's "truck backer-upper" ( see
appendix A ) also uses this approach ) . Although these latter approaches
might still be viewed as instances of reinforcement learning because they
do not begin with knowledge of moment-to-moment target outputs for
either the plant or controller, they do not employ the kind of direct on­
line search in control space that is usually associated with reinforcement
learning. Is there a role for these more direct reinforcement-learning
I think these methods have a definite role because model-based meth­
ods are not necessarily effective with inaccurate models, and exploratory
behavior is necessary anyway to achieve accurate models. It makes sense
to take advantage of this exploration to directly modify control rules via
reinforcement learning in order to obtain improvements in performance
while model parameters are being adjusted. This strategy is likely to be
especially useful in tasks involving the optimization over time of a non­
linear plant's performance. Considerable time may be required to form
sufficiently accurate models, and there can be sensitive dependencies be­
tween a model and the control actions it implies. Moreover, the range of
assumptions made in the model-based approach may not be justified to
the degree necessary to obtain the desired refinements in performance.
Adjusting the control rule by direct appeal to the performance measure,
or a consistent subgoal measure, can shorten the chain of assumptions on
which a method is dependent. Additionally, in some problems there may
not be enough time to take full advantage of an elaborate model, espe­
cially if dynamic programming methods are required to generate control
actions from it. Reasons such as these, which all involve aspects of the
the usual tradeoffs between the acquisition of knowledge and its use for
control, suggest that d�dal control law via reinforce-
ment learning can play a useful role in control, and that when combined
with adaptive critic methods, it may be most useful for perfecting plant
performance with respect to complex performance measures.
By emphasizing the continuity of connectionist learning methods with
those from more established traditions, I have highlighted some of the
significant features of connectionist research while minimizing discussion
of features that I think will prove more transient. I have not discussed
all the learning methods that connectionists have devised, leaving some
for other chapters of the present publication, and focusing on those that
seem immediately relevant to control. I have argued that the most dis­
tinctive character of connectionist learning systems lies not so much in
their technical specifications as in the methodology with which they are
applied. Connectionists have adopted an experimental approach more
like that of AI than of the engineering disciplines to which, technically,
connectionist methods have the closest ties. This experimental, heuris­
tic approach is characterized by what I termed representational freedom
and a willingness to plunge ahead when theoretical guarantees are lack­
ing. It is easy to criticize this freewheeling nature of much connectionist
research, but theoretical guarantees are often obtained at the cost of ex­
tremely restrictive assumptions about tasks ( e.g. , the assumption that a
plant is linear and time-invariant ) , which are almost always violated in
practice. Certainly rigorous theory is important, and a valid criticism
of any research is that it proceeds in ignorance of relevant theoretical
frameworks and previous research, but an experimental methodology
seems necessary for developing control applications involving complex
Despite the association of connectionist methods with applications
involving weak assumptions about system structure, it is not appro­
priate to characterize these methods as knowledge-free, or tabula rasa,
approaches to learning. Networks can be applied with little regard for
prior knowledge (and often are) , but they need not be. Indeed, as I
pointed out in this chapter, networks provide numerous avenues for in­
troducing bias into the learning process on the basis of prior knowledge.
I think it is more accurate to characterize connectionist methods as spe­
cial kinds of memory-intensive approaches to computation sharing many
of the characteristics of memory-based reasoning as discussed by Stan­
fill and Waltz ( 1986) an(t�erdl Mat9Imfrol application by Atkeson
and Reinkensmeyer ( 1988; see chapter 9) . Networks generally have a
large number of parameters compared with more traditional uses of pa­
rameter estimation techniques, and some networks are basically lookup
tables that automatically provide interpolation between, and extrapola­
tion beyond, the data stored. As in the case of conventional computer
memory, how this storage capacity is initialized depends on how it is to
However, the connectionist approach provides a generalized concept of
memory that I think will turn out to be its most important contribution
to control engineering. Storing information in connection weights gener­
alizes the idea of inserting information into memory locations, and on­
line parameter estimation, whether used in supervised or reinforcement­
learning modes, generalizes the process of memory storage. Because
adaptive connectionist networks fit in the range between structureless
lookup tables and highly constrained model-based parameter estimation,
they seem well suited for the acquisition and storage of control informa­
tion. These methods suggest how new techniques for adaptive control
can be developed which take full advantage of the possibilities for fab­
ricating associative memory systems having high capacity, high speed,
and the ability to usefully interpolate and extrapolate in real time.
The author gratefully acknowledges the support of the Air Force Of­
fice of Scientific Research, Bolling AFB , through grant AFOSR-87-0030,
which made this chapter possible, and the support of the King's College
Research Center, King's College Cambridge, England, where some of it
was written. Special appreciation is expressed to Chuck Anderson, Judy
Franklin, Mike Jordan, Mitsuo Kawato, Rich Sutton, and Paul Werbos
for useful discussions of the material presented in this chapter and many
helpful suggestions on improving its presentation.
Aizerman, M. A. , Braverman, E. M . , and Rozonoer, L . I. {1964} . On
the method of potential functions. A utomatika i Telemekhanika.
Albus, J. S. (1979). Mechanisms of planning and problem solving
brain. Mathematict!t�fJ/iJtBfJrMatN181293.
Anderson, C. W. (1986) . Learning and problem solving with multi­
layer connectionist systems Ph.D. diss., University of Massachusetts,
Anderson, C. W. ( 1987) . Strategy learning with multilayer connectionist
representations. Technical report TR87-509.3, GTE Laboratories,
Arbib, M. A. (1987) . Brains, machines, and mathematics, 2nd ed. New
Atkeson, C. G. and Reinkensmeyer, D. J. ( 1988). Using associative
content-addressable memories to control robots. In Proceedings of
the IEEE Conference on Decision and Control, 792-797. Austin,
Barto, A. G . , Sutton, R. S . , and Brouwer, P. S. (198 1 ) . Associative
search network: A reinforcement learning associative memory. IEEE
Transactions on Systems, Man, and Cybernetics 40: 201-2 1 1 .
Barto, A. G. , Sutton, R . S . , and Anderson, C. W . (1983) . Neuronlike
elements that can solve difficult learning control problems. IEEE
Transactions on Systems, Man, and Cybernetics 13(5) : 835-846.
Reprinted in J. A. Anderson and E. Rosenfeld, eds., Neurocomput­
ing: Foundations of Research Cambridge, MA: MIT Press, 1988.
Barto, A. G. ( 1 985) . Learning by statistical cooperation of self-interested
neuron-like computing elements. Human Neurobiology 4: 229-256.
Barto, A. G. and Anandan, P. ( 1985 ) . Pattern recognizing stochastic
learning automata. IEEE Transactions on Systems, Man, and Cy­
Barto, A. G. and Anderson, C. W. (1985 ) . Structural learning in con­
nectionist systems. In Program of the Seventh Annual Conference
of the Cogni tive Science Society. 43-45 Irvine, CA.
Barto, A. G . ( 1 986) . Game-theoretic cooperativity in networks of self­
interested units. In J . S. Denker, ed. , Neural networks for computing,
41-46. New York: American Institute of Physics.
Barto, A. G. and Jordan, M. I. ( 1987). Gradient following without
back-propagation in layered networks. In M. Caudill and C. B utler,
eds . , Proceedings of the IEEE First A nnual Conference on Neural
Networks, II629-II636. San Diego, CA: SOS Printing.
Barto, A. G . ( 1 989) . From chemotaxis to cooperativity: Abstract ex­
ercises in neuronal learning strategies. In R. Durbin, R. M aill , and
G . M i tchison, eds. , The computing neuron. Reading, MA: Addison­
Barto, A. G . , Sutton, R. S . , and Watkins, C. (To appear) Learning and
sequential decision making. In M. Gabriel, J. W. Moore , eds . , Learn­
ing and computational neuroscience Cambridge, MA: MIT Press.
Barto, A . G . , An approach to learning control surfaces by connectionist
systems. In M. A. Arbib and A. R. Hanson, eds . , Vision, brain, and
cooperative computation Cambridge, MA: MIT Press.
B aum , E. B . and Haussler, D . , ( 1989) What size net gives valid gener­
alization? Neural computation 1 : 151-160.
Booker, L . B. ( 1982) . Intelligent Behavior as an Adaptation to the task
environment. Ph.D . diss . , Ann Arbor, MI: University of M ichigan.
Bryson, A . E . , Jr. and Ho, Y.-C. ( 1969) .
Bush, R. R. and Mosteller , F. ( 1955) . Stochastic models for learning.
Denker, J. S . , Schwartz, D. B . , W i ttner, B. S . , Solla, S. A . , Howard,
R. E. , Jackel, L. D . , and Hop field, J. J. ( 1 987) . Automatic learning,
rule extraction, and generalization . Complex Systems 1: 877-922.
Derthick, M . { 1 988} . Mundane reasoning by parallel constraint satisfac­
tion. Technical Report CMU-CS-88-182, Department of Computer
Science, C arnegie-Mellon University, Pittsburgh, PA.
Duda, R. O. and Hart, P. E. ( 1 973) . Pattern classification and scene
( 1950) . . Toward a statistical theory of learning. Psycholol­
Farley, B. G. and Clark, W. A. ( 1 954) . Simulation of self-organizing sys­
tems by digital computer. IRE Transactions on Information Theory
Feldman , J. A. and Ballard, D. H. ( 1 982) . Connectionist models and
their properties. Cognitive Science 6: 205-254.
Franklin, J. A. ( 1987) . Learning control in a robotic system. In Proceed­
ings of the 1 987 IEEE International Conference on Systems, Man,
and Cybernetics, 466-470, Alexandria, VA, October 20-23.
Franklin, J . A. ( 1 988) . Refinement of robot motor skills using reinforce­
ment learning. In Proceedings of the twenty-s eventh IEEE Confer­
ence on Decision and Control, 1 096-1101 , Austin, TX, December
Fu, K. S . ( 1970). Learning control systems-Review and outlook. IEEE
Transactions on Automatic Control, 210-221 .
Gaines, B. R . and Andreae, J . H. ( 1 966) . A learning machine i n the
context of the general control problem. In Proceedings of the Third
IFA C Congress, pages 1 4B. 1-14B.8 Institute of Mechanical Engi­
Gallant, S. I. ( 1986). Three constructive algorithms for network learning.
In Proceedings of the Eighth Annual Conference of the Cognitive
Science Society 652--6 60, Hillsdale, NJ: Erlbaum.
Giles, C. L. and Maxwell, T. ( 1987) . Learning, invariance, and gener­
alization in higher-order networks. Applied Optics 26 (December) :
Goodwin, G. C. and Sin, K. S. ( 1984). Adaptive filtering prediction and
control. Englewood Cliffs, NJ: Prentice-Hall.
Grossberg, S . and Kuperstein, M. ( 1 986) . Neural dynamic of adaptive
sensory-motor control: Ballistic eye movements. Amsterdam: Elsevier.
Gullapalli, V. ( 1988 ) . A stochastic algorithm for learning real-valued
functions via reinforcement feedback. Technical Report 88-91 , Uni­
Hampson, S. E. ( 1 983) . A neural model of adaptive behavior. Ph.D.
diss. , University of California, Irvine.
Hinton, G. E. and Anderson, J. A . , eds . , ( 1981 ) . Parallel models of
associative memory. Hil lsdale, NJ:Erlbaum.
Hinton, G. E. ( 1 984) . Distributed representations. Technical Re­
port CMU-CS-84-1 57, Department of Computer Science, Carneg­
Hinton, G. E. ( 1986) . Learning distributed representations of concepts.
In Proceedings of the Eighth A nnual Conference of the Cognitive
Science Society 1-2 Hillsdale, NJ: Erlbaum.
Hinton, G . E. ( 1 988) . Representing part-whole hierarchies in connec­
tionist networks. In Proceedings of the Tenth A nnual Conference of
the Cognitive Science Society, 48-54. Hillsdale, NJ: Erlbaum.
Hinton, G . E. ( 1989 ) . A rtificial Intelligence 40: 185-234.
Holland, J. H. ( 1 986) . Escaping brittleness: The possibility of general­
purpose learning algorithms applied to rule-based systems. In R. S .
Michalski, J. G. Carbonell, and T. M. Mitchell, eds., Machine learn­
ing: An artificial intelligence approach, Vol. II, San Mateo, CA:
Honavar, V. and Uhr, L. ( 1 988) . A network of neuron-like units that
learns to perceive by generation as well as reweighting of its links. In
D. Touretsky, G. Hinton, and T. Sejnowski, eds., Proceedings of the
1 988 Connectionist Models Summer School San Mateo, CA: Morgan
Hornik, K . , Stinchcombe, M., and White, H. ( 1 989) . Multi-layer feed­
forward networks are universal approximators. Neural Networks 2:
Ivakhnenko, A. G. ( 1971) . Polynomial theory of complex systems. IEEE
Transactions on Systems, Man, and Cybernetics 1 : 364-378.
Jordan, M. I. ( 1988) . Sequential dependencies and systems with excess
degrees of freedom. COINS Technical Report 88-27, Department
of Computer and Information Science, University of Massachusetts,
Jordan, M. I. ( 1 990) Learning and the degrees of freedom problem. In
M. Jeannerod, ed. , Attention and performance XIII. Hillsdale, NJ:
Judd, J. S. ( 1987a) . Complexity of connectionist learning with various
node functions. Technical Report 87-60, Department of Computer
and Information Science, University of Massachusetts, Amherst.
Judd, J. S. ( 1 987b ) . Learning in networks is hard. In Proceedings of the
IEEE First Annual Conference on Neural Networks, 685--692 . San
Judd, J. S. ( 1 988) . On the complexity of loading shallow neural net­
works. Journal of Complexity: Special issue on neural computation
Kawato, M., Furukawa, K . , and Suzuki, R. ( 1 987) . A hierarchical neural­
network model for control and learning of voluntary movement. Bi­
Kawato, M., Uno, Y., Isobe, M., and Suzuki, R. ( 1988) . Hierarchical
neural-network model for voluntary movement with application to
robotics. IEEE Control Systems Magazine 8(2): 8-16.
Klopf, A. H. ( 1972 ) . Brain Function and Adaptive Systems-A Hetero­
static theory. Technical Report AFCRL-72-0164, Air Force Cam­
bridge Research Laboratories, Bedford, MA. (A summary appears
in Proceedings of the International Conference on Systems, Man,
and Cybernetics IEEE Systems, Man, and Cybernetics Society, Dallas, TX, 1 974. )
Klopf, A. H. ( 1982 ) . The hedonistic neuron: A theory of memory, learn­
ing, and intelligence. Washington, D.C.: Hemisphere .
Klopf, A. H . ( 1988 ) . A neuronal model of classical conditioning. Psy­
Kohonen , T. ( 1977) . Associative Memory: A system theoretic approach.
Kuperstein , M. ( 1987 ) . Adaptive visual-motor coordination in multi­
joint robots using parallel architecture. In Proceedings of the IEEE
International Conference on Robotics and A utomation, 1 595-1602.
Lapedes, A . and Farber, R. ( 1 987 ) . Nonlinear signal processing using
neural networks: Prediction and system modelling. Technical Re­
port LA-UR-87-2662 , Los Alamos National Laboratory, Los Alamos,
Ie Cun, Y. ( 1985 ) . Une procedure d'apprentissage pour reseau a se­
qui! assymetrique ( A learning procedure for asymmetric threshold
network ) . Proceedings of Cognitiva 85: 599-604.
Ie Cun, Y. ( 1988 ) . A theoretical framework for back-propagation. In
D. Touretzky, G. Hinton, and T. Sejnowski, eds., Proceedings of the
1 988 Connectionist Models Summer School 2 1-28 San Mateo, CA:
Ljung, L. and Soderstrom, T. ( 1983 ) . Theory and practice of recursive
Mendel, J. M. and Fu, K. S. ( 1970) . A daptive, learning, and pattern
recognition systems: Theory and applications. New York: Academic
Mendel, J. M . and McLaren, R. W. ( 1970 ) . Reinforcement learning
control and pattern recognit ion systems. In J. M. Mendel and K. S.
Fu, eds., A daptive, learning and pattern recognition systems: Theory
and applications, 287-318. New York: Academic Press.
Michie, D. and Chambers, R. A. ( 1968). BOXES: An experiment in
adaptive control. In E. Dale and D. Michie, eds . , Machine Intelli­
gence 2, 137-152. London: Oliver and Boyd.
Miller, W. T. ( 1987) . Sensor based control of robotic manipulators us­
ing a general learning algorithm. IEEE Journal of Robotics and
M. L. ( 1 954) . Theory of neural-analog reinforcement systems
and its application to the brain-model problem. Ph.D. diss., Prince­
Minsky, M. L. ( 1961 ) . Steps toward artificial intelligence. Proceedings
of the Institute of Radio Engineers 49: 8 30. Reprinted in E. A.
Feigenbaum and J . Feldman, eds . , Computers and thought. 406450. New York: McGraw-Hill.
Minsky, M. L. and Papert, S. A. ( 1988). Perceptrons: An introduction
to computational geometry. Expanded ed. Cambridge, MA: MIT
and Darken, C. (1988) Learning with localized receptive fields.
In D. Touretzky, G. Hinton, and T. Sejnowski, eds., Proceedings of
the 1 988 Connectionist Summer School, 1 33-143. San Mateo, CA:
C . and Smolensky, P. (1989) . Skeletonization: A technique
trimming the fat from a network via relevance assessment. In D.
Touretzky, ed. , A dvances in neural information processing I, 1071 15 . San Mateo, CA: Morgan Kaufmann.
( 1 987) . A dual back-propagation scheme for scalar reward
Proceedings of the Ninth Annual Conference of the Cog­
nitive Science Society, 165-176, Hillsdale, NJ: Erlbaum.
K. S. and Thathachar, M. A. L. (1974) . Learning automata­
A survey. IEEE Transactions on Systems, Man, and Cybernetics 4:
Report CUED/F-INFENG/TR 22, Engineering Department, Cam­
Parker, D. B. ( 1 985) . Learning logic. Technical Report TR-47, Mas­
sachusetts Institute of Technology, Cambridge, MA.
Pollack, J. ( 1 988) . Recursive auto-associative memeory: devising com­
positional distributed representations. Technical Report MCCS-88124, Computing Research Laboratory, New Mexico State University,
Psaltis, D . , Sideris, A . , and Yamamura, A. A. ( 1 988). A multilay­
ered neural network controller. IEEE Control Systems Magazine
Raibert, M . H. ( 1 978) . A model for sensorimotor control and learning.
Rosenblatt, F. ( 1961 ) . Principles of neurodynamics: Perceptrons and
the theory of brain mechanisms. Washington, D.C.: S part an Books.
Rosenfeld, F. and Touretzky, D. S. ( 1988) . A survey of coarse-coded
symbol memories. In D. Touretsky, G. Hinton, and T. Sejnowski,
eds. , Proceedings of the 1 988 Connectionist Models Summer School.
Rumelhart, D . E., Hinton, G. E., and Williams, R. J. ( 1 986) . Learning
internal representations by error propagation. In D . E. Rumelhart
and J. L. McClelland, eds. , Parallel distributed processing: Explo­
rations in the microstructure of cognition. Vol. 1, Foundations.
Samuel, A. L. ( 1 959) . Some studies in machine learning using the game
of checkers. IBM Journal on Research and Development. 3: 2 10229. Reprinted in E. A. Feigenbaum and J. Feldman, eds., Comput­
ers and Thought. New York: McGraw-Hill, 1963.
Samuel, A. L. (1967) . Some studies in machine learning using the game
of checkers. II-Recent progress IBM Journal on Research and
Saridis, G. N. and Gilbert, H. D. ( 1970) Self-organizing approach to the
stochastic fuel regulator problem. IEEE Transactions on Systems,
Selfridge, 0 . , Sutton, R . S . , and Barto, A. G . ( 1985) Training and track­
ing in robotics. In A. Joshi, ed. , Proceedings of the ninth interna­
tional joint conference on artificial intelligence. Los Angeles, CA:
Sklansky, J. and Wassel, G. N. ( 1981 ) . Pattern classifiers and trainable
Solla, S. A. ( 1 989) Learning and generalization in layered neural net­
works: The contiguity problem. In G. Dreyfus and L. Personnaz,
eds . , Neural networks from models to applications 168-1 77. Paris:
Srinivasan, V., Barto, A. G., and Ydstie, B . E. ( 1988) . Pattern recog­
nition and feedback via parallel distributed processing. Paper pre­
sented at the A nnual Meeting of the American institute of chemical
engineers, Washington D . C . , November.
Stanfill, C. and Waltz, D. ( 1 986) . Toward memory-based reasoning.
Communications of the A CM 29(December): 1 21 3-1228.
Sutton, R. S . and Barto, A. G. ( 1981 ) . Toward a modem theory of adap­
tive networks: Expectation and prediction. Psychological Review 88:
Sutton, R. S . ( 1 984) . Temporal credit assignment in reinforcement learn­
ing. Ph.D. diss. , University of Massachusetts, Amherst.
Sutton, R. S . and Barto, A. G . ( 1 987) . A temporal-difference model of
classical conqitioning. In Proceedings of the Ninth A nnual Confer­
ence of the Cognitive Science Society, Seattle, WA. Hillsdale, NJ:
Sutton, R. S . ( 1988) . Learning to predict by the methods of temporal
Sutton, R. S. and Barto, A. G. ( to appear ) Time-derivative models of
Pavlovian conditioning. In M. Gabriel and J. W. Moore, eds. , Learn­
ing and computational neuroscience, Cambridge, MA: MIT Press.
Touretzky, D. S . ( 1986) . BoltzCONS: Reconciling connectionism with
the recursive nature of stacks and trees. In Proceedings of the Eighth
A nnual Conference of the Cognitive Science Society, Amherst, MA.
Touretzky, D. S . and Hinton, G. E. ( 1985 ) . Symbols among the neurons:
Details of a connectionist inference architecture. In A. Joshi, ed. ,
Proceedings of the Ninth International Joint Conference on A rtificial
Intelligence. 238-243 Los Angeles, CA. San Mateo, CA: Morgan
Tsetlin, M. L. ( 1 973) . A utomaton theory and modeling of biological
Waltz, M. D . and Fu, K. S. ( 1 965 ) . A heuristic approach to reinforcment
learning control systems. IEEE Transactions on A utomatic Control
Watkins, C . ( 1 989) Ph.D. diss., Learning from delayed rewards. Cam­
Werbos, P. J. ( 1974) Beyond regression: New tools for prediction and
analysis in the behavioral sciences. Ph.D. diss. , Harvard University,
Werbos, P. J. ( 1 977) Advanced forecastin methods for global crisis warn­
ing and models of intelligence. General system yearbook 22: 25-38.
Werbos, P. J. ( 1 988) . Generalization of back propagation with applica­
tions to a recurrent gas market model. Neural Networks, 1 : 339-356.
Widrow, B. and Hoff, M. E. (1960) . Adaptive switching circuits. In
1 960 WESCON Convention Record Part IV, 96-104, Reprinted in
J. A. Anderson and E. Rosenfeld, Neurocomputing: Foundations of
Widrow, B. and Smith, F. W. (1964) Pattern-recognizing control sys­
tems. In Computer and Information Sciences (COINS) Proceedings,
Widrow, B . , Gupta, N. K . , and Maitra, S. (19 73 ) . Punish/reward:
Learning with a critic in adaptive threshold systems. IEEE Trans­
actions on Systems, Man, and Cybernetics 5: 455-465 .
Widrow, B . , McCool, J . , and Medoff, B . ( 1 978) . Adaptive control by
inverse modeling. In Twelfth A silomar Conference on Circuits, Sys­
Widrow, B. and Stearns, S. D. (198 5 ) . Adaptive Signal Processing. En­
Widrow, B . ( 1 986) . Adaptive inverse control. In Proceedings of the
Second IFA C Workshop on Adaptive Systems in Control and Signal
Processing, 1-5, Lund, Sweden: Lund Institute of Technology
Williams, R. J. (1986 ) . Reinforcement learning in connectionist net­
works: a mathematical analysis. Technical Report ICS 8605, Insti­
tute for Cognitive Science, University of California. San Diego, La
Williams, R. J. ( 198 7 ) . Reinforcement-learning connectionist sys­
tems. Technical Report NU-CCS-87-3, College of Computer Science,
Williams, R. J. ( 1988) . On the use of backpropagation in associative
reinforcement learning. In Proceedings of the IEEE International
Conference on Neural Networks 263-270. San Diego, CA, July 2427.
Witten, I. H. ( 1 977) . An adaptive optimal controller for discrete-time
Markov environments. Information and Control 34: 286-295 .
This book discusses tools, applications, potential applications, and
benchmark problems for neurocontrol. The range of applications should
be clear from the titles of the various chapters in parts II and III, and
appears fairly complex as it spans both parts I and II. This chapter is
an overview of some of the tools discussed in other chapters.
The chapter by Barto in part I of the book provides a deeper in­
troduction to this field, its intellectual roots, and its relation to other
theoretical paradigms. This section will simply outline five basic neural
network controller designs that lie at the core of many of the examples
In supervised control, an artificial neural network (ANN) learns to
imitate a human or a computer program which already knows how
2. In direct inverse control, an ANN learns the mapping from the po­
back to the actuator signals (usually the angles
would move the arm to that position. The ANN is then used to
make the arm follow a desired trajectory or reach a desired target
point. The desired trajectory is supplied by a human or a different
3. In neural adaptive control, ANNs are substituted for the (linear)
mappings used in conventional adaptive control.
adaptive control includes designs like the Self-Tuning Regulator
(STR) and Model-Reference Adaptive Control (MRAC). These de­
signs, like inverse control, try to achieve
4. In backpropagation-through-time (BTT), the user specifies a utility
function or performance measure to be maximized and a model of
the external environment. Backpropagation is used to calculate the
derivative of utility summed across all future times with respect to
curre nt actions. These derivatives are then used to adapt the ANN
which outputs the actions, or to adapt a schedule of actions. (This
chapter will ignore other uses of BTT in pattern recognition, etc.)
5. In adaptive critic methods, the user again supplies a function or
by adapting an additional ANN, called a critic network,
which evaluates the progress that the system is making. In other
utility function (or its derivatives), that somehow represents the
sum of the original utility function across all future time.
network which outputs the actions is adapted to maximize this
secondary utility function in the immediate future. Many different
adaptive critic designs have been proposed.
Table 2.1 compares these methods against four of the capabilities of
network made up of differentiable functions; they are not restricted to
Supervised learning, by definition, does not involve any long-term plan­
ning. Some researchers have argued that supervised control is "cheat­
ing," since it only works if someone already knows how to perform the
desired task. However, Widrow pointed out in discussion that this kind
human expert to a robot. It can be seen as a kind of
neural expert system, which pays attention to what people actually do
instead of what they say they do. Also, if humans can only perform a
task at slow speed, in simulation, a supervised controller can learn to
Widrow's early pole-balancer was the first working example of super­
supervised control in aviation. Kawato mentioned that Fuji is now using
Supervised control is similar, in a way, to the old pendants which are
used to record desired motions and store them in a robot controller.
However, the pendants recorded only a fixed schedule of movements.
Supervised controllers record both actions and sensor inputs, so that
the system can learn how movements vary in response to sensor inputs.
To implement supervised control, one begins by choosing one of the
many known methods of supervised learning. In any form of supervised
learning, the user plugs in a history of inputs and targets (desired out­
puts), and the learning method then adapts the neural network. (This
may also be done in real time.) Among the supervised learning methods
described in this book are CMAC, basic backpropagation, and Atkeson's
In the case of backpropagation, many tutorials have been published
The chapters by Williams, Werbos, and Narendra contain
citations. Numerical methods that may be useful in getting backprop­
agation to converge quickly are described in the chapter by Shanno, a
world-renowned leader in the area of numerical optimization.
Direct inverse control is also based on supervised learning. Once again,
any supervised learning method may be used. T he targets are the actua­
tor signals (angles), and the inputs are the coordinates of the robot
(or other system). The history of inputs and targets comes from letting
the system move aroun<l1opj(li�i\IgItatahl positions and angles.
Atkeson's chapter provides a rather straightforward example of direct
inverse control. Kuperstein's work is also rather straightforward in prin­
ciple; the main complexity lies in the hard-wired preprocessor he uses
on the sensor inputs, inspired by biology. He reports position errors on
the order of 3-5 percent. Miller (see chapter 6) has developed different
refinements, including the use of earlier spatial coordinates as an addi­
tional input to the network. With these refinements, he reports position
errors small enough to be of real commercial interest.
Kawato and Jordan address the problem of trajectory following with­
out using direct inverse control. They stress the limitations of direct
inverse control, in the case where there is no one-to-one mapping from
spatial coordinates to actuator signals. For example, when there are
more actuators than there are coordinates, a one-to-one mapping does
Table 2.1 shows question marks for neural adaptive control because the
research in that area is at an early stage. The chapter by Kraft and
Campagna discusses conventional adaptive control and its performance.
Narendra is a recognized leader in the field of adaptive control, cur­
rently working on a variety of topics in neurocontrol. His chapter begins
to explore links between adaptive control and neural networks.
Backpropagation-through-time (BTT) can be used to solve problems
of optimization over time because: (1) the user or designer is allowed
to pick any utility function, performance measure, or cost function to
maximize or minimize; (2) the method accounts precisely for the impact
The chapter by Williams focuses on the theory of BTT and on related
methods or approximations that are popular in some quarters. The
chapters by Narendra and Werbos also mention this method.
BTT is basically equivalent to the calculus of variations, a well-known
method in control theory. The only essential difference is that BTT
includes a way of calculating the derivatives of utility, which is much
faster to calculate when working with large sparse systems. Like the
calculus of variations, it has two disadvantages: (1) it requires a model of
the external environmelC�atd�atinciple, must be noise-free
it requires calculations backwards through time (in the
derivative calculations), which are not consistent with true real-time
Still, one may implement something like real-time learning,
by dividing experience up into distinct "experiments" or "strings" and
updating the weights after each experiment is analyzed.
Four applications of BTT as a neurocontroller were discussed at the
workshop. The simplest example was by W idrow, whose chapter may be
useful as an introduction to how to implement the method. It is crucial,
however, to backpropagate through the correct sequence of blocks, as
Kawato's new cascade method and Jordan's "in­
verse control" method are both based on BTT, as discussed in Kawato's
chapter. Kawato's cascade method includes a "first pass" which adapts
a model of the environment in an unusual but powerful way; the "sec­
considerable effort in crafting utility functions which represent what we
really want a robot arm to accomplish. F inally, Werbos briefly discussed
an application to maximizing profits in the natural gas industry, used
in the official forecasts of the Department of Energy (chapter 3). That
example should be useful to applications that involve general networks
of functions, which may or may not be neural networks.
Adaptive critic designs include a large family of methods, discussed in
the chapters by Barto and by Werbos (chapter 3).
are capable of maximizing any utility function or measure of reinforce­
They are less exact than BTT, insofar as they
an approximation-the critic network-to represent the effect of current
On the other hand, they can be derived as
approximations to dynamic programming, which is an exact method for
handling noise and stochastic models of the environment.
Formally speaking, Kawato's feedback error learning method may be
seen as a kind of critic design, in which a preprogrammed feedback con­
troller acts as a (nonadaptive) critic. Naturally, the performance of that
method depends on how good the feedback controller is.
Several workers-including Mayhew of Sheffield University in England
-have reported great success with adaptive critic designs, in simple
physical experiments (not just simulations).
was not discussed here. Likewise, there was no discussion of the work
by Grossberg and Levixm��rate "secondary reinforce-
though its mathematical properties require further analysis.
All these adaptive critic designs are consistent with real-time learning,
including those which use backpropagation.
me&-llrE' of utility. reinforcement or perfor­
repre-senting the de,iation betWE'en the acmal
trajectory and the desired trajecTOry. and the Olher represeming eneIID­
e..-q>enditure. Jordan (1989) ha.5 shown that this approach acmally works
in simulation tE'::>',s. Kawaro. in chapter
trajecTOry planning. a.5 do �gu�-en and 'Yidrow in chapter 12. Psychol­
ogb',s haw smdied the idea of learning ba..'E'd on reinfo rcement si..,anals
for decade.s. :\Iany probleIlli in indlb-rry can be stated a.5 probleIlli in
designing neural networks TO :;.olw Tne.se probleIlli. the bigga-.-r
for the link betWE'en present actions and
an explicit model of the enernal emironmem which the neural
�ing TO control. and lL'E' bacl...-proJ>8ca-arion through time (BTT) to
calculate the derinnlYI:':' of furore utility with re:,-pecT TO pre.sem actions.
(All reierE'nces to BIT in this chapter will refer to this particular way
of u..� baci...-proJ>8ca-ation. See Weroo.s (l990b) for a tutorial on back­
propa."a-arion through time in more general applicarions). The other way
is to adapr a -critic network. a special network which ourput,s an �--ti­
mQt� of the total furore utiliTY which will arL.� from pre-sent simations
or actions. This chapter will focus on the adaptiw critic approach.
This book pn>Sellt.s th� two approach€5 a.5 ways of adapting artificial
neural nenrorks_ Howt'Yl'x, both approach€5 can be applied jlb, as ea.."­
ih- to adapting any network of d.ifi'e.rE'ntiable functions. They can both
� newro as general methods in control theory. For e.."GlIpIl le., Weroo.s
(19S9a) u..� BIT to ma."'llinize profit.s in the natural gas indlb-rry. as
part of the official1� forecast.s of The Energy Information _-\.d.minblni­
tion: the model of the emironmem in that ca..� Vi"8S a simple economic
model of that indm.u� CIi�c1�J of connol theory. BIT is
the same as the first-order calculus of variations (Bryson and Ho 1969),
modified only by the use of a more efficient, parallel algorithm to cal­
culate the derivatives (Lagrange multipliers). Likewise, the adaptive
critic can be derived as a way of approximating dynamic programming;
dynamic programming, in turn, is the only exact and efficient method
available to control motors or muscles over time, so as to maximize
a utility function in a noisy, nonlinear environment, without making
highly specialized assumptions about the nature of that environment.
Section 3.7 will describe how to implement the ideas in this chapter in
the general situation, not limited to neural networks as such.
BTT is easy to use and exact, but it has no provision for handling noise
or error in one's model of the plant, and it is not suitable for real-time
learning as in biological systems (Werbos 1988b). Thus the very first
journal article mentioning backpropagation (Werbos 1977) focused on its
use in a subordinate role, as an adjunct to the adaptive critic approach.
In this approach, backpropagation may be used from cell to cell at a
given time, but not backwards through time. Many people believe that
backprop�gation is not biologically plausible even in that limited role;
however, arguments can be made for its plausibility (e.g., Werbos 1988a,
1988b), especially in light of new evidence that the cytoskeleton inside
of mammalian cells can transmit information at speeds on the order of
millimeters per millisecond (Zhu and Skalak 1988).
This chapter will focus in depth on the adaptive critic approach, which
is a large and important subject in its own right. Other chapters and
papers cited above already describe BTT (chapter 4 also describes some
variants of BTT). For background information on all these methods,
see chapter 1 and chapter 2. For concise mathematical definitions and
Adaptive critic designs come in many shapes and forms. The sim­
plest and best-known designs have been criticized for their inability to
handle very large control problems. This chapter will argue that these
criticisms are legitimate, but that we can overcome them by using more
complex adaptive critic designs. Many applications do not require such
complexity, but in the long-term-to duplicate or explain the capabili­
ties of the brain-we will need to work with it. This chapter will begin
by presenting a simple, generic design for an adaptive critic system, and
then-section by section-show how additional features can be added,
Section 3.2 will present a simple design, made up of only two neu­
ral networks-a critic network and an action network. Designs like this
have worked well in reDp��ts (not just simulations);
however, the examples I know of are proprietary, confidential, or so re­
cent that I have only heard oral presentations. (For example, John May­
hew of Sheffield University in England has spoken about applications to
controlling a simple autonomous vehicle.)
Section 3.3 will describe two minor limitations of the simple design,
related to the possibility of divergence and the impact of unobserved
variables. It will describe how to overcome these limitations, by go­
ing back to the literature on dynamic programming, and upgrading the
Sections 3.4 and 3.5 will focus on the deeper, more difficult problem
of how to handle a large number of control variables (e.g., many motors
or muscles to control) or sensor variables. Certainly the human brain
is capable of handling millions of control variables, and there are many
engineering applications where a number of motors must be controlled in
tandem. Section 3.4 will describe a way of coping with these problems,
in adapting the action network, and will draw some parallels with the
human brain. Section 3.5 will go further, by describing alternative ways
Section 3.6 will discuss a few topics for future research, such as the
problem of high-speed motor control and the problem of extending the
effective planning horizon. Section 3.7 will give equations and examples
A Simple Two-Component Adaptive Critic Design
This section will describe the design of a neural network system to solve
the problem of reinforcement learning. Before describing that system, I
will first describe the problem which it tries to solve.
The problem of reinforcement learning has very deep roots in the
literature of psychology. Sutton (1984) has traced this problem back to
Marvin Minsky, in the early 1960s and earlier.
Figure 3.1 illustrates the problem. Imagine a little man (or computer)
sitting in front of a row of levels labeled Ul through Un. Beside him is
a big meter (which looks like a thermometer in the cartoon), labeled
U. The meter gives a measure of how well the man is doing. His job is
to control the levers so as to make U as large as possible. The blinking
lights, labeled Xl through Xm, are simply a source of information which
the man can use in deciding which levers to pull and when to pull them.
In principle, the man &e�(ttM�ho knowledge of the lights
or the meter; he may have to learn everything from scratch, allowing
for the possibility of nonlinear fluctuations and noise in the external
This notation is similar to that used in decision and control theory.
The collection of variables Ul through Un form a vector, y, called the
control vector (Bryson and Ho 1969). The observation variables Xl
through Xm form a vector X, similar to the observation vector of control
theory. The variable U-called "reinforcement" in psychology-plays
the same role as the utility function U in the theory of cardinal utility
developed by John Von Neumann and popularized by Howard Raiffa.
(U may also be used to represent a "performance index" or a "cost
Williams ( 1988) has discussed earlier formulations of reinforcement
learning, in which the actions g(t) are chosen to maximize U (t), as
if later times did not matter. This chapter will focus entirely on the
problem of reinforcement learning over time, in which the goal is to
maximize the long-term expected value of U(t). We will always try to
account for the effect of current actions on the state of the world at later
Figure 3.2 shows one way to build an adaptive critic system, taken (with
modification) from Sutton (1984). Two neural networks are adapted
over time-an action network and a critic network. The action network
outputs the actual control signals, g(t), while the critic network guides
how the action network is adapted. The critic network inputs a descrip­
tion of the state of the world (X(t)) and outputs a single number, J(t),
which is an evaluation of how well the action network is doing in creating
a "good" situation. (The letter J stands for "Judgement," and is also
used by Bryson and Ho (1969) and Raiffa (1968).) Then, in each time
t, the action network is "rewarded" or "punished," based on what kind
of situation it produces; in other words, actions g(t) are "rewarded" if
they lead to good results (larger J{t + 1)) and "punished" if they lead
To translate this psychological intuition into a working design, we
1. A learning rule which states how the weights in the action network
change in response to reward and punishment (J(t + 1»;
2. A learning rule which states how the weights in the critic network
To adapt the action network, Barto, Sutton and Anderson (1983) ad­
justed the weights in response to correlations between various variables
calculated in the network and the variable J. I will not discuss the details
of their method, in part because they are already well known, and in
part because section 3.4 will discuss some alternatives.
The remainder of this section will describe a simple form of heuristic
dynamic programming (HDP), a method for adapting the critic network
proposed in Werbos (1977). This chapter will not describe the better­
known method of temporal differences used by Barto, Sutton and An­
derson to adapt their critic networks; however, Werbos (1990a) argues
that HDP may be viewed as a generalization of that method. Grossberg
and his collaborators have also developed critic-like networks, which are
As noted above, the critic network yields an output, J, which is a func­
tion of its current inputs, X, and of its weights, w. Let us write this
Intuitively, we want to find weights w which make J(X(t),Yl.) an ac­
curate assessment of "how good" X(t) is. (Section 3.3 will provide a
more rigorous basis for this idea.) We want to know how good X(t) is,
relative to the problem we started with-maximizing the expected value
of U across all future times. Ideally, then, we would want J(X(t),Yl.) to
assuming that this sum converges. (Section 3.3 describes what to do if
How can we train the critic network to provide such an estimate?
HDP, like backpropagation, can be implemented in a variety of ways.
For example, it can be implemented through real-time learning (where
the weights are updated after each pattern is analyzed), or it can be
implemented through batch learning (where the weights are updated all
at once after a big pass through all the patterns). For simplicity, I will
In order to simplify our future discussion, it will help to remember that
U(t) is available at time t as one of the observed inputs to the system.
Thus we can assume that it is available as one of the components of the
vector X; in other words, Xk(t) U(t) or some k. This lets us write U
as a function of X, U(X) Xk. Let us also assume that we are given
a time series of vectors, X (t), for t going from 1 to T. Once again, our
goal is to adapt the weights W. in the critic network.
In HDP, we use some sort of supervised learning method to adapt
the critic. Any supervised learning method will do, so long as we use
the right inputs and targets. For example, anyone familiar with basic
backpropagation should be able to fill in the rest of the details, when
the inputs to the net and the targets are fully specified, for the training
In the simplest form of HDP, we carry out several passes through
the training set. We start with an initial set of weights w.(O); in pass
number n, we derive a new set of weights w.(n). We keep going through
the training set, over and over, until the weights settle down, Le. until
On each pass, our training set consists of T 1 pairs of inputs and
target, for t 1 through T 1. (There is only one target for each pair,
because the critic network has only one output, J.) The inputs for time
t are simply the vector X(t). The target for time t, within pass number
In other words, before we begin the adaptation of the weights in pass
number n, we have to plug in X (t + 1) into the critic network, using the
old weights, in order to calculate the target for each time t. The targets
are then fixed throughout pass number n. Then, in the adaptation phase
of pass number n, we update the weights to try to reach the targets. We
could do this in an exhaustive way, by searching the entire weight space,
or by using a supervised learning method (like Kohonen's pseudo-inverse
method) which converges in a single pass; alternatively, we could simply
Theory tells us that we should simply throw out the case where t T,
when we are trying to control an infinite, continuous process, because
equation 3.2 is not well defined in that case (since X(T+ 1) is unknown).
In many practical applications, however, our set of observations X(I)
through X(T) is actually made up of several strings of observations,
where each string repr��Mtajetiment on the system to be
controlled. In those applications, X(T) usually represents the comple­
tion of the last experiment; thus the case t T should be added to the
data set, with a target of U(X(t». In fact, the target for the final time
t of any such string is simply U(X(t». In some applications, we may
even want to use a different utility function, U, for the final time, to
measure how well the experiment is completed; this can be done quite
Werbos (1990a) has shown that this simple form of HDP converges to
exactly the right weights, for a simple class of linear problems. More pre­
cisely, it converges whenever the external environ�ent is a linear system,
governed by a matrix equation with multivariate normal noise, when the
Critic network has the appropriate form (which is simply linear in this
case), and when the supervised learning method is itself statistically
Figure 3.3 illustrates the basic trick used in dynamic programming, which
is one of the fundamental tools used in control theory. Dynamic pro­
gramming is discussed in standard textbooks like Bryson and Ho (1969)
and Gale (1979), but the extension of dynamic programming developed
Dynamic programming requires as its input a utility function U and
a model of the external environment or plant, which I denote as "E".
Dynamic programming produces, as its major output, another function,
J*, which I like to call a secondary or strategic utility function. The key
insight in dynamic programming is that you can maximize the expected
value of U, in the long term, over time, simply by maximizing the func­
tion J* in the immediate future. Whenever you know the function J*
and the model F, it is a simple problem in function maximization to
pick the actions which maximize J*. The function J of section 3.2 may
be viewed as an approxiTl!-ation to the function J*.
Why should we train our Action network to maximize an approxima­
tion like J instead of the exact function J*? Clearly it is better to use
the exact function, when this is possible. But the computational cost of
finding J* grows exponentially with the number of variables in the prob­
lem. To cope with complicated problems in the general case, we really
have only one choice; we have to approximate dynamic programming, by
using a model or netwo�Nitelili/function or its derivatives.
What dynamic programming requires and produces.
The derivatives of J* (X, Yl.) with respect to the variables Xk form a vec­
tor II *; the Lagrange multipliers used in the calculus of variations are an
example of this vector. In fact, all approximation methods-including
neural net methods-for "solving" the reinforcement learning problem
over time in a general environment may be viewed as general approxi­
mations to dynamic programming; in all cases which I am familiar with,
they do include specific functions which try to approximate J* or ll* in a
general way. Examples may be found in numerous fields, including arti­
ficial intelligence, physics, economics, and schools of psychology ranging
from the behaviorist to the avowed mystic (Werbos 1986).
By looking at dynamic programming more closely, we will see two
features-involving two new terms, 11 and Uo-to enhance the design of
Dynamic programming assumes the availability of a model of the exter­
nal environment or plant which we may write as:
where yet) refers to the vector of actions at time t, noise is a vector
of random numbers, and 11 is a vector describing the current state
of Reality-the plant or the environment. For mnemonic purposes, it
helps to recall that "R" stands for "recurrent," "representation" and
"reconstruction" -all of which apply to some extent.
At this point, some readers may believe that the shift from X to 11
is an oversight or an unnecessary inconvenience. However, this shift is
truly fundamental to the validity of the method. If equation 3.3 were
an accurate description of reality with 11 = X, then the shift would
indeed be unnecessary; however, this is usually not the case in practical
applications and is certainly not the case in human psychology. For
example, when a person sees a ball roll under a chair, he does not act
as if the ball has vanished from reality. Likewise, in control theory, the
state vector and the vector of observables are usually quite distinct.
How can we build a neural network to represent the function F? A
simple, popular approach is to build a network which inputs X(t), which
uses X(t + 1) as its targets, and which includes recurrent hidden units.
In that case, the vector R(t) would consist of the outputs of the hidden
units and all the components of X(t). Unfortunately, this approach has
a number of weaknesses, involving its robustness over long time periods,
its representation of noise, and problems involving real-time convergence.
Werbos ( 1987a) offers a few suggestions for overcoming these weaknesses,
based on extensive empirical work, but more research is needed. The
first stage of Kawato's cascade method is an example of what Werbos
(1977) calls the "pure robust method" ; it is a good first step, but very
far from the whole story. Difficulties in this area are arguably the most
serious obstacle now facing us in neurocontro!'
In any event, it is safe to treat X as part of the vector R, since what we
observe is part of reality. For this reason, we may treat U as a function
of 11, U (E). Henceforth, I will also defi�e:
Howard has proven that the function J* can be found by solving a
modified form of the Bf!HfHi{fig_Mi,iferial
where Uo is an intercept term used to prevent drift off to infinity, and
where the angle brackets denote the average or expected value. (Nota­
tional standards vary greatly here; angle brackets are standard in physics
and some branches of statistics, because they minimize confusion in long
calculations.) Howard's equation was slightly more complicated, in that
he allows U to depend on y as well as H; that feature is of little value
here, and would lead to unnecessary confusion.
Comparing equation 3.5 with equation 3.2, it is clear that "Uo" is an
additional complication. If there is no possibility of drift off to infinity
(i.e., if the expected value of equation 3.1 converges), then Uo is unnec­
essary. Such convergence can result either from uncertainty about the
future which grows with time (Werbos 1990a) or from discount rates (as
in Barto, Sutton and Anderson 1983). When equation 3.1 does not con­
verge, one can modify HDP by estimating Uo explicitly. For example,
one can define J(H) + Uo as the output of the network, and keep the tar­
gets as in equation 3.1; one can adapt Uo as one adapts any other weight
in the network. Werbos (1979) talks at length about problems related to
the existence of Uo; fortunately, these problems seem far removed from
Equation 3.5 assumes the existence of a model, F, while section 2
did not. The point here is that Section 3.2-following Barto, Sutton
and Anderson (1983)-used the world itself as a model of itself. Instead
of using a model, F, and simulating F(H,y, noise) to get a simulated
R(t + I), it used reality itself to generate H(t + 1). This should yield
the correct expectation values, after enough experience, but there is one
catch: we may need to develop a model F anyway, to help us develop a
representation vector R, in applications where that is important.
Howard has proven many theorems which are relevant to HDP. One of
them may be roughly translated as follows. Suppose that we have a critic
network J(H, w ) and an action network u(R,w') so rich that they can
represent any functions J*(H) and Jl* (H) by choosing the appropriate
weights. Suppose that we adapt these networks by going back and forth
1. Update Y!. such that J(B.(t) , y!'(n) equals J( < H(t+ 1) >, y!'(n-l)
U(H(t» - Uo, for all possible vectors H(t), where the expecta­
tion value of H(t + 1) refers to the expectation assuming that
y(R,!ll(n-l) is used to control system actions;
2. Updatey!" such th�fl:«iJ(�(M'mruHmizes
Then the weights :!Q and :!Q' will converge to give the optimal strategy of
Alternative Ways to Figure 3.2 in Adapting the
The design shown in figure 3.2 has been criticized by Guez (1987) and
others, because it provides such little feedback to guide the action net­
work. There is only a single number, J(t + 1), used to evaluate all
aspects of action. This is very different from the usual situation in su­
pervised learning, where separate error measures are available to each
of the output cells. Here, there is no indication of which output cell
should have had a different output, or of which way the output should
have been different (smaller versus larger). For this reason, when there
are many outputs to be controlled, one can expect much slower learning
than one would get with supervised learning. The problem here is like
the problem of a student trying to figure out what to study when his
professor only gives him an overall grade (like J), instead of giving him
feedback targeted to individual questions on the exam.
These intuitive arguments can also be expressed in statistical terms.
When there are a very large number of variables (like all the weights
in a network) correlated against a single dependent variable (like J),
one faces a problem called multicollinearity (Wonnacott and Wonnacott
1977). With such multicollinearity, it becomes nearly impossible to es­
timate the correct weights, unless one has an astronomically large data
base. The point is that the system learns slowly, when there are many
output variables, for reasons which have nothing to do with the nu­
merical slowness reported with some versions of backpropagation. The
system is slow in the sense that the estimated weights, after numerical
convergence, still converge very slowly to the true weights as the size of
the database, T, goes to infinity. (Multicollinearity is an issue in super­
vised learning as well (Werbos 1987a), but the assumption of a sparse
structure tends to keep it from getting out of hand.)
Figure 3.4 illustrates an alternative to figure 3.2, based on the
Figure 3.4 is based on the idea of following equation 3.5 more exactly.
After all, our goal is to find a vector y. which maximizes J(F(R, y)) .
Figure 3.2 is a lot like plugging in values of u and looking only at the
values of the function J(F(R,y')) . But with backpropagation, we can
calculate the derivatives of J(F(E,y.)) with respect to all the compo­
nents of y', in a single pass through the system. In numerical analysis,
at least, we can find the maximum of a function much faster if we ex­
ploit the gradient information, instead of looking only at the value of the
function. (When the gradient is used with steepest ascent, convergence
may be very slow, but Werbos (1988a) describes other ways of using the
The basic idea here is very straightforward. The action network can
be represented by a function, y(R,!Q'). Our goal is to adapt the weights
w�. To do this, we use backpropagation to calculate the derivatives of
J(F(R,y'(R,yj))) with respect to the wi. Then we adjust the weights
in response to the derivatives, as in conventional backpropagation. In
this case, we are backpropagating through the critic to the model (F)
and then to the action network, as if the three networks formed one
large feedforward network. Information is passed backwards (the dotted
lines) from network to network in figure 3.4, but it is not propagated
backwards in time. (Se£�MalftriIbre details.)
This approach may be viewed as a synthesis of the best features of
figure 3.2 and of backpropagation through time. As with backpropa­
gation through time, we have specific feedback to specific components
of y., rather than one gross evaluation. This feedback accounts for t he
cause-and-effect information embedded in the network F. If a machine
or person never uses knowledge of cause and effect to guide his choice
of actions, then there are limits to how well he can cope with a complex
Williams (1988) has pointed out that we have to be very careful in
propagating derivatives through a network (like F in figure 3.4) which
has noise attached to it. Therefore, I will be very specific about how to
There are two different ways to handle the noise vector in the F
network-the simulation approach and the imputed-noise approach. I
will describe both of these approaches for the case of batch learning, as
Assume that we have a set of vectors R(t) for t 1 through T. Assume
that the model network F and the critic network J will be held constant.
Our goal for now is to adapt the action network y'(R, w').
In the simulation approach, we begin by simulating the noise vectors.
For each value of t and each component of the vector noise, we pick a
value at random, based on the probability distribution for that compo­
nent. This procedure will yield vectors noise(t) for all t, which we hold
constant thereafter. Then on each pass through the data, calculate the
derivatives of J with respect to a weight w� as:
treating both noise and R( t ) as constants. Many variations are possi­
ble, such as the simulation of multiple noise vectors for each R(t), res­
imulation after each major pass, and comparison tests across different
Many readers will ask where we find a probability distribution for each
component of noise. In fact, this probability distribution is really part
of our model of the plant. It may come from our substantive knowledge
of the plant, or it may come from a neural network adapted to represent
the plant; as discussed in section 3.3, it is not a simple issue how we
should adapt such a ne&mtrrighted Material
One way of developing such a model of the plant is to build a network
which inputs R(t) and !!(t), and aims at R( t + 1) as the targets for its
output. We can then assume the following stochastic model of the plant:
where the only form of noise is error in the forecast R(t + 1). We can
assume that the noise for each component is normally distributed, and
we can estimate the variance by simply looking at the variance of � R..
across the training set. (For high performance, one can modify this very
slightly, to assume that the noise acts on the activation level of the
When our model of the plant looks like equation 3.6, we have a very
simple alternative to the simulation approach. For each pair of vec­
tors R(t) and R(t + 1) in our data, we can simply plug R(t) into the
In fact, this procedure turns out to be very simple in practice. It amounts
to plugging in the actual R( t + 1) into the critic for the first part of the
backpropagation (through the critic), but backpropagating straight on
through to the forecasting network and the action network as if there
had been no noise. We don't even have to estimate the variance of the
noise, because we never really use it for anything.
The approach of section 3.2 is really an example of the imputed noise
approach. However, the simulation approach could also be used with
HDP as well. Furthermore, it is also possible to simulate the vectors
R(t) themselves, when using the simulation method. The imputed noise
approach has the advantage of staying close to observed data, and there­
fore being more robust and reliable. The simulation approaches (like
"dreaming") have the advantage of allowing the system to prepare for
situations it can predict but has never experienced; for example, if hu­
mans were not capable of dreaming, we would not be capable of embark­
ing on projects like space programs or building new devices which have
never existed in the past. It is impossible to map out a complex deci­
sion space without including some provision-either dreaming or actual
exploration-to ensure that all regions in the space are considered.
Figure 3.4 has certain advantages over figure 3.2, which have been dis­
cussed. However, it h�cM�n. For example, its validity
depends on the validity of the model of the plant, F. Figure 3.2 may be
slower but more reliable at times, since it sticks to observed data only.
If there is a sudden, unexpected painful event, a system like Figure 3.2
may be able to respond immediately, by changing its weights, instead
of waiting for a change in the model of the plant which can explain the
situation. (On the other hand, models of the plant like Kawato's may
be a better guide to action than the empirical data, at times, because
they can filter out confusing short-term fluctuations.)
These relative strengths and weaknesses are similar in flavor to the
strengths and weaknesses of backpropagation versus associative memory
in conventional supervised learning (Werbos 1987a, 1988a). Once again,
one would like to find a blend which combines the best of both methods.
This blend is really needed only in the most sophisticated applications,
where it is important to combine the advantages of both.
One possible blend has emerged from discussion between Barto and
myself this past year. In essence, one can start with HDP, as in section
3.2, but also adapt an auxiliary network J' (,!!, B.) which tries to predict
the error, J(R(t)) minus the targets in equation 3.2. Then one can
backpropagate through the critic and the model network back to ,!!, as
in figure 3.4, but also backpropagate through the J' network directly
to '!!. If J' is adapted by a high-speed supervised learning method (like
associative memory), then this will provide an immediate reward or
punishment to weights which lead to unexpected good or bad results.
There are many refinements possible here, and other alternatives as well.
This approach can also be extended quite easily to the methods of section
3.5. A similar structure, based in different mathematical objectives, has
worked well in simulations by Lukes et al. (1990).
The human brain is not a homogeneous mass of identical neurons, but
it is not a preprogrammed hierarchy of cells with fixed specific tasks
either. During the workshop which led to this book, James Houk stated
that the brain is actually quite flexible and modular; it is made up of
five or so major subsystems, each containing a high degree of flexibility
(albeit with different cell types) within it. From a functional point of
view, standard texts like Nauta and Feirtag (1986) suggest that the five
major adaptive subsystems are: (1) the limbic system; (2) the basal
ganglia; (3) the thalamus/cerebral-cortex system; (4) the brain stem;
Figure 3.4 shows only three major subsystems. Werbos ( 1987b) cites
extensive physiological evidence consistent with the idea that the limbic
system is like a critic network, the corticothalamic system like a model
network ( F ) , and the brain stem like an action network. But this leaves
out the basal ganglia, including the nucleus basalis, which recent research
points to as very important. The connections of the basal ganglia are
more like figure 3.2 than figure 3.4, suggesting that the brain might
represent a kind of blend of these architectures. This blend may be very
different in quality from the blend discussed in this section; if so, it is
all the more important to study these differences in detail, because they
may point to new opportunities on the engineering side. The cerebellum
is well known to be responsible for smoothing out high-speed motor
Alternatives to HDP in Adapting the Critic
Section 3.4 began with a discussion of the challenge of large problems, as
it affects the adaptation of the action network. The exact same problem
also affects the adaptation of the critic network. If we use HDP, as
described in section 3.2, then we still have that problem, even if we use
the design of figure 3.4 to adapt the action network. HDP still only uses
one piece of information-the target for J as given in equation 3.1-to
One useful step in solving this problem is to exploit our knowledge ( if
any ) of the utility function, U(R). Section 3.2 showed how the reinforce­
ment learning problem can be represented as a special case of utility
maximization, of maximizing some function U(R). However, the meth­
ods of this chapter can all be applied to an arbitrary differentiable utility
function. If we know apriori how to connect our performance measure
to more concrete variables ( like the position of a robot arm ) , then we
can avoid wasting time as our system relearns this information. The
examples of backpropagation through time cited in section 3.1 all take
Klopf ( 1 982) has placed great stress on the importance of these effects.
He defines the problem of drive reinforcement learning as the problem
where the Vi are treated as fixed weights . This is more plausible biolog­
ically than the situation shown in figure 3.1, because it allows for the
fact that primary reinforcement comes from a variety of ce lls , not just
It is very straightforward to extend HDP to the case of drive reinforce­
ment learning. We need to define our critic as a network with multiple
output s, Ji+, one for each component of the vector V. Conceptually, we
but we never really need to calculate J. We can still use supervised
learning, as in section 3.2, but we now need targets for each output Ji.
with allowance for UiO if convergence is a problem, as discussed in Section
3.2. (Alternatively, we could weight the sum in equation 3.8 by Vi, and
in e quation 3.9.) To adapt the action network, we can
backpropagate through the critic just as easily as we could before; we
begin by noting that the derivative of J with respect to each Ji is simply
1, and proceed backwards in the usual way.
This does not tell us how to exploit our knowledge of U(R) if U is a
nonlinear function or network. To do that, we must go beyond HDP to
In examining figure 3.4, Barto has asked whether we really need the critic
network at all here. All we are really getting from the critic network are
the derivatives , the dotted lines feed ing back from the critic to the model
of the plant (F). If we could estimate the derivatives of J* with respect
to R(t + 1) directly then we would not need the critic at all. This is
Stric tly speaking, DHP does not eliminate the critic. It replaces the
critic with a new network, whose output is an estimate of the derivatives
of J* with respect to R. We can think of this new network as an alter­
native type of critic. The derivatives of J* with respect to R are simply
the vector �", defined iG�flc:hlllatwifJlnay say that DHP adapts
a A-type critic, rather than a J-type critic. When we adapt the action
network, we use this new critic to calculate �, and then propagate these
derivatives back through the model of the plant and the action network
just as we would if � had come out of backpropagation.
How can we adapt a A-type critic? First let us consider the theory
behind DHP. DHP is based on differentiating equation 3.5 on both sides
(and suppressing the expectation operator and maximization, as in Sec­
Note that the unpleasant Uo term has dropped out. Recalling our defi­
nition of �*, and defining the rightmost term as Vi(R), we get:
Using DHP, we no longer need to assume the availability of U(R); in­
stead, we assume the availability of the reinforcement vector V, which­
unlike the V of the previous section-truly varies over time, as a function
of the situation. The summation in the middle of the equation is simply
an application of the chain rule, a calculation of derivatives; to calculate
such derivatives efficiently in practice, we can simply use backpropaga­
All of this theory leads to the following procedure. With DHP, as in
HDP, we adapt a critic network. Our critic now has multiple outputs,
Ai. The input to the critic is R(t), exactly as in HDP. We can use any
supervised learning method to adapt this network, to make the outputs
match the targets. We have to iterate through many passes, as in HDP,
even if we use a one-pass supervised learning method in each pass.
The only difference with HDP is in where we get the targets. Instead
of using equation 3.2, we now try to use the right-hand side of equation
3. 1 1 . In order to use the right-hand side of equation 3.11, we have to
use backpropagation as a way of calculating the summation term in that
equation. In other words, DHP forces us to use backpropagation as a
way to obtain the targets, not as a way to adapt the network to match
the targets. (Backpropagation can be used in both places, of course, if
More precisely, in each pass n , we assume that the weights 1Q(n-l)
are available (as in HDP). Our first step (as in HDP) is to calculate the
1 . Inserting R{t + 1 ) as the input into the A-critic, using the old
weights :!Q(n - l) in that network. The output will be �(t 1 ) .
2. Backpropagating these derivatives {Le., the components o f �(t +
1 » through the model of the plant (F) , all the way back to the
R;. (t) variables input to that model. This yields an estimate of the
derivatives of J* with respect to each of the R;. {t).
3. Set the target for Ai{t) to be Vi {R{t» plus this new estimate of the
derivative of J* with respect to R;. {t).
The next step is to adapt the weights so as to make the outputs closer
Just like the BAC design of figure 3.4, OHP takes fuR advantage of the
cause-and-effect information embedded in the model of the plant (E.).
It focuses the computational effort on estimating the slope of J* , which
is usually more important to making good decisions than is the absolute
As in section 3.4, there are numerous possibilities for blends between
the two basic methods. GOHP is one such blend.
HOP has the advantage of coherence. Because there is only one J
function, there is one consistent evaluation of how well one is doing.
OHP, however, is not guaranteed to be internally consistent; the deriva­
tive of Ai {R) with respect to Rj ought to equal the derivative of Aj {R)
with respect to R;., but it may not come out that way in our approx­
imation. Ideally, one would want to know the absolute level of J-for
use in making big decisions-while also learning about the slope in fine
GOHP makes this possible. In GOHP, we adapt a J-type critic, as
in HOP, but we try to minimize the error in equation 3.10, as in OHP.
(As in HOP, we treat the weights on the right-hand side as constants, to
avoid the problems discussed in Werbos ( 1 990a).) In fact, we could even
expand this error measure by adding it to the error function implicit in
Unfortunately, the only way I know to minimize such an error measure
is to use backpropagation to adapt the critic network. In using back­
propagation, we need to calculate the derivatives of error with respect to
the weights in the critic network; however, the error measure itself con­
tains derivatives, so that we need to calculate second derivatives. The
details of this are given �/iJI1dHtitJW!Wterial
Better methods may be found to achieve the same objectives, but
I am not aware of any such at present. The best I can imagine at
present is the adaptation of further networks to approximate some of
3.6. 1 The Problem of "Vision" (Effective Long-Term
Even in Howard's dynamic programming, the current estimate of J*
in any iteration will tend to represent the future H periods into the
future ( Le. , it will really tell you how to maximize the sum of U(t)
through U(t + H ) , not U(t) through U (oo)). On each full application
of the Bellman equation, H grows to H + 1 . But if the cycle time for
calculation is, say, a fifth of a second, and the unit time period is a fifth
of a second as well, it would take years to build up a time horizon of
years at best; realistic inefficiencies, lind a lack of a complete update in
neural networks, could lead to a Hme horizon of only a few days after
If adaptive critics were adapted by backpropagation, one could replace
steepest descent by an accelerator method which could work far faster­
at the risk of instability ( a risk which may have its biological counter­
part ) . Also, most accelerator methods do not allow for this kind of
"moving target" problem, where parameter changes themselves change
Werbos ( 1987b, 1990a) mentions a few ideas on how to cope with this
problem, but it is a wide-open area. It is also a difficult area, and less
than essential to near-term engineering applications. Still, it may be
3.6.2 The Problem of High-Speed Motor Coordination
GDHP takes a long time to go through a cycle of calculations, because
there are so many calculations to go through. Werbos ( 1987b ) compared
it in detail with literature on the human cerebral cortex and limbic
system, which are also relatively slow and relatively coherent. The brain
needs a faster system to smooth out actions-the cerebellum.
How can we link a primary neurocontrol system, based on some­
integrated, less statistically efficient (i.e. , learns slowly) , but much faster
in operation (smaller cycle time)? How could we build an artificial cere­
bellum? Perhaps we might use something like Ai (t + 1 )
estimated by the primary system-to be used as Vi in a subordinate,
fast HDP system without a model. The multivariate richness of the
�(t + 1) �(t) vector would partly overcome the usual slowness of HDP.
Or perhaps a very simple associative model could be used with DHP,
or other inputs could be used in HDP with a goal of simply smoothing
motion. The biological studies discussed by Kawato in chapter 9 sug­
gest very strongly that the human cerebellum does indeed minimize a
cost measure (torque change) over time. In general, the interface be­
tween multiple sets of neurocontrollers-including humans as well-will
be important to many practical applications.
This is almost certainly the most critical research area for now. All the
trade-offs discussed above seem fairly clear from the mathematics, but
concrete tests are needed, across a wide spectrum of problems, to clarify
and communicate the nature of these tradeoffs in practice. Likewise, a
creative approach to "making things work" -by diagnostic analysis and
modification as necessary-is vital to solving realistic problems using any
of the methods in this book. Few things in this field are likely to work out
perfectly the very first time they are tried, when they are implemented
in the most trivial way; in chapter 7 Shanno cites examples of the same
phenomenon in numerical analysis and function minimization, which
present very similar challenges. Diagnostic analysis--drawing on a wide
range of disciplines as well as immersion in the behavior of concrete
examples-will be essential, as will mathematical analysis of simplified
problems which abstract the essence of more realistic ones.
Many engineers would find it difficult to understand the ideas above
without seeing a few equations. Unfortunately, the equations will look
very different, depending on the type of network to be used (neural net
versus econometric model versus fuzzy logic net versus fluid dynamics
code versus . . . ) the type of learning schedule, the type of computer (se­
quential versus parallel versus dedicated) , etc. This section will describe
the key details, as they might look in a sequential computer, applied
to real-time learning, uQOI!I�lelVatpproach to handling noise
and basic backpropagation with steepest ascent to handle all supervised
learning problems. This is not the best way to go, but it should at least
The flowcharts in this chapter exploit a modular way of thinking about
systems design. To preserve this modularity, I will present the key cal­
culations in terms of subroutines which are then linked together to form
a system. The result will look a lot like real code, but there has been no
effort to optimize its efficiency or carry out numerical testing/debugging.
A functional network may be defined as a subroutine NET(X; !Y.; ;f; Y ) ,
which inputs arrays X and !Y. and outputs arrays ;f and Y , and performs
where m , n and N are constants built into the subroutine, and the Ii
are twice-differentiable functions also built into the subroutine. Y is
the "real" output of the network, but the entire array ;f is sometimes
Artificial neural networks are a special case of functional networks,
where In+2j + 1 for all j calculates a weighted sum of the outputs of earlier
neurons, and In+2j+2 calculates 1 / (1 + exp( -Xn+2j + t } )-the output of
the (j + 1 )st neuron. It is possible to handle networks where Ii is allowed
to use Xi+! , x H 2 etc., as arguments, in addition to the arguments shown
here; however, this requires special methods (Werbos 1988b) .
For any functional network, we can construct a dual subroutine,
F_NET(F1; ;f; F.-!Q; F-.Xi F�) . The inputs to this subroutine are
the arrays F1 and ;f. The main outputs are F.-!Q and F....J{ . (F�
is usually just scratch space.) The dual has the following key property:
if F1 represents the gradient of some quantity L with respect to Y,
and Y is the output of the subroutine NET, then F.-!Q and F-.X will
be the gradients of L with respect to the weights !Y. and the inputs X ,
respectively. (Werbos ( 1 989a) gives a more rigorous definition of what
The dual can be coded as foUoW� based .ofl backpropagation:
Finally, for GDHP, we need to pay special attention to networks NET
which have only a single output. For the critic network, we need to
construct a doubly dual subroutine, G_F_NET(.f.j F�; WeightXi G w),
whose first three arguments are inputs and whose last argument is an
output. If the output of the subroutine NET is a single scalar, J, then
To calculate these derivatives efficiently, we may use the tricks in Werbos
(1988a) , which lead to the following equations:
For relatively sparse networks (and neural networks especially), these
second derivatives tend to be very sparse as well, and the summations
turn out to be relativel�gt(tedMWerlals 1988a) .
To implement the BAC method of figure 3.4, we require three subrou­
tines or networks, whose arguments are ordered by the conventions just
MODELC R (t) , y.(t) , noisej �" j �" j R (t + 1) )
Our goal is to adapt � and �' j we assume that �" is already known.
Note that the input array to MODEL consists of the concatenation of
three vectors. As in a real computer program, the order of arguments­
not their names-is what controls the process.
When we use the simulation approach to handling noise, and we use
real-time learning, we can assume that we start from a vector R describ­
ing reality. We next simulate a noise vector noise, and must then adjust
the weights. To adjust the action network according to figure 3.4, we
CALL F_CRITIC( l j �j scratchj F_R2j scratch)
CALL F_MODEL(F_R2j �" j scratchj scratch, F_u, scratchj scratch)
CALL F_ACTION(F_uj �' ; F_w' ; scratchj scratch)
where "scratch" refers to scratch space ( Le. , unused outputs ) .
To implement DHP under the same conditions, we change our CRITIC
To adapt this critic network, we calculate:
CALL F_MODEL(lambda2j �" j scratchj F_R, F_u, scratchj scratch)
CALL CRITIC(Rj �j �j lambda) error lambda2 + Y(R2) - lambda
CALL F_CRITIC(errorj �j F_wj scratchj scratch)
w �+ learning_rate*FQiJpyrighted Material
where the function V is as defined in Section 3.4. To adapt the action
network, along the lines of Figure 3.4, we go on to calculate:
CALL F_ACTION(F_u; �' ; F_w'; scratch; scratch)
Section 3.5 proposed that we minimize an error measure based on equa­
tion 3.10. For the sake of generality, let us assume that we minimize
a weighted sum of square error, where the error for each value of i is
weighted by an arbitrary constant Ai ' ( Those who are disturbed by this
may simply use Ai 1, as we did implicitly with DHP. ) Our calculations
CALL MODEL(lf. .u., noise; Yl." ; �" ; R2)
CALL F_CRITIC (1; �; scratch; F_R2; scratch)
CALL F_CRITIC( I ; �; scratch; F_R; save)
delta(i) = error(i) * ;A(i) ( for all i )
Adapting the action network is then straightforward. Note how the
error vector here is equivalent in meaning to the error vector we used
in DHP; the two methods are minimizing the same measure of square
error, but in GDHP the lambda vector ( i.e. , F_R) has to be computed
Barto, A. G . , Sutton, R. S. and Anderson, C. W. (1983) . Neuronllke
elements that can solve difficult learning control problems. IEEE
Transactions on Systems, Man and Cybernetics 13(5) :835-846.
Bryson, A. E. , Jr. and Ho, Y. C . (1969) .
Gale, D. ( 1 979) . Theory of Linear Economic Models. Chicago, IL: Uni­
Guez, A. ( 1987) . Neurocontrollers. Tutorial C- l presented at the Third
International IEEE Symposium on Intelligent Control, Virginia.
Howard, R. ( 1 960) . Dynamic programming and Markhov processes.
Klopf, A. H. ( 1 982) . The hedonistic neuron: A theory of memory, learn­
ing and intelligence. Washington, D.C.: Hemisphere.
Lukes, G. , Thompson, B . , and Werbos, P. ( 1 990) . Expectation driven
learning with an associative memory. In Proceedings of the Interna­
tional Joint Conference on Neural Networks. Hillsdale, New Jersey:
Nauta, W., Jr. and Feirtag, M. ( 1986). Fundamental neuroanatomy.
Raiffa, H. ( 1968). Decision analysis: Introductory lectures on making
choices under uncertainty. Raeding MA: Addison-Wesley.
Sutton, R.S . , (1984) . Temporal aspects of credit assignment in reinforce­
ment learning. Ph.D. diss . , University of Massachusetts, Amherst.
VonNeumann, J. and Morgenstern, O. ( 1953) . Theory of games and
economic behavior. Princeton, NJ: Princeton University Press.
Werbos, P. J. ( 1977) . Advanced forecasting methods for global crisis
warning and models of intelligence. General systems yearbook, Ap­
Werbos, P. J. ( 1 979) . Changes in global policy analysis procedures sug­
gested by new methods of optimization. Policy Analysis and Infor­
Werbos, P . J. ( 1 986) . Generalized information requirements o f intelli­
gent decision-making systems. In SUGI-1 1 Proceedings. SAS Insti­
tute, Cary, NC. ( The revised version, available from the author, is
easier to read and discusses more issues in psychology. )
Werbos, P. J . , ( 1987a) . Learning how the world works. In Proceedings
of the 1 987 IEEE International Conference on Systems, Man and
Cybernetics, New York: IEEE Press, 1:302-310.
Werbos, P. J. (1987b) . Building and understanding adaptive systems: A
statistical/numerical approach to factory automation and brain re­
search. IEEE Transactions on Systems, Man and Cybernetics 17:719.
Werbos, P. J. ( 1988a) . Backpropagation: past and future. In Pro­
ceedings of the IEEE International Conference on Neural Networks.
New York: IEEE Press, I: 343-353. (The transcript of the talk­
which is something of a broad, basic introduction-is available from
Werbos, P. J. ( 1 988b). Generalization of backpropagation with ap­
plication to a recurrent gas market modeL
Werbos, P. J. (1989a) . Maximizing long-term gas industry profits in two
minutes in Lotus using neural network methods. IEEE Transactions
on Systems, Man and Cybernetics 19(2) :315-333.
Werbos, P. J. (1989b) . Backpropagation and neurocontrol: A review
and prospectus. In Proceedings of the IEEE International Joint
Conference on Neural Networks. New York: IEEE Press, 1:209-216.
Werbos, P. J., (1990a) . The consistency of HDP applied to a simple rein­
forcement learning problem. To appear in Neural Networks, March,
Werbos, P. J., (1990b). Backpropagation through time: What it is and
how to do it. To appear in Proceedings of the IEEE, August, 1990.
Williams, R. J. ( 1 988) . On the use of backpropagation in associative
reinforcement learning. In Proceedings of the IEEE International
Conference on Neural Networks, San Diego, CA.
Wonnacott, T. H. and Wonnacott, R. J . ( 1 977) . Introductory statistics
for business and economics. 2nd ed. New York: Wiley.
Zhu, C . , and Skalak, R. (1988) . A continuum model of protrusion of
pseudopod in leukocytes. Biophysics Journal 54( December) : 1 1 1 51 1 47.
The purpose of this chapter is to provide an introductory overview of
some of the current research efforts directed toward adapting the weights
in connectionist networks having feedback connections. While much of
the recent emphasis in the field has been on multilayer networks hav­
ing no such feedback connections, it is likely that the use of recurrently
connected networks will be of particular importance for applications to
the control of dynamical systems. Following the approach taken in the
chapter by Barto, this chapter will emphasize the relationship of connec­
tionist research in this area to strategies used in more conventional en­
gineering circles for modelling and controlling dynamical systems, while
at the same time noting what there is in the connectionist approach
that is novel. In particular, I will argue that while much of the connec­
tionist approach to adapting the weights in recurrent networks having
interesting dynamics rests on the same sort of mathematical strategies
used commonly within the engineering sciences, there are noteworthy
differences in the overall emphasis, which represents an important con­
tribution which connectionist research has to offer. I agree completely
with the idea Barto has presented that the connectionist approach may
serve as a bridge between the two seemingly disparate disciplines of ar­
tificial intelligence (AI) on the one hand and the engineering areas of
estimation and control on the other. The use of parameter estimation
techniques familiar to engineers on problems often treated within AI
via discrete symbolic computational approaches represents a refreshing
synthesis of two research directions which deserve unification. At the
same time, I also agree with his point that there is a important need
for much additional mathematical analysis to bolster our understand­
ing, currently obtained primarily through extensive simulation, of the
interesting systems which connectionist researchers study.
It is common in connectionist research circles to distinguish between net­
works having feedback connections, which are usually called recurrent
networks, and network�AlMst8Rl:t1ack connections, which are
/eed/orward or layered networks. Because of the existence
gradients in feedforward networks, a great deal of attention has been fo­
cused recently on this particular type of network, especially in situations
where learning from examples of input-output pairs is required.
A particular type of recurrent network, a
been widely recognized as important in connectionist circles
1977, Hinton 1977, Grossberg 1980, Feldman and Ballard 1982,
Hopfield 1984). Such a network converges to a stable state from any
In the terminology of dynamical systems theory, such
networks have only point attractors. The final state of such a network
the solution to a certain constraint-satisfaction-type of
relaxation labeling ( Rosenfeld, Hummel, and Zucker 1976),
or it might be viewed as a retrieved item from a content-addressable
essentially the same backpropagation computation may be used to com­
pute error gradients for settling networks
this can be derived as a special case of a general approach to computing
error gradients for a recurrent network having arbitrary
Because convergence to a point attractor represents a very limited
form of dynamical behavior, a growing number of connectionist re­
searchers have begun to explore techniques for training recurrent net­
works to manifest a much wider range of dynamical behaviors than sim­
outline some arguments why studying such net­
Unfortunately, current research efforts have only
made limited progress in identifying situations where the necessary er­
ror gradient computations have all the properties one might wish them
to have for this more general class of networks.
connectionist research in this area thus consists of either attempts to
apply the currently available algorithms in spite of their limitations or
attempts to discover better algorithms and/or network architectures.
Gradient-Based Learning Algorithms for Recurrent
Here I will briefly outline several algorithms which have been proposed
and studied for training networks having a broad range of dynamical
behaviors. Each of these is based on the same strategy used in the back­
Adaptive State Representation Using Recurrent Networks
1985; Ie Cun 1985; Rumelhart, Hinton, and Williams 1986), that of com­
puting the gradient in weight space of an error measure based on the
mismatch between the actual performance and the desired performance.
The type of task for which these algorithms are appropriate is what
might be called a sequential supervised learning task, which means that
the output values of certain units in the network are to match specified
target values at specified times.l A more extensive description of these
and related algorithms, along with a detailed analysis of their computa­
tional requirements, may be found in (Williams and Zipser, To appear).
Two completely general approaches to training an arbitrary recur­
rent network by adjusting weights along the error gradient are: (1 )
the backpropagation-through-time algorithm (Werbos 1974; Rumelhart,
Hinton, and Williams 1986; Robinson and Fallside 1987), extended to
continuous time by Pearlmutter (1989); and (2) the real-time recurrent
learning algorithm (Robinson and Fallside 1987, Bachrach 1988, Mozer
The backpropagation-through-time algorithm can be derived from the
more familiar backpropagation algorithm for multilayer networks by un­
folding an arbitrary recurrent network into a multilayer feedforward net­
work that grows by one layer on each time step. It is not hard to show
that the partial derivative of error with respect to any weight in the
recurrent network is equal to the sum of the partial derivatives of the
error with respect to each of the weights in the unfolded network which
correspond to this weight in the recurrent network. Computation of
these error derivatives in the unfolded net is carried out using the usual
backpropagation algorithm. The algorithm derives its name from the
fact that the computation of these error derivatives is based on infor­
mation propagating from later times to earlier times in the recurrent
network. The actual implementation of the algorithm of course requires
the storage of the history of activity of the network as it runs.
The real-time recurrent learning algorithm, on the other hand, keeps
a running tally of the partial derivatives of the outputs of all units with
respect to all the adaptable weights in the network. Whenever there
is an error in network operation, the appropriate error derivatives are
then computed directly from this information. This algorithm derives
its name from the fact that the weight updates can be computed (and
1 For ease of exposition, we consistently assume a discrete-time model throughout
the discussion here; many o( the techniques described also carry over to continuous
Each of these algorithms is capable of computing the error gradient
in weight space, but they each have certain drawbacks which limit their
usefulness. The backpropagation-through-time algorithm can be viewed
as having an unbounded memory requirement for situations when the
network is assumed to run through time indefinitely; alternatively it can
be applied in situations where the network is assumed to run through a
fixed time interval in each trial, with a reset to some given start state at
the outset of a trial. The real-time recurrent learning algorithm is par­
ticularly useful in situations where the network runs through time indef­
initely and is never reset to a start state, because it has a fixed memory
requirement of nr real numbers, where the network has n units and r
adaptable weights. For short fixed time intervals the backpropagation­
through-time algorithm uses less memory, but for continual operation
the real-time recurrent learning algorithm obviously requires less mem­
ory. Both of these algorithms suffer from the disadvantage that their
computations require communication and /or storage far beyond that re­
quired for the simple nonadaptable network operation itself. While the
backpropagation-through-time algorithm ostensibly sends signals (back­
wards) along only the connections which are present in the network, close
examination of the details of the algorithm shows that to implement it
within the network being trained would require the simultaneous trans­
mission of tagged messages along every connection and the storage and
update of multiple values at every unit. The problem with the real-time
recurrent learning algorithm is that it requires the storage and update of
information corresponding to every unit/weight pair in the network, and
such information involves nonlocal co�munication within the network
In spite of these limitations, these two algorithms continue to be of
interest for various reasons. One reason is that they can be used as
off-line techniques for creating networks having desired sequential be­
haviors, and various simulation studies ( e.g., Rumelhart, Hinton, and
Williams 1986; Williams and Zipser 1989b) have demonstrated the abil­
ity of these algorithms to create networks capable of performing interest­
ing sequential processing. Another reason is that they can be specialized
to situations where their disadvantages disappear. Still another reason
is that they can serve as the basis for more computationally attractive
algorithms which may represent useful approximations to the full error
As an example of the usefulness of the backpropagation-through-time
algorithm in a specializea settin,.L.l�p'!����ing to note that it reduces
to a much simpler form w Rel'lKl!WcH-sTbffl'Ulhual and desired dynamics
Adaptive State Representation Using Recurrent Networks
consist of settling to a fixed equilibrium state. This is essentially because
storage of the activity history is not required because all past states
along a constant trajectory are equal to the current one. The resulting
algorithm (Almeida 1987, Pineda 1988) involves an indefinitely iterated
backpropagation computation through the network. This computation
itself converges stably whenever the forward computation does.
Approximations to the full backpropagation-through-time algorithm
have also been studied. A natural way to simplify the computation is to
truncate the backward propagation of information to a fixed number of
prior time steps. This is, in general, only a heuristic technique, although
in those situations where the actual backpropagation computation leads
to exponential decay in strength through (backward) time, this can give
a reasonable approximation to the true error gradient. A particular
case which has been investigated by several researchers (Elman 1988;
Cleeremans, Servan-Schreiber, and McClelland 1989; Rumelhart, per­
sonal communication, 1988) is to truncate the computation at one prior
time step. Such an algorithm thus involves the saving of state informa­
tion only for one time step and is very simple to implement. The partic­
ular architecture used by Elman consists of a layered network having an
input layer, a hidden layer, and an output layer, with the hidden layer
having lateral feedback connections within it. Only the lateral feedback
connections in the hidden layer are assumed to include a delay. In such
an architecture, the prior state need be saved only for the hidden layer.
By not performing the backpropagation beyond one prior time step, the
algorithm essentially treats as given with each training instance not only
the input to the network but the prior state of the network as well. It is
not hard to discover tasks which such networks are incapable of learning
(Cleeremans, Servan-Schreiber, and McClelland 1989) but which can be
learned by using the full gradient co�putation (Smith and Zipser 1989).
The real-time recurrent learning algorithm can also be restricted to
certain specialized settings where its disadvantages disappear. In par­
ticular, Bachrach (1988) and Mozer (1988) have noted that for a single
unit it reduces to an entirely local computation involving the update
and storage of one additional number per weight. Mozer has applied
this idea to develop an entirely local learning algorithm for certain re­
stricted architectures in which the only recurrent connections allowed
are self-recurrent connections on the initial layer of hidden units in what
is otherwise a multilayer feedforward network. As in the Elman archi­
tecture, only these recurrent connections incorporate a delay. For such
networks an appropriate comJ:>ination of ba. ckpropagation
multiple layers and reaF-�nMettmfl(ing within the recurrently
connected units can be used to perform the exact computation of the
Investigation of another potentially useful approximation to the real­
time recurrent learning algorithm has been undertaken by Zipser (To
appear). In this version, the network learning the sequential task is
viewed as consisting of several communicating subnetworks. Each of
the subnetworks is trained using the full real-time recurrent learning
algorithm, but communication between the subnetworks during training
is simplified from what the full gradient computation would require.
The resulting algorithm has lower space and time complexity than the
full real-time recurrent learning algorithm, but preliminary simulation
results suggest that it can still be used to train networks to perform
Another learning algorithm designed for a certain specialized class of
architectures is that proposed and studied by Jordan (1986). In this
architecture, there is an output layer, a hidden layer, an input layer,
and a group of units called the state units. There are connections from
the input layer to the hidden layer and from the hidden layer to the
output layer, and all of these are adaptable, just as in a typical feedfor­
ward network to which backpropagation is applied. In addition, there
are adaptable connections from the state units to the hidden units, so
the state units may be viewed as augmenting the input representation.
Depending On the application, there are also fixed connections from the
output units to the state units and/or fixed connections among the state
units, and these connections have a delay of One time step. The purpose
of these fixed connections is to capture (in a handcrafted manner) the
information necessary to distinguish states of the network. For example,
in a task where the network is to cycle through a certain sequence of
output patterns, the fixed connections between the state units may rep­
resent a clock which cycles at the desired frequency; in a task where the
correct output is a function of previous outputs, the connections from the
output units to the state units may represent a one-for-one copy opera­
tion, as in the net studied by Elman (1988). Under the assumption that
training information is available for the output layer at each time step,
the algorithm used for training this network is just the usual multilayer
backpropagation algorithm. Just as with the Elman network, backprop­
agation is applied to the network representing the mapping from current
input and prior state to output. In situations where training information
is not ostensibly available on e�h time step! Jordan imposes smoothness
constraints which attemffe8Yml{J�iK@"erence between outputs at
Adaptive State Representation Using Recurrent Networks
neighboring time steps, and this essentially gives desired output values
Another technique which has been singled out in the connectionist lit­
erature as being of value in training recurrent networks is that of teacher
forcing. This is an intuitively sensible strategy which can be used with
any recurrent network learning algorithm. The idea is to replace, dur­
ing the training phase, the actual output of any unit in the network
with the desired output of that unit ( if specified) before computing the
subsequent state of the network. A number of the algorithms described
above either use it routinely or can be easily modified to include this
strategy. For example, Jordan's (1986) algorithm uses it, and Pineda
(1988) has called attention to its importance for creating new stable
points in settling networks. Williams and Zipser (1989a, 1989b) who
coined this name for it, have demonstrated that this technique is gener­
ally required whenever one wishes to alter the qualitative dynamics of a
network which is operating in its steady state. For example, they have
used it in conjunction with the real-time recurrent learning algorithm to
turn settling networks into oscillating networks.
However, it should also be noted that there are situations for which
teacher forcing is clearly of no use or may be otherwise inappropriate.
One obvious case where it is of no use is when the units to be trained do
not feed their output back to the network, as in the networks used by
Elman (1988). Furthermore, a gradient algorithm using teacher forcing
is actually optimizing a different error measure than its unforced coun­
terpart, although any setting of weights giving zero errol' for one also
gives zero error for the other. This means that, unless zero error is ob­
tained, the two versions of a gradient algorithm need not give rise to the
same solutions. In fact, it is easy to devise examples where the network
is incapable of matching the desired trajectory and the result obtained
using teacher forcing is far different from a minimum-error solution for
One important aspect of the use of teacher forcing is that it can help
simplify the computational requirements of the gradient calculation. An
extreme case of this occurs when every unit in the network has a target
on all time steps, in which case training the weights amounts to a simple
one-layer mapping problem. Rohwer (1989) has used this observation to
suggest an interesting algorithm for training recurrent networks. The es­
sential idea is to treat the output of those units not having an externally
specified target as variables, along with the network weights. The error
measure used takes into;���ancies between the actual
and "desired" values for all units, including those for which no external
Relationship to Standard Engineering Approaches
In the next section I will discuss the potential relevance of adaptive re­
current networks to control applications, including some novel aspects
they have to offer. Here I want to note briefly how the techniques de­
scribed above, and our understanding of them, might be viewed from
First of all , since the basis of the algorithms described here is the
commonly used method of estimating parameters by moving along the
gradient of an error measure, it should not be surprising that at least
some of these techniques are rediscoveries or simple nonlinear exten­
sions of techniques already described in the engineering literature. In
particular, the real-time recurrent learning algorithm coincides with an
approach suggested in the system identification literature (McBride and
Narendra 1965) for tuning the parameters of an arbitrary dynamical
system. Also, the teacher forcing idea appears in the adaptive signal
processing literature (Widrow and Steams 1985, 250-253) as an "equa­
tion error" technique for synthesizing recursive linear filters.
Another observation is that we have far less analytic understanding
of the behavior of recurrent networks themselves or these learning al­
gorithms than would be considered desirable from the point of view of
engineering theory. For example, except in the case of associative mem­
ory networks, where convergence to a stable equilibrium is required for
proper operation, questions of stability or convergence of network ac­
tivity (ignoring the effects of learning) are not usually addressed. This
is partly because an analysis of the dynamics of such highly nonlinear
systems is extremely complex, and partly because the use of units hav­
ing bounded outputs guarantees trivially that bounded inputs lead to
bounded outputs, which is a major concern for the linear systems often
treated in conventional control theory. Of course, this in no way simpli­
fies the important question of stability of a combined plant/controller
system when a connectionist network is used as a controller, but it does
insure that, for example, whenever a recurrent network is to be used as
a plant model (over a bounded range), this model itself cannot blow up.
However, these remarks should definitely not be taken to suggest that
connectionist research has nothing new to offer. Below I will offer sup­
port for the contention 16��approach, while borrowing
Adaptive State Representation Using Recurrent Networks
techniques from estimation and control theory, is actually intended to
address a much broader class of adaptation problems, including many
treated using an entirely different set of techniques.
In this section I want to describe three different ways that one might
attempt to use adaptive connectionist networks in control-related appli­
cations. Some of these ways necessarily involve recurrent networks of
one form or another, but some need not. I will call these three ways the
conservative approach, the liberal approach, and the radical approach.
In the chapter 1, Barto described several strategies for applying su­
pervised training of feedforward networks to the problem of generating
control signals for a dynamical system. Each of the three approaches
I introduce here is potentially applicable to any of those strategies, in­
cluding copying an existing contr.oller, developing an adaptive predictor,
identifying the system or a.n inverse of it, and differentiating a model.
For simplicity, the description here will treat only the system identi­
fication case, in which the network is to learn to model a plant from
observation of its input-output behavior, but many of the points being
made here apply to the other strategies as well.
Before considering the three approaches, recall the important distinc­
tion between a memoryless system and one possessing internal state or
memory. We will use the term static mapping to describe such a mem­
oryless system. Ignoring the effect of weight adaptation, a feedforward
network is usually viewed as implementing such a static mapping. Even
a settling network used as a content-addressable associative memory is
often treated as a means of performing a static mapping from initial
state to final state. 2 While one is a feedforward network and the other
is a recurrent network, both are used to perform static mappings. 3 It
is clear that the only way to model a system in which internal state
plays an essential role in its input-output behavior is for the model itself
to allow for the possibility of nontrivial internal state. The distinction
21n this case it must be assumed that network settling occurs on a very fast time
scale in relation to the external time scale of input-output events.
3 An interesting distinction between the resulting mappings, not important for
our purposes here, is that the settling network generally implements a discontinuous
function even when the come,.utation l'erformed by the individual units
among the three approaches to be described is simply their means of
straightforward way to capture certain kinds of state information is
If the plant's state is a function only of
k plant inputs, then using a sufficiently long delay line on these
inputs obviously allows the model to capture the same state information.
The concepts of a moving average model, a transversal filter, or a finite­
impulse-response filter are all equivalent embodiments of this idea in
engineering circles. Such a model or filter is constructed out of a static
linear mapping whose input comes from one tapped delay line for each
straightforward generalization of this idea is to replace the static
linear mapping by a static mapping which is allowed to be nonlinear,
such as that implemented by a feedforward connectionist network or that
implemented by a settling network. Let us call such a network, consist­
ing of any static mapping network together with tapped delay lines on
transversal filter network. Use of such a network, particularly
where the static mapping is implemented by a feedforward network, is
widespread in connectionist attempts to deal with time-varying input.
The use of adaptive transversal filter networks for such purposes will be
conservative approach. Such an approach is obviously appeal­
ing because adjusting the model involves nothing more than training a
Similarly, there are situations when delay lines on the plant outputs,
perhaps combined with delay lines on plant inputs, provide sufficient
moving average model, a recursive filter, or an infinite-impulse-response
filter are all equivalent embodiments of this idea in engineering circles.
Such a model or filter is constructed out of a static linear mapping whose
input comes from one tapped delay line for each plant input as well as
one tapped delay line for each plant output.
Once again, it is straightforward to generalize this idea by replacing
the static linear mapping by a static mapping which is allowed to be
recursive filter network to the architec­
ture consisting of a static mapping network together with tapped delay
lines on input and with (6�flt4pq,Mlltlfi4M.put through tapped delay
Adapt.ive State Representation Using Recurrent Networks
lines. While this scheme is less commonly used in connectionist circles,
certain forms of the network architecture studied by Jordan (1986) can
be viewed as having this form, where the static mapping is implemented
by a feedforward network. Furthermore, although the resulting network
has recurrent connections, from output back to input, in cases where
training information is available on every time step (as, for example,
in system identification ) , the use of teacher forcing essentially reduces
the problem of training the network to that of training a static net­
work. The use of adaptive transversal filter networks for control-related
applications will be called the liberal approach.
In connectionist parlance for describing feedforward networks faced with
supervised learning tasks, one often distinguishes the visible units, which
are those that either carry input signals or produce output signals, from
the hidden units, which perform intermediate computations. We build
on this terminology here and say that a system has weakly visible state
if it is input-output equivalent to a system whose state is a function of
only a fixed set of finitely many past values of its input and output.
Otherwise, we say that the system has strongly hidden state. Recursive
filter networks are thus capable of modelling systems having weakly vis­
ible state, but, by definition, are incapable of modelling systems with
strongly hidden state. Let us call the use of an adaptive recurrent net­
work which is not a transversal filter network or a recursive filter network
for control-related applications the radical approach. By definition, in
order to accurately model a system having strongly hidden state we must
A central thesis of this chapter is that the radical approach to using
adaptive connectionist computation for temporal processing is a highly
novel research thrust and thus offers the greatest potential for provid­
ing systems having significantly new capabilities. As will be discussed
below, it is designed to address a type of problem which does not even
come up in more standard control-theoretic formulations. Furthermore,
while other approaches to certain such problems have been developed,
most often within linguistics and computer science, the techniques used
are quite different, so it is not clear how to integrate such methods with
more standard control-��tMlMatttfial The radical connectionist
approach, on the other hand, treats all such problems within a unified
framework-that of fitting an unknown system with a member of a con­
It is not hard to show that any linear system having finite-dimensional
state cannot have strongly hidden state. This implies that the radical
approach is superfluous when attempting to model any such system.
One manifestation of this result is that any recurrent network consist­
ing entirely of linear units (and delays) is input-output equivalent to a
recursive linear filter. That is, any such network having "hidden" recur­
rent connections has identical input-output behavior to one in which all
recurrent connections are ''visible,'' from output back to input.
thermore, the entire network can be simplified to a single layer as well.)
However, it is easy to construct a simple example of a system having
strongly hidden state. Consider a system which is capable of recognizing
that a particular triggering event has occurred at any time after some
previous enabling event. Such a sy�tem can be designed using one flip­
The flip-flop records the occurrence of the
enabling event (thereafter emitting a 1) and the
simultaneous occurrence of the triggering event and the output 1 from
the flip-flop. To actually make this an interesting system identification
task for a learning algorithm, the occurrence of the triggering event
should also reset the flip-flop to 0 so that the process can be repeated.
It is clearly not possible to model this system exactly using a transver­
sal filter network because of the potentially arbitrarily long span of time
between the enabling event and the triggering event. It is also clear that
adding any finite (or even infinite) length of history of output of the sys­
tem does not help, so a recursive filter network also cannot do the job.
However, a simple two-unit network using units which have saturating
nonlinearities can easily be designed to model this system.
portant for our purposes here is that such a recurrent network can also
be trained to model such a system from examples of its input-output
behavior (Bachrach 1988, Williams and Zipser 1989b).
Furthermore, the same two-unit network can also be trained to model
systems whose state consists of any simple function of the immedi­
ately prior value of the output and one of the inputs.4
adaptive recurrent network can also discover delay line solutions when
they are appropriate. While the tapped delay lines in an adaptive re­
cursive filter network encode a predetermined state representation, a
more general adaptive recurrent network is free to develop its own state
that CQRJ«igh�Q.M�a/single-time-step delay as well.
Adaptive State Representation Using Recurrent Networks
r�presentations, as appropriate for the task. Thus the radical approach
might also be characterized as involving the
This example of a system having strongly hidden state can also be
used to illustrate another point concerning the
capable of adapting its representations versus a network using tapped
delay lines as its only means of state representation. Suppose that we
are willing to settle for an approximate model whose probability of er­
ror, although not zero, is tolerably small. We could use a transversal
filter network whose tapped delay lines store input to the system over
k time steps, where k is chosen so large that delays of
more time steps between the enabling event and the triggering event are
exceptionally rare. However, to learn such an approximate model can
be harder than learning an accurate model using the simple recurrent
network. This is because more taps on the delay line means more ad­
justable weights, which means more parameters with which to fit the
training data, which implies that more data is required to obtain a fit
giving useful generalization. Williams and Zipser (1989b) observe th�t
having more that four taps on the input delay line guarantees that the
simplest possible network of this type will necessarily have more weights
than the two-unit fully recurrent network which is capable of learning
Thus any tapped delay line approach to such a task is
almost certain to require much more training to achieve a satisfactory
level of performance than the simple recurrent network which can solve
it exactly. The point of this argument is that the radical approach may
not only be necessary for accurate modeling but it also may be more ef­
ficient in terms of its need for training data even when perfect accuracy
Of course, this example of a system having strongly hidden state is a
simple finite-state machine, having just two states. In fact, very recent
research into the use of adaptive recurrent networks has begun to focus
on just this class of problem: the inference of a deterministic or stochas­
tic finite-state machine from observation of its input-output behavior.
For example, Williams and Zipser (1989b), Smith and Zipser (1989),
Cleeremans, Servan-Schreiber, and McClelland (1989), and Rumelhart
(personal communication, 1988) have all experimented with the use of
adaptive recurrent networks on problems of this type. The study of
such problems is, of course, not new. Linguists and computer scientists
studying formal langua�!, �:Jh Gold 196?, Angluin and Simith 1983)
problems have also been discussed in the syntactic pattern recognition
literature (e.g., Gonzalez and Thomason 1978).
However, the usual approach to these problems generally involves at­
tempting to enumerate the states, which is obviously different from the
connectionist parameter estimation approach described here. Further­
more, not only does the radical connectionist approach offer an interest­
ing and promising alternative to existing techniques for inferring such
finite-state machines, it may represent a workable approach to dealing
with problems where the state structure of the system to be modelled
contains aspects of both the discrete-state systems treated especially
within theoretical computer science and the continuous-state systems
typically considered within systems theory.
imagine trying to discover a model for a system having a state space
which consists of several disconnected pieces, each of which is some sub­
region of a Euclidean space. This would correspond to a system which
operates in one of several discrete modes.
In this chapter I have tried to give a broad overview of some of the tech­
niques used for training recurrent connectionist networks and to discuss
their relationship to techniques used within engineering circles for re­
lated problems. I have also described three approaches to using adaptive
networks in control-related applications. Of these three approaches, the
conservative approach and the liberal approach are natural extensions
of the idea of using an adaptive transversal linear filter and an adap­
tive recursive linear filter, respectively.
other hand, has no natural counterpart in the realm of adaptive linear
filters but is a unique outgrowth of the connectionist use of parameter
estimation methods in conjuction with highly nonlinear computation.
The conservative approach does not require the use of recurrent net­
works and is already studied widely. It is likely that this approach will
continue to predominate over the near term because it represents the
most conservative blend of techniques already well established within
both the connectionist and engineering camps.
approach is not yet used to nearly as great an extent, I suspect that,
to the extent it also represents a straightforward combination of tech­
niques already in use, it will find growing use in certain control-related
Adaptive State Representation Using Recurrent Networks
While the conservative and liberal approaches to integrating adaptive
connectionist computation into control-related applications will both
likely prove to be valuable, I have particularly emphasized here the
promise of the radical approach, which necessarily requires recurrent
networks of a fairly general type. A major thesis of this chapter is the
suggestion that it is the radical approach which may lead to the most
interesting and novel results in the long term. How widespread a use
this approach may find in such applications will depend on the extent
to which the adaptive discovery of strongly hidden state turns out to
further progress is made in developing algorithms which can train more
efficiently the recurrent networks required to implement it.
This research was supported by Grant IRI-8703566 from the National
A learning rule for asynchronous perceptrons
the IEEE First International Conference on Neural Networks, vol.2"
with feedback in a combinatorial environment.
Anderson, J. A. (1977). Neural models with cognitive implications. In
J. Samuels, eds. Basic processes in reading per­
ception and comprehension. Hillsdale, NJ: Erlbaum.
Angluin, D., and Smith, C. H. (1983). Inductive inference: Theory and
Bachrach, J. (1988). Learning to represent state. Master's thesis. Uni­
Cleeremans, A., Servan-Schreiber, D., and McCleland, J.L. (1989).
Finite-state automata and simple recurrent networks.
Elman, J. 1. (1988). Finding structure in time CRL Technical Report
J. A. and Ballard, D. H. (1982). Connectionist models and
their properties. Cognitive Science 9:205-254.
Gold, E. M. (1967). Language identification in the limit. Information
Gonzalez, R. C. and Thomason, M. G. (1978). Syntactic pattern recog­
nition: An introduction. Reading, MA: Addison-Wesley.
Grossberg, S. (1980). How does the brain build a cognitive code? Psy­
University of Edinburgh, Edinburgh, Scotland.
Neurons with graded response have collective
computational properties like those of two-state neurons.
ings of the National Academy of Sciences, USA 81:3088-3092.
Attractor dynamics and parallelism in a con­
nual Conference of the Cognitive Science Society, Amherst, MA:
Une procedure d'apprentissage pour reseau a se­
quil assymetrique A learning procedure for asymmetric threshold
networks. Proceedings of Cognitiva 85:599-604.
IEEE Transactions on Automatic Control 10:289-
focused back-propagation algorithm for tempo­
ral pattern recognition. Technical Report, Departments of Psychol­
ogy and Computer Science, University of Toronto, Canada.
Adaptive State Representation Using Recurrent Networks
Learning-logic. Technical Report 47, MIT Center
for Computational Research in Economics and Management Science.
Learning state space trajectories in recurrent
neural networks, Neural Computation 1:263-269.
Pineda, F. J. ( 1988 ) . Dynamics and architecture for neural computation,
Cambridge University Engineering Department, Cambridge, Eng­
cal Report, Centre for Speech Technology Research, University of
Rosenfeld, A., Hummel, R. A. and Zucker, S. W.
by relaxation operations. IEEE Transactions on Systems, Man, and
E., Hinton, G. E., and Williams, R. J. (1986 ) . Learn­
ing internal representations by error propagation. In D. E. Rumel­
hart and J. L. McClelland, eds., Parallel distributed processing: Ex­
plorations in the microstructure of cognition, Vol. 1, Foundations.
Experience with the real-time recurrent learning algorithm. In Pro­
ceedings of the International Joint Conference on Neural Networks
Beyond regression: new tools for prediction and
analysis in the behavioral sciences. Ph.D. diss., Harvard University,
ually running fully recurrent neural networks.
algorithms for recurrent networks. In: Y. Chauvia and D. E. Rumel­
Back-propagation: Theory, architectures and applications.
Zipser, D. ( To appear ) . A subgrouping strategy that reduces complexity
and speeds up learning in recurrent networks. Neural Computation,
Institute for Cognitive Science, University of California San Diego,
Mathematical systems theory has made major advances in the past four
decades and has evolved into a scientific discipline which cuts across
boundaries, extending from design and development on the one hand
to mathematics on the other. The best developed part of the theory
concerns linear systems and important concepts as well as major the­
oretical results have been introduced in such areas as stability theory,
optimal control, multivariable theory and adaptive control. The pow­
erful techniques developed in the area of adaptive control complement
current computing technology and have enormous potential in the world
of applications, where systems have to be controlled in the presence of
uncertainty. Although adaptive systems are by their very nature non­
linear, most of the theories of such systems are deeply rooted in linear
systems theory (Narendra and Annaswamy 1989).
In recent years, the multilayer neural network and the recurrent net­
work have emerged as important components, which have proved to be
extremely successful in pattern recognition and optimization problems
(Hopfield 1982, Hopfield and Tank 1985, Burr 1988, Gorman and Se­
jnowski 1988, Sejnowski and Rosenberg 1987, Widrow, Winter and Bax­
ter 1988). From the system-theoretic point of view, these networks can
be considered as components which can be effectively used in complex
The main objective of this chapter is to explore how well-established
adaptive identification and control techniques can be applied to the anal­
ysis and synthesis of dynamical systems, which contain neural networks
as subsystems. It is the author's conviction that this new area will evolve
in the near future in the same general directions along which adaptive
control did in the past two decades. Further, many of the same theoret­
ical difficulties will also be encountered, though in more complex forms
due to the nature of the subsystems involved. In spite of the seeming
differences, the concepts developed in adaptive control theory will have
In this chapter, new models for the identification and control of com­
plex nonlinear dynamical systems with unknown parameters are intro­
duced. The assumptions that must be made at every stage to have well
posed problems, as well as the theoretical difficulties encountered in as­
suring stability while �mlf.Wl'J8pdentification and control,
are briefly discussed. F inally, areas where further work is needed are
Multilayer Networks and Recurrent Networks
Two classes of networks which have received considerable attention in
the area of artificial neural networks are ( i ) multilayer neural networks,
and ( ii) recurrent networks. A typical multilayer neural network with an
input layer, an output layer and two hidden layers is shown in figure 5.l.
For convenience, this can be denoted in block diagram form as shown
in figure 5.2 with three weight matrices U, V and W. The multilayer
network represents a nonlinear map j where
and the elements of U, V and W are adjustable weights. Such networks
have been used successfully in pattern recognition, where the weights are
adjusted to minimize a suitable error function. In the following discus­
sions the terms "parameters" and "weights" are used interchangeably.
In contrast to the above, the recurrent network, based on the work of
Hopfield (1982), has been used as a content-addressable memory and in
optimization problems. One version of the Hopfield network is shown
in figure 5.3 and consists of a single layer neural network in the forward
path connected to a delay in the feedback path. The choice of the
weights determines the equilibrium states of the dynamical systems and
hence the specific equilibrium to which the state trajectory converges
depends upon the initial conditions. This fact has been used for content­
addressable memories as well as optimization problems.
From a system-theoretic point of view, the multilayer network repre­
sents merely a versatile nonlinear map. The recurrent network on the
other hand, in most cases, represents a dynamical system with no in­
put. In system analysis, both types of operators play an important role.
Hence it is desirable to study the properties of feedback systems ( refer to
figs. 5.5 and 5.6) which contain both types of networks as components.
In ( Narendra and Parthasarathy 1988b) the backpropagation method
used for adjusting the weights in a multilayer neural network was also
suggested for adjusting the weights in a recurrent network to increase
Block diagram of a three layer neural network.
Our main interest is in the control of nonlinear dynamical systems and
systems While one of our primary objectives is to determine theoretical
all) stability of dynamical systems with mult i l ayer neural networks
controllability, observability, and (most important of
components, the enormous di ffi cult y of the endeavor is evident right from
Hence, we realize that parallel empi rical studies based on
computer simulations should form a vital part of any investigation in this
general area. In the following sections both aspects of the problems are
discussed. Empirical st �eG IttfBtlBriBto justify the choice of the
problems proposed for further investigation. Theoretical questions which
have to be examined in each case are also addressed in each section.
In the dynamical systems that are considered in the following sections,
multilayer neural networks are used as subsystems. For an efficient anal­
ysis of the dynamic behavior of the overall system, it is desirable that the
mapping properties of the static subsystems be well understood. The
following comments concerning the approximating capabilities of neural
networks are consequently very relevant for our investigations.
Consider maps in C(JRn, JR) and C(JRn, JRm). By Weierstrass's fa­
mous theorem, it is known that any function in these classes can be
approximated to any d�_t.at polynomial, i.e.,
where O!O,O!i,O!ij E JR. Similarly, if ¢i(Xl,X2,"',Xn) (i
elements of a complete orthonormal set, f can be expressed as
the structure of a three layer neural network was briefly
described and the block diagram representation of such a network was
More generally, static maps realized by a multilayer
hidden layers and an output layer have the
Multilayer networks in dynamical systems.
The first question that arises is whether such networks can be used to
C (rn.n, rn.) or C (rn.n , rn. m) to any desired
It has recently been shown in Hornik, Stinchombe, and White
using the Stone-Weierstrass theorem, that a two-layer network
arbitrarily large number of nodes in the hidden layer can approximate
C (rn.n , rn. m) over a compact subset of rn.n .
This implies that neural networks with even one hidden layer are ade­
Since polynomials and orthogonal expansions can also approximate
to any degree of accuracy, the advantages of
neural networks over such representations are less obvious and have to
be justified on the basis of practical considerations.
following questions have to be addressed if representations using neural
Are neural networks more parsimonious representations of special
classes of continuous functions in that they need fewer parameters?
If so, what are the characteristics of such functions?
Given a nonlinear map f which has to be approximated, what
dictates the choice of the number of layers and the number of
nodes in each layer of the network (i.e. size of the weight matrices
These questions, which have received considerable attention, have only
partial answers at present. However, such questions are bound to arise
with increasing frequency, even as more precise understanding of the
behavior of complex dynamical systems is sought in the future. In par­
ticular, if neural networks are to be used for the identification and control
problems in dynamical systems, their approximation capabilities must
be well understood when a finite number of layers with a specified num­
ber of nodes in each layer is present in the network. In the absence of
such quantitative results, it will be assumed in all the problems that we
shall study that the nonlinear function f to be approximated belongs
to the class N of functions that the given neural network can generate
for different choices of the weight matrices. This will assure that the
identification or control problem is well posed.
The adjustment of the weights of a neural network when the latter is
used as a component in a dynamical system can be considered as the
adaptive part of the control process. In conventional adaptive control
theory, the rules for up dating the parameter values are called adaptive
laws. Methods for generating both deterministic and stochastic adaptive
laws for controlling linear systems with unknown parameters, based on
an output error, are currently well understood. However, when the
desired static or dynamic maps are nonlinear, the development of such
Backpropagation is one of the computat ionally efficient methods for the
adjustment of the weights of a neural network . In such a method, the
partial derivatives of an error criterion with respect to the weights in
a multilayer neural network are determined and the weights in turn
are adjusted along the negative gradient to minimize the error function.
Figure 5.7 shows a diagrammatic representation of the architecture given
in Narendra and Parthasarathy (1988a) for backpropagation and has
been extensively used by them in simulation studies involving both st atic
and dynamic systems. The structure of the weight matrices used to com­
pute the derivatives (called the sensitivity network) is seen to be identical
to that in the original network, while the signals flow in the opposite di­
rection, justifying the t 'Om bac!mr<W'M,alio� According to the authors ,
while the diagrammatic l'$'J5I!-A\\'tio1!A r£'ngure 5.7 is informationa1ly
algorithmic representation, it enjoys inferential and
computational advantages over the latter.
more evident in the following sections where backpropagation is used in
t he identification and control of dynamic systems.
Most physical systems are dynamic in nature and the identification of
such systems using input-output data will naturally involve dynamic
elements. Fu r ther , if such systems are to be controlled efficiently,
lems , feedback systems must .invariably c�ntain dynamic elements
used for either identification or control, the overall system must include
If backpropagation is to be the principal method for adjusting the
weights of the neural network, the method must be extended to dy­
namic systems as well. For example, figure 5.8 shows that if time delays
are present in cascade in a neural network, the same method shown in
figure 5.7 for static neural networks can be suitably modified to yield
the desired partial derivatives. The more important question is to deter­
mine the partial derivatives when neural networks and linear dynamic
elements are connected in arbitrary configurations.
The introduction of dynamics in a system requires a more detailed
investigation of the concept of a partial derivative. The reader is referred
to Narendra and Parthasarathy (1989b) for a more extensive treatment
of this subject. Figure 5.9 shows structures in which neural networks
and dynamic elements are connected in cascade and feedback. These
represent the basic building blocks of identification and control models.
If dynamic backpropagation methods can be developed to determine
the partial derivative of the output error with respect to any weight as a
function of time, gradient methods can be extended to dynamic systems
also. It is precisely this approach that is used in sections 5.6 and 5.8.
Given a dynamical system with a multilayer network as a component
and an output error e(k) at time k, the backpropagation method yields
as the output of a dynamical sensitivity network, where Wij is a
typical weight. It is assumed that Wij is a constant so that a partial
derivative can be rigorously defined. This implies that changes in Wij
should be small and should be made over large time intervals. When
Wij is adjusted at every instant and the step size is large, it is no longer
possible to theoretically justify the adjustment of the weights of the
network. However, local stability as well as satisfactory performance can
be achieved by making the parameter adjustments sufficiently small.
It is of historical interest to note that in the early 1960s, gradient meth­
ods were used widely in the identification and control of linear dynamical
systems (Narendra and McBride 1964, Kokotovic 1964, Cruz 1973). The
performance of such systems, demonstrated through simulations, was for
the most part satisfactory. However it was shown by Parks (1966) that
systems in which parameters are adjusted in such a fashion can lead to
instability. It was this inability to prove stability of rapidly adjusting
systems using gradient tf;le�y led to their demise and
Architecture for backpropagation when delays are present.
Building blocks of identification and control models.
witnessed the rise of stability methods that are currently used in the
A large literature currently exists on the stab ility of adaptive systems
where the process to be controlled is linear and time-invariant. The
adaptive procedure makes the overall system nonlinear in such cases.
Liapunov's direct method and hyperstability theory have been applied
extensively to prove the stability of such systems. The concept of posi­
tive realness is intimately related to stability and hence to the generation
of stable adaptive laws. The Kalman-Yacubovich lemma is an impor­
tant link in relating the positive realness of a transfer function and the
The problem of stability is vastly more difficult when artificial neu­
ral networks are used either for identification or control and the system
is nonlinear. Unlike linear systems, simple algebraic conditions are not
available for assuring the stability of the overall system. Even if stability
is assumed, a corresponding I:iapunov fun<;tion cannot be easily deter­
mined. Further, Liapun�WItntfly�Mfltfrfjpbymptotic stability) of the
equilibrium state does not assure bounded-input bounded-output stabil­
ity. The concept of positive realness has to be replaced by the concept
of the stability of dynamical adaptive systems using artificial neural
networks is quite rudimentary at the present time and that consider­
able work remains to be done. A large body of work exists in adaptive
control theory in which the adaptive systems, linearized around nom­
These methods will also find application in
the design of stable adaptive controllers using artificial neural networks.
However, in the author's opinion, such methods based on linearization
will be of limited use in problems where the initial weight vector in the
neural network is far from the desired value. It is precisely in problems
where the system has to adapt to large uncertainty that controllers based
on neural networks will be needed in practical applications.
problems, new concepts and methods based on stability theory will have
Until such methods are developed, we have to resort
to dynamic backpropagation methods to evaluate different architectures
Models for the Identification of Nonlinear
In this section four models which were introduced in Narendra and
Parthasarathy (1989a) for the representation of a single-input single­
output ( SISO ) nonlinear plant are presented. These models were chosen
are motivated by corresponding models which have been used in the
both for their generality as well as analytical tractability. The models
adaptive systems literature for the identification of linear systems and
can be considered as their generalizations to nonlinear systems.
backpropagation is the principal method that we shall use for the ad­
justment of parameters of the identification model, the parameterization
of the plant ( and hence the model) is such as to make the application of
the procedure relatively straightforward.
� ses 0tililW� tlJ:tro� uced here can be described
luCk), x(k)] represents the input-output pair of the SISO plant at
k. The functions! and 9 are assumed to be differentiable func­
tions of their arguments. It is evident that Model IV subsumes Models
I-III. However, Model IV is analytically the least tractable and hence
for practical applications, some of the other models might prove more
attractive. In this section, each of these models is briefly described.
The output of the unknown nonlinear plant in this case is
assumed to depend linearly on its past values and nonlinearly on the
past values of the input. The latter is realized as shown in figure 5.10
and consists of tapped delay lines at the input and the feedback path.
This model is realized as shown in figure
nonlinearly on its own past values. The advantage of this model is that
it lends itself readily to control in practical situation as shown in section
The unknown plant in this case is described by a nonlinear
and hence depends nonlinearly on both its past values as well as those
of the input. However, the effects of the input and output values are
As mentioned earlier, this is the most general of all models
introduced here and subsumes the earlier models.
the i n put and the output. Once again, the representation of the model
instant in this case is a nonlinear function of the past values of both
using tapped delay lines is shown in figure
The identification model of the plant is composed of neural networks
In each case, the neural network is assumed
to contain sufficient number of layers, and nodes in each layer, so as to
be able to match exactly the input-output characteristics of the corre­
sponding nonlinear mapping in the given plant. From the point of view
of mathematical analysis, this implies that the nonlinear functions in the
difference equations describing the plant can be replaced by neural net­
fixed but unknown weight matrices Wt. Hence, a theoretical
solution to the adaptive identification problem is assumed to exist from
To identify the plant, an identification model is chosen based on prior
r'�tructure'�scf1�ed by Model III, the model
is chosen to have the form shown in figure 5.14. The aim then is to
determine the weights of the two neural networks Nt and N2 so that
the mapping Nt is equal to g[.] and the mapping N2 is equal to J[.]. If
x(k + 1) and x(k + 1) are respectively the outputs at stage k + 1 of the
plant and the identification model, the error e(k+ 1) � x(k+ l)-x(k + 1)
is used to update the weights of Nl and N2. As described in Section
5.5, either static or dynamic backpropagation is used, depending on the
a. Parallel Model: In this case, the structure of the identifier is
identical to that of the plant with J and g replaced by N2 and Nl
respectively. This is shown in Figure 5.14. Since N2 is in a dynamic
feedback loop, the parameters of Nt and N2 have to be adjusted using
h. Series-Parallel Model: In this case x(k + 1) rather than x(k+ 1)
is used to generate the output of the model. This implies that the model
Since the model does nQt include a feedback loop containing a nonlinear
element, static backpropagation rather than dynamic backpropagation
of the error can be used to adjust the weights of the neural network.
The two methods outlined above have been discussed extensively in
the context of the identification of linear time-invariant systems with un­
known parameters (Narendra and Annaswamy 1989). While the series­
parallel method has been shown to be globally stable, similar results are
difficulties encountered, as well as to assure stability and simplify the
identification procedure, the series-parallel model was used in Narendra
vealed that a large class of nonlinear plants can be identified using the
above procedure. However, theoretical studies concerning stability and
convergence are still in the initial stages and numerous questions have
The identification of an unknown nonlinear plant
may be needed for off-line or on-line control. The nature of the identi­
fication in the two cases may be substantially different.
considerations that determine the models to be used and the test sig­
nals to be applied need �P¥riglrJl.tirJcMat$bieJI.
Model III: Structure of identification model.
Comment 2: The back-propagation method yields the partial deriva­
tives of an error function with respect to a set of parameters at every
instant of time. However , in practice, the parameters are continuously
varied , violating the assumptions made in deriving partial derivatives.
The additional assumptions that have to be made concerning the rate
at which the parameters are adj usted to assure overall system stability
Comment 3: In all the simulation studies, the output error tends to
zero rapidly but the convergence of the weights cannot be assured. Con­
cepts similar to persistent excitation encountered in adaptive systems
theory are needed in this case also to assure convergence . This is briefly
The importance of the class of inputs to be used to train adaptive and
learning systems is generally acknowledged. The training set has to
be representative of the entire class of inputs that the system may be
fashion even when an input not included in the training set is applied to
it. This concept, referred to as persistent excitation, has been extensively
treated in conventional adaptive control theory both in the context of
function as the model. The vector ()* may be unique or belong to a set S.
Let () be an adjustable parameter vector in a system and let
In the latter case, the given system has the same input-output behavior
well as the reference model, the output error
to zero asymptotically with time even if () ¢:.S. However, if
is persistently exciting, ()(k) -+ ()* E S. If ()* is unique, this
the given model for all constant values of
also implies that the parameters of the unknown system can be identified
exactly. Conditions that ensure the persistent excitation of the reference
input which will result in parameter convergence are well established in
The concept of persistent excitation is also found to be important
while dealing with the identification and control of nonlinear systems
sinusoid), the output error rapidly approaches zero if back-propagation
is used to adjust the weights of the neural network.
represent the operators of the plant and the identification model
respectively, .c is not "close" to.c in any sense i.e. the norm lI.cu - .cull
is not small for all u E U. This becomes evident by considering an input
If the input to the plant (and the identification model) is sufficiently
general and the weights of the neural network are adjusted for a suffi­
ciently long time, the output error can be made small for any input in
U. The general input referred to above may then be considered to be
persistently exciting. Needless to say, methods of characterizing persis­
tently exciting inputs, which assure the convergence of the identification
model to the desired set, are highly desirable.
used off-line in the place of the plant for prediction purposes. However
if accurate model following is achieved only by adjusting the parameters
of the model at every instant, the latter has limited predictive ability,
but can be used effectively on-line. It is this method that is used in the
Parametric adaptive control is the problem of controlling the output of
a system with a known structure but unknown parameters.
the problem analytically tractable, in adaptive systems theory the plant
to be controlled is assumed to be linear time-invariant with unknown
If p is known, the parameter vector 0 of a controller can be
parameters. These parameters can be considered as the elements of a
so that the plant together with the fixed controller behaves
equation with constant coefficients. If p is unknown, the vector
like a reference model described by a linear difference (or differential)
to be adjusted on-line using all the available information concerning the
Two distinct approaches to the adaptive control of an unknown plant
parameters of the controller are directly adjusted to reduce some norm
of the output error. In indirect control, the parameters of the plant are
at any time instant and the parameter vector
the plant parameter vector. Even when the plant is assumed to be linear
and time-invariant, both direct and indirect adaptive control result in
nonlinear systems. When the plant is nonlinear and dynamic (i. e. the
present value of its output depends upon the past values of the input and
the output respectively), a neural network can be used as a controller
In conventional direct adaptive control theory, meth­
ods for adjusting the parameters of a controller based on the measured
output error rely on concepts such as positive realness and/or passivity.
By making suitable assumptions concerning the plant and the reference
model, it is shown that the direction in which a parameter is to be ad­
justed can be obtained by correlating two signals that can be measured.
Using either Liapunov theory or hyperstability theory it is shown that
the adjustments of all the controller parameters based on such adaptive
laws result in the stability of the overall system.
At present, methods for directly adjusting the parameters of the con­
on the output error are not available. This is due to the nonlinear na­
ture of both the plant and the controller. Even backpropagation cannot
be used directly, since the plant is unknown and hence cannot be used
Direct adaptive control using neural networks.
methods are developed, adaptive control of nonlinear dynamical systems
has to be carried out using indirect control methods.
Indirect Control: As mentioned earlier, when indirect control is used
to control a nonlinear system, the plant is parameterized using one of the
models described in the previous section and the parameters of the model
are updated using the identification error. The controller parameters in
turn are adjusted by backpropagating the error ( between the identified
model and the reference model outputs ) through the identified model.
A block diagram of such an adaptive system is shown in figure 5.16.
Both identification and control can be carried out at every instant or
after processing the data over finite intervals. When external distur­
bances and /or noise are not present in the system, it is reasonable to
adjust the control and identification parameters synchronously. How­
ever, when sensor noise or external disturbances are present, identifi­
cation is carried out at every instant while control parameter updat­
ing is carried out over a slower time scale, to assure robustness ( Le.,
control parameters are adjusted with a lower frequency than identificar
Prior Information: In conventional adaptive control theory, where
the plant is linear and time-invariant but with unknown parameters, star
ble adaptive laws have been d�rived only' by assuming that considerable
prior information conce�tpl!M�er function is available. In
Indirect adaptive control using neural networks.
particul ar it is assumed that ( i ) the sign of the high frequency gain, ( ii )
the order of the plant , and (iii ) the relative degree of the plant transfer
function are known, and t hat (iv ) the zeros of the plant lie in the interior
of the unit c ircle . It is o nly reasonable to assume that considerably more
prior information concerning the plant input-output characteristics has
to be assumed when the plant is nonlinear, before theoretical results
can be derived to control it in a stable fashion. The insights develop ed
in controlling linear systems are bound to prove invaluable in this con­
text. For example, if the zeros of the plant transfer function lie outside
fashion even as the output error tends to zero. Assump tions
the unit circle it is possible for the plant input to grow in an unbounded
made to assure the existence of the desired controller as well as the ref­
erence model. Theoretical studies on the control of nonlinear dynamical
systems with unknown parameters will have to address similar questions
structure of the adaptive syst�m prop�s.ed . in this paper to control a
Adaptive Control of Nonlinear Dynamical Systems:
is independent of the specific model used to identify the plant.
latter depends critically on the prior information that is available. The
delayed values of the plant input and plant output form the inputs to
the neural network Nc which generates the feedback control signal to
network Nc are adjusted by backpropagating the control error (between
the output of the reference model and the identification model) through
Assuming that one of the models suggested in section 5.6 is used to
identify the plant, the problem of determining a control input which
will result in the output error tending to zero asymptotically can still
be a formidable one. This is because general methods do not exist even
for controlling nonlinear plants whose input-output characteristics are
completely known. Of the four identification models suggested earlier,
only Model II lends itself readily for control purposes. For example, let
the plant be described by the difference equation
IR, and the poles and zeros of the model transfer
function lie in the interior of the unit circle. Let the estimate of
based on the inputs and outputs observed upto time instant
ik. A is determined by updating the weights of a
neural network by using the identification error
in the backpropagation method. The-control input
= - ik[X(k), x(k - 1),···, x(k - n + l)J +
The analytical problem is then to show that the output x(k) of the plant
tends asymptotically to that of the reference model. If Model I is used
f[x(k), x(k - 1),· . . ,x(k - n + 1)J + g[u(k)]
a similar procedure can be adopted if the right inverse of 9 (denoted by
gt) exists such that ggtu(k) = u(k) over the range of interest. In such
a case, gt has to be estimated on-line for use in the controller. Models
III and IV where the equations describing the plant have the form
f[x(k), .. . ,x(k - n + l)J + g[u(k),···, u(k - m + 1)]
f[xCk),· .. ,x(k - n + 1); u(k),' .. ,u(k - m + 1)]
are considerably more difficult to cont rol theoretically. The simulation
studies merely serve to provide empirical evidence of the effectiveness
of neural networks as controllers in those si tuations where no reliable
In the control problem discussed above, it is
tacitly assumed that the reference model is linear . This is because linear
systems theory is well developed and methods of choosing linear models
that have desired properties are well established. In contrast to this,
methods of determini ng general nonlinear models with desired transient
and steady state properties are not currently available. This accounts
for the use of linear reference models in may practical applications.
Whether the reference model is linear or nonlinear, an important prac­
ti cal consideration is that the dynamic mapping represented by the sys­
tem, containing the controller and the nonlinear plant, should approx­
imate that of the reference model as k -> 00. Hence, an important
theoretical question concerns the generation of reference models which
tion that has to be answered is where neural networks would prove most
effective in adaptive control problems. Since the adaptive control of lin­
ear systems is highly developed at present, the principal applications of
neural networks are bound to be in the control of nonlinear systems.
Four distinct classes of problems where such nonlinear control may be
desi rable are listed below. In each case the plant is assumed to be de­
scribed by a complex nctJ)4jpplgtJilJttrMsteilat differential) equation.
i . The first class includes plants whose governing equations are com­
Hence a controller can at least be designed in
theory. However, since the determination of such a controller may
be too complex for practical purposes, it may be preferable to use
a neural network in an adaptive fashion .
ii. In some situations (e.g . , flexible space structures ) the plant may be
stable but have a poor transient response for initial conditions in a
domain of interest. Further a linear feedback controller may not be
satisfactory for large initial conditions. In this case , identification
of the plant using a neural network over a long period of time
followed by the use of a nonlinear controller may result in
iii . In some cases (e.g. , chemical processes ) the plant may have to
operate efficiently at several equilibrium states.
an adaptive controller in such a case is to stabilize the system
around one of these equilibrium states depending upon the in­
formation available at any moment. While very little theoretical
work has been reported in this area, simulation results indicate
that controllers based on neural networks may prove effective in
iv. In many situations, control may be carried out on the basis of out­
put patterns. The state space in such a case is partioned into dis­
joint regions which are equivalent for purposes of control. Neural
networks may be used to generate the optimal input corresponding
Work on most of the problems described above is still in the initial
In this chapter the use of neural networks for the identification and con­
trol of nonlinear dynamical systems is proposed. Many of the concepts
and methods developed in adaptive control theory, for controlling linear
systems with unknown parameters, are shown to have their counterparts
Models of nonlinear plants are first suggested whose
structures are motivated by m,odels which l.lave proved successful in the
identified plant models are used to determine the weights of the con­
troller. Both in identification and control problems, backpropagation
(both static and dyn amic) is used to adjust the relevant weigh ts .
It is shown throughout the paper t h at to have a well- develo p e d theory
of control using neural networks, a vast number of questions have to be
first resolved. Many of these are close ly r el ated to the stability of dy­
namical systems in which neural networks are use d as subsystems. One
of t h e major problems that is worth investigating and which appears
to be tractable is the gener at io n of stable adaptive laws for adj usting
t he weights of t he neural networks using signals t h at are readily avail­
able. The extens ive simulations carried out using the models suggested
in this paper reveal that neural networks are very effective both for iden­
tification and control. This provides adequate justification for current
efforts to address the truly formidable theoretical questions that have
Supp orted by grant number EET-881 4747 from the National Science
Narendra and K. Parthasarathy, which treats in
t he problems raised here, can be found in the IEEE
Transactions on Neural Networks, volume 1 , March 1 990, pages 4-28.
Burr, D. J. ( 1 988) . Exp erime nts on neural n et recognition of spoken and
written text . IEEE Transactions on A coustic, Speech and Signal
System Sensitivity A nalysis. Benchmark Papers
Gorman, R. P. , and S ej nowski , T. J. ( 1988 ) . Learned classification of
sonar targets using a massively parallel network. IEEE Transactions
on Acoustic, Speech and Signal Processing 36(7) : 1 135-1 1 40.
Neural networks and physical systems with emer­
( 1 985) . " Neural" computation of deci­
Hornik, K . , Stinchcombe, M . , and White, H .
forward networks are universal approximators.
Department of Economics, University of California, San Diego, La
Method of sensitivity points in the investigation
and optimization of linear control systems.
( 1987) . A n introduction t o computing with neural nets.
Acoustics, Speech and Signal Processing Magazine ( April ) : 4-
L. E . , Jr. ( 1 964) . Multiparameter self­
optimizing system using correlation techniques. IEEE
resentation of back propagation . Technical Report
Systems Science, Department of Electrical Engineering, Yale Uni­
Part I: A gradient approach to Hopfield net­
ment of Electrical Engineering, Yale University, New Haven, CT.
namical systems containing neural networks. Technical Report
Center for Systems Science, Department o f Electrical Engineering,
II: Identification. Technical Report 8902,
Center for Systems Science, Department of Electrical Engineering,
Parth asarathy, K. ( 1 989c) . Neural networks and
Systems Science, Department of Electri cal Engineering, Yale Uni­
Liapunov Redesign of Model Reference Adaptive
and Signal Processing 36(7) : 1 1 09-1 1 18 .
L. Gordon Kraft, III and David P. Campagna
Historically, it can be argued that developments in control systems have
in rough form and then tested experimentally either in simulation or
hardware. Each time a new simulation or hardware model is built, more
insight is gained into the basic concepts required to fully analyze and
understand the problem. Eventually, the underlying principles are ex­
posed and a design methodology emerges. Theoretical advances usually
compliment the hardware development process by suggesting new direc­
tions to explore to improve the original postulate and extend the basic
This author believes that neural network control system development
is indeed following the same pattern and currently is in the stage where
hardware and software simulations are ahead of the theoretical under­
standing of the problem. For example, in table
is presented that illustrates what is currently known in classical ser­
vomechanism systems, modern multivariable systems, and adaptive sys­
tems versus the known results in neural network control systems.
seems clear at this point in time that much more work is needed in the
theoretical understanding of neural network-based controllers.
One area of theoretical research that needs more work is the direct
comparison of the new neural network approach to more traditional
In this chapter, a method similar to Miller's
adaptive systems methods, namely the self-tuning regulator
compared under identical conditions on the same simple system. Com­
parisons are made about stability, speed of convergence, noise rej ection,
memory size, control effort, number of required calculations, and track­
While each of the methods has both good and bad
characteristics , the experimental results indicate that the neural net­
work approach exhibits desireable properties not found in the other two
methods. The neural network approach contains no restrictions about
system linearity, performs well in noise, and can be implemented very
In the following section, a representative example of each of three differ­
ent control system designs is explained briefly. More details and exten­
sions of these methods can be found in the references at the end of this
The three examples are the self-tuning regulator by Astrom
( Astrom and Wittenmark 1973), the model reference adaptive control
( MRAC ) system by Parks ( 1966), and a neural network method similar
to Miller's CMAC ( Miller, Gl�nz, and Kr8:ft 1987). Each of the meth­
ods was simulated on thf�&Q'fHJm���1n under identical noise and
CMAC Neural Network and 'I'raditional Adaptive Control Systems
Self-tuning regulator control block diagram.
model mismatch conditions. The results are discussed later in section
The self-tuning regulator is essentially a classical feedback system with
on-line adjustable coefficients. A least-square-error parameter identifica­
tion technique is used to determine an approximate model of the system
The controller coefficients are adjusted during each
control cycle according to the best estimate of the system parameters.
As an example, consider the following discrete first-order system with
unknown parameters a and b. The input-output relationship is
index. The control problem is to make the closed-loop system behave as
d characterize the desired system response
Since the parameters are assumed to be unknown, the least-square-error
will be used in place of the true values of a and b. The param­
eter estimates are derived from input-output measurements by think­
ing of the original system equations as a set of simultaneous equations
and least squares can be applied to calculate the parameter estimates
This system was simulated using the a and
The least square error technique is well-known
and can be found in many references including
One of the advantages of the self-tuning regulator app ro ach is that all
the theoretical advances of least-square-error parameter identification
can be applied to the learning part of the control system. That is, the
theoretical lower bound on the parameter errors is
tion of the parameter error variance can be determined recursively, and
available in the literature for applications to
practical problems. The disadvantages of this approach are that there is
t.l,l�J.���f!1 an� in fact, during the learning
CMAC Neural Network and 'lfaditional Adaptive Control Systems
infinitely large and therefore not realizable. Another underlying prob­
lem, however, is that the method requires an assumption of the structure
of the system being controlled. That is, if the system being controlled
does not match the assumed structure, the least-square-error parameter
estimates may not completely describe the system and the closed-loop
system performance may not match the design specifications.
simple case above, for example, the actual system being controlled must
be completely described by a first order linear model in order to match
the assumed structure. Also, the control system design is fixed in the
sense that the design is intended to achieve a certain tracking band­
closed-loop poles or other design objective ) . If the reference
input should change frequency, for example, the tracking performance
will degrade and the design may need to be altered.
Lyapunov Model Reference Adaptive Control (MRAC)
Model reference control systems are designed so that the output of the
system being controlled eventually follows the output of a prespecified
model that exhibits desirable characteristics.
reference control systems are designed so that the overall closed-loop
eventually tracks the reference model with zero error.
transients during the adaptive or learning phase of the control are guar­
anteed bounded. The overall controller structure is shown in figure
The control problem is to dynamically adjust the parameters of a
feedback controller and precompensator such that the output of the plant
eventually follows the output of the reference model. When adaptation
is complete, the properties of the closed-loop plant should match the
properties of the desired reference model.
The adaptive algorithm is derived from a Lyapunov function that is
positive definite in the output error and the parameter errors. Consider
being controlled are described by equations
p are unknown system parameters. With the feedback
and precompensators in place the closed-loop system equation is
CMAC Neural Network and Traditional Adaptive Control Systems
which is negative definite in the plant/ model err0r e. This is sufficient
to prove that for stable systems (am > 0) the error signal will van­
ish asymptotically for sufficiently rich reference inputs r ( Narendra and
Monopoli 1980) . The system described by equations 6.7-6.14 was simu­
lated as a discrete system (see section 6.3.2) so that the results can be
compared directly with the example in section 6.2.1 and the following
The model reference adaptive method has an advantage over the self­
tuning regulator in that there is an implicit proof of closed-loop stability.
In fact, the adaptive laws are chosen specifically to guarantee that the
plant will always follow the model assuming enough frequency richness
in the input signal. However, the stability proof is not guaranteed if the
model of the system does no� match the I!oCtual system. The stability
proof depends on a lin"9�� for the plant. Moreover,
Neural network system characteristic surface.
very little is known quantitatively about noise propagation in this class
of adaptive systems. The adaptive laws depend on a nonlinear prod­
uct term (equations 6.13 and 6.14, above) that makes noise analysis
The neural network approach to control is quite different from either the
self-tuning regulator or the model reference adaptive methods. Consider
the same first order system example discussed in the previous sections
(equation 6.1). The difference equation represents the input-output re­
lationship of the system. An alternative way to interpret the system
input-output characteristics is to view the equation above as a three
dimensional diagram as shown in figure 6.5. The horizontal axes are
x(k + 1), x(k) and the vertical axis is the system input u(k). The hor­
izontal plane represents the system state space. Each point x(k) and
x( k + 1) corresponds to one system input u(k) from the system descrip­
tion (equation 6.1). In the linear case, the collection of these points
forms a planar surface with slope depending on the values of a and b.
Figure 6.5 shows the case where a = b = l.
A nonlinear system would correspond to a differently shaped surface,
not a simple plane as in the linear case in figure 6.5. An example is
shown in figure 6.6 for the cubic nonlinear term used in the simulations
CMAC Neural Network and Traditional Adaptive Control Systems
The basic idea behind the neural network approach is to generate an
approximation to this characteristic system surface from input-output
measurements and then use the surface as feedforward information to
calculate the appropriate control signal.
parameters were known, the surface could be precalculated and stored
in memory. Then, given the control objective (ie., the desired position in
memory) it would be possible to look up in memory the correct control
signal. The more interesting problem is when the system parameters are
unknown and the surface must be "learned" from input-output data in
There are many ways to generate approximations to the system char­
acteristic surface. One relatively simple and robust technique is to iter­
atively improve the values in memory representing the surface according
is the present value of the memory location,
rate parameter which takes on a positive value between zero and one. If
by subtracting a positive number proportional to the error. If
The algorithm above updates only one memory location during each
control cycle. The drawback is that no information is extrapolated to
''nearby'' memory locations. In order to speed up learning and increase
the information spread to adjacent memory cells, the concept of general­
ization is used. Simply stated, the idea is to update a group of memory
cells which are "close" to the selected memory cell. The concept of close­
ness stems from the assumption that for well-behaved systems, similar
states will require similar control effort. For this two dimensional ex­
ample, the generalization region becomes a square of memory locations
2c + 1 locations wide centered at the selected location. The update law
where mij(k) is the value of the selected memory location and i and j
range from -c to c. The value of the system input needed for control
is found by averaging over the generalization region according to the
The difference between learning with and without generalization is il­
lustrated in figures 6.7 and 6.8. Without generalization the surface is
updated one point at a time, whereas with generalization several memory
locations are corrected during each control cycle. It is also important to
realize that the particular regions of memory that are trained correspond
The control structure is shown in figure 6.9. During each control
cycle the system output x(k) is measured and the desired x(k + 1) is
computed or found from memory. The neural network system surface
region is updated according to equation 6.14 above. The control signal
generated by the network is found by summing the values in the system
surface region associated with the desired x(k + 1) and the desired x(k).
This signal is then added to a fixed gain feedback error controller to
form the control signal that is applied to the plant.
As the controller adapts or "learns" the appropriate surface corre­
sponding to the plant being controlled, the portion of the control signal
derived from the neural network takes over the control of the system.
The fixed gain controller is required only during the first few control
cycles while the system CiiP-,-.d Material
CMAC Neural Network and Traditional Adaptive Control Systems
------ --'I-- OF (\<. + 1) f:>.. ('2.'S)\_'2.'S)
This system was simulated on the same simple
methods. The data are plotted and the results are discussed in section
The three control system algorithms were applied to the same first order
problem using computer simulation. The problem was to track a signal
constructed by passing a square wave of unknown magnitude and fre­
quency through a first order filter. No a priori knowledge of the system
being controlled ( other than the system order ) was assumed. The system
input and output were available for measurement. Three groups of ex­
periments were conducted for each of the three systems being tested. For
the first set, the system being controlled was a discrete linear integrator
free. In the second set, the system measurements contained significant
amounts of white zero mean noise relative to the input and output signal
magnitudes. The third set included a nonlinear term in the plant being
controlled. In all experiments tpe desired response was constructed from
a first order shaping filter with unity low frequency gain and a pole at
z = 0.1. The sampling rate was nine hertz and the plant was initially at
rest. For all experiments statistics were recorded about tracking error,
learning time, and control effort required.
The self-tuning regulator was simulated according to in equations 6.16.6. The desired steady state closed-loop pole was set at z = 0.1 so that
if the system parameters were known precisely the closed-loop system
would exactly follow the desired trajectory (filtered square wave) . Since
the true system parameters are unknown, the controller must learn the
parameters before it can adjust the feedback controller. The error sig­
nal decays more slowly than if the system parameters were previously
The self-tuning regulator can also be implemented recursively to save
calculations ( Sage and White 1977) . The results would be identical to
the results presented he{eOpyrlghted Material
CMAC Neural Network and Traditional Adaptive Control Systems
The model reference controller was simulated with adaptive laws de­
6.13 and 6.14. The adaptive gain parameters rand
as -8 and 8 respectively to make the noise free conver­
gence relatively fast as in the least square error case. The system was
simulated as a discrete system so that direct comparisons with the other
two methods could be made. The discrete system equations are shown
magnitude and frequency one hertz. The system being controlled is a
where j ( k) and g(k) are the adjustable controller parameters and y(k)
the measurement of the system state corrupted by noise.
The process n(k) was zero mean, white noise uniformly distributed be­
tween +0.4 and -0.4 and was the same for all systems. The adaptive
The model reference system was simulated as described
6.19-6.23 and the results are presented in section 6.4.
The neural network approach has two adjustable parameters, the learn­
ing rate gain B and the amount of generalization C. For these ex­
periments a gain of B = 0 . 3 and generalization C = 3 were used as
midrange or nominal values. Other values of C and B were tried with
similar results. The total memory size was 625 locations. There were
no collisions due to memory mapping as in (Miller, Glanz, and Kraft
1987) because there was no need to wotk with a smaller memory for this
The neural network controller uses a memory update algorithm sim­
ilar to the method used by Miller «Miller, Glanz, and Kraft 1987). In
Miller's approach, the memory is updated using the actual input u(k)
and output y(k) measurements taken from the system. In the method
presented here, the memory locations associa.ted with the desired posi­
tion are used in place of the actual measured system positions, similar
to the feedback error learning method discussed by Kawato in chapter
7. During the first few control cycles, the controller "learns" in the area
of the system surface close to the desired trajectory.
The results of the self tuning-regulator simulation are shown in figures
6.10--6. 14. Figures 6.1�glIt8d��r estimates from the least
CMAC Neural Network and Traditional Adaptive Control Systems
. j•.•••.• �.•••••• +. .. .... ! .. .. . . �. ..... + ..
--------------- ------- -------- ------- !------- -------- ------- ------- ------
j. ... .. . �... . .. . .� ... ... .j ..... .. �.....
• • .• : .• I.. I• • :• • •:• j• • !• • .· ',:. , :, : ":i,•• ' , : !. , : . ,;i. :· ., :', .:· -tl. • • •J• • • i • •·.i·,�
--------------. ------�--------�-------�---
: • N • • r • • t• • •r• • :• • l• • :•.• • •l• • :• .• r.J.··l·· j·· !··I
CMAC Neural Network and Traditional Adaptive Control Systems
the noise-free case. The estimates converge quickly to the true system
with zero mean white noise. As expected the parameter estimates still
converge relatively quickly towards the true system parameters. In fig­
a nonlinear term was added to the system equations.
parameter estimates in this case did not converge exactly to the true
system parameters and, more importantly, the system output did not
track the desired reference well. The closed-loop controlled trajectory
for the self-tuning regulator is shown in figures
The case with a system nonlinearity diverged for this test.
shows the borderline case, i.e., the largest nonlinear term
for which the STR approach remained bounded. The tracking error was
small in the noise free case, increased as expected when noise was added
and diverged rapidly when the nonlinearity was added.
The results of the model reference adaptive control technique are plot­
The parameter estimates in the no-noise case
when noise was added to the measurements. As expected the parameter
estimates did not converge as quickly to the exact system parameters
when noise was added but, more importantly, the system still tracked
the desired reference input initially. If the system was allowed to run
beyond the initial transient phase, however, the parameter estimates
and the tracking error diverged. When a system nonlinearity was added
the parameter estimates did not converge at
system parameters, but the system still tracked the reference command.
The closed-loop performance of the model reference approach is shown
for the no-noise case. With noise added, the tracking er­
added, the results showed that the system still tracked the reference
The learning performance of the CMAC neural network system is
In this method, there are no explicit param­
eter estimates. The learning performance is characterized by the system
characteristic surface map generated through equation
no-noise case. With noise added, the learned surface changed as in fig­
a nonlinearity present the surface changed as shown in
.. !... . . �. .. � ...... . f·····+······ ······· ······· ······· ·······t······-j--·····t·······[······- --·····j.····
----.. ( ..... �....... -r ..... ! ..... . �.. ·····r·······i·······i·······t·······l-·-----j·.. ---�-·---·-·r -_ ....j .. ···· -_··
CMAC Neural Network and 'ITaditional Adaptive Control Systems
.. . .. . ... : ::.:: : :. . :.[: ::::�:::...:::::::::::. :
--··-···· ·········· ·········· ·-········ ··········
�: . � �E. tj�.. . .. =f. . .. .+. =.. =:�:�,. . I. :
.. ; .......... !...... : . . .;..........�. .... .i" ... . 1-. .. . . . . .. �-.... .
.. .! .... . . + . .... ;..........L ... )
�;.;:..;. j .. . .. j . .. .. . i . . j . + i
. --j-.--.....-�--.-.-----�-.-.--.--.;... � ..
.., ...--r ; . ... ..;....... L..........: ...... ; ... ...; .........� . .. ... : ........� .......
· ·· · ··· ··· · ··· · · ·· ·· · ····· · ·
·· ········· · -· · · ········· ··-···-· · _···· ···· ····· ········· ···· . ··· ········ ····· ·
CMAC Neural Network and Traditional Adaptive Control Systems
········· ·········· ·········· ·········· ··········
� .......... � .......... �.......... +......... + ......... + ..........; ...... .
;··········t··········i·.········j··········�···.··.···(.·········i··········.f.··········;·······
: fl1illr !1 •·· ··.·1. · · ·)f··=. . 1�1�
..... .. : ......... ....... .. : .........: . ........ : ......... :. ........ .:.. ...... .. �
.. . . . . . !;V. . ;\1!. . !!· · · · i;· . . V:." . . . .�t.·.· . w.·:: .·: ·.· �_·..v. : . .:
CMAC Neural Network and Traditional Adaptive Control Systems
+ ._. __ +_ . ._ _ ,........;........ , ....... ,.........>! .....••, ..••••. , •••.••.. ••••••••••,••••••. ; .•..•.••.•... . ,
i--. . ... .;....... ;......, •••••• ;"""H.
. . ! .. .. .-+.If....,..... +.+ •......, ..... . .
. . .. . + ....... ; ....... + . . . . . . + ...
; ....... + . . . . . . � . • . . . . . : ....... � ... .
••••••• l . . . . . . . � . . . . . i . . . . . . . �. . .
+ . . . . . � . . . . . . L. . . . . . L. .. . . . i. . . ... .; ....... L . . . . L .. . . . l .. .. . ! . . . . . . . !
CMAC Neural Network and Traditional Adaptive Control Systems
took longer but , the steady state performance was about the same as
without noise. With the system nonlinearity, the tracking was very good
Each of the three systems compared had both desirable and undesirable
characteristics. The least-square-error self-tuning regulator was able to
learn the unknown system parameters quickly and functioned well with
noise in the measurements. However, during the adaptation process it
called for large system inputs and it did not track the reference well when
the plant was nonlinear. The Lyapunov-based model reference controller
used less control effort and remained bounded during adaptation but,
was slow to converge and sensitive to noise if allowed to run for many
The model reference approach did track the nonlinear
model well even though there was no guarantee of stability. The neural
network approach called for small control signals during learning and was
relatively insensitive to noise, but took the longest to learn the system.
The neural network also performed well when the plant was a nonlinear
system. The self-tuning regulator controller parameter design must be
changed if the plant is to follow a different reference input. The model
reference approach and the neural network approaches did not require
a different design for each reference signal. Implementation speed com­
parisons favored the neural network approach because the control signal
can be generated virtually from a table look up procedure rather than
multiplication operations as in both the least square error and model
summary of the control system characteristics
The challenge to future control system researchers and designers is
to take advantage of the desireable properties of each of the classes of
systems discussed in this chapter . The future of intelligent control lies in
combining the experience and dependability of classical and traditional
adaptive control with the potential and promise of neural network-based
systems. Research efforts should be directed towards extracting the best
The authors wish to thank Dr. W. T. Miller and Dr. F. H. Glanz and
Judy Franklin of GTE Waltham, Massachusetts, for their helpful com­
ments during the writing of this chapter. This work was funded in part
by the National Science Foundation Grant No. IRl-8813225.
Albus, J . S. (1975). A new approach to manipulator control: The cere­
bellar model articulation controller (CMAC) , 7ransactions of the
A SME Journal of Dynamic Systems, Measurement, and Control.
Astrom, K. J . , and Wittenmark, B. (1973). On self-tuning regulators,
Astrom, K. J . t and Wittenmark, B. (1989) . A daptive
CMAC Neural Network and Traditional Adaptive Control Systems
single-input single-output linear systems, IEEE Transactions on A u­
A control structure using an adaptive observer,
A daptive Control- The Model Reference Ap­
Sensor based control of robotic manipulators
using a general learning algorithm, IEEE Journal of Robotics and
general learning algorithm to the control of robotic manipulators,
International Journal of Robotics Research
Lyapunov redesign of model reference adaptive
control systems, IEEE Transactions on A utomatic Control,
Rumelhart, D. E . , and McClelland, J. L.
Generalization and information storage in networks
of adeline neurons, in Self- Organizing Systems Yovits et al. , eds. ,
An important problem in neural networks is the problem of parameter
estimation in a mathematical model. The number of parameters is gen­
erally very large, and the parameter values are chosen to minimize a
functional norm which is designed to minimize the difference between
desired and actual behavior of the model for any given set of state vari­
ables. It is the purpose of this chapter to give a brief summary of recent
advances in numerical techniques for doing large-scale minimization.
Initially, we will consider the unconstrained nonlinear optimization
estimated. Classical numerical methods for the solution of equation
the matrix of second partial derivatives of
The steepest descent formula has the advantage of being cheap and
simple to implement, and being highly parallelizable,but the distinct
disadvantage of being extremely slow to converge when the contours
method has the advantage of extremely rapid convergence when
For the past twenty-five years, research in unconstrained optimiza­
tion has concentrated on two major classes of techniques, quasi-Newton
methods and conjugate gradient methods. Quasi-Newton methods ap­
where H(k) is a positive definite approximation to ("72 f(x(k»)-l ob­
tained solely from first order information. This has the advantage of
guaranteeing that d(k) is always a descent direction and of not requiring
the evaluation of second partial derivatives. However, H(k) is invari­
ably a totally dense matrix, so for large numbers of variables storage
requirements are extremely large. As memory capacity of contempo­
rary computers is currently becoming extremely large, this may not be
a problem for moderate numbers of variables, say 1000 < n < 2500, but
will still be a problem for large n. These methods have been fully de­
veloped and work extremely well. Interested readers should begin study
of these methods with either the excellent survey by Dennis and More
(1974), or the excellent text by Dennis and Schnabel (1983).
An interesting feature of the methods defined by equation 7.5 are that
various degrees of parallelization are possible. Clearly, each element of
d(k) can be independently computed on a separate processor, and in
a form which utilizes vectorization efficiently.
computing H(k) is also equally parallelizable, and vectorizable as each
row can be computed independently in parallel. Parallel quasi-Newton
methods have been the subject of a great deal of recent study. A survey
of results to date is contained in Byrd schnabel, and shultz (1988).
Conjugate gradient methods compute d(k) as a linear combination of
the current gradient vector and the previous search direction. Starting
with d(O) = - "7 f(x(O»), then sequence d(k) is chosen as
where (3(k) is a scalar parameter chosen to assure that the sequence
of vectors d(k) satisfy a mutual conjugacy condition. Contemporary
conjugate gradient methods are more elaborate than equation 7.6, but
similar in motivation and structure. A full development of conjugate
gradient algorithms is contained in Shanno (1978), and a computer code
is distributed by the Association for Computing Machinery (Shanno and
As is apparent from equation 7.6, conjugate gradient algorithms re­
quire relatively little storage. The code of Shanno and Phua, for exam­
ple, requires a total of 7n double precision words of computer storage.
Further, the algorithm is easily parallelized or vectorized, but in general
cannot benefit greatly from parallel vector processors, for if each element
d(k) of equation 7.6 is c�f/>lItillJMfaJ.o vectorization is possible.
Advances in Numerical Techniques for Large-Scale Optimization
In general, conjugate gradient methods share all the desirable proper­
ties of the steepest descent method, namely low storage, ease of imple­
mentation, and parallelization. However, when properly implemented
they converge far more rapidly. In practice, conjugate gradient methods
generally take approximately the same number of iterations as quasi­
Newton methods. However, the linear searches to determine a(k) must
be done more carefully for conjugate gradient methods than for quasi­
Newton methods. In general, quasi-Newton methods use a(k)
almost every iteration, while conjugate gradient methods must use at
least two trial values of a(k) at each iteration to assure a superlinear
rate of convergence. Thus conjugate gradient methods generally utilize
approximately twice as many function evaluations, and dependent on
the line search algorithm up to twice as many gradient evaluations as
quasi-Newton methods, but save large amounts of memory and signifi­
cant time in computing H(k) for large problems.
Modern conjugate gradient algorithms have proved so successful in
practice that they have become the method of choice for large uncon­
strained problems. However, if function and gradient evaluations are
expensive, it is desirable to attempt to alter the algorithms so that at
a cost of somewhat more memory, the linear search requirement can
be loosened. The first successful attempt to use additional memory to
make conjugate gradient methods look like quasi-Newton methods is
due to Buckley and LeNir (1983). They significantly reduced function
evaluations in some instances at the cost of slightly increased storage
requirements. In a recent paper, Liu and Nocedal (1988) have greatly
improved on the Buckley-LeNir algorithm, and have dramatically re­
duced total time on the computer over conjugate gradient algorithms at
a cost of slightly greater memory requirements. This algorithm is easily
and naturally vectorizable. While it has not to date been parallelized, it
should not prove difficult to do so effectively. At present, this algorithm
seems the most promising overall in terms of maximum efficiency with
reasonable storage requirements and the ability to fully utilize emerging
A more specialized variant of low storage quasi-Newton methods is the
partitioned quasi-Newton method of Griewank and Toint (1982). Here
where each Xi is a small subvector of the vector
are computed i ndependently for each subvector
Xi, and the overall inverse Hessian approximation H
where overlap is possible between some Hi'S, and indeed the block diag­
the two x's from h , so that the contribution of Hl to H
As shown by L iu and Nocedal (1988), thi s method is often very effective,
and as each Hi is estimated indep endently, this extends naturally to
As a final note on nonlinear unconstrained optimiz ation, we reconsider
Newton's method. When second partial derivativ es are readily available,
and storage adequa te, a careful implementation of Newton's method is
ge nerall y more efficient than any other method. Even. if storage of a
full Hessian matrix is imp ossible due to problem size, in many large
problems most elements of the Hessian matrix are zero. In thes e
storage is quite often adequate to store the nonzeros of
direction without using an inordinate amount of storage. Here the linear
Advances in Numerical Techniques for Large-Scale Optimization
the so-called truncated-Newton method of Dembo and Steihaug
These methods are particularly effective for very sparse Hessians from
highly nonlinear problems, providing that the nonzero second partial
derivatives can be calculated cheaply or estimated inexpensively using
Thus far, we have dealt exclusively with unconstrained nonlinear prob­
In general, problems with very large numbers of variables and
large numbers of constraints on these variables are computationally in­
tractable, especially if large numbers of the constraints are nonlinear.
However, some specialized constrained problems are solvable for large
The simplest constrained problem simply puts upper and/or lower
as even guaranteed convergent methods only find a local optimum for a
nonconvex problem, it is sensible whenever possible to restrict the x's
to a region in which it is suspected that the desired optimum lies.
This seemingly simple modification of the problem can add substan­
tial computational complexity in the case where the optimizing point
x· requires that a substantial number of the bounds be active, i.e., that
are at either their upper or lower bound at optimality.
The method is implemented using both Newton's method and a variety
of quasi-Newton methods. To date, the algorithm has not been extended
to limit memory quasi-Newton or conjugate gradient methods, but such
an extension should be straightforward. Substantial work will be needed,
however, to modify the method to efficiently exploit parallelism, as much
of the work of the algorithm is in a segmented line search. It is not im­
mediately obvious that this line search can exploit parallel capabilities,
so this remains a topic for further study.
When the constraints take a more complex form than simple bounds,
the problem becomes much more complex. If only a few nonlinear con­
straints are included in the problem, they may be incorporated into
the objective function via various penalty or barrier methods (F iacco
constraints are present, as previously noted, the problem is generally
An interesting case arises when there is a large constraint set, but the
For truly nonlinear objective functions, the re­
duced gradient algorithm of M.urtagh and S�unders (1978), implemented
large numbers of variables if the number of constraints is not excessively
large and the constraint matrix is suitably sparse.
When the objective function is linear, subject to linear constraints and
bounds on the variables, the problem is a linear programming problem.
Until recently, the simplex method was the sole method for solving prob­
lems of this type and performance often became unacceptably slow as
the number of variables became very large. A new, interior point algo­
ity provably superior to that of the simplex algorithm. Computational
experiments have shown various interior point variants of Karmarkar's
method to be significantly faster than simplex code, with the advantage
As an example of how dramatically solution time may be improved,
we consider a stochastic programming model with approximately
variables. This was run on primal-dual interior
15 minutes on a Silicon Graphics IRIS workstation. The same problem,
using the MINOS linear programming code took approximately
times as long. Of perhaps more interest in neural networks
are problems with many more variables, but few constraints. The same
primal-dual code recently solved a problem with
less than one-half hour on a Convex computer, and a
a CRAY II. Thus the promise of being able to solve very large problems
Linear objective functions are of limited interest in neural networks.
However, the theory for applying these interior point algorithms to
quadratic objective functions with linear or quadratic constraints has
date these algorithms for quadratic models have not been implemented,
as it is only quite recently that codes for linear objective functions have
been successfully developed. Given the success of the linear model codes,
it is certain that quadratic models will be implemented in the near fu­
ture, and it is to be hoped they will greatly expand the size of nonlinear
In summary, there has been a great deal of recent concentration on nu­
merical solution of a variety of optimization problems with large numbers
of variables, with significant advances in algorithms for these problems.
Concentration on adapting these new techniques to specific problems in
neural networks should significantly enlarge the set of problems which
Advances in Numerical Techniques for Large-Scale Optimization
Bertsekas, D. P. (1982). Constrained optimization and Lagrange multi­
Buckley A., and LeNir, A. (1983). QN-like variable storage conjugate
gradients. Mathematical Programming 27: 155-175.
Byrd, R. H., Schnabel, R. B., and Shultz, G. A. (1988). Parallel quasi­
Newton methods for unconstrained optimization. Technical Report
CO-CS-396-88, Computer Science Department, University of Col­
Conn, A. R., Gould, N. I. M., and Toint, Ph. L. (1988). Global con­
vergence of a class of trust region algorithms for optimization with
simple bounds. SIAM Journal on Numerical Analysis 25:433-460.
Dembo R. S., and Steihaug, T. (1983). Truncated-Newton algorithms for
large scale unconstrained optimization. Mathematical Programming
Dennis, J. E., and More, J. J. (1974). Quasi-Newton methods, motiva­
Dennis, J. E., and Schnabel, R. B. (1983), Numerical methods for Un­
constrained optimization and nonlinear equations. Englewood Cliffs:
Duff, I. S., Erisman, A. M., and Reid, J. K. (1986). Direct methods
for sparse matrices. New York: Clarendon Press Oxford University
Fiacco, A. V., and McCormick, G. P. (1968). Nonlinear programming:
Sequential unconstrained minimization techniques. New York: Wi­
Griewank, A., and Toint, Ph. L. (1982). Partitioned variable metric
updates for large structured optimization problems. Numerische
A new polynomial-time algorithm for linear
method for large scale optimization. Technical Report NAM 03 , De­
partment of Electrical Engineering and Computer Science, North­
McShane, K. A., Monma, C. L., and Shanno, D.
tion of a primal-dual interior point method for linear programming.
of Industrial Engineering and Management Sciences, Northwestern
dual algorithms: Part II: Convex quadratic programming. Technical
Report, Department of Industrial Engineering and Operations Re­
search, University of California, Berkeley.
strained optimization. Mathematical Programming
searches. Mathematics of Operations Research
imization of unconstrained multivariable functions. A CM Transac­
How should a robot decide what to do? The traditional answer in AI
has been that it should deduce its best action in light of its current
goals and world model, Le., that it should plan.
widely recognized that planning's usefulness is limited by its computa­
tional complexity and by its dependence on a complete and accurate
world model. An alternative approach is to do the planning in advance
and compile its result into a set of rapid reactions, or situation-action
rules, which are then used for real-time decision making.
approach is to learn a good set of reactions by trial and error; this has
the advantage of eliminating the dependence on a world model. In this
chapter I briefly introduce Dyna, a simple architecture integrating and
permitting tradeoffs among these three approaches.
simple Dyna system that learns from trial and error while
it learns a world model and uses the model to plan reactions that result
Dyna is based on the old idea that planning is like trial-and-error
theory of Dyna is based on the classical optimization technique of dy­
ship of dynamic programming to reinforcement learning ( Watkins
and to AI methods for planning and search. Werbos
has previously argued for the general idea of building AI systems that
approximate dynamic programming, and W hitehead
have presented results for the specific idea of augmenting
reinforcement learning system with a world model used for planning.
The Dyna architecture consists of four primary components, interact·
by the current set of reactions; it receives as input a description of the
current state of the world and produces as output an action to be sent
to the world. The world represents the task to be solved; prototypically
it is the robot's external environment. The world receives actions from
the policy and produces a next state output and a reward output. The
overall task is defined .��bIe�-term average reward per
The Dyna architecture also includes an ex­
The world model is intended to mimic the one-step
input-output behavior of the real world. F inally, the Dyna architecture
evaluation function that rapidly maps states to values,
as the policy rapidly maps states to actions.
the policy, and the world model are each updated by separate learning
For a fixed policy, Dyna is a simple reactive system.
policy is continually adjusted by an integrated planning/learning pro­
tioned by current input. The planning process is incremental and can be
interrupted and resumed at any time. It consists of a series of shallow
searches, each typically of one ply, and yet ultimately produces the same
result as an arbitrarily deep conventional search. I call this
Dynamic programming is a special case of this.
Relaxation planning is based on continuously adjusting the evaluation
way that credit is propagated to the appropriate steps
within action sequences. Generally speaking, the evaluation of a state x
should be equal to the best of the states
in one action, taking into consideration the reward (or cost)
E {. I .} denotes a conditional expected value and the equal sign is
quoted to indicate that this is a condition that we would like to hold, not
one that necessarily does hold. If we have a complete model of the world,
then the right-hand side can be computed by looking ahead one action.
Thus, we can generate any number of training examples for the process
that learns the evaluation function: for any
holds in all states, then the optimal policy is
given by choosing the action in each state
There is an extensive theoretical basis from
dynamic programming for algorithms of this type for the special case
in which the evaluation function is tabular, with enumerable states and
actions. For example, this theory guarantees convergence to a unique
The evaluation function and policy need not be tables, but can be
more compact function (ip�trI� decision trees, k-d trees,
connectionist networks, or symbolic rules. Although the existing theory
does not directly apply to the case in which these machine learning al­
gorithms are used, it does provide a theoretical foundation for exploring
their use. Finally, this kind of planning also extends conventional plan­
ning in that it is applicable to stochastic and uncertain worlds and to
The above discussion gives the general idea of relaxation planning,
but not the exact form used in Dyna. Dyna is based on a closely related
iteration ( Howard 1960), in which the evaluation
function and policy are simultaneously approximated. In addition, Dyna
in which the world model need only be sampled, not examined directly.
Since the real world can also be sampled, by actually taking actions and
observing the result, the world can be used in place of the world model
In this case, the result is not relaxation planning, but a trial-and-error
learning process much like reinforcement learning
In Dyna, both of these are done at once. The same
algorithm is applied both to real experience resulting in learning and
to hypothetical experience generated by the world model
relaxation planning . The results in both cases are accumulated in the
fully justify the algorithm, but it is quite simple and is given in outline
As a simple illustration of the Dyna architecture, consider the nav­
igation task shown in the upper right of figure
grid of possible locations or states, one of which is marked as the
starting state, "8", and one of which is marked as the goal state, "G".
The shaded states act as barriers and cannot be entered. All the other
states are distinct and completely distinguishable. From each there are
four possible actions: UP, DOWN, RIGHT, and LEFT, which change
the state accordingly, except where such a movement would take the
system into a barrier or outside the space, in which case the location is
not changed. Reward is zero for all transitions except for those into the
goal state, for which it is,+l. Upon entering the goal state, the system
is instantly transported back to the start state to begin the next trial.
None of this structure and dynamics is known to the Dyna system a
In this demonstration, the world was assumed to be deterministic,
that is, to be a finite-state automaton, and the world model was im­
O. Decide if this is a real experience or a hypothetical one.
If this is a real experience, use the current state.
5. If this is a real experience, update world model from
than ej this typically involves temporal-difference learning.
S. Update policy�trengthen or weaken the tendency to perform ac­
Inner loop of a Dyna algorithm. These steps are repeated continually, sometimes
with real experiences, sometimes with hypothetical ones.
whenever a new state-action pair was experienced ( Step
The evaluation function was also implemented as a table and was up­
according to the simplest temporal-difference learning
at random uniformly over all states previ­
ously encountered. The initial values of the evaluation function
and the policy table entries Wxa were all zerOj the initial policy was thus
The world model was initially emptYj if a state and
action were selected for a hypothetical experience that had never been
experienced in reality, then the following steps ( Steps
In this instance of the Dyna architecture, the inner loop ( figure
was applied alternately to the real world and to the world model. For
each experience with tQ>��t_thetical experiences were
Learning curves for Dyna systems on a simple navigation task. A trial is one trip
from the start state "S" to the goal state " G ". The shortest possible trial is 14
steps. The more hypothetical experiences ("planning steps") using the world
generated with the model (Step 0). Figure 8.3 shows learning curves
10, and k = 100, each an average over 100 runs. The
k = 0 case involves no planning; this is a pure trial-and-error learning
system entirely analogous to those used in reinforcement learning sys­
tems (Barto, Sutton, and Anderson 1983, Sutton 1984, Anderson 1987).
Although the length of path taken from start to goal falls dramatically
for this case, it falls much more rapidly for the cases including hypothet­
ical experiences (planning), showing the benefit of using a learned world
model. For k 100, the optimal path was generally found and followed
by the fourth trip from start to goal; this is very rapid learning. The
parameter values used were f3 0.1, 'Y = 0 . 9 , and a 1000 (k 0) or
100). The a values were chosen roughly to give
Figure 8.4 shows why a Dyna system that includes planning solves
this problem so much faster than one that does not. Shown are the
policies found by the k 0 and k 100 Dyna systems halfway through
the second trial. Without planning (k = 0), each trial adds only one
additional step to the policy, and so only one step (the last) has been
learned so far. With planning, the first trial also learned only the last
step, but here during the second trial an extensive policy has been de­
veloped that by the trial's end will reach almost back to the start state.
By the end of the third or fourth trial a complete optimal policy will
have been found and perfect performance attained.
This simple illustration is clearly limited in many ways. The state and
action spaces are small and denumerable, permitting tables to be used
for all learning processes, and making it feasible for the entire state space
to be explicitly explored. For large state spaces it is not practical to use
tables or to visit all states; instead one must represent a limited amount
of experience compactly and generalize from it. The Dyna architecture
is fully compatible with the use of a wide range of learning methods for
doing this. In this example, it was also assumed that the Dyna system
has explicit knowledge of the world's state. In general, states can not be
known directly, but must be estimated from the pattern of past inter­
action with the world (Rivest and Schapire 1985, Mozer and Bachrach
1989). The Dyna architecture can use state estimates constructed in
any way, but will of course be limited by their quality and resolution. A
promising area for future work is the combination of Dyna architectures
with egocentric or "indexical-functional" state representations (Agre and
Yet another limitatio� of th,e exampJ!l py{)a system presented here is
the trivial form of searc��'HeJA .� control in Dyna boils down
Policies found by planning and non-planning Dyna systems by the middle of the
second trial. The black square indicates the current location of the Dyna system,
and the arrows indicate action probabilities ( excess over the smallest ) for each
to the decision of whether to consider hypothetical or real experiences,
and of picking the order in which to consider hypothetical experiences.
The task considered here is so small that search control is unimportant,
and was thus done trivially, but a wide variety of more sophisticated
Particularly interesting is the possibility of
using the Dyna architecture at a higher level to make these decisions.
Finally, the example presented here is limited in that reward is only
nonzero upon termination of a path from start to goal.
the problem more like the kind of search problem typically studied in
AI, but does not show the full generality of the framework, in which
rewards may be received on any step and there need not even exist start
or termination states. In the general case, the Dyna algorithm given here
attempts to maximize the cumulative reward received per time step.
Despite these limitations, the results presented here are significant.
They show that the use of an internal model can dramatically speed
trial-and-error learning processes even on simple problems. Moreover,
they show how the functionality of planning can be obtained in a com­
pletely incremental manner, and how a planning process can be freely
intermixed with reaction and learning processes. I conclude that it is
not necessary to choose between planning systems, reactive systems, and
learning systems. These three can be integrated not just into one sys­
The author gratefully acknowledges the extensive contributions to the
ideas presented of his colleagues Andrew Barto and Chris Watkins. I
also wish to also thank the following people for ideas and discussions:
Steve W hitehead, Paul Werbos, Luis Almeida, Ron Williams, Glenn Iba,
Leslie Kaelbling, John Vittal, Charles Anderson, Bernard Silver, Oliver
Selfridge, Judy Franklin, Tom Dean, and Chris Matheus.
Agre, P. E., and Chapman, D. (1987). Pengi: An implementation of a
theory of activity. Proceedings of AAAI-87, 268-272.
Anderson, C. W. (1987). Strategy learning with multilayer connectionist
representations. Proceedings of the Fourth International Workshop
on Machine Learning, lO3-114. Irvine, CA: Morgan Kaufmann.
Barto, A. G., Sutton R. S., and Anderson, C. W. (1983).
elements that can solve difficult learning control problems. IEEE
Transactions on Systems, Man, and Cybernetics, 13: 834-846.
Barto, A. G., Sutton, R. S., and Watkins, C. J. C. H. (1989). Learn­
95, Dept. of Computer and Information Science, University of Mas­
Bellman, R. E. (1957). Dynamic Programming. Princeton, NJ.: Prince­
Craik, K. J. W. (1943). The Nature of Explanation. Cambridge, UK.:
Dennett, D. C. (1978). W hy the law of effect will not go away. In Brain­
storms, by D. C. Dennett, 71-89, Montgomery, Vermont: Bradford
Howard, R. A. (1960). Dynamic Programming and Markov Processes.
reactive environment by exploration. Technical Report CU-CS-451-
Dept. of Computer Science, University of Colorado at Boulder.
supervised learning in deterministic environments. Proceedings of
the Fourth International Workshop on Machine Lea rning, 364-375.
Rumelhart, D. E., Smolensky, P., McClelland, J. L., and Hinton, G. E.
(1986) Schemata and sequential thought processes in P DP models.
Parallel Distributed Processing: Explorations in the Microstruc­
ture of Cognition, Volume II, by J. L. McClelland, D. E. Rumelhart,
Russell, S. J. (1989). Execution architectures and compilation. Pro­
(1984). Temporal credit assignment in reinforcement learn­
ing. Doctoral dissertation, Department of Computer and Informa­
tion Science, University of Massachusetts, Amherst.
(1981). An adapt ive network that constructs
and uses an internal model of its environment. Cognition
(1985). The learning of world models by con­
Proceedings of the Seventh Annual ConE. of
Building and understanding adaptive systems:
A statistical/numerical approach to factory automation and brain
research. IEEE Transactions on Systems, Man, and Cybernetics,
Technical Report 305, Dept. of Computer Science, University of
The chapters in part II concern the use of neural network learning for
motion control in robotics. This problem area was chosen for elabora­
tion because it has received the most attention in past learning control
research. An additional advantage is that, since we are all robots of a
sort, the typical reader has an appreciation for the problems of robotic
motion planning and control. The issues involved in process control or
flight control are equally important but less familiar. Many of the issues
and techniques explored in robotics problems in this section are also
While the chapters in part II address robotic motion control, the au­
thors have taken quite different approaches. This reflects the fact that
the field of neural network-based control is still in its infancy, and that
"standard" techniques have not yet been developed. It also reflects the
fact that any control problem can be solved in many ways, each with its
These chapters illustrate some of the general concepts addressed in
part I. The following additional, more specific, research issues are also
Use of existing knowledge of the problem domain.
basic neural network learning studies assume no knowledge of the
plant being controlled, it is rarely the case that no knowledge is
available for a real control problem. Techniques are needed for ef­
fectively using such a priori knowledge in the design of application
On-line learning. For complex systems operating in unstructured
environments, training with predetermined examples will have to
be supplemented with on-line ( run-time ) training and learning. In
an on-line learning environment, issues such as speed of learning
system convergence, controller performance in poorly trained re­
gions, and learning interference have increased significance.
control is robust system performance in complex, uncertain en­
vironments, and not the mimicing of biological systems. However,
useful control system concepts exploited by nature should not be
This is particularly true of many problems in mo­
tion planning and control, where biological systems are extremely
often try to minimize the number of sensors used in order to re­
other hand, appear to use large numbers of sensors to obtain rer
bust performance. Techniques need to be developed for integrating
large numbers of sensors in learning systems without unreasonably
degrading learning system convergence and generalization.
In the first chapter of this part, Kawato presents a cascade neural
network architecture for robot trajectory planning. An update rule is
described through which the network can be trained to produce arm
paths which are optimal according to a minimum torque change criteria.
T his training technique is related to the backpropagation through time
approach to optimization discussed by Werbos in the first part.
In the succeeding chapter Mel describes a different approach to robot
In his experiment, a neural network, arranged in a
sensory-topographic architecture, is trained to formulate a model of the
"mentally" for a trajectory which successfully reaches a target while
avoiding obstacles. T he mental search uses "trial-and-error" logic, but
the robot is not moved until a successful path is found.
The use of associative memories in robotic control is discussed by
His experiments utilize a nearest-neighbor technique imple­
are relevant to control utilizing other associative memory architectures,
such as those developed by Albus, Kanerva, or Kohonen.
Nguyen and Widrow present an example of the use of backpropagation­
through-time to train a controller for a simulated tractor-trailer truck
which must back up to a loading dock from arbitrary initial positions.
First, one network is trained to formulate a model of the truck's response
to steering signals. This network is then used for training the control
network, which must generate the steering signals required to back the
an overview of sensory-motor information processing in the cerebellum,
followed by a discussion of the implications of the physiological evidence
to motor control in robotics. He then formulates a model for learning
control which uses adjustable central pattern generators, rather than
In the final chapter of this part, Franklin and Selfridge discuss a variety
of issues related to adaptive control in robotics. Rather than reporting
specific technique, they discuss the control challenges presented in
robotics and the limitations of existing control techniques.
In this chapter, we propose a computational model and two neural
network models for trajectory formation and control of multijoint arm
movement based on data from behavioral experiments and physiological
First, in this section, we explain that the three problems, trajectory
formation, inverse kinematics and inverse dynamics must be solved for
control of voluntary movements. In the next section, it is shown that
all the three problems are ill posed in the sense that the solutions to
these problems are not unique. We propose two entirely different neural
network models for resolving the ill-posedness of motor control.
In the first model, the minimum torque change criterion derived from
data from human arm movement experiments is utilized to resolve the
ill-posedness of arm trajectory formation. We propose a neural network
model with a repetitive cascade structure which generates the minimum
torque change trajectory. The model first acquires the forward dynamics
model of a controlled object in the learning phase, then minimizes some
energy to generate a trajectory by relaxation computation in the pattern
In the latter half of this chapter, we will show that the feedback er­
ror learning scheme for motor learning that we proposed can be used
to resolve the ill-posedness of motor control. This scheme is compared
with alternative approaches such as direct inverse modeling or combi­
nation of forward and inverse modeling. We will show experiment data
of feedback error learning for acquiring the inverse-dynamics model of a
A computational model for voluntary movement is proposed (figure
9.1) which accounts for Marr's (1982) first level for understanding com­
plex information-processing systems (Le., computational theory).
Consider a thirsty person reaching for a glass of water on a table. The
goal of the movement is to move the arm toward the glass which in turn
will lead to reducing the thirst. First, one desirable trajectory in the
task-oriented coordinates must be selected from an infinite number of
possible trajectories which lead to the glass whose spatial coordinates
are provided by the visual system (trajectory determination in figure
9.1). Second, the spa�P.)f6�tMat1frlale desired trajectory must
Computational Theory and Information Representation
Computational theories and internal information representation in the brain for
sensory-motor control of voluntary movement.
be reinterpreted in terms of a corresponding set of body coordinates,
such as joint angles or muscle lengths (coordinates transformation in
figure 9.1). Finally, motor commands, that is muscle torque, must be
generated to coordinate the activity of many muscles so that the desired
trajectory is realized (generation of motor command in figure 9.1). The
second and the third problems are called the inverse kinematics problem
and the inverse dynamics problem in robotics literature. Several lines
of experimental evidence suggest that these three sets of information
shown in figure 9.1, i.e., the desired trajectory in visual coordinates,
the desired trajectory in body coordinates and the active torque, are
internally represented in the brain (Kawato 1989).
However, it must be noted that here we do not adhere to the hypothe­
sis of the step-by-step information processing shown on the bottom line
in figure 9.1. Rather, our model indicates that there are other stages
of information proceselJpVi1Ajr£ a��r O the desired trajectory. In
the middle line of figure !f. r,' the motO': command is obtained directly
Schemes and Models for Control of Arm Trajectory
from the desired trajectory represented in the task-oriented coordinates,
that is, the two problems (coordinates transformation and generation
of motor command) are simultaneously solved. Previously, we proposed
that some areas of the sensory association cortex (areas 5 and 7) con­
stitute the locus of this computation by an iterative learning algorithm
(Kawato, Isobe, Maeda, and Suzuki 1988). That is, the motor command
is not determined immediately, but in a stepwise, trial-and-error fashion
over the course of a set of repetitions. In this motor learning situa­
tion, short-term memory of the time history of trajectory and torque
are required. We successfully applied this learning scheme to trajec­
tory control of the PUMA manipulator in the visual coordinates of two
position sensor head cameras (Isobe, Kawato, and Suzuki 1988).
Further, in the uppermost line of figure 9.1, the motor command is cal­
culated directly from the goal of movement, that is, the three problems
(trajectory determination, coordinates transformation and generation
of motor command) are simultaneously solved. The minimum torque
change criterion and the repetitively structured cascade neural network
model for trajectory formation, which I will propose in the first half of
this chapter, correspond to the uppermost line of information processing
On the other hand, the feedback error learning scheme, which will be
explained in the latter half of this chapter, provides a computational
scheme for the final step of the bottom line in figure 9.1 (i.e. generation
Ill-posed Problems in Sensory-motor Control
A problem is well posed when its solution exists, is unique and depends
continuously on the initial data. III posed problems fail to satisfy one or
more of these criteria. Most motor control problems are ill posed in the
We list three ill-posed control problems in figure 9.2. First, consider
the trajectory determination problem for human planar, two-joint arm
movement, when the starting, intermediate and end points, as well as
the movement time, are specified (figure 9.2, top) . There is an infinite
number of possible trajectories satisfying these conditions. Thus, the
solution is not unique and the problem is ill posed.
The second ill-posed problem is the inverse kinematics problem in a
redundant manipulator with excess degrees of freedom. For example,
consider a three-degre� !M8tQm}lttor in a plane (figure 9.2,
Inverse Kinematics in Redundant Manipulator
Inverse Dynamics in Redundant Manipulator
Three ill-posed problems in sensory-motor control.
Schemes and Models for Control of Arm Trajectory
middle). The inverse kinematics problem is to determine the three joint
angles (three degrees of freedom) when the hand position in the Carte­
sian coordinates (two degrees of freedom) is given. Because of the re­
dundancy, even when the time course of the hand position is strictly de­
termined, the time course of the three joint angles cannot be determined
uniquely. We note that human hands have excess degrees of freedom.
The third ill-posed motor control problem is the inverse dynamics
problem in a manipulator with agonist and antagonist muscles (actua­
tors). Consider a single joint manipulator with a pair of muscles (figure
9.2 bottom). The inverse dynamics problem is to determine time courses
of tensions of agonist and antagonist muscles when the joint angle time
course is determined. Even when the time course of the joint angle is
specified, there are an infinite number of tension waveforms of the two
muscles which realize the same joint angle time course, as indicated by
solid and broken curves in figure 9.2 bottom.
To resolve the ill-posedness of these problems, we need to introduce
some performance index other than the above conditions which specify
movement pattern. We will propose such an objective function in the
next section. This kind of objective function is still not a sufficient condi­
tion for resolving the ill-posedness. Computational schemes, algorithms
and neural network models must be compatible with the objective func­
tion so that the objective function is naturally embedded into them. It is
worthwhile to evaluate computational schemes and neural network mod­
els on the basis of whether they can cope with the ill-posedness inherent
Formation of Human Multijoint Arm Trajectory
Early studies of motor control concentrated on single-joint arm move­
ments. Whereas, recently, several studies have heen reported regard­
ing the kinematic and dynamic aspects of multijoint arm movements.
Morasso (1981) provided experimental data which suggests that the de­
sired trajectory is first planned at the task-oriented (visual) coordinates.
He measured human two-joint arm movements restricted to an horizon­
tal plane, and found the following common invariant kinematic features.
When a subject was instructed merely to move his hand from one visual
target to another, his hand usually moved along a roughly straight path
with a bell-shaped speed profile. Morasso also reported that, in contrast
to the simple hand pro%b�il¥M�ns and velocity profiles of
the two joints (shoulder and elbow) were widely different according to
the parts of the workspace in which movements were performed. Abend,
Bizzi and Morasso (1982) investigated not only straight paths but also
curved paths. In contrast to the point-to-point movements, when the
subject was instructed to move his hand while avoiding an obstacle or
along a self-generated curved path, the hand path appeared to be com­
posed of a series of gently curved segments and the speed profile often
In order to account for these kinematic features, Flash and Hogan
(1985) proposed a mathematical model, the minimum jerk model. They
proposed that the trajectory, followed by the subject's arms, tended to
minimize the following quadratic measure of performance: the integral
of the square of the jerk (rate of change of acceleration) of the hand
position ( x, y), integrated over the entire movement.
Flash and Hogan showed that the unique trajectory which yields the best
performance was in good agreement with experimental data in one region
of the workspace, that is, the region just in front of the body. Their
analysis was based solely on the kinematics of movement, independent
of the dynamics of the musculoskeletal system, and was successful only
when formulated in terms of the motion of the hand in extracorporal
Based on the idea that the objective function must be related to the
dynamics, Uno, Kawato and Suzuki (1987) proposed the following alter­
here, Ti is the torque fed to the i-th actuator out of n actuators. The
objective function is the sum of the square of the rate of change of
the torque, integrated over the entire movement. One can easily see
that the two objective functions, CJ and CT, are closely related. How­
ever, it must be emphasized that the objective function CT critically
depends on the dynamics of the musculoskeletal system. Due to this
fact, it is much more difficult to determine the unique trajectory which
minimizes CT. Uno et al. (1987) overcame this difficulty by developing
an iterative scheme, so that the unique trajectory and the associated
motor command (tor�y��**wiatined simultaneously. In this
Schemes and Models for Control of Arm Trajectory
work, locations of the end point and the intermediate point were given
in the task-oriented Cartesian coordinates instead of the body coordi­
nates. That is, the three above-mentioned problems-trajectory for­
mation, coordinate transformation and generation of motor command,
can be solved simultaneously using this algorithm. Mathematically, the
iterative-learning scheme can be regarded as a Newton-like method in a
Trajectories derived from the minimum torque change model are quite
different from those of the minimum jerk model under the following be­
havioral situations: (i) excessive horizontal free movement between two
targets; (ii) constrained and horizontal movement between two targets;
(iii) vertical arm movement between two targets (see experimental data
of Atkeson and Hollerbach 1985); (iv) free and horizontal movement via
a point. Uno, Kawato, and Suzuki (1989) recently examined human
arm trajectories under these conditions and found that the minimum
torque change model reproduced these experimental data better than
the minimum jerk model. Here, only the fourth case is explained. Fig­
ure 9.3 shows prediction by the minimum torque change model (A) and
experimental data (B) of Uno, Kawato, and Suzuki (1989). For hori­
zontal arm movements which had to travel between two targets passing
through a specified point (a via-point), both the minimum jerk model
and the minimum torque change model predicted curved hand paths
with single-peaked or double-peaked speed profiles. It depended on the
location of the via-point whether the hand speed profile had a single peak
or two peaks. In both the models, if the via-point was located near to
the line connecting the initial and the final targets, the hand speed pro­
files were single peaked; on the other hand, if the via-point was located
further away from the line connecting two targets, highly curved move­
ments were produced and the hand speed profiles were double-peaked.
Furthermore, according as the curvature of hand path became larger,
the valley in the double-peaked speed profile tended to be deeper. In
this case, the peak in the path curvature was temporally associated with
the valley in the speed profile (see figure 9.3A(c».
However, when the via-point was located at a certain distance from
the line connecting two targets, the two models predicted quite different
trajectories. Consider two subcases, with identical start and end points,
but with mirror-image via-points (see figure 9.3A(a». That is, the start
point T3 and the end point T5 are the same for these two subcases, but
the two via-points PI and P2 are located symmetrically with respect
to the line connecting the common start and end points. Here, the via­
point PI is located furt &flpy#iwtJtedrMatBNti:ly than the line T3T5 and
Free movements passing through a via-point, PI or P2, PI and P2 are located
symmetrically with respect to the line connecting T3 and T5.
Schemes and Models for Control of Arm Trajectory
the via-point P2 is located nearer to the body than the line T3T5 as
If one notices invariance of the criterion function CJ under transla­
tion, rotation and turning up, it is easy to see that the minimum jerk
model predicts identical paths (with respect to turning up) and identical
speed profiles for the two subcases. On the other hand, the minimum
torque change model predicted two different shapes of trajectories cor­
responding to the two subcasesj for the movement passing through the
via-point PI, a convex curved path was formed and the associated speed
profile had only one peak (figure 9.3A(b»j in contrast, for the movement
passing through the via-point P2, a concave path was formed and the as­
sociated speed profile had two peaks (figure 9.3A(c»). Furthermore, the
convex path (T3 -> PI -> T5) and the concave path (T3 -> P2 -> T5)
were not symmetric with respect to the line T3T5.
In short, the trajectory derived from the minimum jerk model is de­
termined only by the geometric relation among the initial, final and
intermediate points, whereas the trajectory derived from the minimum
torque change model depends not only on the relation among these three
points but also on the arm posture (in other words, the relative location
Uno, Kawato, and Suzuki (1989) examined the same free movements
passing through a via-point in human subjects. Specifying via-points
PI and P2 as shown in figure 9.3B, Uno, Kawato, and Suzuki (1989)
measured via-point movements, T3 -> PI -> T5 and T3 -> P2 -> T5.
The convex path movements (T3 -> PI -> T5) were quite different from
the concave path movements (T3 -> P2 -> T5). In particular, the speed
profiles of the former were single peaked as shown in figure 9.3B(b),
while those of the latter were double peaked as shown in figure 9.3B(c).
These experimental results were consistent with the predictions of the
Repetitively Structured, Time-Invariant Cascade
Neural Network Model for Vector Field of Dynamics
Since the dynamics of the human arm or the robotic manipulator is
nonlinear, the task of finding a unique trajectory which minimizes CT
is a nonlinear optimization problem. The central nervous system does
not seem to adopt the iterative algorithm which we proposed in Uno,
Kawato, and Suzuki (1987). It was reported that some neural network
models can solve diffic1fiD�i.Ma�ems such as the traveling
salesman problem or early visions by minimizing "energy" through the
Previously, we proposed a neural network model
which automatically generates a torque to minimize
This network can be regarded as one example of autonomous motor
pattern generators such as a neural oscillator for rhythmic movements.
propose a repetitively structured, time-invariant, cascade neu­
imum torque change criterion, which is an extension of our previous
neural network model. It can learn the vector field of the ordinary dif­
ferential equation which describes the forward
object. The model consists of many identical four-layer network units
which are connected in a cascade formation through a unit weight serial
connection from the fourth layer of the k-th network unit to the first
layer of the k + I-th network unit, and a unit weight connection bypath
from the fourth layer of the k-th network unit to the fourth layer of the
The network unit consists of four layers of neurons.
represents the time course of the trajectory and the torque. The third
layer represents the change of the trajectory within a unit of time, that
is expected to provide necessary nonlinear transformations for learning
the vector field multiplied by the unit of time. The fourth layer and the
output line on the right side represent the estimated time course of the
We divide operations of this network into the learning phase
and the pattern-generating phase ( figure
this network acquires an internal model of a vector field of forward dy­
namics of the controlled object between the first and the third layers us­
monitoring the actual trajectory from the
a teaching signal. In the pattern-generating phase,
electrical coupling between neighboring neurons in the first layer is ac­
Then the network changes its state autonomously
through feedforward synaptic connections and feedback propagation of
error signals. The stable equilibrium state of the network corresponds to
the minimum energy state and, hence, the network outputs the torque
which realizes the minimum torque change trajectory.
let us consider a two-joint manipulator in a plane. The
hand position is represented in the Cartesian coordinates
Schemes and Models for Control of Arm Trajectory
A repetitive neural network model learns and minimizes energy for generation of
torque waveforms which realize the minimum torque change arm trajectory. Energy
A rep eti tive neural network model learns and minimizes energy for generation of
torque waveforms which realize the minimum torque change arm trajectory. Energy
Schemes and Models for Control of Arm Trajectory
by the following ordinary differential equations.
Here, Tdt) and T2(t ) represent torque waveforms fed to the shoulder and
the elbow. f and 9 are nonlinear functions which determine nonlinear
dynamics and kinematics of the manipulator. The time t is digitized
with the increment ilt. The k-th network unit corresponds to the k­
th time point, kilt. Through supervised learning, each network unit
acquires a model of the vector field of the forward kinematics and dy­
namics equation, that is the right side of equations 9.3-9.6, multiplied
by the time increment ilt. Since the vector field is time invariant, all
the network units are identical. That is, the number of neurons and the
synaptic weights are exactly the same for all units. Operation of each
neuron is assumed as linear weighted summation of synaptic inputs and
the sigmoid nonlinear transformation. The input-output transformation
of the neurons in the first and the fourth-layers is assumed linear. The
fourth-layer neurons in the k-th unit output the estimated trajectory,
x(kilt), y(kilt), x(kilt), i;(kilt) at time kilt, which are the summation
of their two synaptic inputs, i.e., the outputs of the third layer in the
k-th unit and outputs from the fourth layer neurons in the (k-l)st unit.
The first four neurons in the first layer of the first unit represent the ini­
tial hand position X(O), y(O), i(O), y(O). In short, the cascade formation
of the network constitutes a hardware implementation of Euler's method
of the numerical integration of the ordinary differential equations.
Because the model faithfully reproduces the time structure of the dy­
namical system, the following intrinsic properties of the flow of the dy­
namical system are embedded into the cascade structure of the model.
(i) continuity of the solution of the dynamical system with respect to
the time and the initial condition; ( ii ) group property of the flow 1jJ:
1jJ(t + SiX) 1jJ(t; 1jJ(s; x )) ; (iii) causality: trajectory at some time does
not depend on the torque input after that time.
In the learning phase (figure 9.4), the common input torque are fed to
both the controlled object and the neural network model. The realized
trajectory from the controlled object is used as a teaching signal to
acquire the forward model of the manipulator between the first and
the third layers of the {{€JfiY6fgJ1mdtllleepest
introduced for learning synaptic weights in the network unit, with the
following output error function E for the output of the fourth layer.
Here, c represents the pair of input torque and the resulting trajec­
tory. X denotes the vector (x, y, X, iJ)T which represents the trajectory
estimated by the neural network model. X represents the trajectory re­
alized by the controlled object. The backpropagation learning algorithm
(Rumelhart, Hinton, and Williams 1986) can be extended to this case
as follows. The error signal 6t,k of the i-th neuron in the fourth layer of
Il tion of the error between the realized
trajectory and the estimated trajectory at time kat, the error signal of
the i-th neuron in the fourth layer of the (k + 1 )-st network unit and the
error signal of the i-th neuron in the first layer of the (k + 1)-st network
The error signal of neurons in the third layer is the same as that of the
fourth layer. The error signals of neurons in the first and the second
layers are computed from those in the thirrd layer as in the usual back
propagation learning rule. After the incremental changes of synaptic
weights between the first and the second layers, and between the second
and the third layers are calculated independently in every network unit,
all changes for the corresponding synaptic weight are summed to be the
net change. This procedure guarantees identity of all network units.
In the pattern-generating phase (figure 9.5), electrical couplings be­
tween neurons representing torques in the first layer of the neighboring
units are activated while torque inputs to the first layer and the teaching
signal to the fourth layer are suppressed. Instead, the central commands
which specify the desired end point, the desired via point and the lo­
cations of obstacles to be avoided are given to the fourth layer from
the higher motor center which receives the necessary visual information
for the trajectory specification. Locations of the end point, the inter­
mediate point and the obstacle are given in the task-oriented Cartesian
coordinates to the fourth layer of network units.
The error signals for all neurons are calculated from equation 6.6 ex­
actly the same as in�tmtgA1it.1 while replacing the realized
Schemes and Models for Control of Arm Trajectory
trajectory as the teaching signal by the desired end point, the via-point,
obstacles and so on, as the objective signal. The error signals are not
used for synaptic modification. But, the error signals to the torque neu­
rons in the first layer are used for dynamical state change of the model.
That is, the network changes its state autonomously by feedforward
synaptic connections and by error signals backpropagated through the
cascade structure while obeying the following differential equation.
Here, s is the time of neuron state change and has nothing to do with
the movement time t. m represents the membrane potential and the
output signal of the neuron in the first layer, and 9 denotes the electrical
conductance of the gap junction. We note that calculation of the error
signal of le at every instant s in equation 6.9 requires both the feedforward
calculation of every neuron state through the entire network with learned
synaptic weights, and the feedback calculation of the error signal by
This procedure is equivalent to impose the potential function which
corresponds to the conditions of the movement. In the case of obstacle
avoidance, we impose a quadratic potential function which is positive
inside the location of the obstacle and zero outside the obstacle. In the
case of the intermediate point and the end point, the quadratic potential
is minimum at the site of these points. So, the potential of the obstacle
is inverted in comparison with the potential of the end and intermediate
points. However, the calculation of error signals is exactly the same in
It can be shown that the network dynamics has the following Liapunov
{Xd(N�t) - X(N�t)}2 + 9 LL)mh - m;,Ie_l)2
The first term of the Liapunov function requires that the hand reaches
the end point and the second term guarantees the minimum torque
change criterion. The stable equilibrium point of the network dynamics
corresponds to the minimum energy state, and, hence, the network out­
puts the torque, which realizes the minimum torque change trajectory.
The conductance 9 is slowly decreased during relaxation of the state
point to the equilibrium similarly to the temperature in "simulated anCopyrighted Material
The performance of the proposed network model was examined in
computer simulation experiments using a two-joint manipulator model
of human arm as a controlled object (Maeda et al.
al. have successfully shown that the network produced a good approxi­
mate trajectory on minimum torque change criterion for the movements
between pairs of target, movements where the intermediate point is spec­
ified, and movements where obstacles must be avoided. In the via-point
movement case, we did not specify the time when the via-point must be
passed. Instead, the network model autonomously finds the best time
to pass the via-point on the minimum torque change criterion. In the
obstacle avoidance case, we specified only the location of the obstacle in
the task-oriented Cartesian coordinates. Because locations of end points,
via-points and obstacles were given in the task-oriented Cartesian coor­
dinates in this study, the neural network model solved the problem of
inverse kinematics as well as the problems of trajectory formation and
The first term of the Liapunov function is the data constraint imposed
by trajectory specification, and the second term is the smoothness con­
straint. Introduction of the second energy as electrical couplings resolves
the ill-posedness of trajectory formation. The network learns the first
term of energy as synaptic weights, and then minimizes the total energy.
This model has several conceptual similarities with the motor program
subnetwork conjoined with the forward model subnetwork proposed by
Jordan showed computer simulation of trajectory forma­
tion for a one-joint arm based on minimum jerk model. He noted that it
is straightforward to implement the minimum torque change criterion in
his model. Our model and Jordan's model both uses the forward model
of the controlled object. The idea to resolve the ill-posedness of motor
control by introducing smoothness constraint such as the minimum jerk
criterion or the minimum torque change criterion is also common.
The essential difference of the two models is in the method of satisfying
the smoothness constraint. In our model, relaxation of state vectors of
the model (energy minimization) is utilized for attaining the smoothness
Because of this, torque values at different time must be
independently represented in the model, and hence time is represented
spatially and the objective function is guaranteed by electrical coupling.
On the other hand, in Jordan's model, time is represented as time in
a recurrent network and the smoothness constraint is guaranteed by
an error term based on comparing the activations of units at adjacent
moments in time. In this sense, the smoothness constraint is embedded
in synaptic weights of(fbpydghte�network through learning.
Schemes and Models for Control of Arm Trajectory
One of the advantages of Jordan's model is that it can generate a
trajectory in real time once it learns the forward model of the controlled
object and the smoothness constraint. On the other hand, in our model,
the relaxation time ( typically hundreds of iteration ) is required for tra­
We recently found that multigrid, multiresolution
extension of our network can calculate the trajectory
an order of magnitude faster than the original model. Research in this
direction might be able to overcome the shortcoming of our model.
One advantage of our model is that it can generate any trajectory
regardless of locations of the end point, the intermediate points and ob­
stacles to be avoided once it learns the forward model of the controlled
object. On the other hand, in Jordan's model, the motor learning sub­
network must learn many instances of trajectories with various locations
of the end points, the intermediate points and obstacles so that it can
showed that his model can resolve the ill­
posedness in the inverse kinematics, the inverse dynamics and the tra­
We also note that the proposed repetitive network
model can not only resolve the ill-posedness in the trajectory determina­
tion problem but can also resolve the ill-posedness in inverse kinematics
and inverse dynamics problems for redundant manipulators (figure
middle and bottom) . In these situations, the desired trajectory in the
task-oriented coordinates must be fed to the neurons in the fourth layer
of all units as the objective signal in the pattern-generating phase. If we
introduce the electrical junctions between the neurons in the third layer
representing change of the velocity instead of the first layer, the network
can generate torque which realizes the minimum jerk trajectory.
Examination of human multijoint arm movement in the three­
dimensional space while avoiding obstacles, and comparison of the ex­
perimental data with predictions of the present model are our future
Hierarchical Neural Network Model for Control
proposed that the cerebrocerebellar communication loop is
a reference model for the open-loop control of voluntary move­
which accounts for the functional roles of several brain regions in the con­
A hierarchical neural network model for control and learning of voluntary
theoretical model of the cerebre>-cerebelle>-rubral learning system based
on recent experimental findings of the synaptic plasticity. Expanding
these previous models and the adaptive filter model of the cerebellum
( Fujita 1982), we proposed a neural network model for the control and
learning of voluntary movement (Kawato, Furukawa, and Suzuki 1987).
In our model shown in figure 9.6, the association cortex sends the
desired movement pattern (Jd, expressed in the body coordinates, to the
motor cortex where the motor command T, i.e., the torque to be gen­
erated by muscles, is then somehow computed. The actual movement
pattern (J is measured by proprioceptors and returned to the motor cor­
tex via the transcortical loop. Then, feedback control can be performed
utilizing error in the movem!'lnt traj� t.ol1 ' However, both feedback de­
lays and small gains liWfnJm9� �peeds of motions.
Schemes and Models for Control of Arm Trajectory
The cerebrocerebellum-parvocellular part of the red nucleus system re­
ceives synaptic inputs from wide areas of the cerebral cortex but does not
receive peripheral sensory input. That is, it monitors both the desired
trajectory and the motor command but it does not receive information
about the actual movement. Within the cerebrocerebellum-parvocellular
red nucleus system, an internal neural model of the inverse dynamics of
the musculoskeletal system is acquired. The inverse-dynamics of the
musculoskeletal system is defined as the nonlinear system whose input
and output are inverted (trajectory is the input and motor command
is the output) . Once the inverse-dynamics model is acquired by mo­
tor learning, it can compute an appropriate motor command Ti directly
Several Computational Approaches for Learning of
Inverse Dynamics Model of a Controlled Object
In this section we compare several computational approaches for learn­
ing of inverse-dynamics model of a controlled object. Figure 9.7 shows
three different approaches to acquire the inverse dynamics model. In this
figure, the block named "inverse dynamics model" is just a black box
with its input and output. One can realize it by many kinds of neural
network models such as the feedforward multilayer network (multilayer
perceptron), the prepared nonlinearity network, the simple table lookup
method, CMAC, the nearest neighborhood search with content address­
able memories, etc. In this section, we do not mind what kind of neural
network model constitutes the inverse-dynamics model. Our purpose is
to compare properties of different computational approaches to learn the
inverse dynaInics model, which are somewhat independent of types of
neural network model which actually constitutes the inverse dynamics
The simplest approach for acquiring the inverse dynanlics model of
a controlled object is shown in figure 9.7a. The manipulator receives
the torque input T(t) and outputs the resulting trajectory OCt). The
inverse dynamics model is set in the opposite input-output direction to
that of the manipulator, as shown by the arrow. That is, it receives the
trajectory as an input and outputs the torque Ti(t). The error signal
set) is given as the difference between the real torque and the estimated
T(t) - Ti(t). This approach to acquire an inverse dy­
namics model is referred to as direct inverse modeling by Jordan (1988).
Direct inverse modeling��_!MatdriBJed by Albus (1975), Miller
Three computational schemes for learning inverse- dynamics model of a controlled
object. (a) Direct inverse modeling. (b) Forward and inverse modeling. (c)
(1987), Miller, Glanz, and Kraft (1987), Kuperstein (1988), and Atke­
son and Reinkensmeyer (1988). However, in Atkeson and Reinkensmeyer
(1988), the content-addressable memories (CAM) are used as the inverse
dynamics model, and hence the learning can be accomplished within one
triaL In this sense, the error signal needs not be calculated in their case.
Furthermore, the CAM has no direction built in, and hence it can use
the same data for both the forward and inverse dynamics.
Figure 9.7b shows the scheme of combining a forward model and the
inverse model, proposed by Jordan and Rumelhart (see Jordan 1988;
Jordan and Rosenbaum 1988). First, the forward model of the controlled
object is learned by monitoring the input Ti(t) and the output (J(t) of
the controlled object. Next, the desired trajectory (Jd(t) is fed to the
inverse model to calculate the feedforward motor command Ti(t). The
resulting error in the trajectory (Jd(t) - (J(t) is back propagated through
the forward model to calculate the error in the motor command, which
is then used as the error signal for training the inverse model.
Figure 9.7c shows the alternative computational approach which we
proposed and termed, feedback error learning (Kawato, Furukawa, and
Suzuki 1987). This block diagram includes the motor cortex (feedback
gain K and summation of feedback and feedforward commands), the
transcortical loop (negative feedback loop) and the cerebrocerebellum­
parvocellular red nucleus system (inverse-dynamics model). The total
torque T(t) fed to an actuator of the manipulator is the sum of the feed­
back torque Tf(t) and the feedforward torque Ti(t), which is calculated
by the inverse dynamics model. The inverse-dynamics model receives
the desired trajectory (Jd represented in the body coordinates, such as
the joint angles or muscle lengths, and monitors the feedback torque
Tf(t) as the error signaL It is expected that the feedback signal tends to
zero as learning proceeds. This has been proved recently. We call this
learning scheme feedback error learning, emphasizing the importance of
using the feedback torque (motor command) as the error signal of the
We will compare the three computational schemes for acquiring the
inverse model of the controlled object (see table 9.1 for reference).
The direct inverse modeling does not seem to be used in the central
nervous system because of the following reasons. First, after the in­
verse dynamics model is acquired, large-scale connection changes must
be carried out before it can be input from the desired trajectory instead
of the actual trajectory, while preserving the minute one-to-one corre­
spondence, so that it can be used in feedforward control. So, other su­
pervising neural networ�@:deMiltli€iWhen the connection change
Comparisons of the three computational schemes for acquiring the internal inverse
should be done are required. Because this method separates the learning
and control modes, it is not easy to cope with the dynamics change of
a controlled object- From an engineering point of view, memory copy
from the learning hardware, which receives the realized trajectory, to the
control hardware, which receives the desired trajectory, is more conve­
nient and efficient, and was utilized by Miller (1987) and Miller, Glanz,
and Kraft (1987). But memory copy seems unnatural in the biological
the memory copy scheme is utilized instead of the connec­
tion change, the direct inverse modeling approach can cope with the
dynamic change of the controlled object in real time since learning and
control can be performed simultaneously. In summary, the direct inverse
modeling only requires "separated" learning and control with regard to
In engineering application, one drawback of the direct inverse model­
ing approach seems to be that it does not necessarily achieve a particular
target trajectory Od(t}, even when the training period is sufficiently long.
Let us suppose that the control objective is to realize
Schemes and Models for Control of Arm Trajectory
some desired trajectory (Jd(t) by using the direct inverse modeling and
the table lookup-type learning network. Initially, we need some fixed
feedforward controller and feedback controller for generating training
examples T(t) and B(t). If the control problem is difficult and / or the
initial controller is poor, the realized trajectory (J(t) might be outside
the generalization range ( e.g. , range of CMAC characteristic function )
of the desired trajectory Bd (t) . Then, the memory content of the mo­
tor command at the address of the desired trajectory is not refreshed.
Consequently, even if we apply the desired trajectory to the network
many times, the control performance does not improve at all. The near­
est neighborhood search utilized by Atkeson and Reinkensmeyer (1988)
also suffered from the "stuck state" phenomena after several iterations
for the particular target trajectory. The stuck state might be explained
as follows. Let the pair (B N N , TN N ) be the nonempty memory content
which is the nearest neighbor of the desired trajectory point Bd . Be­
cause the dynamics of the controlled object depends on the past history
of the trajectory and the effect of the feedback controller on the motor
command, the actual trajectory point Ba may be further from (Jd than
BNN . Then, for this trial, only the memory content for Ba is refreshed
as the summation of TNN and the feedback motor command. Ba is not
used to generate the next motor command, since (JNN is closer. In the
next trial exactly the same thing happens, and the memory content is
not changed. It will be most interesting to see in the future whether
combinations of multilayer networks, which are expected to have more
generalization capability than table lookup methods, will exhibit goal
directed learning properties with direct inverse modeling.
The forward and inverse modeling approach of Jordan and Rumelhart
is goal directed because the error for learning is defined as the square of
the difference between the desired trajectory and the realized trajectory.
The feedback error learning approach is also goal-directed even if we
use a fine-grained table lookup learning network. The iterative perfor­
mance of feedback error learning with a simple table lookup memory for a
single desired trajectory presented repeatedly is similar to our Newton­
like method in iterative learning control ( Kawato, Isobe, Maeda, and
Suzuki 1988). The feedforward motor command T;t (t) calculated by
the inverse dynamics model in the k + 1-st iteration of the single target
trajectory is the summation of the feed forward torque Tj, (t) and the
here, Kp and Kd are feedback gains of the proportional term and the
differential term, respectively. The upper suffix k indicates the number
of the iteration. Because the above equation for modification of the feed­
forward motor command does not contain the term which is proportional
to the error in the acceleration of trajectory, we cannot mathematically
prove the exponential convergence rate for the feedback error learning
approach, which was guaranteed for the Newton-like method ( Kawato,
Isobe, Maeda and Suzuki 1 988) . However, it is intuitively clear that the
above scheme converges albeit very slowly if the feedback gains are small
The direct inverse modeling method can not cope with the second
and the third ill-posed problems shown in figure 9.2, unless some spe­
cific mechanisms, such as specifying some joint angles or mechanical
impedance, are introduced. Jordan (1988) explained this reason in the
one to many inverse kinematics problem associated with motor control
of redundant manipulators with excess degrees of freedom. The forward
and inverse modeling approach can resolve the ill-posedness by learn­
ing some performance index as synaptic weights in the inverse model.
The feedback controller selects one specific motor command even in the
inverse dynamics and inverse kinematics problems for redundant ma­
nipulators. But the desired trajectory can not be exactly realized only
by the feedback control. The feedback error learning approach can re­
solve the ill-posedness in the second and the third problems shown in
figure 9.2 because of the above-mentioned good characteristics inherent
in the feedback controller. We showed that the feedback error learning
network can accurately realize the desired trajectory in the Cartesian
coordinates for a three-link manipulator in a plane (Kano, K awato, and
Suzuki 1989) . The selected joint angle time courses depended on the ini­
tial posture of the manipulator and the gains of the feedback controller.
The feedback error learning scheme does not require the teaching sig­
nal or the desired output for the neural network controller. Instead, the
feedback torque is used as the error signal. The control and learning are
performed simultaneously. Furthermore, backpropagation of the error
signal through the controlled object or through a forward model of the
controlled object is not necessary. The difficulty of the feedback error
learning approach in its application to the inverse kinematics problem
is that it requires a sophisticated feedback controller with such as the
transpose of the Jacobian or the pseudoinverse of the Jacobian of the
forward kinematics for generating the feedback motor command.
Above comparisons of different computational schemes and those
shown in table 9 . 1 w�@t��/ on sparse experimental data
Schemes and Models for Control of Arm Trajectory
and a small number of literatures within a very short period. So, we
do not intend to draw any conclusions from the comparisons. What
we would like to emphasize is that we have choices from several dif­
ferent computational approaches for learning an inverse model of the
controlled object other than the problem regarding what kind of neural
networks should be used for the inverse model itself, such as the sim­
ple table lookup method, CMAC, the nearest neighborhood search with
content- addressable memories, etc. It will be very important to con­
tinue systematic comparisons of the different computational approaches
in combination with different types of learning networks.
Manipulator Control Experiment by Feedback
There are two possibilities regarding how the central nervous system
computes nonlinear transformations required for an inverse dynamics
model of a nonlinear controlled object. One is that they are computed
by nonlinear information processing within the dendrites of neurons
(Kawato et al. 1 984; Kawato, Furukawa, and Suzuki 1987; Miyamoto et
al. 1988) . The other is that they are realized by neural circuits, and are
acquired by motor learning (Kawato, Setoyama, and Suzuki 1988) .
Examining the first possibility, we have successfully applied the feed­
back error learning neural network to trajectory control of an indus­
trial robotic manipulator (Kawasaki-Unimate PUMA 260) with pre­
pared nonlinear transformations derived from a dynamics equation of
a manipulator-idealized mechanical model (Miyamoto et al. 1988) . A
simple training movement pattern lasting for 6s was elicited three hun­
dred times. Both the error of the trajectory and the feedback torque
decreased dramatically during thirty minutes of learning. Moreover, the
effect of learning for faster and quite different movement patterns from
the training pattern was noted; that is, the network showed the ability
Regarding the second possibility, we succeeded in obtaining learn­
ing control of the robotic manipulator by an inverse dynamics model
formed by a three-layer neural network shown in figure 9.8 ( Kawato,
Setoyama, and Suzuki 1988). In this network, nonlinear transformation
was made only of a cascade of linear-weighted summation and sigmoid
nonlinearity. That is, we did not use any a priori knowledge about the
dynamical structure of the controlled object. In this particular task,
we continued to use th£Op�ltfatedahmand as the error signal.
:- - - - - - - - -hidd�� -I;'Y� r- - - - · - - - - :
A feedback error learning neural network model. The inverse dynamics model is
acquired in the three-layer neural network.
Neurons in t he input layer represent desired joint angles, angular ve­
locities, and angular accelerations. The input-o utput transformation
function in the input layer is assumed linear. The neurons in the output
layer represent the feedfor ward motor command for different actuators.
, Wj k ' Wkm represent t he memb rane potent°al
and the ou tput of the j -th neuron in the input layer, membrane potential
and the o ut put of the k-th neuron in the hidden layer, the membrane
potential and the output of the m-th neuron in the output layer, the
synaptic weight from the j-th neuron in the input layer to the k-th neu­
ron in the hidden layer , and the synaptic weight from the k-th neuron
in the hidden layer to the m-th neuron in the output layer, respectively.
The following equations hold for inputs and outputs of the neurons.
Schemes and Models for Control of Arm Trajectory
Here, f is the sigmoid function. The synaptic weight between the hid­
den layer and the output layer is modified according to the following
heterosynaptic learning rule while using the feedback motor command
Tfm for the m-th degree of freedom as the error signal.
here, 'T is the time constant of the synaptic modification. The error
signal of the k-th neuron in the hidden layer is calculated as the weighted
summation of the responsible feedback motor commands by the synaptic
The rate of change of the synaptic weight from the input layer to the
hidden layer is again given by the product of the input to the hidden­
The motor command fed to the m-th degree of freedom of the controlled
object is the sum of the feedforward torque and the feedback torque.
In this study, the learning task proceeded smoothly and the network
showed the capability of some degree of generalization as shown in figure
9.9 ( Setoyama, Kawato, and Suzuki 1988) . Figure 9.9 shows mean square
errors of the three joint angles for the two training patterns represented
by the two solid curves, and errors for a twofold faster test movement
pattern shown by the one point chain curve, and a slower test pattern
Change of the mean square error of the three joint angles during 90 minutes
learning. Cited from Setoyama, Kawato, and Suzuki ( 1 988 ) .
chain curve. The test movement patterns were
not used in t he training. The network performance for the test patterns
We recently obtained results that information representation of bid­
den units properly corresponds to the inertia force, the viscous friction
force, Co ulo mb frict ion force , and the gravitc:).tional force . However, the
Coriolis forces and the centripetal forces are
not easy to analyze their repres ent at ions .
of our future tasks. We now know that three-layer networks
combined with simulated annealing can learn arbitrary nonlinear func­
t ions . However, in t he futu.re, we must �xamine whether this statement
has any practical imfM'�WI MeI�iH4.d applications.
Schemes and Models for Control of Arm Trajectory
Abend, W. , Bizzi, E. and Morasso, P. ( 1 982 ) . Human arm trajectory
Albus, J. S. ( 1975 ) A new approach to manipulator control: The
cerebellar model articulation controller (CMAC) . Transactions of
the ASME Journal of Dynamic Systems, Measurement, and Control
Allen, G. L, and Tsukahara, N. ( 1 974) . Cerebrocerebellar communica­
tion systems. Physiological Reviews 54:957-1006.
Atkeson, C. G . , and Hollerbach, J. M. ( 1985 ) Kinematic features
of unrestrained vertical arm movements. Journal of Neuroscience
Atkeson, C. G . , and Reinkensmeyer, D. J . ( 1988 ) Using associative
content-addressable memories to control robots. In Proceedings of
the IEEE Conference on Decision and Control, 792-797. Austin,
Flash, T. and Hogan, N. ( 1985 ) . The coordination of arm movements:
An experimentally confirmed mathematical model. Journal of Neu­
Fujita, M. ( 1982 ) . Adaptive filter model of the cerebellum. Biological
Isobe, M., Kawato, M . , and Suzuki, S. ( 1988) . Iterative learning control
of industrial manipulator in joint-angular and visual coordinates.
Japan IEICE Technical Report. MBE87-134, 241-248.
Ito, M. ( 1970) . Neurophysiological aspects of the cerebellar motor con­
trol system, International Journal of Neurology 7:162-176.
Jordan, M. L ( 1988 ) . Supervised learning and systems with excess de­
grees of freedom, COINS Technical Report 88-27, 1 4 1 , Department
of Computer Science and Information, University of Massachusetts,
I. ( 1989) . Indeterminate motor skill learning problems. In
Jeannerod ed . , Attention and performance XIII. Hillsdale, NJ:
Ros enbaum, D. A. ( 1 988) . A ct ion COINS Technical
Report 88-26, 1-68, Department of Computer Science and Informa­
tion, University of M ass achusetts , Amherst .
Kawato, M . , and Suzuki, R. (1989) . Acquisition of inverse­
and i nverse-dynamics model of a redundant arm by the
feedback error learning. Japan IEICE Technical Rep ort MBE88171, 9 1-96.
tary movement by the central nervous system.
Furukawa, K . , and Suzuki, R. ( 1 987) .
network model for control and learning of voluntary movement,
Kawato, M . , Hamaguchi, T. , M ur akami , F. and Tsukahara, N . (1984) .
Quantitative analysis of electrical properties of dendritic spines, Bi­
Kawato, M., Isobe, M., Maeda, Y., and Suzuki, R. ( 1988) . Coordinates
transformation and learning control for visually-guided voluntary
movement with it er at ion : A Newton-like method in a function space,
Kawato, M . , Isobe , M. , and Suzuki, R. ( 1988). Hierarchical learning of
vo lunt ary movement by cerebellum and sensory association cortex.
In M. A. Arbib and S. Amari eds . , Dynamic interaction in neural
networks: Models and data, 195-2 14, New York : Springer-Verlag.
Kawato, M., Setoyama, T. and Suzuki, R. ( 1 988) . Feedback error learn­
ing of movement by multi layer neural network. In Proceedings of
the International Neural Networks Society First A nnual Meeting,
Schemes and Models for Control of Arm Trajectory
Kawato, M . , Uno, Y., Isobe, M . , and Suzuki, R. ( 1988 ) . A hierarchical
neural network model for voluntary movement with application to
robotics, IEEE Control Systems Magazine 8:8-16.
Kuperstein, M. ( 1 988 ) . Neural model of adaptive hand-eye coordination
for single postures. Science 239: 1308-131 1 .
Maeda, Y. , Kawato, M . , Uno, Y. , and Suzuki, R. ( 1989 ) Trajectory
formation for human multi-joint arm by a cascade neural network
model. Japan IEICE Technical Report MBE88-169 , 79-84.
Marr, D. ( 1982 ) . Vision. New York: W. H. Freeman.
Miller, W. T. ( 1987) . Sensor-based control of robotic manipulators us­
ing a general learning algorithm. IEEE Journal of Robotics and
Miller, W. T., G lanz F. H., and Kraft, L. G. ( 1987 ) . Application of a
general learning algorithm to the control of robotic manipulators.
International Journal of Robotics Research 6:84-98.
Miyamoto, H., Kawato, M . , Setoyama, T., and Suzuki, R. ( 1 988 ) .
Feedback-error-Iearning neural network for trajectory control of a
robotic manipulator, Neural Networks 1 :251-265.
Morassa, P. { 1981 } . Spatial control of arm movements. Experimental
Rumelhart, D. E., Hinton, G. E., and Williams, R. J . ( 1 986 ) . Learning
representations by back-propagating errors. Nature 323:533-536.
Setoyama, T. , Kawato, M., and Suzuki, R. ( 1988 ) . Manipulator control
by inverse-dynamics model learned in multi-layer neural network.
Japan IEICE Technical Report MBE87-135, 249-256.
Tsukahara, N. and Kawato, M. ( 1 982 ) Dynamic and plastic properties
of the brain stem neuronal networks as the possible neuronal basis
of learning and memory. In S. Amari and M. A. Arbib eds . , Lecture
Notes in Biomathematics. Vol. 45, Competition and Cooperation in
Neural Nets, 43Q--44 1 . New York: Springer-Verlag.
Uno, Y. , Kawato, M., and Suzuki, R. (1987) . Formation of optimum
trajectory in control of arm movement: minimum torque-change
model. Japan IEICE Technical Report MBE86-79 , 9-16 .
Uno, Y . , Kawato, M . , and Suzuki, R. (1 989) . Formation and control of
optimal trajectory in human multijoint arm movement: minimum
torque-change model. Biological Cybernetics 61:89-1 0 1 .
In comparing the current state of the art in robotics to the sensory-motor
control systems in animals, one difference seems particularly striking.
The difference is in the emphasis placed on sensory processes, particu­
larly vision. The core methods in robotics have typically reduced the
sensory representation of the workspace to the simple position and ori­
entation of the manipulator. Moreover, once this "sensory" state has
been encoded, such as to specify the desired final location of the ma­
nipulator, arm control computations often proceed blindly, i.e., without
further reference to the sensory input channel. Motion planning algo­
rithms capable of moving a multilink arm in the presence of obstacles
have typically finessed the sensory problem altogether, assuming com­
plete advance knowledge of the shapes, locations, and trajectories of all
obstacles in the environment. It seems odd that the role of vision should
be limited to such simple (or unexplained ) functions, since it is unlikely
that it will be possible to achieve true sophistication in robot control
in real environments ( i.e. , with moving targets and partially occluded,
nonpolyhedral objects ) without dealing squarely with sensory processing
The undervaluation of sensory processes in robotics seems particularly
anomalous when we look at the primate brain. First, the visual input
channel to the primate brain is approximately ten times "wider" than
the output interface to the musculature. In addition, visual processing
is by far the dominant modality in terms of sheer neural real estate,
although this is partly due to the fact that vision plays many different
roles in behavior other than the control of movement. Vision may in fact
be a more important and difficult problem in the control of movement
than the control of movement itself. The motor plant ( i.e., the body ) is
relatively well identified and unchanging through time, while the envi­
ronment on which the body must act changes from moment to moment
with the position and orientation of the body, head, and eyes, and the
layout of possibly moving targets and/or obstacles. This view further
illustrates the need for integration of the sensory and motor modalities
A more detailed discuss� of th�m�t!fWl!1)�lhapter can be found in Mel, B.
(forthcoming 1990). ConneklMlJ6fI!rM1Ml'lnMf3Wpmnning, Academic Press, Boston.
Neural networks have already been applied by a number of workers to
problems in the kinematic and dynamic control of robots, primarily for
their ability to learn nonlinear mappings from examples. Other benefits
of neural representation have been noted, including (i) fault tolerance
with respect to isolated synaptic or unit failures, and (ii) the sufficiency
of components with relatively poor dynamic range (Hinton 1986). More
germane to the preceding arguments, the massively parallel, imagelike
(Le., topographic) character of a neural representation in and of itself
can play at least two behaviorally relevant functional roles in sensory­
motor control. The first concerns the computation of local features
over the sensory epithelia. The vertebrate brain has a large number
of topographically mapped sensory areas devoted to visual, auditory,
and somatosensory processing. In primates, vision is dominant, with
well over a dozen topographically mapped visual representations. The
image like character is retained while making explicit, in the response
properties of single neurons, a broad spectrum of visual parameters,
including position, orientation, depth, color, direction and velocity of
movement, and complex textural information (Van Essen 1985). A great
deal of local feature extraction takes place within these (mostly cortical)
areas. The maintenance of topography has often been cited as a means
for allowing local features, such as center-surround or oriented intensity
discontinuities, to be computed as inexpensively as possible, i.e. with
only local circuitry (see Mel 1987, for review).
A second, higher-order role of topography in a neural representation
can be exemplified for the case of vision in motor control. The image­
like character of a topographically mapped visual representation makes
it ideal for capturing a rich array of visual information useful for a wide
range of common motor control tasks. For example, essentially all of
the information needed to direct the movements of an arm in a clut­
tered environment is available in relatively explicit form in an imagelike
representation of the visual field. This includes the positions of the
hand, arm, and body relative to the objects in the environment, as well
as the patterns of optic flow that specify the movements of the arm and
body in the visual field and the movements and relative movements of
other objects. From such representations, progress of the arm towards a
visually specified goal, as well as events such as target interception and
collisions with obstacles, can be computed as fast, parallel operations.
In the remainder of this chapter we review a recently developed neural
network system for vision-based robot motion planning, called MURPHY,
that shares a number of architectural features in common with sensory
and motor systems �BlliOOf£� _ritJJ89). MURPHY first builds a
visual-kinematic model of his arm through a combination of physical
and "mental" practice, and then uses simple heuristic search with men­
tal images of the arm to solve planar, visually guided reaching problems
in the presence of obstacles. The approach differs from previous work in
robot motion-planning primarily in the use of an explicit full-visual-field
representation of the workspace, upon which a variety of visual heuris­
tics relevant to the control of limb movements are easily computed. In
concluding sections, detailed comparisons are made to other, nonneural
MURPHY's algorithms and data structures constitute a new approach
to vision-based robot arm control. In order to ensure both represen­
tational and task realism to the greatest degree possible, MURPHY has
been implemented with a real camera and a real robot arm. The phys­
ical setup consists of a 512 x 512 JVC color video camera pointed at
a Rhino XR-3 robotic arm. Only the shoulder, elbow, and wrist joints
are used, such that the arm moves only in the image plane of the cam­
era at a distance of 7'. (A fourth, or waist joint, is not used). Figure
1O.lA shows MURPHY's view of his arm amidst a target (white cross
stuck to backdrop) and "obstacles" (other white paper cutouts). Figure
10.lB shows MURPHY after successfully "reaching" for the target, i.e.,
with hand visually superimposed with the target from the perspective of
the camera. A "collision" is said to occur when the visible (i.e., white­
spotted) parts of the arm, or their imaginary lines of interconnection,
fall into superposition with any of the backdrop obstacles.
White spots are attached to the arm in convenient places; when the
image is thresholded, only the white spots appear in the image. This
arrangement allows continuous control over the complexity of the visual
image of the arm, which in turn affects computation time during learning
(for a serial computer). A Datacube image processing system is used for
the thresholding operation and to "blur" the image in real time with
a gaussian mask. The degree of blur is variable and can be used to
control the degree of coarse-coding (Le., receptive field overlap) in the
camera-topic array of visual units (see figure 10.3). The arm is software.
controllable, with a stepper motor for each joint. Arm dynamics are not
dealt with in this work. Copyrighted Material
configuration. Target ( white cross ) and obstacles ( other white shapes ) are paper
cutouts stuck to backdrop 6" behind the plane of the arm's movement.
MURPHY 'S internal architecture has been based on the observation that
a surprisingly large fraction of even the most highly evolved brains is
devoted to the explicit representation of the animal's sensory and motor
state, exemplified by the several dozen sensory, higher sensory, and mo­
tor areas of the primate cerebral cortex (Merzenich 1980). During nor­
mal behavior, each of these neural representations carries behaviorally
relevant state information, some of them yoked to the sensory epithelia,
others to the motor system. The effect is a rich set of online associative
learning opportunities. Not surprisingly, these distinct unit populations
have the possibility of interacting bidirectionally through long range
interconnection pathways in the brain. Following a basic tenet of con­
nectionist computation, it is likely that these synpatic pathways are an
important locus for associative learning.
For the work presented here, MURPHY uses only two populations of
neuron-like units (figure 10.2), though up to four have been used in work
reported elsewhere (Mel 1989). The principal sensory representation is
the visual-field population, organized as a rectangular, cameratopically­
mapped 64 x 64 g�9fJ¥(i�attii.@IJ. visual units, each of which
MURPHY's Machine Architecture. Two populations of neuron like units implement
MURPHY's forward kinematic model. Visual input from camera enters at left,
responds when a visual feature ( i.e., a white spot on the arm ) falls into
its receptive field ( figures 10.2, 1O.3A). A coarse coded population is one
in which the units have response fields (i.e. receptive and / or projective
fields) that overlap among neighboring units, and whose peak sensitiv­
ities vary systematically from unit to unit across the population. This
type of representation is extremely common in biological sensory sys­
tems and has been attributed a number of representational advantages,
including finer stimulus discrimination and increased immunity to noise
and unit malfunction ( Ballard 1983, Hinton 1986, Erikson 1984, Baldi
and Heiligengerg 1988, Sejnowski 1986). Coarse coding has also been
shown to contribute to continuity-based generalization in connectionist
learning systems ( Ballard 1987, Walters 1987, Moody and Darken 1988,
Mel 1989). The primary motor representation is the joint-angle popu­
lation consisting of 373 units in three subpopulations; the angle of each
joint is value coded individually in a subpopulation dedicated to that
joint ( figures 10.2, 1O.3b). Each unit in the population is "centered"
at some joint angle, and is maximally activated when the joint is to
be sent to that angle. Neighboring joint units within a joint sub pop­
ulation have overlapping projective fields and progressively increasing
( ) Looking down on t e 64 x 64 map of visual-field units,
grey levels proportional to unit activity in response to the given view of the arm
abov ) . (b) Three joint-angle subpopulations encoding
During both his learning and performance phases to be described in sub­
sequent sections, MURPHY is able to carry out simple sequential opera­
tions driven by an unelaborated controller external to his connectionist
architecture (figure 10.2). Since these sequential control functions and
their "neural" implementations currently fall outside the scope of this
project, they have been compartmentalized and kept as simple as possi­
ble in order that the bulk of MURPHY's competence derive from portions
of the model that have been fully elaborated.
Drawing on the language of traditional kinematics, MURPHY builds a
"forward-kinematic" model of his arm by stepping his arm through a
small, uniform sample (approximately 17,000) of the 3.3 billion legal arm
configurations. At each step, MURPHY learns the mapping from joint an­
gles to states of his visual .field, bl. mo4ifying synaptic weights between
the joint-angle and �OO!lcP}96i4(ftfl@l:lDS (as discussed in the next
section). In his current state, this process takes around five hours, and
entails the development of approximately 2.5 million weights from the
joint-angle to visual-field populations. Importantly, the mapping com­
prises both the kinematics of the arm as well as the optical parameters
and global geometry of the camera/imaging system, a smooth nonlinear
mapping that contains considerable complexitiesl Furthermore, in place
of the traditional, explicit coordinate representations of the manipulator
tip and other vertices, and vertices of workspace obstacles, MURPHY's
workspace representation consists of a full visual image of the arm amid
Post training, a state of activity on the joint-angle population can gen­
erate a "mental image" on the visual-field population via the weighted
interconnections developed during training. The mental image is a effec­
tively a picture of the arm in the specified joint configuration, approxi­
mating that which would have been produced directly by the camera.
Since the focus of this chapter is on issues of representation and planning,
only a brief overview of MURPHY's synaptic learning rule is presented
here; a more complete discussion can be found in Mel (1989). During
training, each unit in the visual-field population is occasionally activated
when one of the white spots that defines the shape of the arm falls into
its receptive field. This "unconditioned" (i.e. camera-driven) activation
function may therefore be described as a smooth nonlinear function over
the 3-d joint space, as exemplified by the plots in figure 10.4. The learn­
ing of forward kinematics is thus complete when each visual-field unit
has learned this activation function over the three-d joint space, that is,
is fired by the conditioned input pathway from the joint-angle population
just as it was fired unconditionally by the camera input during training.
MURPHY's neural populations are based on the
sigma-pi unit, a neural implementation of a locally generalizing func­
tion lookup table ( Albus 1975, Miller 1988, Moody and Darken 1988),
closely related to the method of radial basis functions (Broomload and
Lowe 1988). A sigma-pi unit computes its level of activation by summing
1 An interesting and rather quirky complexity is present in
setup that illustrates the practical unattainability of an accurate analytic model for
camera has autofocus; when any part of the
arm happens to fall in the focusing field of the camera, the camera shifts focus to
the depth of the arm, otherwise it remains focused on the background. This erratic
shift in focus causes a sign ificant optical expansio!l and ontraction that alters the
apparent locations of objec£opJiii�JWatliWat field.
Graph of the sigma-pi activation function for three typical visual-field units,
approximating the unconditioned camera-driven visual activation functions
the contributions of a set of independent, multiplicative clusters of in­
put weights (figure 10.5). The output Cj of cluster j is computed by
multiplying the input signals Xi to that cluster together with their input
ili WiXi. The output y of the entire unit is com­
puted as a weighted sum of the outputs of each cluster and then applying
a soft-threshold, or "squashing," nonlinearity, giving y
Each sigma-pi unit also receives a single unconditioned teacher input
capable of "clamping" the unit at an arbitrary level of activation.
During MURPHY's visual-kinematic learning regime, a sigma-pi clus­
ter is formed as follows: Each time a visual-field unit is activated by
its unconditioned, or teacher, input from the camera, a new cluster is
formed on the unit's "dendritic" tree. A cluster consists of a group of
unit-strength input connections from the three most active units in the
joint-angle population, which encode the current global joint configura­
tion. The weight Wj assigned to each such cluster is proportional to
the intensity of the camera input to the unit in question. In this way
then, each visual-field unit "records" each of the joint states in which
it was unconditionally activated during training, along with the inten­
sity of activation in that state. A cluster behaves like a table entry in
the following sense: in the presence of the same, learned joint state in
a subsequent trial, all input lines to the cluster will be active, causing
the visual-field unit to be "fired" just as it was during training by the
camera. When pres�tiI �fore-seen joint state, the visual
A sigma-pi unit computes a continuous analog of an
the contributions of a set of multiplicative clusters of synaptic weights.
Functionally, the sigma-pi units is used here as an multidimensional interpolating
sigma-pi unit smoothly combines the partial contributions of clusters
Eventually, each unit in the visual-field population learns its activa­
tion function over the three-d joint space. When presented with an arbi­
trary joint state, then, the visual-field population "lights up" in a visual
pattern that corresponds to an image of the arm in that configuration.
Up to this point, we have seen how MURPHY can generate a mental
image of his arm in an arbitrary joint configuration that is sufficiently
close to the real thing as to be useful for reasoning about visual goals.
In this section, we shovPJ1OWVl1lt� �_� search using these visual
mental images can be used to plan movements for a multi link arm in the
presence of obstacles, even when the necessary paths are highly complex
and traverse much of the configuration space.
MURPHY has been endowed with several simple visual heuristics.
First, on the basis of their size, MURPHY can label the blobs in the vi­
sual field as either target or obstacle. Second, he can identify his hand,
wrist, elbow, and shoulder by their visual characteristics. Third, he can
measure the distance from his hand to the target in the following man­
ner. When there is no obstacle in the line of sight from hand to target,
he computes the straight-line distance. If an obstacle lies in the line of
sight to the target, then he measures distance to the target circumvent­
ing the obstacle to one side or the other, whichever results in a shorter
total path. Finally, he understands two simple facts about collisions. If
any part of the arm visually overlaps with one of the obstacles, or if any
part of the arm passes through any one of the obstacles during a mental
step of the arm, then a collision has occured. Armed with this simple
battery of visual heuristics, MURPHY follows these steps in reaching for
1. Label the target and any obstacles present in the visual field.
2. Generate a mental image of the arm in the current joint configu­
ration. Identify and label any parts of the arm that are visible in
the mental image. (Some may be out of view.)
3. Measure the distance from hand to target, taking into account
4. Mentally move the arm in a random direction (but remaining on
a quantized grid in joint configuration space). This involves ran­
domly perturbing the internal joint-angle representation, and re­
generating the mental image of the arm-all without allowing the
arm to move physically. If this arm configuration either (i) has
been tried before, (ii) pushes the hand out of the field of view,
or (iii) causes a collision with an obstacle, then reject it and try
another move. If there are no more moves to try from here, go
back a step and try the next most promising move. If all paths
5. Measure the new distance to the goal. If it has decreased, loop to
4 and keep moving closer. If it has increased, temporarily reject
it, i.e., push it on the stack of things to try later, and go back to
6. If and when the hand makes it to the target, move the arm physi­
cally to the target following the remembered path, dispensing with
Figure 10.6 demonstrates MURPHY's behavior for a difficult reaching
problem. The first frame in the top row shows MURPHY's arm in its ini­
tial configuration relative to a visual target (white cross) and obstacles.
The second frame shows the sequence of intermediate hand positions
generated during the mental search process. Once a complete mental
trajectory has been found, MURPHY moves his hand physically to the
target along a path that has been pruned of any backtracking dead­
ends encountered during the search process (third frame). The lower
panel shows a "movie" of MURPHY's intermediate steps in the physical
execution of the plan, read from left to right by row. (As a technical
note, it must be remembered that MURPHY can only see the white spots
on his arm-as such, only these parts and the links connecting them
will reliably avoid the obstacles, while other parts are free to bump and
crash-see figure 10.6, frames 14-17 for an illustration of the problem.
In the limit, the arm can be completely covered with white spots and
made entirely visible to MURPHY; under the handicap of serial comput­
ing however, processing time and storage requirements increase linearly
with the visual-field activity load.) The reaching problem of figure 10.6
is particularly difficult since the arm must be completely unwound in a
cramped space before the target may be acquired, involving a complete
traversal of the arm's configuration space. In the process of unwinding,
MURPHY is temporarily lured toward what appears to be a promising
(but alas, impossible) path over the top of the vertical obstacle (frames
8-12). As is the nature of heuristic search, the complete path is not
optimal (e.g., as short as pOSSible), but is usually a reasonable, if naive,
path according to aesthetic criteria. Unfortunately, the level of "intelli­
gence" necessary to understand how the paths are suboptimal is beyond
MURPHY's current ability to reason (or heuristisize).
It is significant that for this type of reaching problem, MURPHY's search
space is definitely nontrivial: the branching factor in the search tree
is 26 (Le., all possible steps gotten by perturbing one or more of the
joints in combination, in both positive and negative directions), and the
typical solution paths are between 10 and 50 steps in length. The to­
tal number of paths of length 10 from a given starting configuration is
nearly 1014 , and of lengMplfWlUltll@CiMCf�l1alMost are not solutions and
Reaching for a target in the presence of obstacles.
do not contain solutions. MURPHY's searches typically entail from 10
to the low 100s of internal steps, each step consisting of a run through
the (virtually) parallel kinematic model and evaluation functions. This
relatively successful search performance is attributable principally to the
fact that the search space for reaching with obstacles is not particularly
unfriendly, unless specifically contrived to be so (as in a maze). Thus,
MURPHY's visual heuristics, while trivial to compute from his visual­
field representation, define an evaluation function over the joint space
that has only a modest number of local minima; only a modest degree of
backtracking is therefore needed on the path to a goal. The existence of
a relatively smooth evaluation function is also that which allows MUR­
PHY to search his configuration space at a relatively coarse grain of
quantization (Le., 20° /joint-step) . This reduces total path length dur­
ing search without impairing his ability to find solutions. On the other
hand, coarse-quantization of the search space will prevent MURPHY from
finding a path for his arm when he must "squirm" through too tight a
space--i n these cases, he "bounces" back and forth between obstacles
in his mental search, never finding the clear path up the middle. Figure
10.6 shows typical minimum obstacle clearances below which MURPHY
has difficulty maneuvering. Ideally, the quantization of search could be
determined adaptively during search, as some heuristic function of the
Were one only able to observe MURPHY's behavior externally, it would
appear that he is able to skirt obstacles in reaching for his very first tar­
get, with no prior experience in reaching for objects, and no explicit
knowledge as to how to do so. This is simply to say that it may be
difficult to differentiate from the outside a system such as MURPHY that
uses simple heuristic search on a forward-going mental model of its arm,
from one relying on an explicit inverse model of the task and/or one of
the variety of highly complex algorithms used for the explicit computa­
tion of arm trajectories in the presence of obstacles (Lozano-Perez 1987,
Brooks 1983a and 1983b, Newman and Hogan 1 985). Yet the difference
is fundamental: in the traditional task decomposition in robot control,
the problem of sending the tip of a robot arm to a desired location
in workspace has been viewed as a problem of inverse kinematics, an
approach that intrinsically ignores the path of the arm. Obviously, this
approach can only work in the special case where no obstacles are present
in the workspace; in the more general case, explicit path-planning algo­
rithms are needed (Lozano-Perez, 1987) . In the current formulation,
moving the arm to a desired location is always considered to be a prob­
lem of path planning, r�MfijfJlltJtQIM�-wi�ce or absence of obstacles.
A very simple motion planner can therefore subsume the function of
Interestingly, one of the most perplexing problems in the study of
biological motor control has been to determine in what coordinates
movement plans are generated in the brain. Teleological concerns argue
strongly for planning in workspace coordinates, since the characteristics
of a movement are most naturally specified in terms of the layout of ob­
jects in the workspace. Unfortunately, plans made in hand/workspace
coordinates must eventually be translated into joint actuator commands,
requiring an inverse kinematic transformation as discussed above. On
the other hand, whereas planning directly in the language of joint vari­
ables makes an inverse kinematic transform unnecessary, the plans them­
selves must become more complex in order that their effects in workspace
remain well behaved (Hildreth and Hollerbach 1987). MURPHY's style
of planning suggests a possible resolution to this dilemna: plans can
be made in both coordinates simultaneously! MURPHY generates plans
as sequences of arm configurations in joint coordinates, but evaluates
them "online" as sequences of visual images encoding the state of the
workspace. The availability of a fast visual-kinematic model therefore
allows MURPHY to conveniently incorporate workspace constraints, such
as the locations of obstacles, in the construction of joint-space plans.
MURPHY runs on a SUN 3-160 workstation with 16MB of memory. The
complete software system consists of 15,000 lines of C code, which in­
cludes a variety of peripheral utility routines. Using a kd-tree reimple­
mentation of the sigma-pi system (affording a speedup by a factor of
50-100), a single pass through the forward visual-kinematic map, Le.,
the time necessary to generate a single mental image of the arm from
given the joint angles is between 1 and 2 seconds. With obstacles, the
heuristic visual analysis and cosmetic color graphics processing takes up
to three additional seconds, reflecting the inefficiency of the algorithms
used. A reaching episode with obstacles that has been contrived to be
as difficult as possible may take up to 20 minutes to complete. When no
obstacles are present, MURPHY may require less than a minute to find
the target. It is estimated that the execution could be speeded up by a
factor of between 10 and 50 on the existing machine by eliminating obvi­
ous inefficiencies. On a Connection Machine or other parallel hardware,
MURPHY could easilr>W:.��J'Mter than real time. MURPHY
has not been optimizeK.r{of'ah1gli�i;reclsion of hand positioning, but
even under the worst of circumstances (Le., eight weeks after training
with numerous jostlings of the camera, tables, arm, etc.), would typi­
cally miss the target by no more than 1 inch at 7 feet (0.680 of visual
angle), and would usually come within 0.5 inches.
In order to place MURPHY's reaching algorithm in perspective, it is use­
ful to compare to methods that have been previously used to attack the
problem of robot motion planning in the presence of obstacles. One ap­
proach is to move the arm directly towards a target, detect collisions or
impending collisions with obstacles using proximity sensors on the arm,
and then alter the arm's trajectory according to some prespecified rule.
Lumelsky (1987) has proposed just such a scheme, proving convergence
for planar 2-dof arms of varying kinematic configurations. While this
type of scheme is appealingly simple from an algorithmic standpoint, it
lacks a crucial feature of planning, i.e., the ability to carry out mental
operations in advance that improve the chance of first-time success in
the execution of an action or sequence of actions. Schemes of this type
are essentially blindfolded maze-running algorithms, presupposing no
advance or global knowledge of the environment. Motion-planning alger
rithms that do carry out advance planning computations fall generally
into two types, local and global. Each is discussed below.
One technique that has been used by several workers for simple path
planning with obstacles is the use of artificial potential fields (Andrews
and Hogan 1983, Newman and Hogan 1985, Khatib 1985). In this
scheme, a robot is represented as a point mass. Attractive potential
"wells" (representing targets) and repulsive potential "hills" (represent­
ing obstacles) are used to govern the robot's motion through space. The
potential field approach is particularly well suited to the case where a
global motion plan has been precomputed, such that the robot need
only swerve from its intended path to avoid unexpected stationary or
moving obstacles that impede its progress or threaten collision. Within
this approach though, it is possible for the robot to become caught in
local potential minima, for example, trapped in the elbow of a wedge
shaped obstacle that impedes further direct progress to the goal. The
artificial potential field approach to motion planning is therefore inher­
ently a "local" motion tC�Me,tlMiaffficient by itself to capture
global path information. On the other hand, the necessary computa­
tions are mostly parallelizable and can be made very fast-the potential
field approach can therefore be thought of as an important complement
Beyond questions of functionality, the concept of explicit attractive
and repulsive forces controlling the actions of a manipulator is an intu­
ititively satisfying one, closely paralleling the "high-level language" of
obstacle avoidance. In a sense, MURPHY's search heuristics are analo­
gous to gradients established in this class of potential-field model. Unlike
the potential-field models, however, MURPHY's state transitions are not
governed by "physical" laws embodied in an energy landscape, but by
logical laws-in short, an algorithm. Where the algorithm prescribes
downhill steps, MURPHY moves closer to his goal; where the algorithm
detects an obstacle, MURPHY attempts to swerve around it; where the
algorithm prescribes "uphill" steps, such as in untangling his arm caught
in a local minimum, MURPHY moves away from his goal.
Geometric Techniques for Global Motion Planning
According to its standard definition and practice, the problem of global
motion planning for a multijointed arm in a cluttered workspace has
proven to be an exceedingly complex and expensive proposition. Mo­
tion planning algorithms amost universally proceed by building a free
space graph that captures the global topological structure of the prob­
lem at hand, as determined by the form of the robot, and the shapes,
positions, and trajectories of the workspace obstacles. The graph essen­
tially characterizes the set of candidate paths that the robot may use
to reach the goal from its starting point while avoiding obstacles. Once
built, it is searched for an optimal path using a search technique such
as A *. The most difficult and time-consuming operation is generally the
For expository purposes, we briefly describe three recent approaches
to robot motion planning. When the robot is assumed to be a point or
a simple shape with no internal degrees of freedom, the procedure for
building such a graph is greatly simplified. Brooks (1983a) analyzes the
case of a two-d polygonal "robot" moving amongst polygonal obstacles
in a two-d workspace. First, generalized cone primitives are used to
represent the shapes of the free space between obstacles, by performing
pairwise comparisons among the stored list of workspace obstacles. For
each cone, the range of allowable orientations of the robot are computed
( Le., orientations of CePJliigbImtlM*ii/fits completely inside the cone
as it moves down the entire length of the cone's central spine). A graph
is then built linking together intersecting cones. Finally, a path from the
start to goal locations is computed by searching this graph with the A *
algorithm, allowing transitions from cone to cone only where the ranges
Hwang and Ahuja (1988) describe a second approach to the modeling
of free space in a recent paper. A scalar potential function similar to
those described in the preceeding section is constructed from the layout
of the target and obstacles in the workspace. The minimum poten­
tial valleys running between obstacles are located in the resulting field;
these minima capture the topological structure of the free space. Each
such potential valley represents a candidate path for a moving robot. A
graph is built and searched using dynamic programming to detemine a
best first guess at a complete path from start to goal. Various heuristics
and optimization techniques are then applied to generate collision-free
trajectories that minimize length and change of orientation. The ad­
vantage of this approach is that the computation of initial guesses as
to candidate paths is relatively inexpensive in comparison to pure ge­
ometric approaches, which generally perform large numbers of explicit
geometric computations involving the shapes and intersections of poly­
gons (or polyhedra in higher dimensions).
When the moving robot is assumed to be a multijointed arm rather than
a simple fixed polyhedral shape, the computation of candidate paths
through free space become much more involved. Only a handful! of al­
gorithms have been devised that can make claims both to generality
and computational feasibility (Lozano-Perez 1987, Brooks 1983b, Faver­
A multijointed robot arm can be represented as a set of linked poly­
hedra, and the configuration of the arm can be represented as a point
in a k-dimensional configuration space representing the arm's k degrees
of freedom. Much of the machinery of the geometric approach is ded­
icated to determining in advance which configurations of the arm are
legal, and which cause collisions with obstacles in the workspace. These
"bad" arm configurations can be represented as regions in configuration
space called configuration-space obstacles. Path planning for the arm is
thereby transformed into the problem of finding clear trajectories in con­
figuration space for the arm, now represented as a point, that link the
initial and final config�k!lfethMaterialhile skirting configuration
space obstales. Algorithms capable of explicitly computing configura­
tion space obstacles for a multijointed arm in a three-d workspace are
For example, the algorithm of Lozano-Perez (1987) proceeds schemat­
ically as follows: (1) Assume both the initial and final goal configurations
of the arm are known in advance for a particular reaching problem (pre­
supposing an inverse kinematic solution for the arm's goal configuration
is available). Also assume the positions and shapes of all objects in
the environment are known; obstacles are assumed to be composed of
convex polyhedra. (2) Determine the regions of the arm's configuration
space that are illegal, i.e., that represent arm configurations that cause
collisions with obstacles. To accomplish this, step through all config­
urations of all but the most distal joint at some level of quantization
(e.g., 2°) , and for each, compute the legal linear range of the last, most
distal joint, if any. For each quantized global arm configuration, the
arm is "grown" in a simple fashion in such as way as to guarantee that
its new, larger polyhedral volume includes the curvy "swept volume"
that would be generated if all the joints were to sweep simultaneously
through their small quantized ranges of orientations around the given
starting configuration. If this new synthetic object does not intersect
any of the workspace obstacles, then the arm is "safe" in that quantized
range of configurations. If any part of the grown arm does intersect a
workspace obstacle, then the configuration is marked as illegal. Once
the configuration-space obstacles have been computed, a graph is once
again generated that captures the topological structure of the cluttered
configuration space. As before, this graph is then searched with A· to
While this geometric approach is undeniably an algorithmic tour de
force, its applicability is necessarily limited to relatively static, controlled
environments, where the numerous special assumptions may reasonably
be met, namely (i) complete foreknowledge of the kinematic structure of
the arm and the shapes, sizes, and locations of all obstacles; (ii) objects
that are composed of convex polyhedra; (iii) a correct inverse-kinematic
model capable of generating legal goal configurations of the arm; and
(iv) a general purpose computer for the implemetation of the various
In contrast to the type of environment that might be found in a care­
fully designed factory, however, reaching for targets in the real world
is quite a different sort of problem. For example, since the control of
reaching is a body-centered operation, the layout of obstacles effectively
reach for are often moving, as are the things we reach around. Objects
in the world are also not polyhedra in general, and not always in full
view, conditions which do not seem to pose serious problems for bio­
logical visually guided reaching faculties. In short, it does not seem as
if humans or animals must explicitly precompute the complete set of
illegal configurations of our arm through an involved procedure such as
that described above, a procedure that must be repeated from moment
to moment to reflect any changes in the relative layout of the body,
In an important sense, this type of computa­
tion is highly inefficient: the configuration space obstacles constructed
in Lozano-Perez's geometric approach make up a global data structure
that captures geometric constraints due to obstacles over the arm's en­
tire configuration space, even though the arm will typically visit only a
minute portion of the space during an actual reaching trial. The worst
case asymptotic complexity of this processing step for an arm with k
degrees of freedom, in which the joint ranges are divided into
the environment ( or the location of the arm relative to the environment)
is changing through time, the cost of recomputing this elaborate data
structure moment by moment could easily become prohibitive.
exponential slowdown with increasing degrees of freedom k means that
the full-blown configuration-space structure cannot practically be com­
puted for arms with more than two degrees of freedom. Lozano-Perez
suggests heuristics for pruning the traversal of configuration space that
"significantly" reduce the computational requirements of his algorithm,
but whose pragmatic range of applicability is difficult to judge from the
The principal cost of this geometric approach, then, is the combinatorics
of precomputing both legal ( and illegal) arm configurations in advance,
such that the search for a path may be carried out on a much smaller
graph that contains only legal arm configurations. The graph is kept as
small as possible, since in order to find the optimal path, it may, in the
worst case, be necessary to traverse the graph in its entirety.
By trading off optimality for speed, MURPHY circumvents the need for
this expensive pre-processing step in configuration space.
point is that with good heuristics, it is possible to search directly for
MURPHY is content to visit legal and illegal arm configurations alike
during movement planning, i.e., is perfectly content to collide with an
obstacle when he makes a wrong turn, since the collision is a purely in­
ternal, or mental, event. This is the classic advantage of a mental model:
simulating reality can be faster, cheaper, and/ or safer than doing it for
real, and increases the chance of success on the first actual try. In or­
der for search to be efficient, it is only necessary that the computations
involved in running the internal model and computing the heuristics be
as fast as possible. Within MURPHY's architecture, both of these op­
erations consist of a single parallel operation. Furthermore, no inverse
kinematic model is assumed, and the workspace obstacles can be of ar­
bitrary shape since they are simply treated as blobs in the visual field
that the arm is intended to avoid (Le., upon which the arm should never
rest ) . Nor is additional algorithmic complexity introduced in the case
of moving obstacles ( although malicious moving obstacles could make it
impossible for MURPHY to ever reach his goal without collision ) , since
the heuristics for collision detection simply operate on the current state
of the visual field without concern for its past or future. Were MURPHY's
visual system upgraded to three-d, visual ovelap would continue to sig­
nify collision, but now in a visual representation built from binocular
units that are selective for the depth of the stimulus.
In this direct search formulation, extra work is introduced in the search
process only when backtracking becomes necessary to extract the arm
from a local minimum in the heuristic evaluation function. In MUR­
PHY's case, this cost has proven to be acceptable empirically. The most
difficult reaching problems that it was possible to construct in MUR­
PHY's planar world ( and that MURPHY was able to solve--see section
1 0 . 4 . 2 ) required in the low hundreds of internal steps through configu­
ration space, giving rise to plans of between 10 and 50 steps ( e.g., figure
10.6) . For purposes of comparison, we estimate using Lozano-Perez'
complexity bound of O(rk- 1 (mn)2 ) that problems of this approximate
difficulty would, in a standard geometric approach, consume in the tens
to hundreds of millions of serial machine operations2 However in mak­
ing this comparison, we must carefully consider that (i) much in the
2 This estimate is derived under the following assumptions: the constant of pro­
portionality in the complexity bound from ( Lozano-Perez 1987) is estimated to be
6 x 10 - 7 seconds, which the author states consumed 6 seconds of total wall-clock
time on a y mb o lic s 3600 for ( apparent ) parameter values r = 180, k = 2, m = 8,
and n = 30 . For a 3- dof ( i . e . , k = 3) problem with identical obstacle layout but
much coarser grain of quantization ( i . e . , r = 18), we estimate a
�an-clo �k time of 1 1 seconds..for this I}jgoljthll1pr.l; iq the tens of millions of machine
serial/geometric algorithmic approach could in principle be parallelized,
and (ii) the emphasis on path optimality is in great part that which adds
computational complexity to the serial/geometric approach.
Interestingly, in contrast to such geometric motion-planning algo­
rithms, which as indicated above grow exponentially slower with increas­
ing degrees of freedom in the manipulator, it would seem that MURPHY'S
direct heuristic search method could possibly become easier. As degrees
of freedom are added to an arm without adding joints (for example, the
human arm has 7 degrees of freedom distributed across the wrist, elbow,
and shoulder), the arm becomes more "snakelike." As the arm becomes
more snakelike, more paths become possible since it becomes easier to
"slither" around corners and in and out of the nooks and cranies that
frustrate simpler arms. On the other hand, direct heuristic search has
a generic tendency to become more expensive with increasing degrees
of freedom, since, for a given step size (Le., search space quantization) ,
the branching factor for enumerating next moves increases rapidly. It is
difficult to precisely predict the relative weights of these two factors.
It seems clear that the intelligence and flexibility of future robots will
come in large part from the seamless integration of rich sensory pro­
cesses, particularly vision, into the nitty-gritty motoric control functions.
The style of visual representation seen in the primate brain suggests the
utility of topographic, imagelike neural representations, which can allow
inexpensive computation of many of the heuristic quantities necessary
for the visual guidance of movement. In addition, the cognitive utility
of iconic visual representation is evident in the psychological data on
mental rotation and other transformations of visual images (Shepard
and Metzler 1971, Kosslyn 1980, Shepard and Cooper 1982, Mel 1986).
In this work, the specific problem of motion planning for a multilink
arm has been used to argue that the massively parallel architecture of a
neural network can play a central algorithmic role in the solution of the
difficult real-world problem of motion planning for a multilink arm. It
is reasonable to expect that other, future sensory-based robotic control
applications will benefit from this style of representation as well.
Ackley, D. H. ( 1 988 ) . Associative learning via inhibitory search. Paper
Presented at the 1988 Snowbird Conference on Neural Networks for
Albus, J. S. (1975 ) . A new approach to manipulator control: The
cerebellar model articulation controller ( CMAC ) . Transactions of
the ASME Journal of Dynamic Systems, Measurement, and Control
Andrews, J. R. and Hogan, N. ( 1983) . Impedance control as a frame­
work for implementing obstacle avoidance in a manipulator. In D.
E. Hardt and W.J. Book, eds. , Control of manufacturing processes
and robotic systems. New York: American SOciety of Mechanical
Baldi, P. and Heiligengerg, W. ( 1 988 ) . How sensory maps could enhance
resolution through ordered arrangements of broadly tuned receptors.
Ballard, D. H. (1987 ) . Interpolation coding: A representation for num­
bers in neural models. Biological Cybernetics 57:389-402.
Ballard, D. H . , Hinton, G. E . , and Sejnowski, T. J. ( 1983) . Parallel
Bentley, J. L. (1975 ) . Multidimensional binary search trees used for
associative searching. Communications of the Association for Com­
Brooks, R. A. ( 1983a ) . Solving the find-path problem by good repre­
sentation of free space. IEEE Transactions on Systems, Man and
Brooks, R. A. ( 1983b) . Planning collision-free motions for pick-and­
place operations. International Journal of Robotics Research 2: 19-
Elsley, R. K . ( 1 9 88). A learning architecture for control based on back­
propagation neural networks. Paper presented at the 1 988 Confer­
ence on Neural Networks for Computing, Snowbird, Utah , April.
Erikson, R. P. ( 1 984) . On the neural bases of behavior.
Faverjon, B. ( 1 984) . Obstacle avoidance using an octree in the configu­
ration space of a manipulator. In Proceedings of the IEEE Interna­
tional Conference on Robotics, Atlanta, GA, March.
Feldman, J. A. and Ballard, D. H. ( 1 9 82) . Connectionist models and
their properties. Cognitive Science 6:205-254.
Hildreth, E. C. and Hollerbach, J . M. ( 1 987) Artificial intelligence:
Computational approach to vision and motor control. In F. Plum,
ed. , Handbook of Physiology, Sec. 1 : The Nervous System. Vol. 5,
Bethesda, MD: American Physiological Society.
Hinton, G. E. ( 1 986) . Distributed representations. In D.E. Rumelhart,
J.L. McClelland, eds. , Parallel distributed processing: Explorations
in the microstructure of Cognition. Vol. 1 , Cambridge, MA: Brad­
Howden, W. E. ( 1 968) . The sofa problem.
Hwang, Y . K . and Ahuja, N . (1988) . Path planning using a potential
field representation. Coordinated Science Laboratory Technical Re­
port UILU-ENG-88-225 1 , University of Illinois, Urbana-Champaign.
Jordan, M. I. (1988) . Sequential dependencies and systems with excess
degrees of freedom. COINS Technical Report 88-27, Department
of Computer and Information Science, University of M assachusetts,
Khatib, O . ( 1 9 85) Real-time obstacle avoidance for manipulators and
mobile robots. In Proceedings of the IEEE International Conference
on Robotics and A utomation, St. Louis, MO, March.
Kosslyn, S . M. ( 1980) . Image and mind. Cambridge, MA: Harvard
Kuperstein, M. (1988) . Neural model of adaptive hand-eye coordination
for single postures. Science 239 ( March ) : 1308-1311.
Lozano-Perez, T. (1987) . A simple motion-planning algorithm for gen­
eral robot manipulators. IEEE Journal of Robotics and A utomation
Lumelsky, V. J. (1987). Effect of kinematics on motion planning for
planar robot arms moving amidst unknown obstacles. IEEE Journal
Mel, B. W. (1986) . A connectionist learning model for 3-d mental rota­
tion, zoom, and pan. In Proceedings of Eighth Annual Conference
of the Cognitive Science Society, 562-571.
Mel, B. W. (1987) . A computational model for neural feature extraction.
Technical Report UIUCDCS-R-87- 1357, Department of Computer
Science, University of Illinois, Urbana-Champaign.
Mel, B . W. (1988) . MURPHY: A robot that learns by doing. In Neu­
ral information processing systems, 544-553. New York: American
Mel, B. W. (1989) . A connectionist approach to learning and perfor­
mance in vision-based robot motion planning. Technical Report
CCSR-88- 1 7A, Center for Complex Systems Research, University of
Merzenich, M. M and Kaas, J. (1980) . Principles of organization of
sensory-perceptual systems in mammals. In Progress in psychobiol­
Miller W. T. , Hewes, R. P. , Glanz, F. H . , and Kraft, L. G. (1988) . Real
time dynamic control of an industrial manipulator using a neural
network based learning controller. Technical Report, Department of
Electrical and Computer Engineering, University of New Hampshire,
Moody, J. and Darken, C. (1989) . Fast learning in networks of locally
tuned processing units. Neural Computation 1:281-294.
Newman, W. S. and Hogan, N. ( 1985) . High speed robot control and
obstacle avoidance using dynamic potential functions. In Proceed­
ings of the IEEE International Conference on Robotics Information,
Omohundro, S . ( 1987) . Efficient algorithms with neural network behav­
Rumelhart, D. E., Hinton, G. E. , and McClelland, J. L. ( 1986) A general
framework for parallel distributed processing. In D. E. Rumelhart,
J. L. McClelland, eds . , Parallel distributed processing: Explorations
in the microstructure of cognition. Vol. 1 , Foundations. Cambridge,
Sejnowski, T. J. ( 1 986) . Open questions about computation in cere­
bral cortex. In In D. E. Rumelhart, J. L. McClelland, eds . , Parallel
distributed processing: Explorations in the microstructure of cogni­
tion. Vol. 2, Psychological and Biological Models. Cambridge, MA:
Shepard, R. N. and Cooper, L. A. ( 1982 ) . Mental
transformations. Cambridge, MA: MIT Press.
Shepard, R. N. and Metzler, J . ( 1 97 1 ) . Mental rotation of three­
dimensional objects. Science 171:701-703.
Van Essen, D. C. (1985) . Functional organization of primate visual
cortex. In A. Peters and E. G. Jones, eds . , Cerebral Cortex Vol. 3.
Walters, D. (1987) . Biological vs. artificial representations of variables
in fine-grain parallel systems. In Proceedings of the IEEE Computer
Society Workshop on Computer Vision, 298-300. Miami Beach, FL,
Christopher G. Atkeson and David J. Reinkensmeyer
Models of robots and their tasks are useful for planning, control, and
learning (An, Atkeson and Hollerbach 1988). For many robot systems,
good models are hard to formulate, difficult to identify and calibrate,
and computationally expensive to use. This chapter explores a memory­
based approach to modeling. In this approach models are learned by
storing experiences in a memory, and predictions are made by searching
the memory for relevant experience. The memory used is an associa­
tive content-addressable memory (ACAM) (Kohonen 1980). We have
implemented an ACAM on a parallel computer, the Connection Ma­
chine (Hillis 1985). In order to explore the feasibility of the approach
and identify research issues and problems, we used the ACAM to model
and control a simulated planar two-joint arm and a simulated running
machine. We found that simply using only the nearest experience to
predict new commands produced surprisingly good performance. The
results presented in this paper provide a reference point for more sophis­
An associative content-addressable memory can be implemented in
many ways. There are implementations appropriate for serial comput­
ers, parallel computers, dedicated electronic hardware, and model neu­
ral networks. We have implemented an associative content-addressable
memory on a parallel computer, but the issues raised and experience
gained are useful in assessing many proposed memory-based motor con­
trol schemes. Recent research on the application of neural networks to
robotics and biological motor control has highlighted the usefulness of
associative content-addressable memories for control. Control can be im­
proved by storing experiences in a memory, and using these experiences
to generate future commands. Several research issues have been raised
by this work. How should inputs, outputs, and states be chosen and
represented? What teaching method should be used to generate data?
What distance metric should be used to compare experiences, and how
should experiences be generalized? How should noisy and time-varying
systems be handled? These questions are often confounded with the is­
sues involved in a particular implementation of an ACAM. For example,
implementing an ACAM using a neural network requires an algorithm
to find the weights or parameters that cause the network to represent
a given set of experien<Gop9>fghtec:t�/descent algorithm is used,
which requires experiences to be applied many times to the network.
The measured learning rate of the controlled system may be determined
by the rate of convergence of the weights, rather than the convergence
of a given ACAM-based learning algorithm. In addition, the distance
metric and generalization method are implicit in the network architec­
ture rather than explicitly specified by the control system designer. It
is possible that a unique and useful distance metric and generalization
scheme will be induced by the network architecture, but our present re­
search focuses on explicitly specified distance metrics and generalization
This section describes how a memory can be used to represent a model.
As an example, the dynamics of a one-joint arm are modeled, and the
model is used to compute feedforward commands for a robot controller.
The robot controller combines these feedforward commands with feed­
back control to drive the arm along a desired trajectory.
dynamics are affected by gravitational, viscous, frictional, and inertial
forces. The state of the arm is given by its current joint position and
These values can be stored together as a single experi­
where i designates the ith experience. In this study new experiences are
generated by a feedback controller, which refines the feedforward torques
according to position and velocity errors.
In order to predict the torques necessary to achieve a desired trajec­
tory, it is useful to have a model of the arm's inverse dynamics,
can be represented by a memory that contains experiences of partic­
ular positions, velocities, torques, and the resulting accelerations. Com­
mand torques can be predicted by addressing the memory with the de­
similar to the desired performance. The appropriate variables of each
stored experience are compared with the desired positions, velocities,
�c;&II}...,-5!��alJGHrametrif' A simple distance
the Euclidean distanc'e'"B'eiWe1n t'tie cEb'lPesponding values:
Alternative distance metrics such as the sum of the absolute values of
the distances in each dimension could also be used. The torque from the
experience with the smallest distance, di, is used at each point along the
This process can be implemented on a standard serial computer. Ev­
ery time a particular set of positions, velocities, and accelerations is
requested, the entire memory can be searched for the closest experience.
Viewed as a black box, this process implements an associative content­
addressable memory. The memory is content-addressable in that part
of the contents of the memory are used to select the rest of the memory;
in our example a position, velocity, and acceleration, (Oi, iJi, Bd, were
used to choose an experience, and the torque from that experience was
used, ( 7i ) . In the case where an exact match is not available this process
finds a reasonable answer by finding the most similar experience in the
memory. In this way the process acts as an associative memory.
With large computer memories now available, the limiting factor for
this simple implementation of an ACAM is the time required to search
the memory. The time required to find the closest experience increases
linearly with the number of experiences in the memory. Other more
efficient ways to implement an ACAM are reviewed in a later section.
We have used a parallel computer to implement an ACAM. In our
ACAM implementation each experience is stored in a processor of the
Connection Machine, a massively parallel computer (Hillis 1985). The
Connection Machine can have up to 216 (65536) processors, and can sim­
ulate a parallel computer with many more processors. Each experience
can be compared to the desired experience simultaneously in each proces­
sor, and then a hardwired global-OR bus can be used to find the closest
match in constant time independent of the number of stored experiences.
This approach is similar to many Connection Machine algorithms that
The feasibility of the ACAM was tested on two simulations, the first of
which involved a simulated robot arm following a trajectory. The ACAM
was used to learn a model of the robot inverse dynamics and helped
improve performance of a single trajectory and a similar trajectory by
The arm used in the simulation was a two-joint robot arm (figure 1 1 . 1)
where jj is the joint acceleration vector produced by application of the
torque vector r to the current state of the arm as given by the joint
angle and joint velocity vectors (J and 0. As the arm is servoed along a
trajectory, a model of the inverse dynamics is built by storing values of
In order to use the model to calculate feedforward torques, the mem­
ory is searched using (Jd' 0d' and jjd as indices, where the subscript d
denotes desired values as supplied by a trajectory planner. This search
is done before each attempt at the trajectory, and an array is filled with
the feedforward torques (rtf) wh ich are to be added to the output of a
K and B are constant position and velocity feedback gain matrices.
The combination of feedback terms and feedforward terms drives the
robot. If Ttf is poorly modeled, the feedback terms are responsible for
moving the arm in a reasonable way. This is the situation when the
memory has not stored many experiences: the feedback terms act as
a teacher in generating the initial commands that enable the ACAM
to store appropriate experiences. As the model becomes more accurate,
the contribution of the feedback terms to the controller becomes smaller.
Miller, Glanz and Kraft (1987) use a similar training paradigm in their
The two test trajectories shown in figure 11.2 were planned using a
straight line path and a fifth-order polynomial position function which
started and ended at rest and had a duration of one second. The move­
ments were in a vertical plane so gravity points downward in this figure.
The links of the arm were modeled as thin uniform rods each with a
mass of lkg. The feedback controller used decoupled position and veloc­
ity gains of Kl1 65.7NmJrad, K22 13NmJrad, Bl1 42NmsJrad,
8NmsJrad, and applied torques at a rate of 100 Hz. The
equations describing the arm dynamics were integrated using Euler's
method with a time step of 0.001 seconds.
The ACAM improved trajectory following for a single trajectory. Fig­
ure 11.3 shows the performance of trajectory 1 with the feedback con­
troller alone. After storing these experiences in the ACAM, feedforward
torques were computed using the memory. Figure 11.4 shows that on
the first attempt using the ACAM there was a performance improve­
ment. Figure 11.5 shows how closely the feedforward torques on the
first attempt corresponded to the ideal feedforward torques for the de­
sired movement. In this and other graphs of feedforward torques the
smooth solid lines are the correct feedforward torques.
After three cycles of storing the previous movement's data in the
ACAM and computing a new feedforward command, the performance
improved further. Figure 11.6 shows this tracking performance. Fig­
ure 11.7 shows that the feedforward torques corresponded more closely
to the ideal feedforward torques. The ACAM built a better model of
The accuracy of trajectory following for trajectory 1 stopped improv.
ing after about six atte�gfJt'�_d torques for attempt ten
............................... _.-:':................................ ..
and show the best performance the memory was
able to achieve. We call this phenomena a "stuck state." The problem
of stuck states is discussed in a later section.
The model stored in the ACAM after eight attempts at trajectory
1 generalized to a similar trajectory, trajectory
used only the experiences stored during the attempts at trajec­
tory 1 to calculate the feedforward torques for trajectory
shows the trajectory following performance on trajectory
torques with the id&J1�MaMfi.it/les suggests that the ACAM
can act as a local model by generalizing from limited experience, but it
should be noted that the modeling error is large for some parts of the
The improved single-trajectory following and the generalization to
a similar trajectory seen in these and other experiments demonstrate
that an ACAM can build a useful model of the arm dynamics. The
simple distance metric was effective in retrieving stored experiences, and
reasonable generalization was obtained by using the closest experience.
Stuck states present a problem deserving future attention.
The second simulation involved control of running. Memory-based tech­
niques are well suited for control of a dynamically stable, legged robot
( Raibert and Wimberly �gltt6ti1__�ystem is difficult to model,
Trajectory I-feedforward torques for first
contains a large number of variables, and encounters only a small number
of experiences (hops). In this simulation an ACAM was used to build a
correction to a model of legged running. This learning proved useful in
improving control of the simulated robot beyond the performance with
Figure 1 1 .12 shows a simulated legged, hopping machine originally
developed by Raibert. It has a body, a springy leg, a leg actuator, and a
hip actuator. Control of the hopper is partitioned into three algorithms
(Raibert 1984). In order to control hopping height, the amount of energy
contained in the system is adjusted by changing the length of the leg
spring during stance. In order to control body angle, the hip actuator
servos the body to the appropriate position during the stance portion
of a hop. The hip actuator also controls balance and running speed
by servoing the foot during flight to an appropriate placement. If the
foot is placed at a point called the neutral point, the machine runs with
constant velocity. If the foot is placed behind the neutral point, the
machine accelerates. If the foot is placed ahead of the neutral point, the
The ACAM was used to improve the algorithm controlling balance
and running speed. The memory acted as a correction to an analytical
model of foot placement which is made by predicting the projection
that the center of gravity will sweep out during stance. The projection
is called a CG-print (Raibert 1984), and is approximated by Tsx where
Ts is a stance duration predicted from the characteristics of the springy
leg, and x is the average velocity of the machine during stance, and is
approximated by the velocity at takeoff. If on landing the foot has been
servoed to the center of a perfect model of the CG-print, the machine
experiences no net acc�hftFcHlIIaierlainter of a perfect model of
Trajectory I-feedforward torques for third attempt.
I-feedforward torques for tenth attempt (stuck state).
Trajectory 2-using trajectory 1 experiences.
Trajectory 2-feedforward torques using trajectory
modeled foot placement are cal cula ted in the
following way. Each time the hopper leaves the ground, an appropriate
foot placement for the next landing is calculated using the CG-print
model and a cor r ec t i on from the ACAM:
In orde r to improve t h e CG prin t model,
Controlling hopping flight velocity-without ACAM.
where jPcg represents an evaluation of the CG-print model, A represents
an ACAM lookup, and state is the state of the machine as defined by
the variables X, y, ii, B, 0, 4>, ;p, w, and tV (see figure 11.12). The leg is
servoed to this foot placement during flight, and the machine accelerates
when it lands, resulting in a new takeoff velocity xnew. The resulting
knowledge of the actual physical transformation for this particular foot
placement is used to store a correction to the CG-print model. First, a
foot placement is calculated using the CG-print model, given the output
The difference between what actually happened and what the model
predicted, jp- jp, is then stored in the ACAM so that it can be accessed
The ACAM learned how to correct the CG-print model and improved
running performance in simulation. Figure 11.13 shows an attempt at
a series of desired flight velocities using the CG-print model with the
parameters and controlcef)fWlii§htfMt�ateriftl Raibert (1984). The flat
parts of the plot of the actual center-of-mass velocities are the flight
velocities. Figure 11.14 shows the first attempt at the series of running
velocities using the ACAM for corrections after having stored 100 hops
whose desired velocities changed every three hops and varied randomly
from -1.0 mls to 1.0 m/s. Figure 11.15 shows the second attempt at
the same series after having stored the results of the first series. Steady
state errors and response time were reduced. The model that the ACAM
learned enhanced the legged machine's performance. Stuck states were
encountered in this simulation also, and limited the performance to a
level only slightly better than that shown in figure 11.15.
Raibert has experimented with table-based control of running (Raib­
ert and Wimberly 1984). There are several differences between his
table-based approach and this implementation of an ACAM. The first
difference involves keeping the table size manageable. With the intent
of reducing table size, Raibert partitioned the state variables into two
groups: a group that varies. from h!,p �o !J.�p (x, </>,�, 8) and a group that
varies periodically (f,9��c;I � the periodic variables should
Controlling hopping flight velocity-second attempt with
be about the same value at takeoff for any given hop, it is not essential
to use them as table indices, and thus table size can be substantially
reduced. The ACAM allows retention of the periodic variables without
taxing memory size because the size grows only linearly with the num­
ber of variables or fields in a data point. The benefit of retaining the
periodic variables is that they might become more important in certain
situations. For example, the vertical takeoff velocity (y) might be more
important for runs where hopping height is varied.
The second difference between the table-based and ACAM approaches
is that this implementation used as simple a distance metric as possi­
ble. The distance metric considered all variables equally important.
The running velocity was the only variable controlled. Raibert's table
controlled running velocity, body angle, and body angular velocity and
weighed each variable differently depending on which was deemed more
important to control. His distance metric took into account weight­
ing factors which assigned the relative importance of different types of
The third difference between the table-based approach and the ACAM
implementation is that the tal?le used interpolation in order to compen­
sate for its coarse quan�IltffiElfJt��1lII! wanted to investigate the
simplest generalization scheme possible (nearest neighbor), the ACAM
was implemented without interpolation. This strategy was not unrea­
sonable because the ACAM quantization depends only on the similar­
ity of the experiences collected, and thus can be arbitrarily small. An
interesting point to note is that tabular interpolation becomes compu­
tationally expensive as the number of dimensions increases because the
number of neighboring table entries grows exponentially with the num­
ber of dimensions. Raibert also explored using polynomial surfaces to
F inally, Raibert filled the table by running many simulations before
using the table to control the robot. The table replaced the CG-print
In this implementation, the ACAM was used to correct the
CG-print model, experiences were stored during the actual control of
the robot, and corrections to the CG-print model were used as soon
the memory had stored a small number of experiences (100). This model
correction scheme allows better performance in areas of the state space
for which the ACAM has not stored many experiences.
These feasibility studies demonstrate that useful models of robots can be
learned by storing experiences in a memory. There are several issues that
needed to be addressed in this exploration of memory-based modeling,
such as the source of the experiences, how to measure similarity, and
The source of the experiences is usually referred to as the "teacher."
The experiences can be generated in a variety of ways.
random torques can be applied, or a feedback controller can be relied
on as a source of torques. In this work the feedback controller served as
A simple distance metric is used to measure the similarity between
experiences: the Euclidean distance between the experiences represented
points in a vector space. Even with this distance metric it was not
clear how to weight or scale the different dimensions, which often had
different units of measurement. How should a position be compared to
Other alternatives would be to scale the dimensions SO that
the range ofvalues in each dimension are the same, or so that a given
change in each dimension had the same effect on the outputs. Designing
a perfect distance metric is difficult, but the examples in this paper
show that a simple distance metric can be effective in retrieving stored
In the version of ACAM presented in this chapter, only the closest
experience is used, although a weighted sum of the closest experiences,
or an approximation based on a surface which is fit to the closest expe­
riences could be computed. More complex generalization schemes will
allow interpolation and extrapolation from the stored data, and may also
help in dealing with noisy data, an issue not addressed in this chapter.
In our version of ACAM, every experience is stored. At some point
memory-size limitations will be reached. To use memory space more
sparingly, only the experiences which are sufficiently different from pre­
vious experiences could be stored. Memory size could also be reduced
by "forgetting" certain experiences, perhaps those that have not been
referenced for a long time. We have not needed to implement any form
of "forgetting" or memory consolidation process.
With a poor teacher, such as a low-gain feedback controller, the perfor­
mance in both feasibility studies improved, but errors were not reduced
to zero. We refer to this phenomena as a stuck state, and expect it to
be an issue in many ACAM based approaches to control (Miller, Glanz
and Kraft 1987). The teacher, in our case the feedback controller, is no
longer generating improved performance over the stored performance,
and only old data is used in generating new commands. Possible s0lutions include changing the teacher, deliberately adding random noise
or other perturbations to the commands, and improving generalization
and the distance metric. Preliminary experiments using a generalization
function that fits a hyperplane to a set of similar experiences have elim­
inated the stuck states. Model-based approaches to learning may also
prove useful ( Atkeson, Aboaf, McIntyre, and Reinkensmeyer 1988).
Two forms of modeling previous experience are those approaches that
represent the experiences directly, as in this study, and those that rep­
resent the experiences using parameters or weights, such as in many
table-based schemes, perceptrons and multilayer connectionist or model
neural networks. For a more extensive review of related work see Atke­
Memory-based modeling has a long history. Approaches which repre­
sent previous experiences directly and use a similar experience or sim­
ilar experiences to form a local model are often referred to as nearest­
neighbor or k-nearest-neighbor approaches. Local models (often polyno­
mials) have been used for many years to smooth time series (Whittaker
and Robinson 1924, Macauley 1931) and interpolate and extrapolate
from limited data. Barnhill (1977) and Sabin (1980) survey the use
of nearest neighbor interpolators to fit surfaces to arbitrarily spaced
points. Eubank (1988) surveys the use of nearest neighbor estimators
in nonparametric regression. Lancaster and Salkauskas (1986) refer to
nearest-neighbor approaches as "moving least squares" and survey their
use in fitting surfaces to data. Farmer and Sidorowich (1988) survey the
use of nearest neighbor and local model approaches in modeling chaotic
An early use of direct storage of experience was in pattern recogni­
tion. Fix and Hodges (1951) suggested that a new pattern could be
classified by searching for similar patterns among a set of stored pat­
terns, and using the categories of the similar patterns to classify the
new pattern. Steinbuch proposed a neural network implementation of
the direct storage of experience and nearest-neighbor search process for
pattern recognition, and pointed out that this approach could be used
for control (Steinbuch and Piske 1963). Lorenz (1969) explored the use
of nearest neighbors to predict the weather. Stanfill and Waltz (1986)
proposed using directly stored experience to learn pronunciation, using
a Connection Machine and parallel search to find relevant experience.
They have also applied their approach to medical diagnosis (Waltz 1987)
and protein structure prediction. Loftsgaarden and Quesenberry (1965)
proposed using a nearest neighbor approach to estimate probability den­
Nearest neighbor approaches have also been used in nonparametric
regression and fitting surfaces to data. Often, a group of similar expe­
riences, or nearest neighbors, is used to form a local model, and then
that model is used to predict the desired value for a new point. L0cal models are formed for each new access to the memory. Watson
(1964) , Royall (1966), Crain and Bhattacharyya (1967), Cover (1968),
and Shepard (1 9 68) proposed using a weighted average of a set of near­
est neighbors. Gordon and Wixom (1978) analyze such weighted aver­
age schemes. Crain and Bhattacharyya (1967), Pelto, Elkins, and Boyd
(1968), Palmer (19���tWbittle (1970), Falconer (1971),
and McLain ( 1974) suggested using a weighted regression to fit a local
polynomial model at each point a function evaluation was desired. All
of the available data points were used. Each data point was weighted by
a function of its distance to the desired point in the regression. Stone
(1975) and Franke and Nielson ( 1 980) suggested fitting a polynomial sur­
face to the nearest neighbors, also using distance weighted regression.
Stone scaled the values in each dimension when the experiences where
stored. The standard deviations of each dimension of previous experi­
ences were used as the scaling factors, so that the range of values in each
dimension were approximately equal. This affects the distance metric
used to measure closeness of points. Cleveland (1979) proposed using
robust regression procedures to eliminate outlying or erroneous points in
the regression process. Stone ( 1977, 1 982), Devroye (1981 ), Lancaster
and S alkauskas (1981), Cheng ( 1 984), Li (1984), Farwig (1987), and
Miiller ( 1 987) provide analyses of nearest neighbor approaches. Franke
(1982) compares the performance of nearest neighbor approaches with
other methods for fitting surfaces to data. Devroye (1978) has explored
nearest neighbor approaches to function optimization.
Several direct implementations of associative content-addressable
memories have been proposed. As mentioned previously, a standard
serial computer searching experiences sequentially can be used to simu­
late an ACAM. The problem with this approach is that the search time
increases as more experiences are added to the memory. The search
time is proportional to the number of experiences. One way to reduce
the search time is to use a table. The values of the desired experience
are used to compute an address in the table, and all experiences that
fall into a particular table entry are stored there. In order to find the
nearest neighbor, all experiences in that entry and all neighboring en­
tries must be examined. The dimensions of such a table must match the
dimensions of the data, and therefore the number of entries in the table
grows exponentially with the number of dimensions of an experience, as
does the number of table entries that must be searched on each access.
A benefit of the approach we have used to implement an ACAM is that
memory requirements are much less than those of tabular memories. We
only store experiences that have actually occurred, rather than allocate
storage for all possible experiences. Memory size is proportional to the
number of experiences encountered. It is also proportional to the size
of each experience, and therefore the number of dimensions stored. The
memory required grows only linearly with the number of experiences
and the number of dimensions; rather than. exponentially. However, the
memory used is subst�4�{han a table memory. This
is particularly well suited for systems with a large number
Another approach to implementing an ACAM that represents the data
directly is to use k-d trees (Friedman, Bentley, and Finkel 1977). This
approach uses a tree data structure to speed up the search for the best
match. However, the number of data points that must be examined to
find the best match grows exponentially with the number of dimensions
in the k-d tree search should be used instead, as it
Using this first match greatly reduces the search time.
of generalization functions to model chaotic dynamic systems.
There is a single layer of neurons, often referred to
units," which each compute an inner product of the weights
on their input connections with the corresponding inputs to the net­
work. This layer is followed by a maximum detection or winner-take-all
The representation is local since only one "hidden unit" is
This type of network can find nearest neigh­
bors or best matches using a Euclidean distance metric (Kazmierczak
comparison between network models using a local and a distributed rep­
nary network learns a local rather than distributed representation using
It is important to keep in mind that all of the forms of direct data
representation will give the same outputs if the design choices for the
teacher, distance metric, and generalization function are the same. How
should be implemented is a function of the hardware
available. The direct implementation of an
Parametric models whose structure matches the structure of the system
being modeled are often u�ed to fit an �ntire training data set (see An,
Atkeson and Holler�J('fg� qd16� references from robotics, and
Kawato, FUrukawa, and Suzuki ( 1 987) for an approximate parametric
model of robot arm rigid body dynamics using a connectionist network ) .
These models are often referred to as global models. If the structure of
a global model is correct, the model is capable of wide generalization.
However, it is often difficult to find the appropriate model structure for
a system. Most of the other approaches to modeling described in this
paper are motivated by a desire to avoid having to know an appropriate
model structure prior to fitting the data.
We have previously described how a table can speed access to directly
represented experiences sorted into the table entries. The experiences as­
signed to each table entry can also be averaged or combined in some way
to form a set of parameters or weights. In this case the directly stored
experiences do not need to be examined when the table is used. Raib­
ert and Wimberly's ( 1 984) table was of this form. Michie and Cham­
bers (1968) implemented an early table-based controller to balance an
inverted pendulum on a moving cart. Connectionist network represen­
tations ( described below ) with a local representation can be used to
implement a table of weights ( Barto, Sutton, and Anderson 1983) . Al­
bus used a hashing algorithm and a fixed overlap of tabular weights to
reduce problems of table size and generalization ( Albus 1 975a, 1 975b,
An alternative way to implement an associative content-addressable
memory is in a distributed connectionist network ( Rumelhardt and Mc­
Clelland 1986) . The desired experience is presented to the network as a
set of activations to the input nodes. The output is represented as a set
of activations on the output nodes. There may be several intermediate
layers of nodes. Each node computes a weighted sum of the activations
of the nodes connected to it, and some function of that weighted sum
of the inputs becomes its activation. The weights are the parameters
modified to make the network represent a particular function. Single
experiences are represented in a distributed fashion by the weights, and
each new experience may modify all the weights. Single layer connection­
ist networks were used early on to solve control problems ( Widrow and
Smith 1 964). An example of a recent distributed connectionist approach
to modeling robot dynamics is provided by ( Goldberg and Pearlmutter
Computing the output is straightforward for connectionist networks,
as each node computes in parallel a simple function of its inputs. The
appeal of these networks is that they can be implemented using simple
priate set of weights for the network is a difficult problem and usually
involves some form of iterative gradient descent search, and additional
hardware to implement the weight finding algorithm.
the direct forms of data representation representing the data is straight­
forward; the data is simply stored without any conversion or change
Finding the appropriate output, however, usually
involves a substantial amount of computation with relatively complex
in solving a pronunciation task. Farmer and
provide a comparison of a direct data representation
system with a connectionist network representation for modeling chaotic
Representing data in terms of distributed weights has several impli­
The need to find the appropriate set of weights to represent
a given set of experiences adds an additional learning problem to that
of generating useful experiences. If an iterative gradient descent search
algorithm is used to find the appropriate set of weights, previous experi­
ences must be presented several times to the network or new experiences
collected during the iterative search. Learning new experiences may de­
grade the representation of older experiences, unless the older experi­
ences are stored in another form of memory and presented again during
It is not certain that the search for appropriate weights
will converge to a useful set of values due to the presence of locally op­
timum weights, and it is not certain that a useful set of weight values
exists. It is difficult to detect when there are no close experiences and
the best match is too dissimilar to be useful. The original distribution
of experiences is not represented in the weights of the network.
Although the algorithms connectionist networks use to compute out­
puts are easily specified, they are difficult to analyze. It is not clear how
to design networks to represent a particular function: How many nodes
and layers are required, for example? Although successful network learn­
ing procedures have been demonstrated it is not yet clear how well they
will scale up to realistically sized problems, how effectively they use data
ization function is implicit in the network design, rather than explicitly
design, and the teaching issues are similar for ACAMs implemented us­
ing direct data representations and distributed representations.
Direct implementations of ACAMs raise their own set of implementa­
tion issues. For example, while the question of how to find the appropri­
ate weights or parameters for a representation is avoided, the distance
metric and generalization algorithm must be explicitly provided by the
implementor. We should not only explore different neural network de­
signs, but also explore the full range of methods available to implement
associative content-addressable memories. The question of whether an
ACAM is useful for control should be separated from the question of
This initial feasibility study demonstrates that it is possible to learn
a useful model by storing experiences in a memory. The use of paral­
lel search in the implementation of an associative content-addressable
memory allowed quick searching of stored experiences, and reasonable
retrieval was obtained using a simple distance metric and a simple gener­
alization scheme. The memory was able to generalize after storing only
a small number of relevant experiences. The use of search by parallel
processors also allowed us to avoid many of the problems of previous
memory-based or tabular approaches to modeling such as search speed
and memory requirements. Further experimentation and development
is required to solve problems such as stuck states.
This article describes research done at the Whitaker College, Depart­
ment of Brain and Cognitive Sciences, and the Artificial Intelligence
Laboratory of the Massachusetts Institute of Technology. Support was
provided in the Whitaker College under Office of Naval Research con­
tract NOOOl4-88-K-0321. Support for the A. I. Laboratory's research is·
provided in part by the Advanced Research Projects Agency of the De­
partment of Defense under Office of Naval Research contract NOOOl4-85K-0124, and the Office of Naval Research University Research Initiative
Program under Office of Naval Research contract N00014-86-K-0685.
Support for CGA was provided by a National Science Foundation En­
gineering Initiation Award an� Presidentia} Young Investigator Award
and a Whitaker Health §�tCiiffi &ffKW.aculty Research Grant.
B. Widrow made the authors aware of early work on neural net­
work implementations of direct associative content addressable memory
Albus, J. S. (1975a). A new approach to manipulator control: The
cerebellar model articulation controller ( CMAC ) . ASME Journal of
Dynamic Systems, Measurement, and Control 97:22D-227.
Albus, J. S. (1975b). Data storage in the cerebellar model articulation
controller ( CMAC ) . ASME Journal of Dynamic Systems, Measure­
An, C. H., Atkeson, C. G., and Hollerbach, J. M. ( 1988). Model-based
control "of a robot manipulator. Cambridge, MA: MIT Press .
Atkeson, C. G . (1989). Learning arm kinematics and dynamics.
Atkeson, C. G., Aboaf, E. W. McIntyre, J., and Reinkensmeyer, D. J.
(1988). Model-Based Robot Learning. in Robotics Research: The
Fourth International Symposium 103-110, R. C. Bolles and B. Roth,
Barnhill, R. E. (1977). Representation and approximation of surfaces. In
Mathematical Software III, 69-120, J. R. Rice, New York: Academic
Barto, A. G., Sutton, R. S. and Anderson, C. W. (1983). Neuron­
like adaptive elements that can solve difficult learning control prob­
lems. IEEE Transactions on Systems, Man, and Cybernetics SMC13(5):834-845.
Baum, E. B., Moody, J., and Wilczek, F. ( 1988). Internal representa­
tions for associative memory. Biological Cybernetics 59, 217-228.
Cheng, P. E. ( 1984). Strong consistency of nearest neighb or regression
function estimators. Journal of Multivariate Analysis 1 5:63-72.
Cleveland, W. S. ( 1 979) . Robust locally weighted regression and smooth­
ing scatterplots. Journal of the A merican Statistical A ssociation
Cover, T. M. ( 1 968) . Estimation by the nearest neighbor rule.
Transactions on Information Theory IT- 14:50-55.
Crain, 1. K., and Bhattacharyya, B. K. ( 1967 ) . Treatment of nonequis­
paced two dimensional data with a digital computer. Geoexploration
Devroye, L. P. ( 1 978) . The uniform convergence of nearest neighbor
regression function estimators and their application in optimization.
IEEE Transactions on Information Theory IT-24: 142-1 5 1 .
Devroye, L. P . ( 1981 ) . O n the almost everywhere convergence o f non­
parametric regression function estimates. The Annals of Statistics
Eubank, R. L. ( 1 988) . Spline smoothing
Falconer, K. J. ( 1 971 ) . A general purpose algorithm for contouring over
scattered data points. National Physical Laboratory Report NAC
Farmer, J. D. , and Sidorowich J. J. ( 1 988) . Exploiting chaos to predict
the future and reduce noise. Technical Report LA-UR-88-901, Los
Alamos National Laboratory, Los Alamos, NM.
Farwig, R. ( 1987) . Multivariate interpolation of scattered data by mov­
ing least squares methods. In J. C. Mason and M. G. Cox eds . , A l­
gorithms for Approximation, 193-2 1 1 . New York: Clarendon Press.
Fix, E. and Hodges, J. L., Jr. ( 1951 ) . Discriminatory analysis, non­
parametric regression: Consistency properties. Project 2 1-49-004,
Report No. 4. USAF School of Aviation Medicine Randolph Field,
Fix, E. and Hodges, J. L . , Jr. ( 1952 ) . Discriminatory analysis: Small
sample performance. Project 2 1-49-004, Rep. 1 1 USAF School of
Franke, R. ( 1982) . Scattered data interpolation: Tests of some methods.
Mathematics of Computation 38( 1 57) : 1 81-200.
Franke, R. and Nielson, G. ( 1980) . Smooth interpolation of large sets
of scattered data. International Journal Numerical Methods Engi­
Friedman, J. H . , Bentley, J. L . , and Finkel, R. A. ( 1977) . An algo­
rithm for finding best matches in logarithmic expected time. A CM
Transactions on Mathematical Software 3(3) :209-226.
Goldberg, K. Y. and Pearlmutter, B. (1988) . Using a neural network to
learn the dynamics of the CMU direct-drive arm II. Technical Report
CMU-CS-88-160, Carnegie-Mellon University, Pittsburgh, PA.
Gordon, W. J. and Wixom, J. A. ( 1978) . Shepard's method of metric in­
terpolation to bivariate and multivariate interpolation. Mathematics
Hinton, G. E. ( 1986) . Learning in massively parallel nets. In
ings of the 5th National Conference on A rtificial Intelligence,
Kawato, M . , Furukawa, K . , and Suzuki, R. ( 1987) . A hierarchical neural­
network model for control and learning of voluntary movement. Bi­
Kazmierczak, H., and Steinbuch, K . ( 1 963) . Adaptive systems in pat­
tern recognition. IEEE Transactions on Electronic Computers EC12:822-835.
Lancaster, P. , and S alkauskas, K. ( 1981 ) . Surfaces generated by moving
least squares methods. Mathematics of Computation 37( 1 55) : 1411 58.
Lancaster, P. , and S alkauskas, K. ( 1 986) .
Li, K. C. ( 1984). Consistency for cross-validated nearest neighbor esti­
mates in nonparametric regression. The A nnals of Statistics 12:230240.
Lodwick, G. D . , and Whittle, J. ( 1970) . A technique for automatic
contouring field survey data. A ustralian Computer Journal 2 : 104109.
Loftsgaarden, D . O. and Quesenberry, C . P. ( 1 965) . A nonparametric
estimate of a multivariate density function. A nnals of Mathematical
Lorenz, E . N. ( 1969) . Atmospheric predictability as revealed by nat­
urally occurring analogues. Journal of the A tmospheric Sciences
Macauley, F. R. ( 1931) . The smoothing of time
McLain, D. H. ( 1 974) . Drawing contours from arbitrary data points.
Michie, D., and Chambers, R. A. ( 1 968) . Boxes: An experiment in adap­
tive control. In E. Dale and D. Michie eds . , Machine Intelligence 2,
Miller, W. T., Glanz, F. H., and Kraft, 1. G . ( 1 987) . Application of a
general learning algorithm to the control of robotic manipulators.
International Journal of Robotics Resear'ch, 6:84-98.
Miiller, H. G. ( 1 987) . Weighted local regression and kernel methods
for nonparametric curve fitting. Journal of the A merican Statistical
Omohundro, S. M . , ( 1 987) . Efficient algorithms with neural network
behavior. Journal of Complex Systems 1 (2) , pp. 273-347.
Palmer, J. A. B. ( 1 969) . Automated mapping. Proceedings
Australian Computer Conference 6:463-466.
Pelto, C. R. , Elkins, T. A., and Boyd, H. A (1968) . Automatic contour­
ing of irregularly spaced data. Geophysics 33:424-430.
Raibert, M. H. ( 1984) . Hopping in legged systems-modeling and simu­
lation for the two-dimensional one-legged case. IEEE Transactions
on Systems, Man, and Cybernetics SMC-14(3):451-463.
Raibert, M. H . , and Wimberly, F. C. ( 1 984) . Tabular control of balance
in a dynamics legged system. IEEE Transactions on Systems, Man,
Royall, R. M. ( 1966). A class of nonparametric estimators of a smooth
regression function. Ph. D. dissertation and Tech Report No. 14,
Public Health Service Grant USPHS-5T1 GM 25-09 , Department of
Statistics, Stanford University, Stanford, CA.
processing: Explorations in the microstructure of cognition,
Sabin, M . A. ( 1 980) . Contouring: A review of methods for scattered
data. In Mathematical methods in computer graphics and design,
63-86, K.W. Brodlle ed. , New York: Academic Press.
Scalettar, R . , and Zee, A. (1988) . Emergence of grandmother memory
in feed forward networks: Learning with noise and forgetfulness. In
Connectionist models and their implications: Readings from cogni­
309-327, D. Waltz and J.A. Feldman eds . , Norwood,
Shepard, D. ( 1 968) . A two-dimensional function for irregularly spaced
data. In Proceedings of Twenty-third ACM National Conference,
517-524, Princeton, NJ: Braudon/Systems Press.
Communications of the A CM, 29( 1 2 ) : 1 2 1 3-1 228.
Steinbuch, K . , and Piske, U. A. W. ( 1 963) . Learning matrices and
their applications. IEEE Transactions on Electronic Computers EC12:846-862 .
Steinbuch, K., and Widrow, B. ( 1 965) . A critical comparison of two
kinds of adaptive classification networks. IEEE Trans. on Electronic
Stone, C. J. ( 1975) . Nearest neighbor estimators of a nonlinear re­
gression function. Proceedings of Computer Science and Statistics:
Eighth A nnual Symposium on the Interface, 4 1 3-41 8 , Los Angeles,
Stone, C. J. ( 1982 ) . Optimal global rates of convergence for nonpara­
metric regression. The A nnals of Statistics 1 0(4) : 1 040-1053.
Journal of Statistics, Series A 26:359-372.
Widrow, B . , and Smith, F. W. ( 1 964) . Pattern recognizing control sys­
tems. In J . T. Tou and R. H. Wilcox, eds . , Computer and informa­
tion sciences, Washingt on, D . C . : Spartan Books.
The control of severely nonlinear systems has for the most part escaped
the attention of control theorists and practitioners.
dresses the issue from the point of view of utilizing self-learning tech­
niques to achieve nonlinear controller design. The methodology shows
promise for applications to control problems that are so complex that
analytical design techniques either do not exist or will not exist for some
time to come. Neural networks can be used to implement highly non­
linear controllers whose weights or internal parameters can be chosen or
Backing a trailer truck to a loading dock is a difficult exercise for all
but the most skilled truck drivers. Anyone who has tried to back up a
house trailer or a boat trailer will realize this. Normal driving instincts
When watching a truck driver backing toward a loading dock, one
often observes the driver backing, going forward, backing again, going
forward, etc., and finally backing to the desired position along the dock.
The forward and backward movements help to position the trailer for
successful backing up to the dock. A more difficult backing up sequence
would only allow backing, with no forward movements permitted. The
specific problem treated in this paper is that of the design by self-learning
of a nonlinear controller to control the steering of a trailer truck while
backing up to a loading dock from an arbitrary initial position.
backing up is allowed. Computer simulation of the truck and its con­
troller has demonstrated workability, although no mathematical proof
The experimental controller contains twenty six adaptive
up control. The trailer truck can be initially "jackknifed" and aimed in
many different directions, toward and away from the dock, but as long
as there is sufficient clearance, the controller appears to be capable of
Figure 12.1 shows a computer-screen image of the truck, the trailer,
and the loading dock. The critical state variables representing the po­
The truck, the trailer, and the loading dock.
the truck, Xcab and Ycab, the Cartesian position of the yoke, Xtrailer and
Ytrailen the Cartesian position of the rear of the center of the trailer,
and Xdock and Ydock, the Cartesian position of the center of the loading
dock. Definition of the state variables is illustrated in figure
The truck backs up until it hits the dock, then stops. The goal is to
cause the back of the trailer to be parallel to the loading dock, and to
point ( Xdock, Ydock ) . The controller will learn to achieve this objective.
The approach to self-learning control that has been successfully used
with the truck backer-upper involves a two-stage learning process. The
first stage involves the training of a neural network to be an emulator
of the truck and trailer kinematics. The second stage involves the train­
ing of a neural-network controller to control the emulator.
Once the controller knows how to control the
emulator, it is then able to control the actual trailer truck. Figure
Example of Self-Learning in Neural Networks
to the controller which in turn provides a
(hard right) and +1 (hard left) to the truck. The time index is
time cycle, the truck backs up by a fixed small distance. The next state
is determined by the present state and the steering signal, which is fixed
shows a block diagram of the process used to train the
The truck backs up randomly, going through many cycles
with randomly selected steering signals. By this process, the emulator
"gets the feel" of how the trailer and truck behave. The emulator, chosen
two-layer neural network, learns to generate the next positional
state vector when given the present-state vector and the steering signal.
This is done for a wide variety of positional states and steering angles.
The two-layer emulator is adapted by means of the backpropagation
The first layer had six present-state inputs plus the present steering
signal input. This layer contained forty-five hidden adaptive ADALINE
units producing six next-state predictions. Once the emulator is trained,
it can then be used to train the controller.
The identical blocks labeled C represent the
controller. The identical blocks labeled T represent the truck and trailer
Suppose that the truck is engaged in backing up.
be chosen randomly and be initially fixed.
is fed to C, which produces the steering signal output which sets the
truck and trailer soon arriving at the next state
fixed, the backing up process continues from cycle to cycle until the
truck hits something and stops. The final state sKis compared with the
desired final-state (the rear of the trailer parallel to the dock with proper
positional alignment) to obtain the final state error vector EK. This error
vector contains three elements (which are the errors of interest),
Ytrailer and ()trailer, and is used to adapt the controller C.
The method of adapting the controller C is illustrated in figure
The final-state error vector EK is used to adapt the blocks labeled C,
which are maintained identical to each other throughout the adaptive
process. The controller C is a two-layer neural network. The first layer
has the six state variables as inputs, and this layer contains twenty five
adaptive ADALINE units. The second or output layer has one adaptive
ADA LINE unit and produces the steering signal as its output.
The procedure for adapting C goes as follows. The weights of C are
The initial position of the truck is chosen
Example of Self-Learning in Neural Networks
cycles, until it stops. The final error is used by backpropagation to adapt
Each of the C blocks could be tentatively adapted by
backpropagation if they were independent of each other, but the actual
weight changes in C are taken as the sum of the tentative changes. In this
way, the C blocks are maintained identical to each other. The weights
are changed by this constrained backpropagation algorithm to reduce
the sum of the squares of the components of the final-state error EK
by following the negative of the gradient, using the method of steepest
descent. The entire process is repeated by placing the truck and trailer
in another initial position, and allowing it to back up until it stops. Once
again, the controller weights are adapted. And so on.
show details of one of the state transition
stages of figure 12.5. One can see the structure of controller C and of
emulator T, and how they are interconnected. Each stage of figure 12.5
amounts to a four-layer neural network. The entire process of going from
an initial state to the final state can be seen from figures
to be analogous to a neural network having a number of layers equal to
four times the number of backing up steps when going from the initial
state to the final state. The number of steps varies of course with initial
The diagram of figure 12.5 was simplified for clarity of presentation.
The output error does not go directly to the C-blocks as shown, but
backpropagates through the T-blocks and C-blocks.
used to adapt each of the C-blocks does originate from the output error
Training the controller with backpropagation.
Example of Self-Learning in Neural Networks
of back-propagation of the error, the T-blocks are the truck emulator.
But the actual truck kinematics are used when sensing the error EX
The truck emulator was able to represent the trailer and truck when
jackknifed, in line, or in any condition in between. Nonlinearity in the
emulator was essential to represent the truck and trailer.
between truck and trailer were not s mall. Sin () could not be represented
Nonlinearity in the controller was also essential.
were used to determine the parameters of both
the emulator and the controller. Thousands of backups were required to
amounts of human effort and design time would have been required to
The training of the controller was divided into several "lessons." In
the beginning, the controller was trained with the truck initially set to
points very near the dock and the trailer pointing at the dock.
the controller was proficient at working with these initial positions, the
problem was made harder by starting the truck farther away from the
dock and at increasingly difficult angles. This way, the controller learned
to do easy problems first and more difficult problems after it mastered
the easy ones. There were sixteen lessons in all. In the easiest lesson
the trailer was set about half a truck length from the dock in the
direction pointing at the dock, and the cab at a random angle between
degrees. In the last and most difficult lesson the rear of the trailer
truck length from the dock in the y direction. The
cab and trailer angle was set to be the same, at a random angle between
degrees. The controller was trained for about
The controller learned to control the truck very well with the above
training process. Near the end of the last lesson, the root mean square
was about 7 degrees. There is no error in
them different weightings during training.
Results with the adaptive controller are illustrated in figures
The controller has already been trained and its weights
remained fixed for all the experiments. The truck and trailer were placed
in a variety of initial conditions, and backing up was effected in each case.
Initial and final states are shown in the computer screen displays, and
the dynamics of backing up is illustrated by the time-lapse plots.
The truck backer-upper learns to solve sequential decision problems.
The control decisions made early in the backing up process have sub­
a direction to reduce error, but they position the truck and trailer for
ultimate success. In many respects, the truck backer-upper learns a con­
trol strategy that is like a dynamic t>rogr.amming problem solution. The
learning is done in aq��flxlKl�rk. Connecting signals from
Example of Self-Learning in Neural Networks
one layer to another corresponds to idea that the final state of a backing
the initial state of the next backing up cycle.
Determination of complexity of controller
Determination of convergence and rate of learning for emulator
Analytic derivation of nonlinear controller for truck backer-upper,
Example of Self-Learning in Neural Networks
Exploration of other areas of application for self-learning neural
T his research was sponsored by SDIO Innovative Science and Technol­
ogy Office and managed by ONR under contract #NOOOl4-86-K-0718,
by the Department of the Army Belvoir Research, Development, and En­
gineering Center under Contract #DAAK70-89-K-0001, and by a grant
T his material is based on work supported under a National Science
Foundation Graduate Fellowship. Any opinions, findings, conclusions,
or recommendations expressed in this publication are those of the au­
thors and do not necessarily reflect the views of the National Science
Jordan, M. 1. (1988). Supervised learning and systems with excess de­
COINS Technical Report 88-27, Massachusetts
Parker, D. B. (1985). Learning logic. Technical Report TR-47, Center
for Computational Research in Economics and Management Science,
Massachusetts Institute of Technology, Cambridge, MA.
Rumelhart, D. E ., and McClelland, J. L., eds.
tributed processing: Explorations in the microstructure of cognition.
Vol. I, Foundations, chap. 8. Cambridge, MA: MIT Press.
analysis in the behavioral sciences. Ph.D. diSs., Harvard University,
W idrow, B. (1986). Adaptive inverse control . In Adaptive systems in
control and signal processing, International Federation of Automatic
Control, July. Pergaman Press, Lund, Sweden.
Widrow, B., and Hoff, M. E., Jr. (1960). Adaptive switching circuits.
Example of Self-Learning in Neural Networks
Widrow, B. and Stearns, S. D. (1985). Adaptive signal proccessing. En­
James C. Houk, Satinder P. Singh, Charles Fisher,
In spite of much recent progress in the field of robotics, it is widely ac­
knowledged that the control of robot limbs is crude in comparison with
the facile manner in which primates use their limbs. While primates
may have more efficient effectors (muscles) and better sensors (sensory
receptors), we believe that the most important reason for superior per­
formance is because primates have more effective mechanisms for the
The cerebellum is one of the main regions of the brain implicated in
the adaptive control of movement (see Ito 1984). This brain structure
has long been celebrated as a beautifully organized network of massively
interconnected neurons (Eccles, Ito, and Szentagothai 1967). The ele­
gant structure of the cerebellar cortex has inspired the construction of
perceptron-like models that are capable of learning complex functional
maps between inputs and outputs (Marr 1969, Albus 1971), and a va­
riety of adaptive controllers and filters based on these network models
have been designed by subsequent authors (cf. section 13.8).
These models were inspired mainly by the knowledge of the synap­
tic connectivity of the cerebellum that was obtained in the late sixties
with electrophysiological and neuroanatomical methods (Eccles, Ito, and
Szentagothai 1967). Knowledge regarding information flow through cere­
bellar pathways and cellular properties of neurons was scanty at that
time, so major assumptions had to be made. We now know consider­
ably more about these latter topics, although admittedly much is yet to
While recent information about synaptic plasticity supports one of
the key assumptions of these models, namely that synaptic inputs to
cerebellar Purkinje cells are subject to adaptive modification, new data
regarding information processing, cerebellar circuits, and biophysical
properties of neural membranes were unanticipated by the earlier mod­
els. These new data suggest novel approaches that might appreciably
impact our concepts of sensorimotor processing in adaptive neural net­
works. One purpose of this chapter is to provide a brief summary of
cerebellar anatomy and physiology that incorporates these recent find­
ings. However, the maifj;wrig§taBlN�e a novel approach to the
design of adaptive sensorimotor networks, an approach that is inspired
by our new knowledge of cerebellar function.
Anatomical and physiological studies have revealed a large number of
cell types in the cerebellar network and a formidable number of intercon­
nections between these cell types. In addition to revealing complexity,
these studies have also unveiled a high degree of structural order that
is expressed at both microscopic and macroscopic levels (BrodalI981).
On the microscopic level, each small region of the cerebellum contains
approximately the same densities and ratios of neuron types, and local
connections between the different types show approximately the same
patterns from one region to another. On the macroscopic level, different
categories of input project to different global regions of the cerebellum,
and each of these regions in turn projects differentially to specific targets
in the brainstem and diencephalon. In this section we discuss first the
notion of modularity that has emerged from the macroscopic order seen
in the cerebellar network. We then review the microscopic structure
of the modules and lay the groundwork for treating these modules as
The microscopic similarity of local circuitry coupled with macroscopic
variety in input-output connectivity has suggested to a number of au­
thors that the cerebellum is organized in a modular fashion (Eccles, Ito,
and Szentagothai 1967; Voogd and Bigare 1980; Andersson et al. 1987;
Ito 1984). Module boundaries have differed from one author to another,
and the module proposed here is yet another design. However, the basic
notion has remained quite similar. Each module is presumed to operate
in the same manner on whatever inputs come to it. Modules in different
regions of the cerebellum are able to perform different functions in a
behavioral sense by virtue of their specific input-output connections.
As a first approximation, one can consider the cerebellum as consist­
ing of three medial to lateral regions (Ito 1984; Stein 1986). (1) Much of
the medial region connects with vestibulospinal and reticulospinal path­
ways, which fits with its specialization for the control of whole body
movements and posture. (2) The intermediate region connects with the
connects prominently with the premotor frontal cortex, and it appears
specialized for target prediction, motor planning and other high-level
modes of processing. Each of these global regions contains a large num­
The hypothesis developed here, that the cerebellum functions as an
array of adjustable pattern generators, takes full advantage of the mod­
However, unlike most previously proposed cerebellar modules, an ad­
justable pattern generator is a module that includes feedback loops be­
tween the cerebellum and other brain structures, in addition to strictly
cerebellar circuitry. Since each region of the cerebellum connects with
different brainstem or cerebral structures, modular designs for adjustable
pattern generators must accommodate to these regional specializations
( Houk 1989). Rather than attempting to discuss several variations of
pattern generator modules, our approach here will be to focus on the
intermediate cerebellum, and then upon one particular pathway linking
the cerebellum to the spinal cord. Thus, we will emphasize the cere­
bellorubrospinal pathway which has been studied extensively in Houk's
laboratory ( see Houk and Gibson 1987). This pathway is known to
play a particular role in the control of limb movements ( Kuypers 1981).
The reader may wish to consult Ito's (1984) excellent monograph for a
discussion of the anatomy and physiology of other cerebellar systems.
Circuit Diagram of a Pattern Generator Module
Figure 13.1 summarizes some of the main connections in the cerebel­
lorubrospinal pathway, and it also serves as a circuit diagram of the pos­
tulated module, an individual adjustable pattern generator. The output
of this circuit is a rubrospinal fiber ( for simplicity, let us consider just
one) . Each rubrospinal fiber carries a motor command from the red
nucleus ( R in figure 13.1) to the spinal cord where it acts upon motor
neurons and interneurons, thus contributing to the control of limb move­
ments. Rubrospinal fibers also send collaterals to the lateral reticular
nucleus (L) which projects back into the cerebellum, both to the deep
cerebellar nucleus (N) and to Purkinje cells (P) in the cerebellar cortex
by way of the mossy-fiber/ parallel-fiber pathway that is discussed later.
The main input to the red nucleus comes from the cerebellar nucleus,
but there are weaker inputs coming from the sensorimotor cortex and
The pathway from the cerebellar nucleus to red nucleus and back
to the cerebellar nucletl8�ElflitMiitEW�ular nucleus is a positive
Circuit diagram of an adjustable pattern generator. This diagram applies
specifically to cerebellorubrospinal pattern generators (but see H ouk , 1989, for
extensions to motor cortical loops). Note the positive feedback loop from cerebellar
nuclear (N) cells, to red (R) n ucle us , to the lateral (L) reticular nucleus, back to
nuclea r cells. I nh ibi tory input from a band of P urk i nj e (P) cells (only 3 shown)
located in the cerebellar cortex regulates the intensity of loop activity. Basket (B)
feedback loop. Inhibitory input to the cerebellar nucleus from Purkinje
cells normally restrains this positive feedback, but when the inhibition is
blocked, sustained high-frequency discharge is observed. The adjustable
pattern generator model treats this tendency for sustained loop activity
as the fundamental driving force for the readout of a motor program
(Houk 1987). The detailed, time-intensity profile comprising a motor
program is thought to be sculpted out of this tendency for sustained
loop activity, the sculptor being the powerful inhibitory input to the
Purkinje cells are located in the cerebellar cortex where they are dis­
tributed in a monolayer. Each cerebellar nuclear cell receives inhibi­
tion from several hundred Purkinje cells (see Ito 1984), only three of
which are shown in figure 13.1. The Purkinje cells innervating a given
nuclear cell are situated in a longitudinal band that is aligned in an
anterior-posterior, or parasagittal, plane. This arrangement is illus­
trated schematically in figure 13.2 by a band of twelve Purkinje cells
viewed from the surface of the cerebellar cortex. The box surrounding
these cells is meant to outline the domain of cerebellar cortex that is
associated with one adjustable pattern generator.
The dendritic trees of Purkinje cells (stippled in figure 13.2) are fan­
shaped and longitudinally aligned. Parallel fibers running perpendicular
to the dendritic fans make synapses at points of crossing to provide a
large number of excitatory inputs (up to 200,000) to each Purkinje cell.
Parallel fibers are the axons of the numerous small granule cells (two
shown in figure 13.1) contained in the granular layer of the cerebellar
cortex. Granule cells receive their inputs from mossy fibers which trans­
mit a variety of sensory and efference copy signals that will be described
Considering mossy fiber signals to be elements of input vectors, the ge­
ometrical arrangement of the mossy-fiber/parallel-fiber pathway seems
optimal for presenting a very large input vector to each Purkinje cell (fig­
ure 13.2). Each signal is first distributed longitudinally by the branching
of a mossy fiber to many granule cells (filled circles along the fiber). Then
the axons of the granule cells, the parallel fibers, distribute the signals
in the mediolateral direction to intersect the dendritic fans of Purkinje
cells. In this manner an input vector with many similar elements can be
presented to a large group of Purkinje cells. This group would comprise
the Purkinje domains of many adjustable pattern generators.
A second major category of input to Purkinje cells comes from the
inferior olivary nucleus. The �ons of olivary neurons are called climbing
fibers because of the di���t{Hiilhich they wrap themselves
Organization of a Purkinje domain. The view is looking down onto the surface of
the cerebellar cortex. Mossy fibers distribute input to a series of granule cells along
the longitudinal (shown vertical) direction of the body, within the parasagittal
plane of tbe cerebellum. The diagram shows two mossy fibers as heavy vertical lines
punctuated by filled circles representing the granule cells they innervate. Each
granule cell axon bifurcates to form a parallel fiber that travels medially and
laterally (thin lines running left and right) to innervate large numbers of Purkinje
cells (only 12 are shown for simplicity). The dendritic trees of Purkinje cells are
fan-sh ap ed and are portrayed as oblong stippled zones as viewed from above. A
P urkinje domain is defined as a group of Purkinje cells that innervates a particular
nuclear celL Purkinje cells in a Purkinje domain are innervated by a coherent set of
climbing fibers and receive inhibitory input from a common basket cell.
around Purkinje cells (figure 1 3.1), and these fibers also send collaterals
to the cerebellar nuclei (not shown for simplicity) . Each Purkinje cell
receives input from only a single climbing fiber, in marked contrast to
the massive convergence of parallel fiber input onto individual Purkinje
cells. Climbing fibers produce large excitatory potentials in Purkinje
cells that are followed by periods of depression. In addition, and perhaps
more importantly, they appear to transmit training signals that adjust
the synaptic weights of the parallel fiber inputs onto Purkinje cells, as
On a more global scale, the projections from the inferior olive to the
cerebellum are exquisitely organized into longitudinal bands that are
oriented perpendicular to the parallel fibers. Recalling that the Purk­
inje domains of adjustable pattern generators are also longitudinally
organized, it is reasonable to postulate that the entire climbing fiber
innervation of an adjustable pattern generator (large arrow in figure
13.2) may derive from a small cluster of cells in the inferior olive. Since
adjacent olivary neurons typically have similar response properties, the
climbing fiber inputs to each adjustable pattern generator could be a
Considerably less is known about a third type of input to the cere­
bellum, the innervation supplied by aminergic fibers (figure 13. 1). The
neurotransmitters used by these fibers are two amines, norepinephrine
and serotonin, which are usually assumed to function as global mod­
ulators of neuronal activity. This would fit with the diffuse nature of
the input seen in anatomical studies. These systems seem well suited to
broadcast general messages to the whole network.
Several types of inhibitory interneuron are found in the cerebellar
cortex, only one of which will be mentioned here. We have chosen to
highlight the basket cell (B in figure 13. 1 ) because of its potential for
coordinating groups of Purkinje cells subserving individual pattern gen­
erators. Basket cells receive the same types of input as Purkinje cells,
although the innervation is less elaborate. They project asymmetrically
to a longitudinal band of about 30 Purkinje cells in the manner illus­
trated in figure 1 3.2. This pattern seems ideal for sending a common
message to the cells in an individual Purkinje domain.
Having described the circuit diagram of an adjustable pattern gener­
ator in the cerebellorubro spinal pathway, we now need to discuss the
manner in which these modules are likely to process information.
The most direct approach to an assessment of information processing in
cerebellar modules is to monitor the signals that are transmitted along
outputs should suggest the manner in which information is processed as
it passes through cerebellar modules. The account provided here is based
heavily on studies which have used the cerebellorubrospinal pathway
as a model system for exploring sensorimotor processing through the
cerebellum (Houk and Gibson 1987), though
Mossy fibers originate from several brain sites, transmitting a diver­
sity of information about the internal state of the body and the state
of the external environment. Some of these inputs are clearly sensory.
They come quite directly from cutaneous, muscle or vestibular receptors.
Other mossy fiber inputs are routed more indirectly via the cerebral cor­
tex. They transmit highly processed visual, auditory or somatosensory
information. Yet another category of mossy fiber transmits information
about central motor commands. These signals are designated efference
copy since they transmit copies of the efferent signals that are sent to
and transmits its information through the lateral reticular nucleus.
the spinal cord and onward to muscles. Figure 13.1 shows an example
efference copy signal that arises from collaterals of the rubrospinal
In general, the discharge rates of mossy fibers are modulated over a
wide dynamic range which permits them to transmit detailed paramet­
An example particularly relevant to the operation of
adjustable pattern generators concerns sources of information about the
Signals derived from muscle proprioceptors
provide sensory information about actual limb position whereas signals
derived from efference copy provide predictions of the position the limb
will soon assume if the motor command is successfully completed.
types of mossy fiber have discharge rates that code limb position, and
The sole source of climbing fibers is from cells located in the inferior
olivary nucleus. These neurons respond to various types of soma.tosen­
sory, vestibular or visual stimuli. Different cells have different receptive
motion IS well preserve'tt'TtW'contr��, o vary neurons appear to 19nore
information about the intensity or duration of a stimulus. This is be­
cause the cells have unusually long refractory periods which typically
limits their responses to a single action potential. As a consequence, oli­
vary neurons become selectively responsive to the occurrences of stimuli
and not to their parameters. This is why climbing fibers have been
characterized as somatic event detectors.
There are also efference copy inputs to the climbing fiber pathway,
which appear to be inhibitory. These inputs gate off responsiveness to
self-induced (or expected) stimuli, thus converting olivary neurons into
detectors of unexpected sensory events. For example, one category of
climbing fiber fires when the limb bumps into an object in the course
of a movement but not when the movement is gracefully completed.
Another category fires when the limb is disturbed at the termination
of a movement. Later we speculate on how information of this type
might detect dysmetria and then be used as a training signal for the
adjustment of the synaptic weights of parallel fiber inputs onto Purkinje
Less is known about the signals transmitted by aminergic inputs. The
norepinephrine input comes from cells in the locus coeruleus that are
believed to fire in relation to novel stimuli. The serotonergic input comes
from cells in the raphe nucleus that discharge at low, steady rates which
The signals on the output side of cerebellar modules appear to repre­
sent motor commands. The different categories of output cell (Purkinje,
cerebellar nuclear and red nucleus) are similar to each other in several
important respects. Their signals are closely related to movement per­
formance and poorly related to sensory input. This is why they are
considered to be motor, and not sensory, signals. The responses typi­
cally lead movement by about 100 msec and may correlate closely with
motor parameters such as movement onset, velocity of motion or rate of
The example in figure 13.3 shows signals recorded from a red nu­
cleus cell while a monkey subject tracked visual targets (Gibson, Houk,
Kohlerman 1985). The responses correlate closely with movement ve­
locity and appear to represent velocity commands. When the target
was a step (panel A), the cell discharged at a high rate for a short­
duration, corresponding to the high-velocity, short-duration movement
that followed. When the target was a slow ramp (panel B), the tracking
movement showed several discrete phases, each of which was preceded by
an appropriate velocity �)1Iighle�t to the strong movement-
Examples of central motor commands. Panels A and B show responses recorded
from a red nucleus cell while the monkey subject was tracking step and ramp
targets respectively. The lower traces show the targets (dashed lines) and actual
movements (solid lines) while the upper traces show the discharge rate of the cell.
Discrete bursts of discharge precede each movement, and each movement segment.
The solid vertical lines show some of the burst onsets, and the dotted vertical lines
show the corresponding movements. Note that the rate of discharge within each
burst corresponds to the velocity of movement, and the duration of each burst
corresponds to the duration of the corresponding movement segment. In contrast,
discharge rate correlates poorly with the time course of target motion.
related response, there was no noticeable sensory response to the visual
target (target motion is shown by the dashed line in figure 13.3).
Output neurons also show little or no response to sensory stimula­
tion produced by passive joint rotation. This finding is particularly
remarkable since the mossy fiber input to the cerebellum from muscle
proprioceptors is very prominent, as discussed earlier. However, propri­
oceptive feedback during movement might still be important, so this was
tested by interrupting ongoing movements ( Harvey, Porter, and Rawson
1979; Gibson, Houk, and Kohlerman 1985). If continuous feedback from
the limb were important, this maneuver would be expected to produce
prompt and conspicuous changes in discharge rate. In contrast, the in­
terruption of movement produced little or no change in discharge rate,
indicating an absence of continuous feedback. However, the responses of
red nucleus neurons did show evidence for an unusual kind of feedback
(see Houk 1987). There was a prolongation of the velocity command
beyond the expected time of its termination in many of the trials in
which the movement e� int�l.'WA1cI t Jta�e� we propose a special type
of feedback that accounf'lflfh'nese oCfJservatlOns.
In summary, comparison of the signals that enter and leave the cerebel­
lum reveals some prominent differences. Mossy fibers appear to transmit
a rich variety of parametric information to the cerebellum. Sensory in­
puts signal external state, and efference copy inputs furnish information
about internal state. In marked contrast, climbing fibers transmit much
less parametric information to the cerebellum. Instead they appear to
be specialized for the detection of unexpected somatic events that may
signify error conditions. The signals on climbing fibers might be suit­
able for training the cerebellar network. Finally, the signals present
at the output stages of cerebellar modules are surprisingly insensitive
to sensory inputs and instead appear to represent preprogramed mo­
tor commands. The finding that much sensory information enters the
cerebellum whereas the outputs follow time courses that are relatively
independent of sensory events is particularly striking. This contrast be­
tween sensory input and motor output places a major constraint on the
type of information processing that must occur in cerebellar modules.
The finding of a marked transformation in cerebellar modules between
sensory-driven input and movement-related output suggests that motor
commands are not created by simply combining inputs to the cerebellum.
Instead, it implies that these commands are generated by mechanisms
intrinsic to cerebellar modules that are capable of self-sustained activity,
independent of the time course of sensory input. This is one of the
main reasons for postulating pattern generation as the basic operation
performed by cerebellar modules and for referring to these patterned
There are two ways in which self-sustained activity might be pro­
duced in a neural network. One is by virtue of recurrent circuits such
as the positive feedback loop between the cerebellum and red nucleus
that was discussed earlier ( figure 13.1). The other mechanism is for the
neurons themselves to have intrinsic dynamics that makes them capable
of self-sustained activity. The model of an adjustable pattern genera­
tor developed in the next section incorporates both recurrent loops and
intrinsic dynamics as mechanisms for the production of self-sustained
If the extensive sensory input known to enter the cerebellum is not
used to shape the time course of motor signals, what might its function
be? We would postulate that sensory information has several alternative
uses: (1) to preselect ��Mf4tlt.�lto trigger their initiation,
(3) to influence when they terminate, and (4) to evaluate the success or
failure of the patterns in controlling a motor behavior. As an analogy,
consider how we control an electronic function generator to produce
trains of electrical pulses. First we adjust all the dials on the stimulator
to preselect pulse duration and amplitude, train duration and repetition
rate. A transient input then triggers the entire preselected sequence. If
we are not happy with the way the output looks, we readjust the dials
on the function generator before trying it out again.
The mode of operation outlined in the previous paragraph represents
a type of adaptive feedforward control. Elsewhere it has been suggested
that most physiological control systems use adaptive feedforward mech­
anisms to generate command or reference signals and then use local
feedback systems to translate these commands into actual performance
(Houk 1988a). Confining feedback to lower-level processes with rela­
tively short time delays minimizes the stability problems inherent in
closed-loop control systems. Also, one can capitalize on predictive ca­
pabilities and other special advantages of feedforward systems. What
we actually end up postulating in the next section is a cerebellar mod­
ule that in most respects operates as an adaptive feedforward processor.
However, the postulated module does use feedback, though in a lim­
ited manner that is less subject to the stability problems encountered
in conventional feedback systems. Correspondingly, we will refer to this
mechanism as a quasi-feedforward process.
In section 13.2 we described the circuit diagram of a cerebellar module
that included reciprocal connections between the cerebellum and the
brainstem. While some arguments were given for postulating that these
modules function as adjustable pattern generators, we did not explain
how they might actually operate. In section 13.3 we reviewed the sig­
nals that enter and leave the cerebellum and concluded that cerebellar
modules are repositories of motor programs. They appear to use sen­
sory inputs to adjust parameters and to start a motor program, but
then operate in a quasi-feedforward manner until the program runs to
completion. In this section we describe how these features might result
if we were to introduce elements capable of undergoing state transitions,
inspi�ed by the phys �b0,Q',.;Q[&}irMBi� �9Is and by the anatomy and
Cerebellar Elements Capable of State Transitions
Some neurons are reasonably well characterized by quasi-linear static
properties which yield graded outputs (firing rates) over a range between
threshold and saturation. Correspondingly, in most network models the
neurons have outputs that are proportional to weighted sums, or some
possibly nonlinear function, of the synaptic inputs. This type of static
model may provide a reasonable approximation to the characteristics of
In contrast, Purkinje cells in the cerebellar cortex have unusual dy­
namic properties that warrant special consideration, since they might
give rise to interesting computational capabilities. Their unusual prop­
erties arise because these cells and their dendritic membranes contain a
high density and a rich variety of specialized ion channels. Particularly
intriguing are the non-inactivating sodium and calcium channels that
produce prolonged plateau potentials in response to brief depolarizing
currents (Andersson et al. 1984, Llinas and Sugimori 1980). Plateau po­
tentials give rise to a bistability of membrane potential. A brief positive
input leaves the membrane in a depolarized state that persists, and a
brief negative input will reset membrane potential back to its prior state.
These properties resemble hysteresis, except that the plateau potentials
tend to reset automatically after a period of a few hundred msec.
Since plateau potentials are particularly prominent far out on the dis­
tal dendrites, each dendritic branch of a Purkinje cell may be capable of
independent bistable behavior. As a consequence, the overall firing char­
acteristics of the cell might exhibit multistable properties. While this
is an interesting possibility, the lack of experimental evidence for multi­
stability at the present time led us to choose a simpler, more abstract
The model utilized here is intended to capture the capacity for state
transitions in its most elementary form. Purkinje cells are represented
as simple bistable devices, having the characteristics graphed in Figure
13.4a. When the summed synaptic input exceeds an on-threshold value,
the Purkinje cell is set to an on-state, and it is then assumed to fire at
a constant rate. It stays on until the synaptic input falls below an off­
threshold value at which point the cell is set to an off-state, and firing
Our model of the recurrent pathway between the cerebellum and
red nucleus (figure 13.1) also shows a capacity for state transitions, al­
though the postulated mechanisms and characteristics are appreciably
different from the simptePJBrs�f:l�tp(j�inje cells. The postulated
Operating characteristics assumed for Purkinje cells and for the cerebellorubral
loop. A. Purkinje cells are assumed to have bist ab l e properties. As a consequence,
they are unresponsive to inputs except at two points, the on-threshold where
transitions from the off- to the on-state occur and the off-threshold where
transitions from the on- to the off-state occur. B. The cerebellorubral loop is also
assumed to have off- and on - states , but with more complex features . When the loop
is in its off-state, it has a zero output ind epen dent of its Purkinje input. Transitions
to the o n- state are caused by trigger signals, and, after a tr an siti on , output
(discharge rate) depends on the fraction of cells in the Purkinj e domain that are in
characteristics of the cerebellorubral loop are graphed in figure 13.4b.
This diagram shows: (1) an off-state characterized by no activity in loop
neurons, (2) an on-state characterized by a whole range of intensities of
loop discharge rate, and (3 ) two examples of transitions from the off-to
The off-state is a condition in which most loop neurons are below
t hreshold , and the output of the loop, transmitted by a rubrospinal fiber,
remains constant at zero. The on-state is a condition in, which positive
feedback sustains continuing loop activity. When the loop is in its on­
state, the intensity of activity is assumed to be regulated by the strength
of the inhibitory input provided by Purkinje cells ( figure 13.1). Thus, if a
larger number of cells in the Purkinje domain ( figure 13.2 ) of this pattern
generator are in their on-state, the loop will receive more inhibition and
be less active. Alternatively stated, there is a direct proportionality
between the intensity of sustained loop activity (expressed as d ischarge
rate in figure 13.4b) and the fract io n of Purkinje cells that are in their
off-state. While the loop's capacity for state transitions is attributed
mainly to the coupled effects of positive feedback and thresholds in loop
neurons, some of these neurons may have non-inactivating ion channels
that could also contribute to the expression of persistent states and state
Transitions of loop activity from the off-state to the on-state are pos­
tulated to be triggered by extrinsic inputs to the loop, such as the input
from sensorimotor cortex shown in Figure 13.1. A transition would occur
when an input brings a sufficient number of loop cells above threshold,
at which point positive feedback would sustain the elevated s tate . A
transition back to the off-state is postulated to occur when all ( or most )
of the Purkinje cells return to their on-state, a condition of maximum
inhibition. These state transitions in turn determine the onsets and
offsets of motor programs, as discussed in the next section.
The execution of a motor program begins with the arrival of a trigger
signal, which serves as its start pulse. We assume that the positive
feedback loop is initially in its quiescent state. While figure 13.1 shows
the trigger being sent to the red nucleus, any sufficie ntly strong transient
excitation to any of the cells in the loop would initiate a transition to a
state of sustained activity. While we do not pursue it here, a transition
might also be initiatedClf1Pj!ri9l1tblt �nous removal of Purkinje
inhibition. Thus, there are several ways to start the execution of a
Once started, a program is regulated by the state of cells in the Purk­
inje domain of the pattern generator. If the fraction of cells in the
off-state is large, there will be a transition to a state of intense loop
activity as shown by transition 1 in figure 13.4b. The correspondingly
intense discharge of the red nucleus cell that participates in this loop
will serve to specify a high movement velocity, and this command will
be transmitted to the spinal cord in the rubrospinal fiber (figure 13.1).
In contrast, if many of the cells in the Purkinje domain are in the on­
state, inhibition will restrain positive feedback, the transition will be to
a state of modest loop activity (transition 2 in figure 13.4b), and the
pattern generator will specify a low velocity of movement. If nearly all
of the Purkinje cells are in the on-state, the loop will be unresponsive
to the trigger, and no motor command will be generated. In this man­
ner, the velocity commanded by an adjustable pattern generator can be
preset to values anywhere between zero (no movement) and a maximum
Once started, the program will continue to execute in essentially an
open-loop manner until parallel fiber inputs tum the off-state Purkinje
cells back on. This occurs when the net synaptic input reaches the on­
threshold of the Purkinje cells (figure 13.4a). Figure 13.5 shows how
position signals, conveyed by efference copy and proprioception in com­
bination, can be used to decide when to terminate a motor program. In
this example, we will, for simplicity, neglect the possibility of using ve­
locity signals to enhance performance. Furthermore, we will not attempt
to explain how rubrospinal efference copy signals, which code movement
velocity, are converted into position signals. Although the mechanism
for conversion is not understood, efference copy position signals have
been recorded from mossy fiber terminals in the cerebellum (Van Kan,
Houk, and Gibson 1987). We simply make use of these signals as one of
the mechanisms for terminating a motor program.
The top trace in figure 13.5 shows a motor program that commands
velocity. It was initiated by the trigger signal shown at the bottom.
The second trace represents the resultant movement, which is delayed
by about 100 msec due to transmission time in motor pathways and
mechanical lags in the limb. The third and fourth traces represent the
two types of position signal, efference copy and-proprioceptive, that
are returned to Purkinje cells via mossy and parallel fibers. These sig­
nals inform the cereb�ftIeItlelfiabut the anticipated and ac­
tual progress of a movement. Their combined input (fifth trace) excites
.....Qf.l:.\I:!r.�§.tlQJ.�L9.!..�.r.�.i.ol�..!<�.H§.. .. . . !
Execution of a motor program. Time plots show relations between (1) a motor
program that commands velocity, (2) the resultant movement, (3) a position signal
provided by efference copy, (4) a position signal provided by muscle proprioceptors,
(5) the comb ine d position input (weighted sum of 3 & 4), crossing the on-threshold
of Purkinje cells, and (6) the trigger signal that starts the motor program.
Purkinje cells, and the motor program is terminated when the combined
input crosses the on-threshold of the cells.
Note that the motor program has to be terminated approximately
100 msec before the movement ends due to the neural and mechanical
delays. One way to do this is to rely solely on efference copy information.
Since these signals are propagated through rather direct pathways, they
suffer little delay in transit back to Purkinje cells. However, they do not
represent the actual movement, but only a prediction of the position the
limb will achieve 100 msec later if nothing interferes with the movement.
Another disadvantage of relying �lely on efference copy position signals
is that they are unlikely to supply accurate information about absolute
limb position. Since efference copy signals are derived from collat erals of
fibers that transmit velocity commands, the signals must be integrated
to provide position information. Integration nearly always involves drift,
so these signals are unlikely to supply reliable information about absolute
Proprioceptive mossy fibers are capable of signaling absolute limb po­
sition, and they naturally detect when disturbances prevent movements
from being completed. However, from the standpoint of a pattern gen­
erator module, this information is delayed by more than 100 msec. If
pattern generators used conventional feedback from proprioceptors, the
system would become unstable and oscillate. In contrast, for the pro­
posed model the effect of the delay is negligible when the movements
are slow, since position would not change much during the delay period
and program termination would occur at the appropriate time. How­
ever, proprioceptive signals would show progressively greater errors as
movements become more rapid, and this would introduce delays in pro­
gram termination. Our suggested solution is to use proprioceptive and
efference copy signals in combination, which helps to overcome the dis­
advantages of using one or the other individually.
This type of quasi-feedforward, limited-feedback mechanism offers
partial compensation for disturbances, without the danger of instability
oscillations. Instability is prevented since the delayed feedback from the
limb is used only momentarily to cause a state transition in Purkinje
cells; before and after this switching point, feedback has no effect on
the output of the pattern generator. This limited use of feedback can
in fact be useful. If an obstacle in the environment interferes with a
movement sufficiently before the time of its expected termination, this
will reduce the proprioceptive component of the combined input and
thus delay the switch�� til.Qe of Purkinj� cells. As a consequence, the
duration of the velociwi�lI MfW� extended in a manner that
matches the observations reviewed in section 13.3. This extension of
the motor command might in some cases be sufficient to overcome the
obstacle and allow at least partial compensation for the disturbance.
In the previous section we discussed how a simple motor program spec­
ifying the velocity and duration of a movement could be executed in a
quasi-feedforward manner by a pattern generator module in the cere­
bellum. While we indicated factors that determine the magnitude and
duration of a motor program, we did not discuss how these parameters
might be adjusted in order to control the velocity and duration of a
movement. In the present section we discuss some mechanisms that are
proposed to take place in a preselection period just before a motor pro­
gram is executed and others that operate during the preselection period
but also continue to function during program execution. These mech­
anisms allow the commanded velocity and duration to be adjusted to
accommodate different initial and final positions and different degrees
The pattern generator module discussed in the previous section is tuned
to produce movement to a particular final position by virtue of the
synaptic strengths of position signals returned to its Purkinje domain
via parallel fibers. The important quantity is the combined position
input that derives from efference copy and proprioceptive components
of the input vector (figure 13.5). In the example given previously, we
tacitly assumed that each cell in the Purkinje domain of the pattern
generator receives an equivalent strength of this combined input. As
a consequence, the off-state cells turn back on nearly synchronously as
the combined input reaches a particular endpoint, thus commanding the
movement to stop at a particular final position.
Now we wish to consider how adjustments of this endpoint parameter
might be used to control movements to different target positions. There
are two ways in which the endpoint of a pattern generator might be
adjusted. One is to change the synaptic weights of all the efference copy
and proprioceptive position signals. We consider this to be a long-term
mechanism that is important in motor learning. Synaptic mechanisms
for storing motor programs by calibrating targets to endpoints are dis­
cussed in section 13.6. Copyrighted Material
For the moment we wish to consider immediate mechanisms that
might permit different targets to adjust the endpoint of a pattern genera­
tor on a trial-by-trial basis. This requires additional parallel fiber inputs
that transmit target information to all the cells in the Purkinje domain.
These inputs would add to or subtract from the combined input shown
in figure 1 3.5 ( subtraction would require inhibitory interneurons ) . Sig­
nals that add would cause the program to terminate sooner, at a shorter
endpoint. Signals that subtract would cause termination at a later time,
at a more distant endpoint. Note that the parameter that is specifically
adjusted is endpoint, rather than program duration.
Mossy fibers bring to the cerebellum a variety of information that
might be useful in specifying the end positions of movements. However,
this information is coded in a variety of coordinate systems. Signals in
the visual system are typically in retinal coordinates, though processed
visual information in some cases incorporates eye position information
derived from efference copy and/or head position information derived
from the vestibular apparatus or from neck
proprioceptors. Signals in the auditory system are typically in head
coordinates. Some regions of the brain are thought to code information
about remembered positions of targets in space. Sequential models have
been proposed for combining various information to form maps of in­
variant target position (see Kuperstein 1988) . However, since Purkinje
cells are exposed to virtually all of this information by way of mossy and
parallel fiber input, it is conceivable that invariant target position infor­
mation is assembled as a single computational step in the cerebellum.
These issues concerning how target information might be coupled to
the execution of motor programs in the cerebellum clearly warrants fur­
ther investigation. However, for the present purposes we wish to main­
tain our emphasis on the motor output side of the problem.
Correspondingly, we provide a relatively simple example to demon­
strate the essence of the proposed computational mechanism.
Consider a set of visual parallel fibers that code the position of a target
in retinal coordinates. For didactic simplicity, we assume that each
parallel fiber in this set has a small visual receptive field that does not
overlap with its neighbors, though the proposed mechanism could easily
be extended to fibers with large, overlapping receptive fields. We further
assume that one of the mossy fibers fires a phasic burst of activity when
the target is presented ( this assumption is not necessary, but is included
to illustrate another potential function for bistability in Purkinje cells) .
In cases such as this, when the target input is phasic, some short-term
storage mechanism iC�a:tcMaltlrfaJhe phasic burst into a tonic
effect on the Purkinje cell. The plateau potentials discussed earlier may
be capable of doing this, since it has been suggested that local plateau
potentials are produced by parallel fiber inputs to distal dendritic regions
(Andersson et al. 1984) . We further postulate that dendritic spines,
which receive input from single parallel fibers, are individually bistable,
and could thus be capable of a short-term storage of local responses.
The mechanism proposed in the previous paragraph converts a phasic
visual input into a tonic local response. The latter then adds to the
combined position input in figure 13.5. As a consequence, the endpoint
of the pattern generator will be adjusted to a shorter position, and the
motor program will command a smaller movement. Now consider the
set of visual parallel fibers, each of which fires to a different target. Each
target will produce a different local event in a Purkinje cell's dendritic
tree, and the strength of each event will depend on the synaptic weight of
the particular parallel fiber. If we assume that the synaptic weights have
already been adjusted to establish a correct correspondence between
target position and final position of the movement, then different targets
would automatically adjust the endpoint of the pattern generator so as
to produce an appropriate amplitude of motion.
The previous example has shown how visual target information can
adjust a pattern generator so that it controls movements to different
target positions. Although this simple mechanism could function to
control eye movements, it would not be effective for controlling limb
movements. This is because target information is supplied in a retinal
coordinate system that moves, depending upon the orientation of the
eyes and head in space, whereas a pattern generator will control the
end position of a limb movement in a coordinate system centered about
the shoulder. However, if information about eye and head position were
included in the input vector to the Purkinje domain, and if these signals
were suitably calibrated by a training procedure, then these inputs would
automatically adjust the endpoint of a pattern generator in order to
accommodate different positions of a target in space. This gedanken
experiment demonstrates the generality of the proposed mechanism for
adjusting the final position of a movement . A large number of factors
can easily be included in the computation, limited only by the 200,000
Starting from Different Initial Positions
A convenient feature of the present model is that it automatically adjusts
In essence, this is because any program, once triggered, will run until
the combined position input reaches a particular endpoint. If the limb
is initially farther from that endpoint, the program will simply run for a
longer duration, and at a higher intensity, until the endpoint is reached.
The mechanisms that mediate these adjustments can be appreciated
by extrapolating from figure 13.5. To make this example concrete, let us
assume that the pattern generator is wired to control elbow flexor mus­
cles and that its proprioceptive input is from dtretch receptors in the
elbow extensor triceps. The targeted position is elbow flexion to, let us
say, 45 degrees, and the movement illustrated in figure 13.5 starts from
90 degrees. Instead, if the initial position were 135 degrees, the pro­
prioceptive input would be proportionately smaller in the preselection
interval. This would have two effects on the pattern generator.
The first effect is an increase in the duration of the motor program.
When the initial position is 135 degrees, the proprioceptive component
of the combined position input would be smaller than shown in figure
13.5. Assuming for the moment that the movement is executed at the
same velocity, it would take longer for the combined input to reach the
on-threshold of Purkinje cells. Thus, the motor program would auto­
matically have a longer duration in proportion to the increased distance
between the initial limb position and its final position, as determined by
A second mechanism that compensates for different initial limb posi­
tions operates in the preselection period prior to the execution of a motor
program. Recall that the velocity commanded by a motor program is
regulated by the fraction of Purkinje cells in their off-state ( figure 13 . 4b) .
If more Purkinje cells were to be put in an off-state just before a trig­
ger arrives, the trigger would produce a transition to more intense loop
activity. The discharge rate of the rubrospinal fiber would be higher
thus commanding a higher velocity of movement. This adjustment of
commanded velocity is postulated to depend on an interplay between
inhibitory and excitatory mechanisms that takes place in the preselec­
tion period. Inhibition from basket cells and other interneurons tends
to set Purkinje cells to an off-state while excitation from parallel fibers
tends to return them to an on-state. We will follow the convention of
designating off-state Purkinje cells as being the selected ones, since it
is specifically these cells that determine the commanded velocity of the
Inhibitory input to Purkinje cells from basket cells is ideally suited for
coordinating the preselection process. The distinctive branching pat­
tern of basket axon e«o�J8f1tHtel-iJ� ) tends to confine inhibition
to individual Purkinje domains. The cells in a given domain are thus
driven to the vicinity of their off-thresholds where they become sensi­
tive to the balance between excitation and inhibition. The amount of
excitation in this period depends on both initial and final position. Us­
ing the elbow flexion example introduced earlier, a target position of 45
degrees would produce less excitation than a target at 90 degrees, and
this would result in more Purkinje cells being selected. Similarly, an
initial limb position of 135 degrees would produce less excitation than
would 90 degrees, which again would result in more Purkinje cells being
selected. In this manner, the number of cells selected from a Purkinje
domain will depend on the distance required to be traveled between the
initial position of the limb and the desired final position designated by
Ordinarily these two mechanisms for adapting motor programs to dif­
ferent initial positions would operate in combination. The preselection
of more Purkinje cells would increase the commanded velocity. This
would help compensate by making the movement faster. Assuming that
the former does not fully compensate for a more distant initial position,
the difference would automatically be made up by a longer duration
command. The same mechanisms would work together to adjust the
commanded velocity and duration to smaller values if the initial limb
position were closer to the desired final position.
A given movement can be made at a variety of velocities depending on
the urgency of the situation. Since the commanded velocity depends on
the fraction of Purkinje cells in the off-state, a relatively simple mech­
anism for varying this parameter would be to let a generalized urgency
signal modulate the responsiveness of basket cells. This would potenti­
a.te the intensity of basket inhibition sent to all the cells in a Purkinje
domain, thus increasing the number of selected Purkinje cells. Changing
the number of selected cells in this manner would control the commanded
velocity without affecting the endpoint of the movement. We propose
that this modulating function is provided by one of the diffuse aminergic
inputs to the cerebellum. Since these inputs branch to widespread areas
in the cerebellar cortex, the urgency of movements could be set globally
and thus influence all of the pattern generators that might take part in
We assume that a given pattern generator targets a particular set of
muscles as a consequence of the branching pattern of the descending
rubrospinal fiber. Since neighboring rubrospinal fibers have different
branching patterns, neighboring pattern generators will target diff-erent
sets of muscles and control motion in a variety of directions. As a con­
sequence, the way to make movements in different directions is to select
different pattern generators, or more likely, different combinations of pat­
tern generators that together produce a vector in the desired direction.
This strategy fits well with the hypothesis that the cerebellum functions
as an array of adjustable pattern generators (houk 1988b) . The output
of the array can be thought of as a composite motor program, with each
pattern generator producing one element of the composite program. We
have not yet dealt extensively with the problem of preselecting a whole
set of pattern generators in order to control movements in space. How­
ever, it would seem that basket cells might be particularly important in
A variety of evidence suggests that the cerebellum is important for sen­
sorimotor learning. Patients with cerebellar lesions produce dysmetric
movements, i.e. , their movements are of the wrong size or in the wrong
direction (Holmes 1939) . Normal subjects can be induced to make dys­
metric movements by, for example, distorting visual input with a prism,
but then they readjust their movements to make them normometric.
Cerebellar patients apparently lack the ability to adaptively adjust mo­
tor programs so as to command accurate movements. Similarly, exper­
imental lesions of the cerebellum in animals severely impairs adaptive
mechanisms for coordinating several types of eye movement (Robinson
and Optican 198 1 ) . Cerebellar lesions also interfere with the formation
of conditioned reflexes, an important model for sensorimotor learning in
animals (Thompson 1986, Yeo 1987) . It seems clear that the cerebellum
is important for learning well-adapted sensorimotor functions.
In section 13.5 we developed a method for representing motor pro­
grams in adjustable pattern generators. We further indicated how the
size of a movement controlled by one of these pattern generators will
automatically accommodate different initial limb positions and differ­
ent target positions, "p!ovi<:J,ed that the . network is properly tuned. In
the present section �ermil��C\(etwork strategies that might
be used to tune an adjustable pattern generator to produce accurate
movements. The adaptive strategies proposed in this section are, like
the operational features proposed earlier, inspired by the anatomy and
From the standpoint of pattern recognition schemes, a particularly fa­
vorable site for adaptive modification is the synapse between a parallel
fiber and a Purkinje cell ( Marr 1969, Albus 1971). Each Purkinje cell is
exposed to a large input vector consisting of 200,000 parallel fiber signals
that pass through its fan-shaped dendritic tree. A Purkinje cell could
be trained to process specific input patterns if there were an appropriate
mechanism for adjusting the weights of parallel fiber synapses. In this
section we summarize current hypotheses regarding cellular mechanisms
that mediate plasticity at parallel fiber synapses. We then propose a
training rule that can be applied to the problem of teaching a pattern
There is now considerable evidence that conjunctive activation of
climbing and parallel fiber inputs to Purkinje cells modifies the weights
of parallel fiber synapses ( Ito 1989) . These data provide important sup­
port for the proposed training function of climbing fiber signals. Figure
13.6 summarizes several of the steps that appear to be involved in this
form of synaptic plasticity ( Ekerot 1984, Crepel and Krupa 1988) . It is
helpful to trace these steps in some detail, since they provide consider­
able insight as to the likely training rules that apply to the cerebellar
Parallel fiber synapses occur on spines that extend from the dendrites
of the Purkinje cell. Activity in a parallel fiber causes release of the neu­
rotransmitter glutamate which binds to quisqualate receptors located in
the adjacent dendritic spine, as indicted in figure 13.6. This reaction be­
tween transmitter and receptor has two effects: (1) it opens ion channels
leading to depolarizing current that excites the Purkinje cell, and (2) it
activates the intracellular messenger diacylglycerol within the membrane
of the dendritic spine. While the activation of diacylglycerol is likely to
be a local event, excitatory current has global effects on Purkinje cell
activity. It combines with currents produced by many other synapses to
regulate the level of activity of the Purkinje cell, designated the response
Flow d iagram showing the electrical and chemical signals believed to mediate
s y n apt i c transmission and to regul ate the s y naptic weights for parallel fiber
synapses onto Purkinje c ell s . The diagram also identifies (underlined labels) the
correspondences between these cellular signals and the three factors in equation
1 3 . 1 , the weight adjustment equation proposed here. One factor i s a training signal
transmitted by a climbing fiber; it is postulated to signal when the movement falls
short of a t arget . The second factor is the response of the Purkinje cell, as
measured by its output act iv ity. The third factor is the e lig ib ility of each individual
synapse for we ight change; it is postulated to correspond with a local intracellular
We postulate that diacylglycerol marks synapses as being eligible for
modification. Eligibility is a concept that has been useful in other models
of learning that were inspired by biology (Klopf 1982, Sutton and Barto
1981). In these former models, eligibility was considered to have a short­
term memory that decayed slowly after synaptic activity had ended.
This trace feature might also apply to parallel fiber synapses, though we
The same Purkinje cell that receives many parallel fibers is innervated
by a single climbing fiber, designated as the training signal in figure 13.6.
The climbing fiber terminals spread over most of the cell body and stem
dendrites. Although the climbing fiber does not fire very often, when it
does, it produces a powerful global effect on the cell (Ekerot 1984) . This
begins with an excitatory synaptic current which then activates calcium
channels, the same channels discussed in section 13.4. 1 . Calcium chan­
nel activation produces a spike followed by a plateau potential with the
latter spreading into the distal dendrites. These events cause substan­
tial amounts of calcium ions to enter the cell, which is important since
calcium also functions as an intracellular messenger. Since the duration
of the plateau potential is very sensitive to the level of Purkinje cell ac­
tivity, the amount of calcium that enters the cell becomes a function of
the product (training signal) x (response) as illustrated in figure 13.6.
The modification of parallel fiber synapses appears to be mediated
by a substance called protein kinase C (Crepel and Krupa 1988 ) . The
activation of this enzyme requires two factors, calcium and diacylglyc­
erol (Nishizuka 1986) . This requirement for coactivation links synapse
eligibility to the global components of the training rule described in the
previous paragraph (figure 13.6 ) . The activated form of protein kinase C
then modifies quisqualate receptors, causing them to be less sensitive to
the neurotransmitter glutamate. These events thus produce a decrease
in the synaptic weights of all eligible parallel fiber synapses. The ex­
periments suggest that synaptic weight decrements can last for one to
Efficient training requires a mechanism for incrementing as well as
decrementing synaptic weights. In the cerebellum, relatively little is
known about incrementing mechanisms. However, there is some evi­
dence for a potentiation of synaptic weight under conditions in which
parallel fibers are activated without conjunctive climbing fiber activity
(Sakurai 1987) . The parallel fiber activity would create eligibility in
the form of diacylglycerol, and the absence of a climbing fiber discharge
would result in modest or low levels of intracellular calcium. Presumably
the lowest levels of calci�cWcMare8atkinje cells were inactive in
combination with an absence of climbing fiber input. In the training rule
chosen here, we postulate that these lowest levels of intracellular calcium
are required for increments in synaptic weight of eligible synapses. We
further postulate that no appreciable changes in synaptic weight occur
The rules proposed for incrementing and decrementing can be com­
bined into the following overall equation for adjusting synaptic weights:
where Wij refers to the synaptic weight between the lh parallel fiber and
the ith Purkinje cell and 0 Wij represents the change in weight during
one time step. Increments and decrements have learning rate coefficients
a and b respectively. A climbing fiber response (designated by z
is required for a decrement whereas its absence (z
for an increment . Activity of Purkinje cells (Yi
inhibit increments and enable decrements. If the climbing fiber does
not fire and the Purkinje cell is active, or if the climbing fiber does
fire and and Purkinje cell is inactive, the weight is unchanged. Finally,
the magnitudes of weight changes are assumed to be proportional to
the eligibility of the particular synapse, where eligibility €ij can vary
between 0 and 1 .0 depending on the firing rate of the parallel fiber.
In section 13.5 . 1 we indicated how target information transmitted by
parallel fiber input can be used to adjust the endpoint of a pattern
generator, thus controlling movements to different final positions. As a
specific example, we outlined how a set of visual targets, each designated
by activity in a different parallel fiber, could be made to specify end­
points that would produce movements to appropriate target positions,
provided the network were properly tuned. The problem of tuning the
network reduces to the selection of an appropriate set of synaptic weights
for the target lines. On what basis might" these synaptic weights be ad­
justed to match particular endpoints with particular targets in space,
and how might a pattern generator correct dysmetria? In this section,
we set up a specific model of an adjustable pattern generator, and in
the following section we describe the results of simple simulation experi­
ments with this model which suggest that these modules can be trained
The present model of an adjustable pattern generator consists of one
cerebellorubral loop ��at,�lnnervated by a domain of 12
Purkinje cells ( figures 13.2, 13 4a ) The output of the pattern generator
is a motor program that commands movement velocity. This command
is zero prior to the arrival of a trigger signal, and then jumps to a
where N represents the total number of Purkinje cells in the domain, 1 2
in this example, and Yi is the output of the ith Purkinje cell. This output
can be either 0 or 1 .0, and the synaptic effect on the loop is assumed to
be the same for all of the Purkinje cells. R thus assumes values between
o and a maximum commanded velocity Vmax , depending on how many
At each time step t, Purkinje cells receive an input vector Xt
[xo (t) , Xl (t) , X2 (t) , X3 (t) , X4 (t)] . Lines 0, 2, 3 and 4 represent parallel
fibers whereas line 1 represents a basket cell axon. This vector was as­
sumed to be identical for all cells in the Purkinje domain. Figure 13.2
shows how an input vector with two parallel fibers and one basket axon
could be distributed to an entire domain of Purkinje cells.
Although Pur kinje cells receive several sources of position information,
for the purposes of the present simulation we lumped these into a single,
real-valued, continuously updated and undelayed position signal, and
presented it on line Xo with a fixed weight of 0.0 1 . The combined use of
accurate but delayed, together with less accurate but undelayed, position
inputs will be important to explore in the future.
The inhibitory input from basket cells was presented on line X l ' Bas­
ket inhibition is postulated to be a transient input that occurs in the pre­
selection period. It drives Purkinje cells near to their off-threshold where
they become sensitive to the other parallel fiber inputs. Depending on
these other inputs, a cell will be switched to its off-state ( i.e. , selected
for participation in the movement ) or left in its on-state. We further
assumed that a stochastic process added noise to the off-threshold of
Purkinje cells. The importance of this noise term is that it randomizes
the preselection of Purkinje cells in successive trials, helping to insure
that all cells in the domain are exposed to the full training regimen. We
found empirically that using a basket input weight of -0.6 counterbal­
anced the excitatory target and position lines such that approximately
half the Purkinje cells would be preselected for average length move­
The remaining lines of the input vector were used to indicate the pres­
ence or absence of 3 different targets. When a given target was present,
the parallel fiber line w@Op;tifPJ�ct�(Jlflril.O, whereas it was 0 when
the target was absent. The pattern generator was taught to command
movements that acquired these targets by a training regimen that ad­
justed the synaptic weights of the target inputs, as will be described
We chose a 100-unit long, one-dimensional external world in which to
run our simulations. Movement position along this domain was deter­
mined by numerically integrating the velocity command. In the future
it will be interesting to explore more complex transformations between
the velocity command and the movement. We arbitrarily selected val­
ues of 28, 52 and 95 to represent the desired endpoints for the 3 targets.
One of the target lines was randomly chosen and set to 1 .0 at the start
of a preselection period. Similarly an initial position somewhere in the
range between 0 and the target position was randomly chosen. Then
the basket cell fired and a certain number of Purkinje cells was selected,
based on whether or not the net input fell below the cell's off-threshold.
At this point the pattern generator was fully adjusted, i.e. , all of its
parameters were set by the preselection process. Upon the arrival of a
trigger signal, it then executed its program.
After exposing this circuit to the training regimen described in the
next section, the pattern generator began to command quite accurate
movements to any of the three targets, starting from any initial position
smaller than the target position. Initial positions larger than the target
position resulted in zero output. Thus, like the muscles they control, ad­
justable pattern generators are capable of commanding movements only
in one direction. Figure 13.7 shows examples of movements performed
from two initial positions to a target located at position 91.
Note how velocity and duration both increase when the larger move­
ment is required. The overshoot is due to slight inaccuracy in training
this target input. In some simulation runs, training inaccuracy resulted
The training signal in the postulated learning rule is carried by climbing
fibers. As discussed earlier, these fibers appear to detect unexpected
somatic events. Here we speculate on how the observed properties of
climbing fibers might detect situations in which movements are dysmet­
ric. Then we demonstrate that a signal sensitive to hypometric move­
ments ( ones that are too s4ort ) is effectjve for training the simulation
obtained after a period o f training. Both of the
simulated movements shown in Panel A were t o the same target, located at position
95, but they started from two different initial positions that had been randomly
selected. Panel B shows the corresponding commanded velocities. The model used
a higher velocity and a longer duration in an appropriate combination to make the
larger movement . Panel C plots the number of Purkinje cells selected (ie,
number switched to their off-state) for the two movements. The preselection
process that occurred before the trigger arrived (at time zero) automatically
selected more Purkinje cells when the initial position was farther from the target
position, and this made the commanded velocity higher . The commanded duration
also automatically increased. This was because it took longer for the position input
One type of climbing fiber is predominantly responsive to cutaneous
input whereas another type is predominantly responsive to propriocep­
tive input (see section 13.3. 1 ) . The responsiveness of both types is mod­
ulated by efference copy signals, but in different ways. The cutaneous
fibers are inhibited toward the end of a movement, whereas the pro­
prioceptive ones are inhibited during movement (Gellman, Gibson, and
Houk 1985). As a consequence, the cutaneous fibers fire when a limb
bumps into an object in the course of a movement. We speculate that
these climbing fibers would also fire when a movement command is hy­
permetric, since in these circumstances the limb would tend to bump
into the target before the movement's intended completion. In contrast,
proprioceptive climbing fibers are depressed during movement, and re­
sponsiveness resumes as a movement is completed. Fibers (presumably
of this type) were found to detect instances in which the object en­
countered at the end of a movement yielded (Andersson and Armstrong
1 987) . We speculate that a failure to find a solid object at the end of
the commanded movement is what triggers this type of climbing fiber
response, and that the significance of the response is that it detects when
By this reasoning, we postulate that some climbing fibers would tend
to detect hypermetric movement commands whereas others would tend
to detect hypometric commands. In the present example, we used hypo­
metric climbing fibers to train the pattern generator. When the move­
ment fell short of the target, the training signal z in equation 13. 1 was
set equal to 1 .0 for one time interval after the motor program ended. All
of the synapses made by the active target line received decrements, since
these synapses were the eligible ones and since all Purkinje cells were ac­
tive by this time. At all other times z was kept at O. The incrementing
part of the learning rule operated throughout each movement. Eligi­
ble synapses that innervated inactive Purkinj e cells (preselected ones)
received increments at each time interval. Good results were obtained
A typical learning curve for the model is shown in Figure 13.8a. Ab­
solute error drops quickly but then remains at a low level indefinitely. It
takes approximately 600 trials for the asymptote to be reached. The in­
definite presence of an absolute error is due to fluctuations of the signed
error about zero. These fluctuations are caused by variations in synaptic
weight, apparent in the weight convergence graph shown in Figure 13.8b.
This graph shows how the weights of three synapses formed by one of
the target lines progressively converge toward the correct weight of 0.48
(value calculated analtiwurjqhtftff IIlitelfal:tuate about this value.
Learning performance of the simulation model . Panel A shows a learning curve ,
expressed as a plot of the average absol ute error in successive 20 trial epoches.
Performance reached an asymptote after about 30 epoches. Panel B shows three
examples of the convergence of synaptic weight toward a precisely correct value
that was calculated analytic al l y ( the horizontal line labeled 3) . A synapse that
started with a large weight (curve 1) initially experienced decrements, whenever the
climbing fiber signalled that the movement was too short, in combination with very
few increments, so the synaptic weight progressively became smaller. In contrast, a
synapse that started with a small weight (curve 4) experienced many increments in
addition to decrements, so the synaptic weight progressively became larger. After
having experienced several hundred training steps, all of the synapses had weights
that fluctuated about the c �Jf1Jted Material
The three curves in Figure 13.8b are representative of the 36 ad­
justable synapses in the model. Each of these synapses was initially
assigned a random weight between 0 and 1.0. For cases in which the
initial weight was high, the excitatory input was large, and there was a
high prob ability that the innervated Purkinje cell would either not be
selected, or, if selected, would return to its on-state early in the move­
ment. Both of these factors reduced the likelihood of receiving weight
increments without affecting the likelihood of receiving weight decre­
ments. Thus, the weight moved downward toward the correct value. In
contrast, when the initial weight was low, the excitatory input from the
target was small, and it was more likely that the P urkinje cell would
be selected for participation in the movement and more likely that the
synapse would receive an increment. In addition, the lower the weight,
the longer the cell was likely to remain inactive, which further increased
the likelihood of receiving increments. Thus, the weight moved upward
How does this learning procedure fit into the categories usually associ­
ated with network learning methods? Is it supervised learning, where
the climbing fiber specifies a desired response or a signed error signal;
is it reinforcement learning, where the climbing fiber plays the rule of a
penalty signal (Barto, Sutton, and Anderson 1983, Barto, chapter 1); or
is it some other form of learning? The method we have described differs
sufficiently from the familiar learning methods that it is not immedi­
ately clear how to characterize it. There are two ways one can view this
learning process: a local view focusing on individual Purkinje cells, and
a global view emphasizing its logic at the level of the pattern generator
Viewed at the level of an individual Purkinje cell, the learning process
is not easily characterized. According to equation 13. 1, if a cell fires,
climbing fiber activity instructs it to decrease its tendency to fire under
similar input conditions. In this case, the climbing fiber signal might be
thought of as a teaching signal specifying a desired response of "off." On
the other hand, when a cell is not firing, its weights always change so
as to increase its tendency to fire under similar input conditions (unless
climbing fiber activity occurs, in which case nothing happens: a case
never exercised in the simulations reported here) . Consequently, for
a cell that is not firing, the absence of climbing fiber activity might
be thought of as a te�(j� a desired response of "on."
However, given that the instructive significance of climbing fiber activity
depends on the output of the cell, the learning rule does not conform
to the usual logic of supervised learning. On the other hand, it also
does not conform to the usual logic of reinforcement learning in which
climbing fiber activity would be interpreted as a "punishment signal"
(Widrow, Gupta, and Maitra 1973; Barto, Sutton, and Anderson 1 983;
Barto, 1985, 1989, and this volume) . Climbing fiber activity would play
the role of a punishment signal if it told the cell to do the opposite of
what it was currently doing (Widrow, Gupta, and Maitra 1 973, called
this "negative bootstrapping" ) . In the case of equation 1 3. 1 , however,
climbing fiber activity does not tell an "off" cell to be "on." It may
be most appropriate to regard a climbing fiber in the present model as
playing the role of a kind of "one-sided" punishment signal. According
to the model, a Purkinje cell autonomously increases its tendency to fire;
climbing fiber activity counters this tendency by ( 1 ) suspending weight
increases of nonfiring cells, and (2) instructing firing cells to stay off if
they happen to fire in inappropriate circumstances.
The significance of this weight-adjustment process becomes more clear
at the global level of a pattern generator module. A pattern generator
module essentially conducts a search for the endpoint of a motor pro­
gram that is calibrated to produce a movement to the specified target
position. It does this by continuing to try, on each trial with a given
position command, shorter programs (the increment part of the learning
rule given by equation 13. 1 ) until the attempted movement is too short
(indicated by climbing fiber activity) , at which point it slightly length­
ens the endpoint of the program (the decrement part of equation 13. 1 ) .
Thereafter, for the simple example we have simulated, the endpoint of
the program oscillates, on a trial-by-trial basis, around the correct value
with an amplitude depending on the constants a and b and the mag­
nitudes of the eligibility terms eij in equation 13. 1 . By dynamically
adjusting the magnitudes of a and b, the magnitude of this oscillation
can be reduced, although at the cost of adaptability to future changes in
task requirements. Alternatively, averaging the output. of many pattern
generator modules will reduce the oscillation.
After learning, a target command arriving via parallel fibers sets the
pattern generator to produce, upon triggering, a program with an end­
point that moves the limb very near to the position of the target. In
other words, after learning, the search aspect of the process is no longer
required. Results of previous searches have been retained so that a
program of appropriate endpoint is immediately selected. This is an ex­
ample of the process c��." (Barto and Sutton 198 1 ;
I t i s a n instance o f reinforcement learning because desired program end­
points are learned even though the training information provided by the
climbing fiber is not based on direct knowledge of the desired endpoint.
Instead of directly specifying this endpoint , as would be necessary in a
supervised-learning module, the climbing fiber effectively terminates a
search for the desired endpoint . The computational significance of this
process is that the system can learn to produce correctly calibrated mo­
tor programs without requiring a teacher which already knows the cor­
rect endpoints for a set of training examples. Although there are other
ways to learn under these conditions, the procedure outlined above is a
natural consequence of the architecture of the pattern generator mod­
ules we have described. Future research will address the question of how
efficient this learning process can be when movements are the result of
the activity of many pattern generator modules executing their motor
We have reviewed recent findings regarding the anatomy and physiology
of the cerebellum and have attempted to apply this knowledge to the
design of an adaptive neural network for controlling limb motion. The
proposed network is comprised of an array of modules that are modeled
after the neural circuitry found in the cerebellum. Each module func­
an adj ustable pattern generator that uses semirealistic neurons
to process information in a manner consistent with single unit studies
in animals. Individual pattern generators learn to combine target infor­
mation with limb position signals to produce velocity commands. The
modules are thus capable of storing, retrieving, and executing simple mo­
The simple programs discussed here should be thought
of as elements of more complete programs that would be required to
lem of coordinating a set of pattern generators in order to produce such
The sensorimotor network presented here is related to several previ­
ous models that were also inspired by the anatomy and physiology of
The most obvious similarity concerns our treatment
of climbing fiber input as a training signal that adjusts the synaptic
strengths of parallel fiber inputs to Purkinje cells, as originally proposed
idea has been applied to the control of robot manipulators ( Albus 1 98 1 ,
Miller 1 987) , the adaptive adjustment o f filters (Fujita 1982 ) , the iden­
tification of dynamics and inverse dynamics ( Kawato, Furukawa, and
Suzuki, 1 987) , adaptive gain control (Grossberg and Kuperstein 1986)
and novel associative memories (Kaner va 1988) . The prop osed network
also shares some features with other previous models of the cerebellum
( Kornhiiber 1971 ; Boylls 1975; Pellionisz and Llinas 1979; Braitenberg
1987; Moore, Desmond, and Berthier, 1989).
While these prior models have yielded interesting and useful ideas,
they incorporate little of the new biological knowledge reviewed in this
chapter. For example, the nature of information processing in the models
is relative ly unrelated to that deduced to occur in the cerebellum on the
basis of sing le unit recordings in behaving animals (see section 13.3) .
The networks were built from static elements that compute weighted
sums of inputs rather than exploring the unusual dynamic properties
resulting from special ion channels in neural membranes (see section
13.4) . Also, the training rules used in these former studies bear little
relation to the biology of synaptic plasticity (see section 13.6). While
the model proposed here remains unrealistic in many of its details, we
have attempted to devise simplifying abstractions that incorporate most
of the salient features of this new information. Undoubtedly changes
will have to be made, so it is probably more appropriate to view the
proposed network as an inspiration based on biology rather than as a
realistic model of the cerebellum. Given this caveat, let us rev ie w some
of the potential merits of this tentative model.
Novel Functional Features Related to Persistent
The design for an adjustable patte rn generator developed here owes
many of its unique features to a capacity for undergoing persistent tran­
sitions in internal state. Pattern generator modules have several stable
internal states because they are composed of elements with persistent
on- and off-states. Inputs to a pattern generator can produce transi­
tions in internal state that then alter the basic manner i n which the
pattern generator subsequently operates. While this feature could lead
to chaotic behavior, instead, state transitions are used in a controlled
manner to regulate the preselection and execution of motor programs. It
is worth reviewing the two types of element that have persistent internal
states, pointing out how they give rise to spe ci al functional features of
One form of persistent state is a direct consequence of a recurrent
pathway from red nucleus output neurons back to cerebellar nuclear cells
In our model, this positive feedback loop has two stable
states-an off-state of quiescence, in which output is zero independent of
the level of Purkinje cell activity, and an on-state in which discharge rate
is a variable regulated by the degree of Purkinje cell inhibition (figure
The existence o f a stable off-state o f the loop i s very important for the
process of preselection. While the loop is in this state , Purkinj e cells can
be freely turned on and off so as to preset the parameters of the pattern
generator, without having any immediate effect on pattern generator
output. The preselection process can be thought of as the retrieval of a
motor program from memory, in preparation for its execution.
The ability of extrinsic inputs to the loop to produce transitions to
an on-state of loop activity is the basis for the triggering feature of the
This transition is analogous to switching on the power to
function generator that has already been adjusted to produce a desired
patterned output . Triggering is important since it allows a control pro­
cess to separate the problem of deciding when to take an action from
the problem of determining what kind of action to take. The kind of ac­
tion (the particular motor program) is specified by the cerebellar module
whereas the timing of the action can be decided by a separate process.
There may be many circumstances when it is desirable to use the same
motor program. Different subsystems can be designed to evaluate these
circumstances and transmit their decision by sending a simple trigger
signal to any of the neurons in the loop.
from off- to on-state, thus initiating the execution of a motor program.
Motor cortical mechanisms that might contribute to this triggering func­
tion are discussed in another article (Houk 1 989 ) .
Another form o f persistent state derives from the dynamic properties
of special ion channels present in the membranes of Purkinje cells. The
bistable input-output curve assumed for these neurons (figure 13.4a) is
an abstraction inspired by non-inactivating sodium and calcium channels
(see section 13.4. 1 ) . However, Purkinje cells definitely show more than
two stable activity states, so this model is clearly provisional. The actual
properties of Purkinj e cells are likely to be multistable , as a consequence
of bistability in several dendritic processes, but this hypothesis needs to
be tested experimentally. The present use of a bistable model is meant
to be a conservative choice, since this simplification is expected only to
diminish the computati6dJt�tMallitialjustable pattern generators.
Bistability of Purkinje cells is responsible for the unique ability of pat­
tern generator modules to operate in a semi-autonomous manner during
the execution of a motor program, and yet respond to a rich interplay
of inputs during the preselection period. The absence of responsiveness
to inputs during most of the execution period is due to the flatness of
the input-output function, except at two values of input, the on- and
off-threshold points for state transitions. This nonresponsiveness is im­
portant since it allows the time course of pattern generator output to be
specified independently of the time course of its inputs. The independent
specification of output time course permits a great deal of flexibility in
controller design, such as the possibility that the same program can be
used in many different circumstances, even though the different circum­
stances supply appreciably different input patterns to Purkinje cells.
While it is useful for Purkinje cells to be minimally responsive to in­
puts during the execution phase, they must respond in a sensitive fashion
to inputs present during the preselection period. This is how an appro­
priate pattern generator is chosen and how the parameters of the pattern
generator are adjusted to accommodate different target distances, ini­
tial positions of the limb and degrees of urgency. In our model, basket
cell inhibition is used to drive Purkinje cells into the vicinity of their
off-thresholds where, due to bistability, they become extremely sensitive
to small differences in input. Purkinje cells are thus made to respond
to an interplay between basket inhibition and the constellation of exci­
tatory input from parallel fibers, with some cells being driven into their
off-state and others remaining in their on-state.
The on-threshold region of bistability is also used to advantage in this
model. It serves as a sensitive switching point for stopping a motor
program. In our model, position signals are continuously fed back to
Purkinje cells, but Purkinje cells do not respond to these inputs until
their on-threshold is reached. At this point the cells quickly turn on
whereupon they again cease responding to position feedback. Because
the position loop is operative only at the switching point, the usual fear
that feedback will cause instability is circumvented. This gives rise to
the quasi-feedforward mode of operation discussed in the next section.
Bistability was also seen as a potential mechanism for prolonging the
persistence of target position information, a simple form of short-term
memory. We postulated that local bistability, due to calcium channels
in the distal dendrites and spines, is capable of converting phasic rep­
resentations of target position into tonic responses that then influence
the final position commanded by a motor program. It is important
to note that pattern ge&tt�UIDabt require this mechanism,
since tonic inputs representing target positions may be provided by other
In summary, the use of elements capable of transitions in internal
state gives rise to many advantageous features in the overall proper­
ties of pattern generator modules, and it seems likely that additional
useful features await future exploration. While our present implemen­
tation deals with individual movements, other investigators have used
networks capable of transitions in internal state to generate action se­
quences ( Jordan 1986; Kleinfeld 1986 ) . This is likely to be a fruitful
topic to explore when we begin to deal with multiple pattern generators
While feedforward has many advantages over feedback as a general con­
trol strategy, on-line compensation for errors in performance and prompt
adjustments to changing conditions are unique capabilities of feedback
(see Houk 1988a) . Conventional feedback loops are very sensitive to
time delays, and their tendency to oscillate when gain is increased so
as to improve regulatory performance is a big problem ( Robinson 1987) .
Therefore, the use of bistability to circumvent this problem, as discussed
in sections 13.4.2 and 13.8. 1 , seems to be a very nice feature of the
proposed pattern generator module. Recall that stability problems are
avoided because position feedback is discontinuous-it is operative only
momentarily, at the switching point of Purkinje cell bistability. We refer
to this feature as quasi-feedforward processing.
The present model is related to two previous models of motor com­
mand generation. These previous models used efference copy signals
driving internal feedback loops as a strategy for avoiding the delay ass0ciated with feedback from the movement. This strategy first appeared in
a model of the saccadic eye movement control system (Robinson 1975)
and was later put to detailed experimental test and refined (Gisbergen,
Robinson, and Gielen 1981 ) . The same basic strategy was applied re­
cently in vector format to limb control problems ( Bullock and Grossberg
In Robinson's model for saccadic burst generation, an efference copy
position signal is subtracted from a target position signal to compute
an error, and this error signal is then processed by an amplifier with
a high gain that rapidly saturates. The saturating amplifier is needed
to reproduce the rapid rise and fall of discharge rate that is recorded
from burster neurons i¢�ock and Grossberg instead
postulate that a similar error signal is fed into a time-varying gain func�
tion specifically tailored to produce symmetrical velocity profiles. The
time-varying gain starts out at zero and grows in an uneven manner that
was chosen to produce slowly rising and slowly falling velocity profiles.
The purpose of doing this is not entirely clear, since recordings from
brain cells that transmit motor commands generally show abrupt onsets
and offsets. The slowly rising and slowly falling features of actual ve­
locity profiles probably arise as the feedback mechanisms in the spinal
cord translate the movement commands into actual motion.
Adjustable pattern generators differ from these prior models in sev�
eral respects. First they use position information from proprioceptive
feedback in addition to efference copy sources. The presence of propri�
oceptive feedback allows a pattern generator to adjust itself automati­
cally to accommodate different initial limb positions ( see section 13.5.2) ,
and it also appears to provide some measure of compensation for dis­
turbances that prevent a movement from being completed ( see section
13.4.2) . Proprioceptive feedback would not be feasible in the other two
schemes, since they use continuous feedback, and the delay in the feed­
back pathway is sufficiently long to cause stability problems. This delay
has some undesirable effects on the performance of pattern generator
modules as well, but these effects are relatively minor compared with
a threat of instability. Delay will present no problem for the pattern
generators when movements are slow. However, at high velocities the
distance traveled during the delay period will become appreciable. Un­
der the latter conditions, it would be desirable for the pattern generator
to depend more heavily on efference copy, and this possibility needs to
A more radical difference between adjustable pattern generators and
previous models concerns the process that combines target and limb po­
sition information to produce a motor program. As pointed out earlier,
the pattern generators use persistence of internal state in module ele­
ments and plasticity in parallel fiber synapses to provide several features
not available in the alternative models. These features include the stor­
age of motor programs and a preselection mechanism for retrieving these
programs from memory. For the designation of target position, pattern
generator modules are thought to be capable of directly combining a va�
riety of signals in several coordinate systems, whereas the other models
typically postulate a sequential set of coordinate transformations that
recode target information into the coordinates of the movement that is
controlled by the command generator. Further advantages may result
from the fact that indi0dpyJ:i�teJ:fa/receive such a large input
effects and other contingencies without a necessity for invoking addi­
the learning mechanisms described here can be extended so that this
variety of information expressed in different coordinate systems can be
combined and motor programs can be calibrated to refine the level of
skill exhibited by the final motor output.
Clearly more work is needed to assess the genuine advantages of build­
ing sensorimotor networks from arrays of adjustable pattern generators.
Adaptive neural networks of this type might also be studied for their
potential utility in the control of other types of process.
The authors wish to thank Steven Epstein for many stimulating ideas
and helpful discussions in the course of this work. We also thank Tom
Buchanan and Joyce Keifer for helpful comments on the manuscript
Brains, behavior, and robotics. Peterborough, NH,
inje cells in the lateral vermis (b zone) of the cat cerebellum during
locomot io n . Journal of Physiology, London
G., Campbell, N. C., Ekerot, C. -F., Hesslow, G., and Os­
O. ( 1984 ) . Integrat ion of mossy fiber and c limbing fiber
inputs to Purkinje cells. Experimental Brain Research Supplement,
Andersson, G . , Ekerot , C. -R., Oscarsson, 0 . , Schouenborg, J. ( 1987) .
Convergence of afferent paths to olivo-cerebellar complexes. In: M.,
Glickstein, C . , Yeo, J . , Stein, eds. Cerebellum a n d Neuronal Plas­
Barto, A. G. ( 1985 ) . Learn ing by statistical cooperation of self-interested
neuron-like computing elements. Human Neurobiology, 4:229-256.
Barto, A. G . ( 1989) . From chemotaxis to cooperativity: exercises in
neuronal learning strategies. In R. Durbin, R. Maill , and G . Mitchi­
son, eds., The Computing Neuron, 73-98. Reading, MA: A ddison­
Barto, A. G . , and S ut ton , R. S . ( 1981 ) . Landmark learning: Illustration
of associative search. Biological Cybernetics, 42: 1-8.
Barto, A. G . , Sutton, R. S . , and Anderson, C. W. ( 1 983) . Neuronlike
elements that can solve difficult learning cont rol problems. IEEE
Transactions on Systems, Man, and Cybernetics, 13:835-846.
B arto , A. G . , Sutton, R. S., and Brouwer, P. S . ( 1 98 1 ) . Associative
search network: Reinforcement learning associative memory. IEEE
Transactions on Systems, Man, and Cybernetics, 40:201-2 1 1 .
Boylls, C . C . ( 1 975) . A theory o f cerebellar function with applications
to locomotion. Coin Technical Report 75 C-6 76( 1 ) : , Department of
Computer and information science, University of Massachusetts.
Braitenberg, V. ( 1 987) . The cerebellum and the physics of movement :
Some speculations. In M. Glickstein, C. Yeo, and J . Stein, eds,
Cerebellum and Neuronal Plasticity, 193-207. NewYork: Plenum
Brodal, A. ( 1981 ) . Neurological anatomy in relation
Bullock, D . , and G ross b erg , S. ( 1 988) . Neural dynamics of planned
arm movements: Emergent invariants and speed-accuracy properties
during trajectory formation. Psychological Review, 95:49-90.
Crepel, F. C . , and Krupa, M. ( 1988) . Activation of protein kinase C
induces a long-term depression of glutamate sensitivity of cerebellar
Purkinje cells: An in vitro study. Brain Research, 458:397-401 .
Ekerot, C. -F. ( 1 984) . Climbing fibre actions of Purkinje cells: Plateau
potentials and long-lasting depression of parallel fibre responses. In
J . Bloedel, et al. , eds. Cerebellar /unctions, 268-274 New York:
Fuj it a , M. ( 1982 ) . Adaptive filter model of the cerebellum. Biological
Gellman, R., Gibson, A. R., and Houk, J . C . ( 1985) . Inferior olivary
neurons in the awake cat : Detection of contact and passive body
displacement. Journal of Neurophysiology, 54:40-60.
Gibson, A. R . , Houk, J. C., and Kohlerman, N. J. ( 1985 ) .
between red nucleus discharge and movement parameters in trained
macaque monkeys. Journal of Physiology, London, 358:551-570.
Gielen, C. C. A. M . , and Houk, J. C. { 1 987}. A model of the motor
ical properties. Biological Cybernetics, 57:217-231 .
Gisbergen, J . A . M . van , Robinson, D . A . , and Gielen, S . ( 1 98 1 ) . A
quantitative analysis of generation of saccadic eye movements by
burst neurons. Journal of Neurophysiology, 45:417-442.
Port er , R., and Rawson, J. A. ( 1979) . Discharges of intrac­
erebellar nuclear cells in monkeys. Journal of Physiology, 297:559580.
Holmes, G. ( 1939 ) . The cerebellum of man (The Hughlings Jackson
Houk, J. C. ( 1 987) . Model of the cerebellum
M. Glickstein , C. Yeo, and J. Stein , eds . ,
Cerebellum and neuronal plasticity, 249-260 , New York:
Houk, J . C. ( 1 988a) . Control strategies in physiological systems . Feder­
ation of A merican Socities for Experimental Biology Journal, 2 : 9 71 07
Houk, J. C. ( 1 988b) . Schema for motor control utilizing a network model
Z. Anderson, ed . , Neural information pro­
cessing systems, 367-376 . New York: Amer. lnst . Physics .
Cooperative control of limb movement by the motor
els of brain function, 309-325. Cambridge: Cambridge University
Houk, J . C . , and Gibson , A. R. ( 1 987) . Sensorimotor processing through
the cerebellum. In J. S. King, ed . , New concepts in cere bellar neu­
ro biology, 387-4 1 6 . New York: Alan R. Liss .
M. ( 1 989 ) . Long-term depression. Annual Review of Neuroscience
M . 1. ( 1 986) . Serial order : A parallel, distributed processing
Technical Report 8604, Institute for Cognitive Science,
University of California, S an Diego, La Jolla.
Kanerva, P. ( 1 988) . Sparse distributed memory. Cambridge, MA: MIT
Kawato, M., Furukawa, K . , and Suzuki, R. ( 1 987) .
neural-network model for control and learning of voluntary move­
ment . Biological Cybernetics, 56: 1- 1 7 .
Kleinfeld, D. ( 1 986) . Sequential state generation by model neural net­
works . Proceedings of the National Academy of Sciences of the
Klopf, A.H. ( 1 982) . The hedonist ic neuron: A
( 1971) . Mot or functions of cerebellum and basal gan­
clear hold regulator, and the basal ganglia ramp
movement ) generator. Kybernetik, 8 : 157-1 62.
adaptive neural model for mapping invariant
position . Behavioral Neuroscience, 102: 148-162.
B . Brooks , ed. , Handbook of Physiology, Section 1:
Journal of Physiology-London, 305 : 197-2 13.
D. ( 1969) . A t heory of cerebellar cortex. Journal of Physiology,
Sensor based control of robotic manipulators us­
general learning algorithm. IEEE Journal of Robotics and
J. W . , Desmond, J . E . , and Berthier, N. E. ( 1 989) . Adaptively
approach. Biological Cybernetics, 62 : 1 7-28.
Studies and p ersp ectives of protein kinase Current
cessor for predictiv�f}{JIMJteHfBscience, 4:323-348 .
Robinson, D. A. ( 1 975 ) . Oculomotor control signals. In G. Lenner­
strand, and P. Bach-y-rita, eds . , Basic mechanisms of ocular motility
and their clinical implications, 337-374. Oxford: Pergamon Press.
Robinson, D . A. ( 1987 ) . Why visuomotor systems don't like negative
feedback and how they avoid it. In M. A. Arbib, and A. R. Han­
son, eds., Vision, brain and cooperative computation, 89-107. Cam­
Robinson, D. A . , and Optican, L. M. ( 198 1 ) . Adaptive plasticity in the
oculomotor system. In H. Flohr, and W. Precht, eds., Lesion in­
duced neuronal plasticity in sensorimotor systems, 259-304. Berlin:
Sakurai, M. ( 1987 ) . Synaptic modification of parallel fibre-Purkinje
cell transmission in in vitro guinea-pig cerebellar slices. Journal
Stein, J. F. ( 1 986 ) . Role of the cerebellum in the visual guidance of
Sutton, R . S . and Barto, A . G . ( 1981 ) . Toward a modern theory of
adaptive networks: Expectation and prediction. Psychological Re­
Thompson, R. F. ( 1986 ) . The neurobiology of learning and memory.
characteristics of mossy fiber afferents during forelimb movements.
Paper presented at 2nd World Congress of Neuroscience, Budapest,
Van Kan, P. L. E. , Houk, J. C., and Gibson, A. R.
Voogd, J . , and Bigare, F. ( 1980 ) . Topographical distribution of oli­
vary and corticonuclear fibers in the cerebellum. A review. In J.
Courville, C. de Montigny, and Y. Lamarre, eds. The inferior oli­
vary nucleus, 207-234. New York: Raven Press.
Widrow, B . , Gupta, N. K . , and Maitra, S. ( 1973 ) . Punish / reward:
Learning with a critic in adaptive threshold systems. IEEE Trans­
actions on Systems�gmedJ.tlMlJI:;s, 5:455-465.
Yeo, C. H. ( 1987). Cerebellum and classical conditioning. In M. Glick­
stein, C. Yeo, and J. Stein, eds. , Cerebellum and n euronal plasticity,
The first question that must be asked of a control system of any kind is
what it is for-for what purposes are we designing, building, and using
it? It is usually a treacherous oversimplification to imagine that the
answer is a single easy evaluation function, constant over time. The
real control system should be able to respond to changing demands and
standards, and should be trying to satisfy many requirements at once-­
not only keeping the plant output within some limits ( and that is meant
in the most general sense), but also doing so cheaply, quickly and effi­
ciently; not interfering with other functions that may be going on; and
The concerns of control theory have in the past been primarily to
. .. essentially all the classical design techniques are directly
inspired by stability considerations . .. designs based on op­
timization theory usually resort to a simplified mathematical
model and deal with crude approximations ... stability is of­
ten the main concern in control, rather than optimality with
respect to a (sometimes somewhat arbitrary) mathematical
performance criterion. In other words, the main issue ... is
to guarantee that a plant will operate in the neighbourhood of
a desired operating point in the face of disturbance.
Complex tasks contain unpredictable events, changing environments,
and systems that are difficult to model. In robotics especially, it
is becoming increasingly understood that the environment we want
our robots to work in requires that more intelligent control is needed
( MeysteI1988). Many of the assumptions made by control theory should
be questioned, if our robots are to handle the tasks we should like them
to. Systems that are subject to multiple purposes and abstract goals
cannot be handled with current control methodologies. We argue that
the basic paradigm of control should be that of an adaptive, hierar­
chical, nonlinear controller for a nonlinear system that contains some
general form of interference or noise; furthermore, that the basic design
model should allow some kind of robust dynamic specification and re­
specification of its eval�p¥�if>Ma".rl8�ssential to make the basic
model of the plant nonlinear, because in t hat way, the linear systems
The other way around, there is no sat isfactory
way to generalize linear systems to any broad range of nonlinear cases.
In the following sections we consider the current state of adapti ve and
learning control. We first indicate a general direction in which adapti ve
examples of robot control problems and consider the needs of control in
solving these problems. Finally, in section
methods--es pecially connectionist techniques-may be used to provide
some of the necessary extensions to control and we propose challenges
to the field of connectionist robotic learning.
Adaptive control is a branch of control theory in which a controlled
system is modeled, typically by means of a set of linear difference or
differential equations, some of whose parameters are unknown and have
states that "while a feedback control system is
oriented toward the elimination of the effect of state perturbations, the
control system is orient ed toward the elimination of the effect of
st ruc tur al perturbations upon the performance of the control system."
The application of the techniques of adaptive control theory can be
extremely powerful, but its range of applicability
certain very restrictive characteristics. These are characteristics of the
system model and the ways in which it may be adapted. For
assumptions of linearity are far broader than is commonly thought-like
Gaussian or Gaussian-like noise, root mean square
continuity of effect, and constancy of evaluation function.
is that many real-life tasks that we would like to handle are excluded.
A sampling of some of these tasks is given in section
The tools and capabilities of conventional and adaptive control should
not be discarded, but their limitations should also be recognized. At the
root of the trouble, we believe, is the primary aim of control t heory to
describe plants with manipulable mathematical models, in order to ana.­
lyze those models and to design controllers for them that can be pr oved
to be stable. However, the plant and environment may be very complex
and difficult to model, especially within the framework necessary to ap­
ply these techniques. A more general and abstract control that can be
or consistently convergent; just as there is no way in programming to
prove a program of any size or competence to be correct (see Fetzer
1988). Indeed, in many cases, the optimum may be impossible to define,
expecially in concise mathematical terms.
A more complex and general control can be developed by (1) not
invariably requiring mathematical justification of our models; and (2)
opening far wider the doors of what is to be considered control, includ­
ing, inter alia, more abstract adaptive decision-making and hierarchical
First of all, there may not be available any adequate mathematical
models: one would hope, for example, that a robot could play
Frisbee without profound aerodynamic knowledge-there isn't any
in the literature. Ordinary trial and error can suffice in many cases
and real-world systems can be difficult if not impossible to model
without experimentation and learning. Second, the mathematical
models may apply only with great restrictions, and not to the
actual system and environment that is to be controlled.
2. Ordinary control, as used in English, includes making decisions.
On renting an automobile, the experienced driver does not need
to adapt to each individual control, because the driver has a set
of examples from his own experiences. If, for example, the car
sometimes idles far too fast, the driver will perhaps learn quickly
to kick the accelerator in order to unstick some linkage. Current
control theory techniques include some aspects of decision theory,
but again, can consider such control only for a restricted set of
It seems clear from the work on adaptive control that hierar­
chical cont rol that includes learning modules is a problem that
is largely unfaced, let alone solved. The research of Kawato et
at. (1988) and Ersii and Tolle (1987) begins to address the is­
sues we are describing here. We may expect that our robots
will often be called on to control other control loops; indeed,
many of the robot's components can be expected to be controlled
by loops internally. As long as forty years ago, Wiener wrote
about how a muscle is controlled, not by the nervous impulses
proceeding directly from the motor cortex to the muscle, but
rather by their affecting the gain of the " ( efferent nerve ) -muscle­
( kinesthetic-end-body H afferent nerve)-( central- [spinal] -synapse) ­
(efferent-nerve) " lCi)Up�ttft!I MMijTi5l0me of those loops may be
inherently nonlinear, like decision making discussed above.
thermore, the system may not initially be able to control all of
these loops, but, rather, this ability must be learned.
In order to begin to consider the framework
networks might be used to expand adaptive control in these ways,
have listed in the next section robotic tasks that involve the need for
In this section we list seven examples of complex robotic tasks that
provide the basis for discussion. We also show in each difficulties that
Adaptive Position Control of a Nonlinear Plant
arm position control techniques, the dynamic
equations of the robot arm are linearized or assumptions are made to
simplify the model into a linear one. In order to deal with nonlinearities
and more complex robotic systems we need to consider the nonlinearities
of the process. The nonlinear relationships may be modeled yet it may
be difficult if not impossible to design controllers for their processes.
Some efforts have been made to approach this problem
tions are irregular. For example, a robot arm may interact, using force or
While stiffness, compliance, and impedance control have
little work has progressed in introducing
adaptation. An example of the need for adaptation is the
of modeling �lay, which exhibits a wide range of stiffnesses under differ­
Initial work in this area includes that of Van Brussel
{1979} who have incorporated connectionist learning
to control jamming for �_1\fati$le insertion task.
Two robot arms that juggle is another example of the general problem
of interacting control loops. The dual arm control problem has been
receiving more and more attention. Often the two arms are considered
as a closed kinematic chain in that they are manipulating a single object
and never break contact with it (Zheng and Lug 1988).
The Incorporation of Both Vision, Touch, and Other
A more complex problem is the integration of two or more senses into the
juggling task: in juggling, the juggler needs to sense the stiffness of the
objects being tossed up (in order to impart the appropriate velocity),
as well as to see where they are, in order to catch them on the way
down. A novel incorporation of vision into a robotic system via the use
of connectionist systems is described by Miller (1989).
The Acquisition, Integration and Application of
Symbolic Knowledge with the Numerical Knowledge
That Is the Usual Commodity of Control Theory
This type of integrated system includes a robot dealing with several
different kinds of objects-for example, a robot to catch moving animals.
The easiest way to identify an animal is naming it. By knowing its name,
the robot may have access to some knowledge base about it and retrieve
information on how to control it. For example, the capture of snails does
not need high speed, and birds and snakes need very different kinds of
capturing tools. Decisions represented symbolically cannot naturally
and easily be made part of the model of a control process in today's
control theory, including adaptive control. As another example, a higher
level decision operator might be: If you fail more than three times at
a certain task, give up. How is that to be represented as part of a
control system? How can this be incorporated into the goal structure of
The Adaptation to Changing Distributions of
A robot arm could intercept moving objects, without having to solve
the equations of motion; not that they do not apply, but they require a
degree of specificity about the world and the task that may be missing as
the environment change€1�e'IrftIt�en: for example, the wind
blow outside its anticipated range, a tool may break, or the moving
object may be a balloon. An exceptional exam ple in which people adapt
extraord inaril y rapidly to unanticipated changes in control parameters
is an experiment reported by Young (1969). Subjects using a joystick to
track a noisy spot on a video monitor typically needed less than a second
to adapt to a total change of sign of the control-that is, exchanging
A Far Wider Concept of the Purpose of Control,
beyond Mere Stability, That Includes Multiple
A robot running a production line may be required sometimes to op­
timize output rate, and sometimes to minimize breakage while main­
taining some minimum rate of processing. Moreover, it must be able
to undertake dynamic planning without knowing the control parameters
or even their ranges ahead of time. It may learn appropriate planning
behavior at the same time as it l earns about these characteristics of the
In robot ics, learning control is a method of controlling a nonlinear system
that may have many sensors providing inputs that cannot necesarily be
i nterpre te d as state variables. Furthermore, the model of the system
may be incompletely known. In addition, such systems may interact with
unknown changing environments and have com plex behaviors and goals.
Connectionist systems may provide more than conventional adaptive
control systems. They may be used to build a model of a little known
Connectionist systems make properly weighted associations between
in puts and desired outputs. Those weights are often ( but not neces­
sarily ) established by means of some form of training, in which weights
are sy stema tically improved by any of several means. The commonest
are some forms of backpropagation. Ideally, these associational powers
exhibited by connectionist systems are not disturbed by extra irrelevant
inputs, so that the problem of balancing large numbers of sensory inputs
is accomodated. But note that it is still not clear how this ability scales
up to very large numbers of those inputs. Moreover, the extra inputs
Connectionist systems offer certain advantages over traditional adap­
tive control algorithms and their implementations. They can and do
learn and adapt, albeit in ways whose limits are not yet thoroughly un­
derstood. In most cases, their programming is simple and flexible. But
there are questions: What should be the representation of the ongoing
tasks so as to enable connectionist systems to work? What preprocess­
ing is needed? What postprocessing? We need especially to resolve how
connectionist systems should serve as a total system concept, or as tools
in a larger architecture. We consider some of these questions below,
point to work that has started in these areas, and offer challenges to
Processing and Preprocessing of Sensory Inputs
It has been estimated that far more equipment in the human brain is
devoted to analyzing sensory inputs than to organizing and executing
complicated motor outputs. There is no reason to believe that that will
not be true of effective robotry. The processing must be as adaptive
and responsive to changing needs as any aspect of the robot. There
have been numerous papers in the literature about the ability of connec­
tionist systems to solve hard problems or to invent and apply ingenious
algorithms ( see the four volumes of The Proceedings of the 1987 IEEE
International Conference on Neural Networks, for example). Whether
those claims are fully justifed or not, most of the work described involves
a large amount of prespecified ad hoc preprocessing and postprocessing.
Our estimate is that, at least for some while, the simple low-level tuning
and adaptive capabilities of connectionist systems will be their truly at­
tractive features. We envision their use, therefore, primarily as adaptive
modules in larger systems. Thus one challenge is the construction of
the larger systems; another is the automation of the preprocessing and
postprocessing stages, especially in dynamic and adaptive ways.
Incorporation of Methods of Adaptive Control Theory
In designing the concept of a learning control system, the methods of
adaptive control should not be discarded. What methods and concepts
that have been developed in adaptive control can be used in designing
more complex learning controllers? As an example, if a goal is system
identification, a traditional method is to build a (structural) model of
the plant and excite it with the same input as is provided to the actual
plant. The difference between the model output and the plant output
is used to update the �cbMat8ri&bdel. We may study this
technique using more extensive plant models, such those attainable by
using a connectionist network. Narendra (see chapter 4) for example
addresses these issues. Model reference adaptive control may similarly
be extended, but the model that indicates the desired behavior of the
system may be more general and the adaptive component may be con­
nectionist network-based. Issues such as the need for similarity between
the model and the actual system (analogous to matching the order of
the plant and reference model) need to be examined. The choice of
performance index is not as constrained and so not as clear cut as the
linear quadratic index currently in use. On a positive note, more general
performance criteria may be employed with connectionist systems, par­
ticularly in systems using reinforcement learning units. Hirzinger and
Landzettel have studied this using a person as the model in force control
experiments (1985). Techniques such as theirs might be used to study
Wired in Capabilities versus Learned Capabilities
There is no reason to develop our robot from a tabula rasa. Indeed,
we doubt that it is practical or feasible to do so. In our examples of
section 14.3, many of the tasks will require hard-wired 'subtasks in or­
der to experiment and learn at all. But there is also every reason to
make sure that whatever capabilities that it is provided with or that it
has learned are themselves capable of being improved. A challenge of
connectionist-based systems is to provide this capability.
Incorporation of Symbolic Knowledge, Such as Natural
This is currently an urgent problem in many branches of AI, especially
machine learning. We envision connectionist systems as initially being
modules inside a larger and more complex structure. One of the reasons
for that is that the kinds of systems that seem at present to be able
to take advantage of knowledge expressed symbolically are much more
deliberately and less adaptively laid out. So much human knowledge is
expressed in natural language that using it would appear to be eventually
Although connectionist systems make decisions as part of their opera­
tion, other derivative decisions are not easily embedded in more com­
plex operational seq�flBYl!gptl/"H.ilt6fi!!/any reasonably complex plan
anticipates that sometimes continual control has to be exerted and that
at other times irreversible decisions have to be made sporadically. This
is another area in which research is needed. For example, in the jug­
gling examples 14.3.3 and 14.3.4, how is it to be decided when the ball is
thrown during the continual arm motions? There is an important effort
in control to incorporate expert systems (Geng and Jamshidi 1988). One
idea is to consider similar work with connectionist-based controllers.
Referring to example 14.3.7, how can the robot system satisfy many
concurrent purposes, while receiving sensory stimuli, many of them ir­
relevant, especially if the methods of resolution are ill determined? In
such situations, there is, in fact, generally no "right" decision that can
be defined. That is exactly the way it is with judgment calls. And if
the system makes a bad decision--or one that is deemed to be bad­
then the user has to find some way to inform the system of that; and to
make some appropriate correction. The problem, then, is not so much
to prevent conflict among the purposes, as to provide a flexible way of
producing a single decision from them. Learning to produce outputs as
weighted functions of many inputs is the most basic operation of con­
nectionist systems, and it works by balancing conflicting and concurring
evidence. That is, connectionist systems can perform the function of
balancing purposes in a very natural way; however, that mechanism
may not be the most appropriate one for higher level decisions. How
multiple purposes can be expressed, and how they should be presented
to the robot are all still open questions.
The issue of multiple purposes arises in hierarchical systems. The
role of hierarchical control in robotics has been studied by researchers
such as Albus and Sanderson (see Albus (1987), and chapters 13 and
16). The incorporation of connectionist modules in a hierarchy with
other types of processes is an area that needs more attention. What
types of nonlinear systems can and cannot be controlled or modeled
via connectionist systems? The very notion of optimization ought to
be questioned. Where is it attainable and where is it not? What does
optimization mean? These are some of the key questions that have to be
faced. One should expect that different domains of robotic application
will need different system architectures. As for the last questions, it will
probably turn out, not tpap�etiBlor cannot be specified and
attained, but that in the long run what matters is the ease and comfort
with which the system can be modified and managed.
We believe that control theorists should express a broad vision about
their ambitions. There are a host of problems in control, of real impor­
tance to engineering and society, that need to be acknowledged, faced,
and solved. They will need, correspondingly, many new approaches and
tools; and there is no reason to expect that these new approaches will re­
semble the mathematical ones of the current theory. It is interesting that
many aspects of developing control theory are being driven by roboti­
It is beneficial to consider the need for new methods of control
in terms of complex tasks similar to those given in section
new methods of control should build on the powers and tools that the
field of adaptive control has shown already; but it must extend its tools
beyond the rigor of mathematical formulations, so that it can work in
a larger range of domains, and under more dynamic and more realistic
conditions. Part of this extension should include connectionist systems,
systems that display many of the characteristics that are needed in the
dramatic future of learning control applied to robotics.
Hierarchical control of intelligent machines ap­
eds., Proceedings of the IEEE International Symposium on Intelli­
gent Control. Philadelphia, PA, January. IEEE Computer Society
applications. In Applications of Adaptive Control.
Monopoli, eds., Applications of Adaptive ControL New
K. F., and Ichikawa, I. (1988). Prerenal failure: A deleterious shift
from renal compensation to decompensation. New York Journal of
International Conference on Neural Networks. San Diego, CA, June.
A survey of methodologies for adaptive control
proach with neuron-like associative memories.
at IEEE Conference on Neural Information Processing Systems­
Natural and Synthetic, Denver, C O , November
J. A. (1988). Refinement of robot motor skills through re­
In Proceedings of the twenty-seventh IEEE
Conference on Decision and Control, Austin, TX, December. IEEE
(1988). Expert self-learning controller for
Proceedings of the twenty-seventh IEEE Con­
ference on Decision and Control. Austin, TX, December. IEEE
G. and Landzettel, K. (1985). Sensory Feedback for Robots
With Supervised Learning. Paper presented at IEEE International
Conference on Robotics and Automation, St.
Y., Isobe, M., and Suzuki, R. (1988). Hierarchical
neural network model for voluntary movement with application to
IEEE Control Systems Magazine, 8(2): 8-16.
Y. D. (1979). Adaptive control: the model reference appoach.
Miller , W., Glanz, F., and Kraft, L. (1987). Application of a general
learning algorithm to the control of robotic manipulators. Interna­
tional Journal of Robotics Research, 6 (2): 84-98.
W. T., (1989). Real time application of neural networks for sensor
based control of robots with visi on. IEEE Transactions on Systems,
concept and its use for automatic assembly by active feedback ac­
comodations. In Proceedin gs of the nineth International Symposiu m
on Industrial Robots. Washingt on DC, March.
Vidyasagar, M. (1978). Nonlinear systems analysis, Englewood Cliffs,
Whitney, D. (1985). Historical perspective and state of the art in robot
force control. In Proceedin gs of the 1985 IEEE International con­
ference on Robotics and Automation. St. Louis, MO, March. IEEE
Widrow, B., and Stearns, S. D.,(1985). Adaptive signal processin g. En­
J. C. (1971). The analysis of feedback systems, 101-102. Cam­
Young, L. (1969). On adaptive manual controls. IEEE
Z heng, Y. F. and Luh, J. Y. S. (1988). Optimal load distribution for
two industrial robots handling a single obj ect. In Proceedings of the
1988 IEEE International Conference on Robotics and Automation.
Philadelphia, PA, April. IEEE Computer So ciety Press.
The chapters in part III present outstanding problems requiring new
control technology from four application domains: manufacturing au­
tomation, autonomous vehicles, process control, and flight control.
is important to extend learning control research in directions that are
most likely to impact critical control problems.
derstanding of the overall control requirements of real-world problems,
expressed in diverse terms such as overall performance, noise tolerance,
control bandwidth, reliability, and ease and cost of implementation. The
following chapters are intended to give insight into such requirements for
In the first chapter of this part, Sanderson discusses applications of
neural network learning to robotics and automation for manufacturing.
The chapters of the previous part discussed motion control in robotics.
In contrast, Sanderson approaches robotics and automation from a man­
Ungar presents an overview of the problems of chemical process control
and the related application of current adaptive control techniques. He
then discusses the potential application of neural network learning to
these problems, and presents a sample bioreactor control problem.
The robust automatic control of aircraft exposed to a variety of en­
vironmental conditions presents many challenges. Jorgensen and Schley
discuss the problems of automatic landing systems and present a sim­
ulation model for aircraft flare and touchdown.
landing experiments using a neural network controller are presented.
The control of autonomous vehicles is a fruitful area for the applica­
tion of neural network learning, since such vehicles operate remotely and
must adapt to any unexpected changes in their environment or system
failures. In the final chapter of this part, Herman and Albus describe
the hierarchical control system developed at the National Institute of
Standards and Technology for the control of autonomous underwater
They review the responsibilities of each level of the control
hierarchy, and the flow of information between levels. While the control
system described does not currently involve neural networks, the discus­
sion of control system, sensors and actuators illustrates the environment
in which a neural network controller must operate in order to contribute
overview of applications of neural networks in
robotics and automation with particular emphasis on potential applica­
tions to manufacturing. The chapter summarizes some individual views
presented at the NSF Workshop and provides examples from the au­
thor's work. It does not attempt to review the literature in these fields.
Background on the basic technologies may be found in standard texts
which arise in those applications that lend themselves to solutions by
techniques involving adaptive or learning systems such as neural net­
Neural network computing methods provide one approach to the de­
velopment of adaptive and learning behavior in robotic systems for man­
Computational neural networks have been demonstrated
which exhibit capabilities for supervised learning, matching, and gener­
alization for problems on an experimental scale. In this paper we point
to a number of issues in the manufacturing applications of robotics where
these capabilities will be extremely important. Supervised
improve the efficiency of training and development of robotic systems.
Matching provides a means to execute the learned behavior and will be
in areas such as industrial inspection and control and task
execution functions. Generalization capabilities of neural networks will
require more long-term research, but could facilitate the flexibility of
systems in their capacity to adapt to new tasks.
these applications are discussed in this chapter.
and learning capabilities in automation systems to simplify the imple­
mentation process and to improve the reliability of these systems, may
tremendous practical impact. Robotics and automation technol­
role in a variety of different manufacturing tasks.
include such areas as parts handling, metal cutting, paint spray­
ing, plastic molding, welding, fastening systems, assembly, and many
other more specialized operations. In manufacturing, most such appli­
are integrated into a larger manual or automated system, and
the requirements of th��te.Jl;MaW_ction often dominate the
success and capability of any particular operation. Therefore, it is im­
portant in evaluating a particular technology or the potential impact of
a new technology such as neural networks to assess the impact on the
overall system. As a means to describe some characteristics of these
manufacturing systems, in the next section we will consider the example
of automated assembly systems in more detail. In section 15.3, we will
discuss overall issues and opportunities for neural networks in manufac­
The key obstacle to making manufacturing systems work economically
and efficiently in industry today is most often the overall systems coor­
dination and not the control of specific devices. An example of a robotic
assembly work cell is shown in the photograph in figure 15.1 (Sanderson
and Perry 1983). This work cell consists of three robot arms, a movable
work surface and fixturing, and several different types of sensors. The
function of this assembly work cell is to acquire parts that are presented
to the system, orient the parts in a prescribed manner, mate the parts
into predetermined relationships, and fasten the parts into a final sta­
ble configuration. While such an assembly work cell depends on the
speed and accuracy of its individual components, the overall capabilities
are most closely related to its capacity as a system to reliably integrate
functions of positioning, grasping, and sensing. Currently, the difficulty
in developing such systems for manufacturing applications is in the im­
plementation, planning, programming, and coordination of the various
devices in order to create a reliable system.
The planning and programming that are required to design and im­
plement an assembly work cell are usually organized into a hierarchical
set of levels such as that in figure 15.2. IvIany similar hierarchies have
been described in the literature, and we will not attempt to discuss dif­
ferences among these models here. The highest level of implementation
involves the planning of the task itself and this is directly related to a
representation or description of the product and its parts. The decom­
position of the task must then be coordinated with the available set of
resources such as robots, fixtures, and sensors. This decomposed set of
tasks is mapped onto a control architecture that defines the coordination
and sequencing relationships among the various devices. The scheduling
of discrete operations, of robot motions, and the continuous real-time
motion control itself CfJPJl1.�1tf9f.f1r181.e lower levels of the structure.
Applications of Neural Networks in Robotics and Automation
Flexible Assembly Workstation Developed for Electronics Manufacturing.
Reprinted from Sanderson and Perry (1983).
Much of the implementation cost in developing such a system is in
the definition of an architecture such that communication between lev­
els remains consistent and reliable, and the implementation of new tasks
or the redefinition of tasks, can be accomplished with minimal redesign.
One important impact of adaptive and learning technologies, such as
neural networks, may be an enhanced capability to develop robust hier­
archical systems. Adaptive behavior at one level will ease the require­
ments for specific coordination with other levels.
learning capabilities in general provide the capability to define these
more abstract sense so that they could be adaptive or
reprogrammed more easily for changing task requirements. In practical
industrial situations a large fraction of the implementation costs may
be spent on development and programming rather than the capital cost
The capacity to provide more efficient implementation
through automatic lear�� MaQlW8tave significant impact on
Hierarchical approach to assembly systems design.
Applications of Neural Networks in Robotics and Automation
the economics of building automation systems for manufacturing appli­
Most planning and programming tasks for industrial applications are
currently carried out manually. In many cases,
manufacturing systems plan, and the final manufacturing systems imple­
mentation may be carried out by different organizations. The evolution
of improved tools and methods to carry out these processes will have
an important impact on the effectiveness of manufacturing organiza­
tions to respond to new technical and economic opportunities.
the development of such improved tools and environments will be the
incorporation of both generic methods for computation and reasoning
with applications specific knowledge and representation of tasks.
successful utilization of neural networks techniques in the planning and
control of manufacturing systems will depend upon the detailed domain
specific understanding of the applications area at hand. The demonstra­
tion of effective neural network approaches to task planning, sequencing,
scheduling, routing, discrete control, sensor-based control, fine motion
control, or error recovery for a given task domain such as assembly or
size the importance of these computational approaches.
demonstration of computational performance
for a domain specific problem such as assembly requires careful atten­
tion to the issue of task representation, including assumptions and con­
straints which are inherent to that manufacturing domain. In our work
on assembly sequence planning (Homem de Mello and Sanderson 1986,
1988; Sanderson and Homem de Mello 1987), we have developed a rela­
tional model of prodnct parts geometry and relationships
us to reason about the feasibility of task operations and, therefore, suc­
cessfully generate and evaluate alternative feasible sequences for accom­
plishing the assembly goals. In the assembly problem, the task decom­
sequence of subassembly mating operations,
is governed by geometric and mechanical constraints. In another
as machining, the task may decompose into
of alternative milling or cutting operations, each of which also has its
own geometric and mechanical constraints.
domains, the search over alternative sequences of operations is closely
coupled to the evaluation of feasibility predicates which incorporate geo­
metrical and physical reasoning problems. A new approach to the repre­
sentation of the geometric or physical relationships appropriate to these
neural networks would be an important contribution to the development
An appropriate domain specific approach may also significantly sim­
plify the representation of the task and lend itself to more efficient plan­
ning of sequences. In our approach (Romem de Mello and Sanderson,
1986, 1988; Sanderson and Homem de Mello 1987), we have introduced
the AND/OR graph structure as a representation of feasible assembly
plans. Such an AND/OR graph representation is a distributed state
representation for the assembly or disassembly process. We have demon­
strated the completeness and correctness of this AND/OR graph rep­
resentation, and have shown the equivalence of this representation to a
directed graph of assembly states as well as to several classes of prece­
dence relation representations. We have shown that the AND/OR graph
representation is more efficient for planning purposes than the directed
graph of assembly states. An example of this AND/OR graph represen­
tation of assembly plans is shown in figure 15.3.
The AND/OR graph plan representation represents alternative as­
sembly sequence plans as tree structures. This same tree structure may
be used as a framework for the planning of more detailed operations and
motions required in the implementation of the task. Figure 15.4 shows
an example of a tree structure which incorporates the discrete operations
associated with devices and mechanisms, including robots and sensors.
In this case, the task plan chosen as a single tree from the AND/OR
graph has been augmented by the incorporation of the robot and fix­
ture devices. Figure 15.5 shows the extension of the same framework
to the description of continuous motion operations at the lowest level of
the control structure. Such a hierarchy between high-level symbolic op­
erations sequence planning and low-level continuous motion planning is
typical of the requirements of a hierarchical control architecture. The use
of a common task representation which maps between levels, facilitates
the integration of low-level functions such as path planning, kinematic
learning and control, dynamic learning and control, and sensor recog­
nition might be approached using neural net computational techniques.
Figure 15.6 shows one example of a control architecture for such a hi­
erarchical system. In this example, the real-time motion is executed in
conjunction with a real-time planning system which modifies the execu­
tion of operations according to changing task constraints. Both the real
time and off-line planning systems require a common task representation
which may be accessed in or�er to reasoI! about alternative sequences
Example of a control architecture which partitions off-line planning, real-time
AND/OR graph representation of assembly plans (Homem de Mello and Sanderson
1986, 1988; Sanderson and Homem de Mello 1987).
Applications of Neural Networks in Robotics and Automation
Mapping discrete operations onto the tree-structured assembly plan.
Mapping continuous map ping continuous device operations onto the tree-structured
In assembly planning as well as other task planning problems, one
cannot explicitly generate all of the feasible plans due to the combina­
torial growth in the number of possible sequences. Instead, one must
invoke an evaluation or objective function in order to choose among fea­
sible sequences and examine in detail only those candidate plans which
are most desirable. In assembly planning, these objective functions are
related to the complexity of the manipulation operations, the cost of
the resources required to execute these operations, the time required for
execution, and the complexity and cost of fixtures and tooling used in
the implementation of the system. Similar goals and constraints occur
in other task-plannin��fibMaterial:ltal cutting or parts molding.
Such objective functions are difficult to specify in an explicit and ana­
lytical form, or as a heuristic knowledge base. An adaptive or learning
system which could synthesize such evaluation functions for a given task
domain could facilitate the process of task planning for these applica­
tions. Within a hierarchy, a neural net might be used to synthesize
the objective function then as a computational approach to minimizing
an objective function. This on-line search problem over a distributed
representation may be well suited to a neural net solution.
The previous section described an example of the hierarchical system of
planning and control which is typical of many manufacturing systems,
and suggested ways in which neural net computation might provide an
effective tool at the levels of planning, discrete control, continuous con­
trol, and sensing. The use of these computational tools will be effective
only if they meet needs or expectations of the users. The manufacturer
has a number of key practical performance goals which he requires from
any system which is being developed. Typically, systems speed, through­
put, accuracy, and overall costs of both implementation and operation
are factors which he must consider. The flexibility of a system is the
ability to change functionality and respond to new requirements, and is
an increasingly important component of such systems. The ability to ef­
ficiently implement a system, to operate the system reliably, and provide
a degree of flexibility which permits an evolution of the manufacturing
system in accord with product changes, are important elements which
influence the effectiveness of automation in manufacturing today.
Table 15.1 summarizes a set of technical issues and opportunities
which must be addressed in order to expand the capabilities of robotics
and automation technology in the manufacturing domain. These issues
are grouped into four separate areas: mechanisms, control, represen­
tation and planning, and architecture and implementation. While the
mechanisms themselves are not directly related to the implementation
of adaptive and learning systems, it is clear that improvements in sens­
ing technology, motor technology, and new mechanisms such as flexible
arms and sophisticated hands, will place increasingly strong demands
on the corresponding control and planning systems to incorporate adap­
tive capabilities for utilization in specific tasks. There are important
opportunities in the development of more robust controllers by utiliz­
ing learning systems to�ig/jted1MEtalfantify robot kinematics and
Technical Issues in Robotics and Automation for Manufacturing
Sensors-Vision, Tactile, Force, Proximity
Sensor-Based Control/ Fine Motion Control
dynamics, to more efficiently adapt dynamic control parameters to par­
ticular tasks, and to more effectively integrate sensory information into
the control process. The capability to adapt to an inherently uncertain
representation or model of the task, is key to the improved reliability of
these systems. An automated system for learning of accurate kinematic
or dynamic parameters of current robot arms, would have immediate im­
pact in terms of the potential performance of these arms. Newer systems
that incorporate lightweight, flexible arms or multiarm interactions will
also require such on-line identification in order to function effectively.
Planning and control of the system depends explicitly on the nature
of the representation of the robot, task, and environment. In part, a
representation is provided by an initial model or description of the sys­
tem, but increasingly this representation must be updated or derived
from sensory informa�yr�etml:;ensory information for the
Applications of Neural Networks in Robotics and Automation
identification of models and for the choice of plans and parameters, are
robot , which in response to changing conditions, can adapt its model of
the task, the parameters of its control, or the sequence of operations,
will provide an important capability for increasingly sophisticated appli­
cations. The integration of sensory information from a variety of sources
vision sensors, tactile sensors, and range sensors has been very
difficult to achieve in a purely analytical approach. Multisensor integra­
tion through adaptive and learning techniques may be another important
opportunity. As discussed previously, the architecture of these systems
is typically hierarchical, and the effective integration and coordination
among layers of this hier archy is facilitated by the adaptation of spe­
cific functions at one layer in response to generalized
a higher level. The basic architectural structure would be retained and
the tuning of the coordination and integration parameters would be left
to the adaptation mechanism rather than the painst aking trial and error
A learning approach that we have used to facilitate the implementa­
an example of parameter learning at the robot operation level. This ap­
pr oach utilizes parameter adaptation within a task which was specified
as a set of discrete operations in a conventional high-level robot program­
ming language . The structure of this system is shown in figure 15.7. The
robot automatically generates its own trial and error procedure by mod­
program. It generates a representation of the
evaluation function parameter space, smooths that optimization surface
based on an anal y tical model of the timing and sampling behavior of
the robot itself, and then employs that optimization surface in order to
modify the parameters of execution of the robot program in real-time.
As the task is repeatedly executed, the evaluation function surface itself
One example of such a parameter learning task used
in this study was the mating of mechanical connectors . In this exam­
ple the two variable robot program parameters were the velocity of the
robot hand and the threshold force for detection of insertion . The per­
formance function J was the time required to accomplish the task. The
resulting performance surface for this robotic insertion task is shown in
figure 15.8. Notice that the initial surface in the top figure is strongl y
affected by the sampling times and the instruction execution
mance surface used in the learning procedure. Given this task, the robot
Structure of the robot parameter learning system described in Weiss, Simon, and
The convergence of the performance with the
number of experiments is shown in figure 15.9. As the robot continued to
perform this task, the optimization surface was appropriately modified
in real time. This example shows how a robot could effectively learn
its performance space on a given task and then update, in real time,
execution parameters in order to improve its own performance.
An example of the application of adaptation to a sensor-based control
problem is the use of sensor-based control in conjunction with partial
state information. In Weiss, Sanderson, and Neuman (1987), we have
described some experiments in which feature information derived from
images is used as a partial state representation of the relative position
of a robot, and this feature information was directly coupled into a
robot joint control loop. Such a hierarchical st ructure illustrated in Fig­
ure 15.10 ut ilizes a lower level feature-based control with partial state
information, and a higher level position-based control which utilizes a
full position interpretation of information from the images. This de­
composition of the problem is effective because it matches the dynamic
c apabil ities of the vision system with the speed requirements for real­
time robot control. The full image interpretation carried out at a lower
speed, sufficient to maintain the stability and reliability of the motion
but delegating the real-time sampling and control to the feature based
loop. Adaptation is r�l1t�iItf� because an accurate model
Applications of Neural Networks in Robotics and Automation
Performance surface calculated for a force-monitored robot placement task (Weiss,
�OO��150 200 250 300 350 400 450 500 550 600
Convergence of the Parameter Learning Procedure in (Weiss, Simon, and Sanderson
of the correspondence between feature space and joint space is never
in fact available and must be learned or identified in real-time.
examples described in Weiss, Sanderson, and Neuman
classic model reference adaptive control system which implicitly identi­
fies the feature-to-joint sensitivity matrix. The on-line identification of
such a nonlinear mapping might be effectively implemented using neural
The choice of evaluation functions is fundamental to the problem of
learning and adaptation, and in these examples, arises in model identi­
fication, parameter learning, and decision making. W hile model-fitting
evaluation functions such as least squared fits are often used for these
problems, they are often not necessarily the most desirable, particularly
We have been interested in the application of
complexity, or representation measures for such problems. In Segen and
we describe an approach to model size identification
trades off between the cothp lE!xity otthe chO sen model and the accuracy
Applications of Neural Networks in Robotics and Automation
Structure of a sensor-based control approach using partial-state information in a
hierarchical system (Weiss, Sanderson, and Neuman (1987).
of its fit to data. In Sanderson and Foster (1989), we describe the use
of this minimal representation size criteria as a basis for model-based
image matching to noisy gray-level images. The further application of
such generalized complexity measures to learning problems using neural
networks will be extremely interesting to explore.
Several areas where neural networks may play a role in the develop­
ment of robotics and automation technology for industrial applications
for the impact of learning systems is on the use of sensing and inspec­
tion technology for industrial applications. In particular, it would seem
that learning systems could offer a marked advantage in the ease of im­
plementation and training of such inspection systems.
inspection tool for new applications is often difficult and expensive, and
the ease of developing this system will often be as important as the out­
right need of the working system. This 'adaptive or learning capability
is of key importance for many-short term applications. Another good
candidate for such an application in the short term is the area of kine­
matic calibration of robot arms, utilizing sensing systems to measure
positions of the arm erliJ�ecVWI.attlri8ig system might identify a
Role of Neural Networks in Robotics for Manufacturing
Improve Reliability Through Adaptive Behavior
Improve Implementation Tools Through Trainable Systems
DON'T IGNORE EXPERIMENTAL APPROACH IN ROBOTICS
complex nonlinear model of the robot arm kinematics which could be
used to improve the positioning accuracy of the robot arm itself. A third
example which is feasible in the short term, would be parameter learn­
ing within the structure of an existing robot program as was illustrated
earlier. In this case, the parameter adaptation is essentially a smooth
adaptation to local changes and might be handled efficiently by existing
neural network techniques. Kinematic path planning is another area of
promise, but is in general more difficult because of the dimensionality of
the geometric representations required, and the resulting complexity of
In the longer term, there are many opportunities for the application
of learning systems to task-planning and task-reasoning problems, par­
ticularly those that confront the issue of uncertainty in the task environ­
ment. Current experiments in learning of robot dynamics parameters
suggest that this is ano&rpYfl§��fe�d certainly the integration
Applications of Neural Networks in Robotics and Automation
of sensory information into an adaptive robot control structure will be
important element of future robot systems.
systems which both converge quickly, maintain stability, and handle the
growing dimensionality of the problem. Learning systems may improve
the capabilities of planning and execution of fine motion operations such
An important element in the development of these adaptive and learn­
ing techniques and in their evaluation, will be the recognition that
robotics is experimental in nature. Development often requires the build­
ing of systems and the testing of new tools on real systems in order to
In applications areas such as manufacturing,
this experimental demonstration becomes even more critical since the
functional capability of these tools is related most to their ability to com­
pensate for characteristics which are not entirely predictable or which
cannot be modeled. Neural network computing systems with capabilities
for supervised learning , matching, and generalization are being devel­
oped and explored in a variety of simulated and experimental contexts.
Robotic systems offer a promising domain for this exploration since the
practical application of complex robotic systems may require adaptive
and a learning behavior in order to achieve their desired functionality. In
manufacturing, these capabilities in particular, may improve the imple­
mentation efficiency, increase the reliability of the system, and improve
the performance and accuracy of inspection and control functions. The
hierarchical nature of a manufacturing systems architecture lends itself
to the integration of these techniques into real systems, the use of neu­
ral network techniques in off-line planning, systems design , and product
area of particular promise. Neural network principles need
to be better understood, and convergence, computational efficiency, and
stability characterized more completely. Robotics and automation pro­
vide an opportunity for evaluation of these capabilities, and a setting
for the development of practical tools to enhance the functionality of
robotic systems in manufacturing applications.
Support for the preparation of this chapter was provided by the New
York State Center for Advanced Technology in Automation and Robotics
and by the NASA Center for Intelligent Robotics Systems in Space Ex­
ploration at Rensselaer. The author would like to thank Ms. R. Laviolette for assistance in
Fu, K. S., Gonzalez, R., and Lee, G. ( 1987) . Robotics: Control, sensing,
resentation of assembly plans. In Proceedings of the National Con­
ference on Artificial Intelligence, AAAI-86, Philadelphia, PA. San
of mechanical assembly sequences. Technical Report
Robotics Institute, Carnegie-Mellon University, Pittsburgh,
ing a minimal representation size criterion.
IEEE International Conference on Robotics and Automation,
control synthesis for flexible assembly systems. In A. K.
and A. Pugh, eds., Machine intelligence and knowledge engineering
Research and applications in electronics manufacturing.
covery by minimal representation methods, Technical Report
Robotics Institute, Carnegie-Mellon University, Pitts­
In Proceedings of the Fifth Yale Workshop
on Applications ·of Adaptive Systems Theory, Yale University, New
Applications of Neural Networks in Robotics and Automation
Weiss, L., Sanderson, A., and Neuman, C. (1987). Dynamic sensor-based
control of robo ts with visual feedback, IEEE Journal of Robotics and
As new techniques for adaptive networks such as those described in the
first part of this book are developed, it becomes important to test these
techniques on a variety of realistic problems to see where they work well
and where they need further refinement. Chemical process control offers
a fruitful set of test beds and benchmarks for the development of new
control algorithms. Chemical processes are often highly nonlinear and
difficult to control, yet easy to make approximate models for. The prob­
lem of controlling them using conventional controllers is widely studied.
This chapter compares chemical and robotic process control and sug­
gests a problem in the control of bioreactors which gives a sequence of
Process control of chemical plants is an attractive application because
of the potential benefits to both adaptive network research and to ac­
tual chemical process control. Control of chemical systems such
actors and distillation columns has been extensively studied (Morari
and Zafiriou 1989). They provide, in fact, the major current applica­
tion of adaptive control, which includes the set of techniques against
which adaptive network-based controllers will need to compete in the
scientific and commercial marketplaces. Many of the approaches devel­
oped in "conventional" adaptive control also apply directly to adaptive
network-based control. Studies of convergence, robustness and stability
that are done on adaptive controllers will eventually need to be done
on network-based controllers as well. However, in the quest for systems
with provable properties, traditional control has neglected some attrac­
tive but more complex control methods. Work on adaptive networks can
stimulate adaptive control by suggesting new approaches, algorithms,
This chapter describes the context of work in adaptive control and the
industrial need for better controllers within which work using adaptive
networks must establish itself. Some of the features that make chemical
reactor control difficult and some of the differences and similarities of
chemical process control and robotic control are described.
section describes a proposed benchmark bioreactor control problem and
the use of an adaptive �er to control it.
One of the most promising uses of adaptive networks is as the model
of the plant or process being controlled in a conventional model-based
controller. Many conventional adaptive controllers use models which can
be recast as being based on linear adaptive networks. In a typical form,
the plant output at time k + 1, 0k+!, is predicted based on the previous
The parameters aj and bj are fit from plant data. This formula can be
written as a linear neural net with 2n inputs and one output. This linear
equation can also be replaced with a nonlinear multiple-layer adaptive
network to allow more accurate modeling of nonlinear plants. See (Bhat
Other model-based architectures can also be used.
control (Garcia and Morari 1982), for example, uses both a plant model
and an inverse plant model. An adaptive network can be used in place
of the model and inverse model (see figure 16.1). The model is trained
using plant input and output as its inputs and outputs, while the in­
verse model is trained by using plant output and input as its inputs
and outputs, respectively. Internal-model control has been extended to
multivariable systems (Garcia and Morari 1982) and specialized so that
the control algorithm can be formulated as a linearly constrained opti­
mization problem (called in this case Dynamic Matrix Control or DMC)
systems (Economou and Morari 1986, Li and Biegler 1987). This work
demonstrates the value of using a separate model of the process for non­
linear as well as linear processes and provides a framework for doing so.
However, conventional control (unlike adaptive network-based control)
does not usually address the problem of determining the model of the
Adaptive networks can also be a part of adaptive controllers such as
self-tuning controllers (Astrom et al. 1977) which adapt parameters as
the process is being controlled. Self-tuning regulators are composed of
two loops: an inner loop which consists of the process and an ordinary
linear feedback regulator, and an outer loop which adjusts the parame­
ters of the regulator in the inner loop. Note that these controllers do not
use an explicit model of the process; knowledge of the process is implicit
The vast majority of the work on adaptive control has been on meth­
ods for linear or weakly nonlinear systems. This has some justification
in that for small enough variations, any continuous system is linear; it
also allows rigorous conclusions to be proven about controller behavior.
Unfortunately, variations in the real world are not always small enough.
Linear adaptive control schemes have been widely studied and are widely
used in industry. (By linear, we mean systems which when not adapting
give linear control. ) Nonlinear adaptive controls are less well developed
and less well understood. Nonlinear self-tuning controllers can offer sub­
stantially better performance than linear controllers (Anbumani et al.
1981, Svoronos et al. 1981, Lachmann 1982), but the methods that have
been used for nonlinear adaptive control are limited to processes with
specific nonlinearities and often have difficulty handling time delays.
Many controllers for nonlinear systems make the unrealistic assumption
that exact nonlinear models of the process being controlled are known.
Even those controllers which can include arbitrary nonlinear functions
of the output and old inputs are typically limited to single input single
output systems (Agrawal and Seborg 1987). Adaptive networks appear
to be ideally suited for modeling highly nonlinear multivariate systems;
it is therefore to be expected that their major contribution will be in
In spite of the extensive work on self-tuning controllers and model­
reference control, there are many problems in the chemical processing
industries for which current techniques are inadequate.
tioned above, many of the limitations of current adaptive controllers
arise in trying to control poorly modeled nonlinear systems. For most
of these processes extensive data are available from past runs, but it is
difficult to formulate precise models. This is precisely where adaptive
networks can be expected to be useful. A representative set of commer­
cially important problems is briefly described below.
Many chemical processes are strongly nonlinear. Nonlinearities may
be intrinsic to the physics or chemistry of a process as in supercritical ex­
traction, in which complex phase behavior leads to sensitive dependence
of operation on operating conditions and control (Panagiotopoulos and
Reid 1987). Nonlinearities may also arise through the close coupling of
simpler processes. For example, when heat integration is used to save
energy, the processes become more tightly coupled, more multivariate
Older technologies also present challenges.
nonlinear process, and one of the most widely studied control problems
Even production of products as "mundane" as soap
flakes can offer major challenges. One major manufacturer has a process
that produces good quality soap flakes during high volume production,
but is erratic (depending on operator skill) during low volume produc­
tion. No good first-principles models of the process exist and there are
too many control parameters to experiment blindly. Processes such as
aluminum casting offer similar challenges. Different rates of cooling lead
to different regimes of operation and produce aluminum with different
properties. Given some simulation data and experience from past runs
one wishes to produce an "optimal" control schedule.
Chemical reaction systems also present important and widely stud­
Extensive theoretical and experimental studies
have been made of both batch and continuous stirred tank reactors
(CSTRs). Although such reactors can be (approximately) described by
simple equations, they can exhibit complex behaviors such as multiple
steady states and periodic and chaotic behavior (Agrawal et al. 1982,
Chang and Chen 198 4). One example of such reactors which presents
of the complexity of the living organisms in them, and can vary from
TlG§1�hlfwJ �/to control because one often
cannot measure online the concentrations of the chemicals being metab­
olized or produced. Bioreactors can also have markedly different oper­
ating regimes, depending on whether the "bugs"
rapidly growing or producing product. Model-based control of such re­
actors thus offers a dual problem: determining a realistic process model
and determining effective control laws in the face of inaccurate process
Of the problems mentioned above, I think that a version of the biore­
actor or CSTR problem is the most accessible to experimentation and
use in the testing of adaptive networks. Strong nonlinearities give be­
havior sensitive to the parameters in models and the structure of the
models. Very different behavior can be observed in different operating
regimes, including complex dynamics, yet one can capture enough of
their behavior in two equations to offer a significant control challenge
Multivariate versions of the problem arise when one wishes
to use control of temperature and feed rates of different nutrients to
optimize production of different products.
Comparison of Chemical and Robotic Process
In spite of their obvious similarities, process control and robotic control
differ in several important ways. Chemical plants are mostly operated
continuously, and try to respond to and minimize the effect of distur­
more work is needed in devising more powerful methods of learning mod­
els of dynamic systemS ( or learning to correct models derived from first
principles so that they describe real plants ) and in developing reinforce­
ment learning schemes appropriate to control applications. Algorithms
must be modified to handle essentially infinite sequences of data, and to
overcome problems such as forgetting. Real-world problems with vari­
able time lags and large amounts of data add to the challenge. Speed is
also a problem for some real-time applications.
On the other hand, some aspects of chemical process control are sim­
have the complicating questions of visual interpretation so common to
temperatures and pressures are virtually continuously available and a
fairly clear identification and control problem can then be posed. If any­
thing, the abundance a.D1JOfJ� .vat.eBia1rements poses a challenge:
how can all of this data be efficiently used to improve one's model of
the process? When one has an accurate set of equations describing the
plant, this is a question of fitting the parameters ("system identifica­
tion" in control jargon). When one does not, more general equational
forms such as those offered by adaptive networks are needed.
One way to approach the problem of developing test problems and
benchmarks is to ask the question, "What makes a system hard to con­
trolT' Much of the difficulty in controlling any process comes from
the complexity of the process being controlled. This complexity can
be described in several ways. Highly nonlinear systems are difficult to
control, particularly when they have complex dynamics (such as insta­
bilities to limit cycles and chaos). Difficulties can often be presented by
constraints, either on the control parameters (e.g., there is a maximum
rate at which the system can be heated) or in the operating regime (e.g.,
heating above a certain point leads to a runaway reaction). Lack of exact
knowledge of the process, of course, makes control more difficult.
The above problems are, of course, common to robotics control prob­
lems. Other difficulties such as significant lag times between the time
a control action is taken and the time a response is observed are more
characteristic of chemical process control. Delays in response mean that
the system is not invertible. They also create a temporal credit assign­
ment problem: it is not trivial to determine which results should be
credited to a given control action. Many chemical processes also have a
spatial credit assignment problem: the systems ha'tre many sensors and
controllers (are multiple-input-multiple-output, or MIMO), and it is not
clear how to connect sensors and controllers or how changes to multiple
Optimal control of many chemical plants also requires systems which
make use of predictions of future behavior. This can occur in time­
varying processes such as batch processes, where one wants to optimize
over time, or it can occur in fairly simple continuous systems, where
nonlinearities can cause "inverse response" (a change to a control pa­
rameter that may initially move the process in a direction opposite to
Finally, chemical problems are different from robotics control prob­
lems in that experimentation must be much more limited and conserva­
tive. Although it may be feasible to have a robot try-but fail-fifty
times to put a peg in a hole, it is not acceptable to have a reactor try­
but fail-fifty times to avoid producing product which is off specification.
Unlike many "toy" problems, one cannot afford in these applications to
stumble around and experiment for a long period prior to achieving
This means-among other consequences-starting
from models based on first principles and improving with a relatively
The pioneering early work on machine learning for control was strongly
supported and shaped by a few benchmark control problems. The char­
acteristics of the problem being solved and the techniques used to solve
it interact closely; different problems call for different approaches and
different controller architectures and algorithms.
be carefully examined to see why they are hard.
model control problem is the problem of balancing a pole
verted pendulum ) on a moving cart ( Michie and Chambers 1968).
figure 16.2) In this version of the problem, the possible actions on the
cart are discretized into a positive or negative force of fixed magnitude,
and the state of the pole and cart is similarly quantized into a small set
of discrete regions ( direction the pole leans, rough position of the cart)
for sensory purposes. Reinforcement or rather, in this case, punishment
comes only when the pole leans over too far, or the cart exceeds the
bounds of the track. This problem has come to be used as a test bed for
trying and comparing new algorithms ( Widrow 1964; Barto Sutton, and
Anderson 1983; Anderson 1987; and earlier chapters in this volume ) .
The difficulty of the problem comes from the delayed reinforcement and
inaccurate measurement of state variables.
The control problem posed below is in some ways easier, and in many
ways different from the pole balancing problem.
problem has been very useful because it captures the aspect of delayed
feedback. Chemical plants also often have delayed feedback, but that
is the the central feature which makes their control difficult. The goal
of chemical process control is quantitative
chemical ) rather than qualitative ( not falling over) . The feedback avail­
able to chemical plants and robots is often of better quality than that
The error signal and the possible actions are
generally continuous rather than discrete and many more measurements
and actions are available. Also, an approximate model of the process is
Chemical systems can be relatively simple in that they have few vari­
ables, but still very difficult to control due to strong nonlinearities which
are difficult to model accurately. A prime example is the bioreactor. In
its simplest form, a bioreactor is simply a tank containing water and
cells (e.g., yeast or bacteria) which consume nutrients ("substrate") and
produce products (both desired and undesired) and more cells. Biore­
actors can be quite complex: cells are self-regulatory mechanisms, and
can adjust their growth rates and production of different products rad­
ically depending on temperature and concentrations of waste products
(e.g., alcohol). Systems with heating or cooling, multiple reactors, or
unsteady operation greatly complicate analysis. For a benchmark, how­
ever, a relatively simple system is best.
The simplest version of the bioreactor problem is a continuous flow
stirred tank reactor (CFSTR) in which cell growth depends only on the
nutrient being fed to the system. The target value to be controlled is
the cell mass yield. Often one wants to produce as much cell mass as
possible. A basic set of equations for such a bioreactor (Agrawal et al.
and C2 are, respectively, dimensionless cell mass and substrate
conversion, with C2 defined as (SF - S)/SF, where SF is the concen­
tration of substrate (nutrient) in the feed to the reactor and S is the
concentration of substrate in the reactor. (See figure 16.3.) The control
is the flow rate through reactor. The first equation says
that the rate of change in the amount of cells is equal to the amount of
cells carried o �t of the tank (C1w) plus the amount by which the cells
(C1(1- C2)ec2l"f). The growth rate is proportional to the current
amount of cells (C1), but depends nonlinearly on the concentration of
nutrient (C2). The second equation says that the change in the amount
of nutrient equals the rate at which nutrient is swept out of the system
plus the rate at which it is metabolized by the cells. The constants {3
and "y determine the rati;�YiflJj�ItMja/J.Utrient consumption. The
equational form implies that cells grow fastest at intermediate substrate
concentrations and slower at very high or low concentrations. (Note
that some researchers define another parameter Da which is equal to
l/w in our formulation.) This is not a completely realistic model of
any bioreactor, but it provides a simple but challenging system that has
been studied in by process control experts (Agrawal and seborg 1987,
This system is difficult to control for several reasons: the uncontrolled
equations are highly nonlinear and exhibit limit cycles. Optimal behav­
ior occurs in or near an unstable region. The problem exhibits multiplic­
ity: two different values of the control parameter-flow rate--can lead
to the same desired set-point in cell mass yield.
This problem has proved challanging for conventional controllers. A
test run with parameter values j3 0.48 and 'Y 0.02 was made. Note
that for these values of j3 and 9 , a Hopf bifurcation occurs at w 0.829.
The system was brought to steady state at a flow rate of w
and then the target cell mass C1 was increased by 0.05. Control was
made at intervals of 0.5 dimensionless time units. The above change
in set-point is sufficient to shift from a stable regime into the domain
of attraction of the limit cycle. Internal Model Control (IMC) required
26 coefficients to achieve model response within 75 percent of its steady
state gain but was unstable due to inaccuracy in the linearized model.
Control schemes which assume that the correct model of the bioreactor is
known can achieve better success by using predictions of future behavior.
However, the quality of control depends critically on having an accurate
model. Assuming that the correct equational form is known and using
nonadaptive control schemes, small errors in the parameters give rise
to very inaccurate control: an error of 2% in 'Y or 20% in j3 leads to
a 50% error in the target cell mass (Brengel and Seider 1989). Note
that these tests assume (unrealistically) that there is no noise in the
measurements (e.g. of C1) and that w actually takes on the exact value
We have implemented a simple, model-based neural-controller using the
architecture by Jordan (1988) described above. (See figure 16.4.) Two
multilayer networks are used, one for the model of the plant and the
second for the controller. In the results presented below, each network
controller and the model of the plant are adaptive
layers of 5 hidden nodes each; more nodes will, of course, give
We trained the adaptive network model using input concentrations
and control actions all varying over the full dimensionless range of zero
to one. W hen the controller is attached, trained, and tested on target
sets of five hidden nodes) produces output concentrations of
The discrepancy between the target value and the actual comes
We have also tested the network with random noise introduced into
show the control to be stable and reasonably accurate. To achieve the
best control on real, noisy systems will require using networks which
have as inputs the outputs from previous times and which attempt to
We suggest that as a benchmark this system be tested on three prob­
of problems of increasing difficulty can be constructed based
on a system similar to the above bioreactor. The system described above
ily created by adding a second feed stream with a concentrated sub­
strate (Se)Se > > SF and flow rate We which would be used to raise the
might also be wanted to lower the substrate concentration. This gi ves
with Ce representing the dimensionless form of Se and W,We
One can now set independent targets for C1 and
C2, giving a multivariable control proble m .
The above problems are the simplest in a large class of problems.
More complex versions of the system occur frequently and are easy to
set UPi one need only consider the products produced by the cells. These
products frequently inhibit further growth, thus increasing the complex­
ity. It is also often desirable to m aximize the quantity of one or more of
the chemicals produced by cells. Other models of cell behavior inclu ding
behavior which varies strongly with growth regimes or history also fre­
quently occur. Rather than moving from one steady state to another in
a continuous flow reactor , one can look at batch or semibatch problems
in which substrate is (periodically) added to the tank, but nothing flows
out. In this case one wants to maxim i ze production of cell mass over the
course of a batch , and an opt imization ( delayed reinforcement ) problem
Although one would not expect every controller to handle this full
range of problems, it is important to know that they are available for
as nonlinearity, interaction of multiple variables , and optimization over
time can be introduced into different problem variations.
Chemical process control problems offer an ideal test bed for many of the
techniques described in the first part of this book. To address the harder
control problems, one must address questions of credit assignment (what
action was responsible for an observed effect), reasoning about dynamic
systems, optimization over time, and incorporation approximate pro­
cess models. Layered feedforward models with smoothness constraints
(Jordan 1988), recurrent networks and reinforcement learning (Williams
1988), and temporal differences (Sutton 1988) all promise major contri­
butions to the area. It will be interesting to see how well some of the
architectures which are starting to be used for robotic control problems
(Kawato et al. 1988, Miller and Hewes 1988) generalize to chemical pro­
cess control problems, and what changes are suggested by the different
For adaptive networks to prove their worth in process control, they
must be directly compared to conventional controllers-i.e., they must
be used to solve the same problems, so that the results can be com­
pared. We have presented a model problem in bioreactor control which
has been tackled with limited success by conventional control methods.
This problem is easily extended to contain additional well-characterized
The next task is now to test the various adaptive
network architectures and learning algorithms on this and other bench­
Agrawal, M., and Seborg, D. E. (1987). Self-tuning controllers for non­
linear systems. Automatica, 23(2): 209-214.
Agrawal, P., Lee, C., Lim, H. C., and Ramkrishna D. (1982). Theoretical
investigations of dynamic behavior of isothermal continuous stirred
tank biological reactors. Chemical Engineering Science 37:453.
Anbumani, K., et al. (1981). Self-tuning minimum-variance control of
nonlinear systems of the Hammerstein model.
Anderson, C. W. (1987). Strategy learning with multilayer connectionist
In Pr:oceedings of the Fourth International Work­
shop on Machine Learning, 103-114, Irvine, CA: Morgan Kaufmann.
Astrom, K. J., et al. (1977). Theory and applications of self-tuning
Barto, A. G., Sutton, R. S., and Anderson, C. W. (1983). Neuronlike
elements that can solve difficult learning control problems.
Transactions on Systems, Man, and Cybernetic, 13(5):835-846.
modeling and control of chemical process systems.
Seider, W. D. (1989). A multi-step nonlinear pre­
dic ti ve controller. Ind. Eng. Chem. Res., 28:1812-1822.
linear systems under conventional PI control. Chemical Engineering
unifying review and some new results. Ind. Eng. Chem. Proc. Des.
Jordan, M. 1. (1988). Supervised learni ng and systems with excess de­
Computer Science and Information, University of Massachusetts,
neural network model for voluntary movement with application to
robotics. IEEE Control Systems Magazine, 8:(2) 8-16.
Lachmann, K. H. (1982). Parameter-adaptive control of a class of non­
Proceedings of the 6th IFAC Symp. Ident. Syst.
Paramo Est. 372-378. Arlington, VA, 372-378.
1. T. (1987). Process control strategies for con­
strained nonlinear systems. Ind. Eng. Chem. Res., 27:1421.
tern formation. In C. H. Waddington, (ed.), Towards a theoretical
biology. Vol. 1, Prolegomena, 206-215. Edinburgh, Scotland: Edin­
Miller, W. T., and Hewes, R. P. (1988). Real time experiments in neu­
ral network based learning control during high speed nonrepetitive
robot operations. In Proceedings of the Third IEEE International
Symposium on Intelligent Control, 24-26, Washington, D.C., Au­
Morari, M., and Zafiriou, E. (1989). Robust process control. Englewood
Distillation control for productivity and energy
Sutton, R. S. (1988). Learning to predict by the methods of temporal
covariance factorization. IEEE Transactions on Automatic Control,
Ungar, L. H. , Powell, B. A. and Kamens, S. (1990). Adaptive networks
A trainable control system: 'The broom balancer.'
Toward a theory of reinforcement learning con­
Problem for Control of Aircraft Flare and
system or "autolander." The first automatic landing was made
England on J une 10, 1 965, using a Trident aircraft o perated by the
Most commercial aircraft currently have available an
British airline BEA. Since then, these systems have been significantly
enhanced and are now used routinely on a variety of planes including
Boeing 747s. They are most often activated in
be used in fog or rain if winds are calm.
When they are used in place of a pilot it is usually for two reasons. First,
automatic systems give a reliably smoo the r landing , leading to increased
passenger comfort and to a reduction of wear on tires and landing gear.
Second, their use in calm weather pro vides an important training func­
tion, accustoming pilots to a system that they must understand and
trust if it is to be used in adverse conditions in the future.
Automatic landing systems rely on an air port based system, called the
( ILS ) , to position an aircraft for the final
phases of landing . The ILS has two radio beacons, called the glide-slope
and localizer beams, which guide the aircraft into the proper height (ele­
vation ) and a pproach angle ( azimuth ) relative to the runway threshold.
The actual distance to the runway threshold may also be measured.
At a pp roximatel y 50 feet abo ve the runway surface, the flare is i niti­
ated to elevate the nose of the aircraft, bleed off airspeed , and cause a
soft touchdown on the runway surface. ILS signals are dispensed with
before the actual flare because ILS signals are too noisy to provi de re­
liable guidance below about 200 feet altitude. Below that al titude , a
radio altimeter or visual data is used as the primary reference. From
the flare-initiation point until touchdown , the aircraft follows a control
program which decreases both vertical velocity and air speed. Because
the full three dimensional d ynamics of an aircraft are quite complex, the
simplified baseline model developed in this chapter considers movements
only in the longitudinal and vertical axes ( Le., horizontal displacements
are not considered ) . Figure 17. 1 illustrates the geometry of the resulting
two dimensional flare and landing . This sim plification permits the dis­
play of aircraft movement and controller errors on a standard p ersonal
A typical strategy used by a conventional controller is to decrease the
vertical velocity in proportion to the altitude above the runway (Le.,
altitude decreases exponentially with time or in space).
technical detail, the complete phases of landing and flare are presented in
It should be noted that existing autolanded systems work reliably only
within a carefully specified operational safety envelope. Today, as spec­
ified in Federal Aviation Administration regulations, these limitations
the system cannot be operated if strong downdrafts called micro bursts
Usually, turbulence is not a significant problem for large
commercial aircraft, but there are limits. A conventional autoland sys­
tem may have difficulties in severe turbulence.
safety and smoothness of landing it would be desirable if new technology
could expand the operational envelope to include safe responses under a
wider range of condi�"SlecilMatedl'Jts may provide one method of
exploring alternative control systems capable of dealing with turbulent,
from a research standpoint to have available a baseline problem within
which alternative network configurations can be tried and compared. In
the following sections we develop one such system defined in terms of
linearized component equations. First, however, it is useful to elaborate
on why a neural controller might be useful and how one might go about
A trained pilot develops skilled intuitions called
him to make appropriate responses in new and unforeseen situations. It
is unclear exactly what flight sense is operationally except that it appears
experienced pilots are able to handle aircraft better than automated
controllers in situations outside normal conditions. An example would
involve control behaviors during weather related events such
shear ( varying head or tail winds at different altitudes ) , unexpected wake
vortices, or microbursts ( abrupt violent changes in vertical wind speed ) .
One possible use of neural net technology would be to have a network
acquire some of these subtle temporal and spatial skills from a human
Current linearized controllers do not do a
good job of emulating pilot responses to emergencies or in generating
creative solutions to rare phenomena. This is perhaps due to nonlinear
or unrecognized linear relationships that pilots may use which are not
captured in a conventional controller. Because a neural net is capable in
principle of generating a mapping from one large set of variables ( e.g.,
sensor streams ) to another (e.g., operational modes or control actions) , it
can potentially capture critical behaviors implemented in very complex
functions and hence may, if provided with the correct training set, be
useful for the construction of such functions.
Another potential application is to let a network adaptively capture
variable interrelationships not specified by a design engineer through the
use of on-line training. Interactive trainability may be possible because
a network can operate in a variety of learning modes, one of which is
to learn by example. Thus, if presented with example pilot behaviors
during selected scenarios, a net may be able to extract critical initiat­
the essence of a correct response without being
explicitly told what to do at each step. A word of caution should be
voiced for such an app61ttUkig.wee,Alj:if66.aluse human factors studies
have tended to indicate that human pilots may use different control
strategies depending upon characteristics of the display and local envi­
behavior from which consistent, more complex control principles can be
Some recent research on neural networks emulating simple classical
control problems has shown encouraging results that indicate an ex­
tension of such technology to more complicated problems may well be
feasible (Ba-83, Gu-87, Ye-88). However, much research remains to be
done before valid comparisons between real world control applications
and neural network methods can be made. In
diagram of some of the most important components of a generic aircraft
landing control system. Notice that there are a number of operational
modes which in turn depend upon the stage of the landing. After several
minutes of altitude hold, ILS capture occurs, followed by ILS tracking,
and finally flare. Output from these modes are a series of control com­
mands, the most important of which for us is
aircraft elevator ser� Md'CH'iabequently the pitch up during
landing. Because the aircraft is often flying under reduced power at
landing, the throttle and autothrottle have minimum effect. These con­
trol variables are then fed into the aircraft dynamic model which in turn
is influenced by the environment. Thus, to train a neural net we must
first create the mathematical models of a standard controller (simpli­
fied and adjusted for our problem), a reduced complexity model of the
airframe upon which the controller will act, and some type of environ­
mental disturbance (in our case a wind model which exhibits logarithmic
shear or different strengths at different altitudes following a logarithmic
variation with altitudes). After these models are generated, a series of
experiments can be conducted using variations on neural network archi­
tectures, training rules, and performance criteria. Later, we will present
the above models in detail. Next, however, we will illustrate how one
might use the baseline by briefly describing our current approach to the
In our laboratory, we plan to study the autoland problem in three stages.
The first stage is complete and investigated whether a neural network
could emulate the performance of a traditional control equation through
observation of input/output controller responses. To do this we had
to specify reasonable aircraft control requirements and provide landing
metrics the network could use to adjust performance. Next, because
of the complexity of a full-scale aircraft system, we feel it is important
to consider how well the network performance would scale up as the
plant model was increased in complexity. The third and final stage will
examine the hardware implications of control algorithm computations in
an actual parallel environment. Each stage requires increasingly complex
models building upon the results and feasibility tests of the previous
steps. In the course of our investigations we have already uncovered a
number of interesting research problems. We provide them as examples
of the kinds of issues which can be explored using the above paradigm.
Our general approach was to apply process control using a trainable
controller like that represented in figure 17.3. It consisted of a teacher
(a human or a linear £ttp:yllgltfpd. MateMile neural network (using a
backpropagation learning algorithm), and the process to be controlled
(e.g., keeping an aircraft icon on a predetermined flight path by issuing
an attitude command Bernd)' The teacher could have used a linear or
nonlinear law, linear in the case of the traditional controller we used
and probably nonlinear if humans were to provide landing values. The
linear control law we used was derived using conventional control theory
methods (Kwakernaak and Sivan 1972). For a human teacher, we would
propose using actual input/output time histories obtained through direct
experiments using a simple interface and a mouse or a joystick. Super­
vised learning in the network was used to provide us with the greatest
amount of control over the mapping so the learning was not taking place
in real time. The true state of the control process was provided to both
the teacher and the network. The teacher defined the desired behavior
of the controller by providing outputs stored in a data base sufficiently
large to provide a detailed sample of the possible input state vectors.
To implement this framework we supplemented the generic aircraft
model with some supporting structures including a flare controller for
the vertical/longitudinal axes (Holley 1975). Once we trained the net
using nominal conditions (i.e., no disturbances), we also studied its per­
formance on off-nominal conditions such as various wind disturbances.
This was because in order to make the neural net perform robust con­
trol, we had first to Cilop�lImt1iMateifatr to perform well understood
linear control behaviors and then determine how well the network could
generalize to new environmental situations.
We also generated a simple graphic tracking program to provide hu­
man input through the interface of a workstation. The neural controller
could learn through observation of a human working with elevator angle
only. In our initial efforts we did not include variable throttle setting.
Regarding application to the real world, we assume that existing trends
in pilot usage of autoland systems will continue. Consequently, a neural
controller may best be used only as a supplementary enunciator device
monitoring pilot landing behavior (i.e., an lLM enhancement). If this
operational mode were selected, the pattern recognition capabilities of
a neural net would have to be significantly enhanced. Particular atten­
tion should be paid to early detection of hazardous external conditions
such as wind shear and the monitoring of ongoing pilot control actions.
In the case of abnormal weather events, it would be necessary to con­
sider whether a neural network could function adequately in a real-time
predictive mode as well, so as to anticipate emerging problems. We
have not implemented such a network but the framework would permit
investigators to consider this type of problem.
We now present the detailed aircraft model for the glide slope and flare
aspects of aircraft flight immediately prior to landing. The model is
in the form of equations of motion and control loops and is oriented to­
ward a reader with a basic engineering background. These equations are
suitable for a simulation that can be used to obtain training data for neu­
ral net based controllers. Their presentation assumes some familiarity
with control systems. The casual reader may find them overwhelming;
but, they are necessary to specify a well-behaved aircraft plant that,
although simple, responds in a realistic way to environment and control
parameters. Linearized longitudinal equations of motion are used for the
aircraft. We have assumed it is initially trimmed in level flight at low
altitude. Data and conventional control system designs were obtained
from a variety of published sources to provide a realistic representative
problem including a we�alrcraft firm. Simplifications
retain only the overall quality of system response (Le., high frequency
hardware and actuator dynamics were neglected).
In addition to linearized equations of motion for the bare airframe,
the baseline system has the following components:
A pitch autopilot to provide adequate damping and a speed of
2. An autothrottle to hold the aircraft at the requisite speed setting.
A glide slope and flare controller to produce pitch commands in
response to desired altitude and altitude change rates during de­
4. Desired altitude and altitude rate commands based on ILS tracking
during glide slope and an adaptive ground speed exponential law
during flare to provide a smooth descent path.
Altitude wind shear and turbulent gusts modeled using a Dryden
distribution (a method for generating random gusts of head winds
The linearized equations of motion define incremental aircraft dynam­
ics in the longitudinal and vertical planes.
airframe velocity components, the pitch rate, the pitch angle, and the
altitude. Figure 17.4 illustrates the aircraft coordinate system in which
these variables are measured. The figure shows some of the complexi­
ties in relating aircraft variables after an aircraft changes pitch attitude.
Basically, the aircraft is flying along an initial direction 'Yo at a speed
Uo with its wing chord (fixed to the aircraft body) elevated above 'Yo
by an initial angle of attack ao in order to generate aerodynamic lift.
The initial wing chord is thus at an angle 90 with respect to the inertial
earth-fixed axes XE, ZE. To rotate the aircraft and change its speed,
a pitch rate q is obtained through development of aerodynamic forces
This causes an incremental angle of attach a to
exist along with an incremental pitch angle 9.
locity changes and has incremental components
rotated axes X, Z. In any case, the main purpose of Figure 17.4 is to
illustrate the number of variables which enter into the equations describ­
ing aircraft motion. In the equations which follow, the time behavior of
these variables and their interrelationships define the effects of changes
in aircraft conditions dtG�ljlttedo)teliGrieent and flare.
We begin by elaborating the incremental aircraft variables. They are
developed by linearizing the force balance equations (derived from New­
ton's law) for aircraft motion. Equation set
equations for incremental response of a generic bare airframe. The re­
lations provide the time behavior of longitudinal and vertical speeds U
pitch rate q, pitch angle 0, and altitude
generic airframe into a specific airframe typical of a large commercial
Zu(u - ug} + Zw(w - wg) + (Zq [71'j180]Uo)q + g(-/I'/180) sin'YoO
Mu(u - ug) + Mw(w - wg) + Mq q + MEOE + MTbT
= incremental velocity components (ft/sec)
= incremental p itch rate (deg/sec), pitch angle (deg),
= incremental elevator angle (deg), throttle settling
Xu, Xw, Xq, XE, XT, Zu, Zw, Zq, ZE, ZT, Mu, Mw, Mq, ME, Mt
stability and control derivatives of aircraft at some
Uo), 'Yo, g = nominal speed (235 ft/sec), flight path angle
The transient response characteristics of these equations are typical
you would notice if you tried to fly such an aircraft without further
control systems operative: a low-frequency ( about
lightly-damped Phugoid response and a higher-frequency ( about
dians/sec ) , well-damped short-period response.
is entirely too oscillatory to apply glide slope and flare control laws.
Hence, we stabilize the aircraft model with a stability augmentation
Such an augmented aircraft model is obtained by combining
the linearized equations of motion with a pitch autopilot and autothrot­
The pitch autopilot is shown in the diagram of Figure
function of a pitch autopilot is to significantly damp the oscillatory bare
airframe behavior while providing reasonably fast pitch response to com­
mands. The pitch autopilot consists of proportional plus rate feedback
combined with a pitch command to develop the aircraft elevator angle.
The function of the autothrottle is to maintain constant airspeed.
illustrates the autothrottle which consists of proportional
plus integral feedback combined with a commanded incremental speed
s indicates the Laplace operator (derivative with respect to time)
Controllers also must be specified for both the glide slope and the
Other than parameter values and an open-loop pitchup
Controller inputs consisted of altitude and altitude rate commands along
with aircraft altitude and altitude rate estimates. Normally, these esti­
mates are obtained by filtering of sensor data (rate gyro, accelerometer,
etc.). In our work, these second order effects were neglected. Thus, fig­
ure 17.7 illustrates the selected controller architecture. The dotted lines
around the figure correspond to the operations which must be performed
Glide-slope commands we developed using a constant sink rate along
an assumed ILS path. Actually, ground speed is calculated by measuring
the distance to the glide slope predicted intercept point (see figure
Flare commands consist of an adaptive ground speed exponential law
(fixed n-space rather than in time). Equations 17.2 and 17.3 show the
steps needed to computC�cM.iWi8,flare commands.
Glide-slope and flare ontroller architecture.
Glide-Slope Altitude and Altitude Rate Commands (hcmd , h md )
is the constant (altitude shear) component of the
Xcmd is the negative of the ground range to the
Flare Altitude and Altitude Rate Commands (hcmd . hcmd)
is the ground speed, ft./sec. (see equations 17.2)
is the negative of the ground range to the
Xcmdo' Xcmdrv are values of Xcmd at beginning of flare and at
Next we introduce the influences of the environment. In this baseline
example they are confined to winds. Wind disturbances are assumed to
have two components: constant velocity and turbulence. The magnitude
of the constant velocity component is a function of altitude (wind shear).
Turbulence is more complex and is a temporal and spatial function as
an aircraft flies through an airspace region. The constant velocity wind
component exists only in the horizontal direction (i.e., head wind or tail
wind) and its value is given in equation 17.4 as a logarithmic variation
is the constant (altitude shear) component of ug,
UwindslO is the wind speed at 510ft. altitude (typical
where: U. is the nominal aircraft speed - ftJsec.
au. u,. are RMS values of turbulence velocity (standard deviations) - ft.!sec.
Gaussian white noise with zero mean and unity standard deviation
and Foster 1970) for spatial turbulence dis­
gen eration of these turbulence velocities is effected by a pply ing
proper correlation to match the desired spectra. F i gure
the same wind profiles we use. Other winds of
To simulate the flight of our aircraft, the five equations for u, w, q,
for elevator and throttle angle forcing functions for equations 17.1
DE and DT) are obtained by implementing the equations implied by the
the symbol s indicates the Laplace operator (derivative with respect to
time). Input to the pitch autopilot Ocmd is calculated by implementing
the controller shown in figure 17.7 where flare takes place at about 50
feet of altitude. Inputs to the controller hcmd and hCmd are determined
by means of equations 17.2 and 17.3. Autothrottle input Ucmd is taken
to have zero value. Finally, the wind disturbance forcing functions for
equations 17.1 (i.e., u9 and W9) are generated by means of the diagrams
of figure 17.8 and the average horizontal speed given by equation 1 7.4.
A sample scenario for the generation of a neural net training data set
involves recording a number of trajectories for descent along a glide slope
from an initial altitude of 500 feet followed by flare at 45 feet altitude
and then touchdown. Both nominal (disturbance-free) conditions and
logarithmic altitude shear turbulent conditions should be considered for
Measures of performance were required to specify the desired landing
conditions of the aircraft. Basically, they require that the aircraft must
land within a desired envelope of dispersions. The following conditions
summarize our touchdown performance measures. We consider four:
sink rate (how fast the aircraft descends), longitudinal touchdown po­
sition (where the aircraft touches the runway), forward speed, and the
Longitudinal Touchdown Position (XTD where Xnom is nominal touch
To facilitate the use o f the baseline model we found it useful to construct
o n a Symbolics computer which provided us
with three types of experimental environments.
environment to simulate the plant to be con­
t rolled and to generate trajectories used for
S ta tistical Analysis environment to study statistical properties
Our base line aircraft simulation was then coded into this environment.
image of the nomin al ( no-win d ) aircraft response pro d uced by the clas­
sical controller during the execution of a simulated landing. The lower
graph shows a landing descending from 500 feet after glide-slop e cap­
ture while the upper graph shows the a ircr aft vertical velocity ranging
At present, alternative neural net input architectures have been ex­
pl ored to address the dynamics inherent in the control problem. Exper
iments centered around the specification of various input combinations
and the issue of incl uding system dynamics. Typically, a control problem
requires three elements: past history by means of integration, current
values by means of prop ort ion al combination, and future predictions
by means of differentiation. Consequently, some degree of information
recurrence must be used to mimic a conventional control system.
within a neural net complicates learning immensely One
separately. Learning takes place very slowly under these circumstances.
variables external to the ne ural net. This stratagem provides
as inputs to the neural net not only current values for aircraft variables
(i.e., altitude, altitude commands, etc. ) , but also values at some point
The neural net can thus be expected to have some basis
to formulate a model of proportion al combination and
system variables. A ddi tional l y the past value of the neural net output
itch attitude commap;iJigl.tle chlrlGlttnrial. as an input
neural net to represent internally the integration of system variables.
This scheme has a beneficial effect of providing recurrence outside of the
neural net. Standard backpropagation can then be used for training.
Clearly, there are other approaches that could be explored using the
Figure 17.10 illustrates the construction of one of our neural net struc­
tures. As mentioned, inputs consisted of current altitude and altitude
rate error values along with values at the previous simulation time step
( taken as 0.025 for our purposes) . Also provided was the value of the
pitch attitude command at the previous time step, providing exterior
The architecture of the neural net shown in figure 17.10 consists of
an input layer with nine inputs, a hidden layer with four neurons and
an output layer with one neuron. Operation of the neural net was as
follows. The input values were scaled so that the input neurons have
activity values from -1 GPmgtMft, l'tfw.mrlabut neuron was connected
net used for glide slope and flare control.
to each hidden layer neuron by a weight value adj u sted during learn­
ing. Hidden l ayer neuron activity values are computed by summing the
input neuron ac tivity values multiplied by the weight value This was
followed by application of a standard nonlinear (sigmoidal) squashing
fu nction w hich provided hidden layer neuron activity values from -1 to
+ 1. Similar weight connections, summation of products and squashing
was used to determine the activity value of the output neuron. Finally,
the output neuron activity was scaled to provide the requisite range for
Our net has been t rai ned using a version of the backpropagation algo­
rithm under a variety of wind conditions. Trajectories involving no wind,
Gusty wind trajectory of neural net controlled aircraft.
constant wind, and logarithmic wind shear have been generated. Also,
turbulent gusts have been added. Figure 17.11 illustrates the result of
controlling the aircraft using a neural net trained in gusty wind condi­
tions. The upper graph of figure 17.1 1 shows the altitude rate of change
while the lower graph shows the altitude. These initial indications imply
that the neural net results provide good comparisons with conventional
controllers when exposed to conditions similar to the training set. Note
that the conventional controller response for these wind conditions is
very similar to the neural controller response shown in figure 17.11.
Further experimentation with learning is needed to show that this or
alternative neural nets can indeed fuse the various modes of flight near
landing (e.g., glide-slope, flare). To accomplish this fusion, networks
need to be trained with a wide variation of environmental conditions.
Also, statistical comparisons among trained controllers must be made
with respect to performance measures such as those listed earlier.
could also be conducted using human-derived
training data. To this end , we have constructed a simple aircraft navi­
gation interface. Currently, the system uses a mouse and does not run
in real time. This system is not intended to be realistic autolander simu­
lator. Rather, it is intended to experiment with training selected neural
nets with other than conventional linear controllers.
The human interface includes a "Flight Manager," similar to that used
in real aircraft, which displays the angle of inclination and height of
the aircraft along wit h additional information A "Preview" indicator
also displays a desired flight-path profile. Figure 17.12 illustrates the
interface. The Flight Manager has the following components:
whose height above/below the horizon indicates
degree of inclination from horizontal. Its position is
pair of triangular guides, located just outside of the aircraft
wingtips, which approximates the correct angle of incli­
nation at the cur�ttp� Mttt�er flight-path navigation.
A circular guide that allows the pilot to direct the aircraft's angle
of inclination with the use of a mouse input device. The pilot's
positioning of the guide above /below the horizon determines the
An altitude bar, which monitors the current height of the aircraft
as a rising/falling horizontal line in the bar. The height is taken
A triangular guide adjacent to the altitude bar indicates the correct
altitude at the current time for proper flight-path navigation. This
value is computed as a "theoretical hemd," as computed from the
A constant, computed at initiation of the flight manager, relates
overall window size to the number of pixels per degree of incli­
This constant is used to draw the aircraft and angle of
inclination guides at consistent distances along the vertical
The desired flight path is displayed on an x-y cartesian graph.
Height is represented along the vertical axis and distance on the
horizontal. The plane must then "fly" the indicated line, with the
current position of the plane represented
series of dots, which mark the plane's actual path.
Upon initiation, the entire flight path is shown.
mined altitude, the flight path is redrawn at a larger scale, to
assist the pilot during crucial flare and touchdown phases of the
In this chapter we have presented a simplified model of a realistic air­
craft which potentially provides a baseline model and control relation­
ships against which alternative neural network controllers can be eval­
uated. We have mentioned several ways in which the problem could be
approached including some methods currently being explored in our fa­
cility. These efforts are only in their early stages and the general area of
should provide a rich topical source for further work. Of particular inter­
est are the relationships between human and machine controllers, how
displays generate conditions that influence human control strategies, and
the generation of mappings between classical control laws and the out­
puts that may be generated by a neural controller as it extracts control
regularities from data. As a result of our experiments, we have observed
the necessity for refinements in a number of neural network concepts
of a neural net to learn discontinuities, the importance of and poten­
tial risks associated with grouping training data around discontinuities,
the vital need for faster convergence methods, and the advantages of in­
corporating a priori knowledge into a network to facilitate convergence.
Although we have successfully generated networks that land the aircraft
in headwinds and tailwinds, we have not found a single network which
has been able to capture the performance capabilities of a linear con­
troller throughout the entire domain of its input space. Further research
is needed, and we hope the present work may interest other researchers
in using this problem for evaluation of their own network designs.
Barto, A . , Sutton, R. , and Anderson , C. Neuron-like adaptive elements
tions on Systems, Man, and Cybernetics 13(5): 834-836.
that can solve difficult learning control problems.
Journal of Robotic Systems, 5(4) : 363-388.
( 1975). Wind modeling and lateral control for automatic
Stanford University, Ph.D. Dissertation, Stanford, CA.
matic aircraft landing system in turbulence,
Ames Research Center, Moffet Field, CA, October .
( 1983) . Autopilot logic for flare maneuver of STOL
( 1987) . Encyclopedia of systems and control.
Supervised learning of action probabilities in asso­
Martin Herman, James S. Albus, and Tsai-Hong Hong
In order to achieve real-time intelligent control of multiple autonomous
vehicles in complex environments, research issues such as distributed
control, knowledge-based systems, real-time planning, world modeling,
value-driven reasoning, intelligent sensing, intelligent communication,
gaming, cooperative problem solving, and learning must be addressed.
The types of activities that must be achieved by these autonomous sys­
tems include aggression, predation, exploration, stealth, deception, es­
cape, communication and cooperation. These activities are required in
order to thrive in a natural and potentially hostile environment.
The goal of the NIST Multiple Autonomous Undersea Vehicles
(MAUV ) project was to examine some of these issues in the underwater
domain by attempting to achieve intelligent, autonomous, cooperative
behavior in multiple vehicles. Our approach was to develop a control
system architecture that fully integrates concepts of artificial intelligence
with those of modern control theory. A first cut at algorithms and soft­
ware to implement this architecture was developed, and this software
was downloaded into computer boards mounted on board the vehicles.
A series of demonstration tests was then planned for two undersea vehi­
cles in Lake Winnipesaukee in New Hampshire.
After presenting the MAUV vehicles and scenarios, this chapter pro­
vides a discussion of issues dealing with intelligent control and then
presents a hierarchical control system architecture which addresses these
issues. The application of this control architecture to the MAUV vehi­
cles is then described, along with how planning, execution and world
modeling are accomplished. Finally, timing issues, implementation, and
experimental results are described. Further details on the project may
Figure 18.1 shows a diagram, and figure 18.2 a photograph, of a MAUV
vehicle. These vehicles were designed and constructed by the Ma­
rine Systems Engineering Laboratory at the University of New Hamp­
shire. They are a derivative of the EAVE-EAST vehicle ( Blidberg 1986)
developed at the same lab. The vehicle is gravity stabilized in pitch and
Diagram of University of New Hampshire EAVE-EAST vehicle.
It is battery powered with the batteries stored in cylindrical tanks at
the bottom of the vehicle. The vehicle carries three acoustic naviga­
tion transponders which are configured as an equilateral triangle. Each
transponder operates on a different frequency
water, allowing range and bearing relative to these buoys to be measured.
The vehicle carries a compass, pressure and tem perature sensors, and
depth and altitude sonars. In front, it has an obstacle avoidance sonar
time. They receive acoustic signals from navigation
five narrow beam acoustic transmitter-receivers. These are
sonar beam points straigh t ahead, 2.10 de­
2.10 deg rees up and down from the center
beam. In addi t ion , the vehicle carries both acoustic and radio teleme­
try systems. All computer b oard s are mounted in card cag es inside the
flotation tanks a�fiWl5i c:t> &ffJtm"iele vehicle.
Photograph showing University of New Hampshire EAVE-EAST MAUV vehicle.
The MA UV project planned to conduct a series of demonstrations by two
vehicles. These tests centered around two scenarios-cooperative search
and cooperative near-target maneuvers. The search scenario involves
traversing an area either to map it out or to seek targets. Figure 18.3
shows a search plan that involves transiting from a base at the island
to a search area and then performing a raster search of the area. The
vehicles may be either near the water surface or near the lake bottom
when performing the search. The concept of using two or more vehicles
to search and map shallow areas is shown in figure 18.4.
In this scenario, the vehicles were to demonstrate the ability to mea­
sure the bottom topology, and to search for and map the positions of
objects on the bottom and in the water. The vehicles were to exe­
cute a variety of search patterns, including several involving separation
and rendezvous for exchange of information. The vehicles were to com­
pute maneuvering tacti&OSilIkti§�MiI&liiWount bottom topology and
Illustration of MAUV search and map scenario.
simulated enemy positions. The vehicles were to demonstrate the use
of topological maps of the bottom for local navigation, and were to use
both visual and acoustic bottom sensors to update these maps in real
time. Obstacle avoidance sonar and bottom altitude sonar were to give
the vehicles the ability to follow bottom topographic features such as
ravines and ridges. The vehicles were to demonstrate tactics using bot­
tom features for shadowing their movement from enemy positions.
The near-target maneuvers scenario involves performing triangulation
maneuvers near a target either to localize it or to take pictures of it. Fig­
ure 18.5 shows how target localization occurs. The two vehicles, either
while patrolling or while performing a search, detect a target in direc­
tion beta using passive sonar. (Passive sonar involves detection of noise
originating at the target.) Passive detection gives only direction but no
range information. At this point, the vehicles determine two positions
perpendicular to and equidistant from the line beta, and each vehicle
travels to its position. The vehicles can then emit sonar pulses and use
triangulation to accurately localize the target. In a separate scenario,
the vehicles use similar maneuvers to achieve the triangle configuration,
and then one vehicle illuminates the target while the other vehicle takes
pictures. Having a light source some distance away from the camera,
and being able to vary the position of this light source relative to the
camera, can often greatly enhance undersea photography.
A third scenario considered for the MAUV vehicles was rendezvous
and docking. This might involve using sonar for the two vehicles ren­
dezvous with one another, and optical tracking methods for docking.
Both side-by-side and end-to-end docking can be considered.
Intelligent Control for Autonomous Vehicles
Autonomous vehicles that operate in complex environments require in­
telligence. Truly intelligent machines will have complex system archi­
tectures in which sensing, acting, sensory processing, world modeling,
task decomposition, value judgments, and goal selection are integrated
into a system which responds in a timely fashion to stimulation from the
environment. Figure 18.6 illustrates the basic elements of an intelligent
Actuators-Within any intelligent system there are actuators
which move, exert �1IR!ti � arms, legs, hands and
eyes. For an intelligenf�emcle, actuators generate forces to point
Target localization Ilsing triangulation.
sensors, excite tranducers, and steer locomotion. The actuators
are motors, pistons, valves, solenoids and transducers.
Sensors-Sensors for an intelligent vehicle may include vision, posi­
tion, distance, vibration, acoustic, pressure, and temperature mea­
suring devices. Sensors may be used to monitor both the state of
the external world and the internal state of the vehicle itself. Sen­
sors provide input to a sensory processing system.
3. Sensory Processing-An intelligent sensory processing system
compares observations with expectations generated by an internal
world model. Sensory processing algorithms perform both tempo­
ral and spatial integration, so as to detect events a.nd recognize
features, objects, and relationships in the world. Sensory input
data from a varie1!X �f s�nsors over e?,tended periods of time are
fused into a consis�_�� of the state of the world.
Sensory processing algorithms may compute distance, shape, ori­
entation, surface characteristics, and material properties of objects
4. Task Decomposition-An intelligent system has processes which
decompose high level goals into low-level actions.
position involves both the planning and execution of actions.
requires the ability to reason about geometry and dynamics, and
to formulate or select plans based on values such as cost, risk, util­
ity, and goal properties. Task planning and execution must often
be done in the presence of uncertain, incomplete, and sometimes
incorrect information. The execution of tasks must be monitored
and existing plans must be modified whenever the situation re­
Task decomposition is a hierarchical process requiring a
multiplicity of planners that simultaneously generate and coordi­
nate plans for many different subsystems with different planning
horizons and different degrees of detail at each hierarchical level.
5. World Model-The world model is the intelligent system's best
database in which is stored knowledge about the world. It provides
information about the state of the world to the task decomposi­
tion system so that it can make intelligent plans and behavioral
choices. It also provides expectations and predictions to the sen­
sory processing system in order to enhance its ability to analyze
sensory data. The world model is kept up-to-date by the sensory
Values-Any intelligent system must have a value system in order
to make judgments as to what is good and bad. The value sys­
tem must evaluate both the observed state of the world and the
risks, and benefits of observed situations and of planned activities.
Without a means of making value judgments, an intelligent task
decomposition system has no basis for choosing one action over
Goal Selection-Goals are selected by a looping interaction be­
tween the goal selection, world model, and value systems.
goal selection system hypothesizes actions, the world model pre­
dicts probable results, aD;d the value sy'stem evaluates the predicted
g�f!¥ltlJ/}t�fli'itfftttfiiM chooses the hypothesized
action with the highest value as a g oal to be pursued. By this
process, the goal selecti on syst em cho oses goals , generates plans,
computes priorities, and ass igns resources to tasks so as to maxi­
S yste m Architecture-An intelligent machine requires an intercon­
necting system architecture that enables the various components
to interact and communicate with each other in an intimate and
s ophist icat ed way. A sys tem architecture is what enables the task
d ecompositi on system to direct sensors, to focus sen sory process­
ing alg orit hms on objects and events worthy of attention, and to
ignore things that are not important to cur re nt goals and t ask pri­
orities. It is what enables the world model to answer queries from
task decomposition m odules , and make predictions and receive up­
date s from sensory processing modules. It is what conveys value
judgments from the value es timat ing system to the g oal selection
system as to the success of behavior and the desirability of states
The MAUV control system architecture is hierarc hically structured into
six levels and is shown in figure 18. 7 ( Albus 1987, 1988). This control
system is based on the one developed for the Automated Manufactur­
ing Research Faciltity at NIST ( Simpson , Hocken, and Albus 1983) .
It is divided into three main components, shown as columns in figure
18.7. These are senso ry processing, world modeling, and task decom­
position. The hierarchy is serviced by a communications system and a
distributed common memory. The task decomposition modules perform
real-time decomposition of task goals by means of real-time planning,
execution and task monitoring. The sensory processing modules detect
and recognize patterns, events and objects, and filter and integrate sen­
sory information over space and time. The world modeling modules
perform the following functions: (a) they maintain a central real-time
data base of information about the state of the wor ld and the internal
state of the system, (b ) they up date this data base with information
from sensory processing, (c ) t hey provide expectations of incoming sen­
sory data, and ( d ) they respond to queries from the task decom position
Block diagram of the MAUV control system architecture.
component based on information in the database and on evaluations of
In the task decomposition hierarchy, the highest level, the mission­
level, converts a commanded mission into commands to each of a set of
groups of vehicles. These commands involve tasks that treat a whole
group of vehicles as a single unit. The group level converts group com­
mands into commands to each of the vehicles in the group. These com­
mands involve large tasks for each vehicle. The vehicle task level converts
task commands into elemental moves and actions for the vehicle. The e­
move ( elemental move) level converts elemental moves and actions into
intermediate poses. These are converted into smooth trajectory posi­
tions, velocities, and accelerations by the primitive leveL Finally, the
servo level converts these into signals to actuators, transducers, etc.
Before describing the elements of hierarchical planning and execution, we
will provide our working definition of a plan, and describe the difference
events. The events are either events in the world or events in the internal
state of the system. We represent a plan as a graph (figure 18.8). The
nodes of the graph represent actions and the arcs represent events. The
purpose of the planner is to obtain a plan graph. It can either generate
We define execution as the process of carrying out a plan. The pur­
pose of the executor is therefore to step through the plan graph. When
the executor arrives at a node of the plan graph, it "executes" the ac­
tion associated with the node. If an action is at the lowest level of the
hierarchy, then executing it involves sending signals to hardware. Oth­
erwise, executing an action involves sending it to a lower level where it
can be decomposed. As the executor sits at a node of the plan graph, it
monitors for events associated with arcs leading out of the node. This
monitoring is done at a fast cycle rate. The process of monitoring for
an event consists of querying the world model data base for that event.
If an event has occurred, the executor follows the arc corresponding to
The notion of hierarchical planning is shown in figure
is first input to the top level as a task command. This task is decomposed
both spatially and temporally. Spatial decomposition means dividing a
task into logically distinct jobs for distinct subsystems. For example, the
group level will have a different planner for each vehicle in the group.
Temporal decomposition means decomposing a task into a sequence of
subtasks to be executed over a period of time.
plan is then the input task to the next lower level, and this, in turn, is
decomposed both spatially and temporally. At each successively lower
level, the actions become more detailed and finely-structured.
shows a single level of this hierarchy in more detail. The
input task to this level first goes to the Planner Manager
Planner Manager performs spatial decomposition by assigning jobs to
The Planner Manager also coordinates plan­
ning among these planners. The planners, operating in parallel, generate
Associated with each planner is a separate ex­
EXi, which executes the plan. The executors also operate in
There are two primary reasons for the hierarchical approach-to
achieve real-time planning and control and to achieve understandability
and programmability. At the higher levels of the hierarchy, actions are
large- scale and they take a long time to execute. Therefore, the search
space used to generate plans is coarse and covers large space and time.
At the lower levels, actions are smaller scale and they take a short time
The search space is therefore fine and covers small space
and time. As a result, the search spaces at all levels are small enough so
that the search is manageable. Furthermore, all levels run in parallel.
The understandability and programmability comes about because the
control system is decomposed into small modules whose functions can be
well understood. Furthermore, different factors are taken into account
at different levels, i.e., mission requirements, group tasks, vehicle tasks,
In this way, when new knowledge is added to
the system, the modules in which this knowledge should reside are more
Three levels of real-time planning activity in the MAUV hierarchy.
Internal structure of the task decomposition modules in the MAUV control system
architecture at every level of the hierarchy.
The MAUV task decomposition hierarchy is shown in figures 18.10 and
18.11. Each module in the task decomposition hierarchy receives input
commands from one and only one supervisor, and outputs subcommands
to a set of subordinate modules at the next level down in the tree. Out­
puts from the bottom level consist of drive signals to motors, actuators,
and transducers. Each large box in figure 18.12 has three levels of small
boxes inside it. The top-level box represents the Planner Manager, the
middle-level set of boxes represent planners, and the lowest-level set of
boxes represent the executors, one associated with each planner. The
output of each executor is a subtask command to the next lower level.
Missions are typically specified by a list of mission objectives, pnon­
ties, requirements, and time line constraints. In our implementation,
the inputs to the mission level are a command and a mission value
function. The command is a task involving a mission strategy, e.g.,
SEARCH-AND-DESTROY, SEARCH-AND-REPORT, and MAP. As­
sociated with each command is a list of subtasks that define the com­
mand. Th.e mission value function is a function used to score the mission,
and is composed of the following elements:
A value for each vehicle-used to assess the desirability of plan
alternatives involving high risk to individual vehicles, or even the
A value for each sub task-specifies the importance of the successful
An information value for each subtask-specifies the importance
of returning information collected while executing each subtask.
A value of stealth for the mission-specifies the importance of
avoiding detection by the enemy during the mission.
Subdivide the vehicles into groups. In our scenario, we have only
2. Determine whether any of the subtasks defining the input mission
3. Provide a coarse description of routes and tactics for the mission
The outputs of the mission level are the group subtasks and priorities.
Priorities are values indicating the importance of the following factors
during lower-level planning; time used, energy used, stealth, and vehicle
As indicated in figure 18.12, the mission level has a Planner Mana ger ,
a planner for each group, and an executor for each planner. The Planner
Manager assigns vehicles to groups, sets priorities for group actions, and
assigns mission objectives to the groups. The planner for each group
schedules the activities of the group and sets the priorities mentioned
A flow chart for a mission level planner is shown in figure 18.13. The
program attempts to generate an optimal sequence of subtasks as fol­
lows. First, a set of promising plan parameters is chosen. These include
a specific sequence of subtasks and an estimate of the time and energy
priorities . Next, the planner uses outcome calculators to determine the
result of choosing these plan parameters. For example , the transit out­
come calculator determines the projected risk and the time and energy
consumption for each transit leg of the mission. In order to do this, the
outcome calculator plans a coarse route . This route will eventually be
The results of the outcome calculators are then scored based on the
mission value function which was input to the mission level. If the score
indicates that a clearly satisfactory set of plan parameters has been
chosen, then these are passed to the lower level. Otherwise, a new set
of plan parameters is chosen and the procedure is repeated. If the time
allocated to the planner to make a decision has terminated, the best set
of plan parameters thus far found will be passed to the lo wer leve l .
Replanning is done at reg';llar intervals throughout the mission by
repeating the program i��I1w.R#Itt{!l{fil.ft nning results in a different
Mission-level planner for a single group.
plan from the one currently being executed, it is installed in place of the
current plan. In this way, the world and vehicle situation is repeatedly
evaluated so that the plan generated from the most recent information
is always being executed. Further details about the mission level may
Group task commands define actions to be performed cooperatively by
groups of MAUV vehicles on multiple targets. The Planner Manager de­
composes group tasks into individual vehicle tasks. This decomposition
typically assigns to each vehicle a prioritized list of tasks to be performed
on or relative to one or more other vehicles, objects, or targets. Tactics
and vehicle assignments are selected to maximize the effectiveness of the
group's activity. The actions of each vehicle are coordinated with the
other vehicles in the group so as to maximize the effectiveness of the
group in accomplishing the group task goal.
Each vehicle planner schedules group task lists into coordinated se­
quences of vehicle tasks. The vehicle planner uses the group-level world
model map to compute vehicle trajectories and transit times. They also
estimate costs, risks, and benefits of various vehicle tactics ( or task se­
In our implementation, the inputs to the group level are a command
The command is a task involving multiple ve­
hicles, e.g., TRANSIT, ATTACK, RASTER-SEARCH. The priorities
are values indicating the importance of stealth, destruction, time, and
energy. These priorities are used as weights in the cost function during
In our scenario, there is only one group of vehicles.
associated with the group is a Planner Manager, a planner
for each of the two vehicles in the group, and an executor for each
A. search during planning. A* search uses a cost
function to calculate optimal trajectories through space. The following
factors are used in the cost function for this search:
This is based on known obstacles ( such as
large land masses ) and known density of clutter
small islands in a given path would result in a low probability of
Probability of detection by enemy sonobuoy fields or by enemy ships
Probability of destruction by enemy minefields or enemy ships con­
Deviation penalty from path specified at level above. The input task
command to the group level may specify a path to be followed.
This path is taken into account by the cost function by means of
The outputs of the group level are the vehicle tasks and priorities.
The output priority values are the same as the input priorities.
The inputs to the vehicle level are a command and a set of priorities. The
command is a task performed by a single vehicle, e.g., GOPATH, WAIT,
RASTER-SEARCH, LOCALIZE-TARGET, RENDEZVOUS. The pri­
orities are the same as the input priorities to the group level.
The function of the vehicle level is to decompose the input vehicle task
into a sequence of tasks for each subsystem of the vehicle. These subsys­
tem tasks are called elemental moves or actions (e-moves). We consider
three subsystems, the pilot, sensors and communications subsystems.
Manager, three planners (one for each subsystem), and three executors.
The Planner Manager decomposes vehicle tasks into work elements to
be performed by the various vehicle subsystems.
synchronizes and resolves conflicts between vehicle subsystem plans.
The pilot planner uses the world model database to search for a path
between the start and goal positions indicated by the input vehicle com­
mand. A. search is used and its cost function has the same factors as
The communications planner schedules the messages to be sent by
deciding if and when to send each message.
determined by computing the value of each message, its urgency, the risk
of breaking communJQ��tR&N,atm<lathe power needed to transmit
The sensors planner schedules the activation and deactivation of pas­
sive and active sonars. Currently, this schedule is also extracted from a
rule database. In the future, the schedule will be determined by comput­
ing the value of taking sonar soundings, its urgency, the risk of breaking
silence for active sonar, and the power needed to take the sonar sound­
The outputs of the vehicle level are the e-move tasks.
The input to the e-move level is a command which is an elemental move
or action involving a single subsystem, e.g., GO-STRAIGHT (pilot sub­
system), ACTIVATE-ACTIVE-SENSOR (sensor subsystem), SEND­
The function of the e-move level is to decompose the input e-move
command into a sequence of low-level commands to the particular sub­
system controller. As indicated in Figure 18.12, a Planner Manager,
planner, and executor exists for each subsystem of each vehicle.
The pilot e-move can be defined as a smooth motion of the vehicle
designed to achieve some position, orientation, or "key-frame pose" in
space or time. The pilot planner at this level computes clearance with
obstacles sensed by on-board sonar sensors and generates sequences of
intermediate poses that define pathways between key-frame poses. A *
search is used to generate these paths. The cost function used during
1 . Traversability. This is based on known local obstacles. The tra­
versability of a given path is either 1 (the path is traversable) or
Distance traveled. A shorter path is always preferred. This helps
Deviation penalty from path specified at level above. As in previous
levels, the input command to the e-move level may specify a path
to be followed. This path is taken into account by the cost function
A communications e-move is a message. The communications planner
at this level encodes messages into strings of symbols, adds redundancy
for error detection and correc.tion, and for�ats the symbols for transCopynghted Matenal
The sensors e-move is a command to activate or deactivate a passive
The sensors planner at this level decomposes sonar
activation commands into a temporal pattern of sonar pings.
The e-move level is the lowest level currently implemented in the
MAUV architecture. The outputs of this level are low-level commands
to the subsystem controllers of the MAUV vehicles. These controllers
were developed by the University of New Hampshire.
completeness, we next describe the lowest two levels in the MAUV ar­
The primitive level computes inertial dynamics and generates smooth,
dynamically efficient trajectory positions, velocities and accelerations.
Inputs to this level consist of intermediate trajectory poses which define
a path that has been checked for obstacles and is guaranteed free of
The outputs of the level consist of evenly spaced trajectory points
which define a dynamically efficient movement.
The servo level transforms coordinates from a vehicle coordinate frame
into actuator coordinates. This level also servos thruster direction and
actuator power. There is a planner and executor at this level for every
Inputs to this level consist of commanded positions, velocities, thrust,
power, orientation, and rotation rates of the vehicle.
level consist of electrical voltages or currents to motors and actuators.
Cooperative behavior between the two MAUV vehicles is achieved as
The vehicles start out with identical software, except for the
vehicle identifier, which is unique for each vehicle.
each vehicle has a mission and a group level, and mission- and group­
level planning is done on both vehicles. If the two vehicles sense the exact
they receive the same sensor input ) , then
mission and group planning will be identical between the two vehicles,
and they will achiev � coor pp.:t.sd b ht\vior. This is because the two
vehicles will generat t' _it!Mef11 � '6£a oth vehicle 1 and vehicle 2,
and each vehicle will simply execute the appropriate plan for itself.
If, instead of always having identical world model databases, the ve­
hicles have the same world model information with regard to significant
world properties (i.e., properties relevant to generating and executing
mission and group level plans), then mission and group planning will
still be identical between the two vehicles. This is the method we cur­
rently use to achieve cooperative behavior. The significant world prop­
erties relevant to our scenarios are the positions of large land masses
such as islands, the positions of sonobuoy and mine fields, the positions
of the two vehicles, and the positions of enemy targets and defenses. Is­
lands, sonobuoy fields and mine fields are input at the beginning of the
mission and do not change. Therefore information about these will be
identical in the vehicles' world model data bases. In order to ensure that
information about the other significant world properties are the same in
both data bases, each vehicle, upon detecting a new target or defense,
immediately communicates this to the other vehicle. In addition, each
vehicle regularly communicates its position to the other vehicle.
A problem with this technique of achieving cooperative behavior is
that, as the scenarios become more complex, more information would
have to be regularly communicated between the vehicles. In addition,
if a group had many vehicles in it, regular communication from each
vehicle to all the others would have to occur. An alternative technique
which seems more promising is to designate one vehicle in each group
as group leader, and to designate one vehicle as mission leader.
mission leader performs mission planning and communicates the plans
to each group learder. Each group leader does group planning and com­
municates the plans to the individual vehicles in the group.
way, if different vehicles have different world model data bases, they will
nevertheless execute cooperative maneuvers determined from the world
model data bases of the group and mission leaders. If communication
cannot occur because of stealth requirements or because a vehicle is out
of communication range, then each vehicle still has mission- and group­
level software and can generate its own plans. Of course, this could lead
to noncooperative maneuvers. Once communication is reestablished, the
This section describes the real-time planning system used at the group
and vehicle levels of the hierarchy. The block diagram in figure 18.14,
which shows this pl am�� MtitEbiaapplied to the group level
as well as the vehicle level. An input task command first goes to the
Planner Manager, which contains two modules. The first, the Job As­
signment Module, divides the input task into several jobs and sends
each to a different planner. The different planners then work on these
jobs in parallel. The second module, the Plan Coordination Module,
coordinates planning among the various planners. Currently, this co­
ordination is accomplished by generating constraints to be met by all
the planners. For example, if each planner corresponds to a separate
vehicle, this module might generate constraints consisting of a position
where all the vehicles are to rendezvous and a time when this is to oc­
cur. Each individual planner would attempt to meet the constraints. If
one of them could not, it would report back to the Plan Coordination
Module which would then generate a new set of constraints. In the fu­
ture, the Plan Coordination Module will also coordinate communication
among the planners. Some constraints can be determined only by the
planners at plan time, and these would have to be communicated to the
other planners. For example, one vehicle planner might want as part of
its plan one of two actions depending on what another vehicle planner
After a planner has finished generating a plan in the form of a plan
graph, the executor associated with the planner steps through the graph.
Each planner contains several modules ( figure 18.14). The Cyclic
Replanning Module accepts an input command (or job ) from the Planner
Manager and, at regular cycle times, generates a new plan. The primary
way in which our system performs replanning is by generating new plans
regularly. The traditional way of doing replanning is to post some simple
conditions on the world which, when met, causes replanning to occur.
Our approach, however, is based on the notion that the best way to know
whether the world has changed in such a way as to require a new plan
is to actually run the algorithm that generates the plan, and then to see
whether the plan has changed. The advantage of doing it this way rather
than posting some simple conditions is that there could be a complex
interaction of events in the world that would require a new plan, and
this complex interaction is exactly what the planning algorithm looks
One issue that must be considered is real-time planning and how it
is handled by the planner. As stated above, we view a plan as being
composed of actions and world events. Execution of the plan by the
executor occurs by monitoring for world events and stepping to the ap­
an arbitrary point in tiftRplDl�'��Ei'K�1 set of events in the world
occurring at ti. We define real-time planning as the process of gener­
ating plans quickly enough so that there is always an action a given to
the executor such that (1) Action a is part of a plan p, and (2) Plan p
represents an "appropriate" response by the system to events E at time
h. Let tl be as defined above and let t2 be the furthermost point in time
at which an action must be executed in order to appropriately respond
to the world events E. Then the planning reaction time is defined as the
Fortunately, the planning reaction time is different at different lev­
els of the hierarchy. At the higher levels, the world representation is
course, planned actions occur over large time scales, and world events
are coarsely represented. Therefore the planning reaction time of the
system can be relatively slow. At the lower levels, the world representa­
tion is detailed, planned actions occur over small time scales, and world
events are represented in detail. Therefore the planning reaction time
The cyclic replanning time at each level is determined by the planning
reaction time. The cyclic replanning times at the higher levels are longer
than at the lower levels. At the end of a cyclic replanning time inter­
val, the next action to be taken must have already been determined by
the planner, for the executor must always have an action to carry out.
However, these time intervals will often not be enough for the planners
to generate new full plans. Therefore, the planner will pass on to the
executor whatever is its best plan at the end of the cycle time, even
though the planner may not have finished planning to completion. In
our implementation, where A * search is used, the best plan at any point
in time is the path in the search tree from the root to the leaf node with
When the Cyclic Replanning Module has generated a new plan, the
plan is passed to the Plan Update Module (figure 18.14), which updates
If a subtask (Le., an action) of the current plan is sent by the executor
to the level below and the subtask cannot be achieved, then a signal is
returned to the current level and the plan is modified by the Subtask
Failure Replanning Module (figure 18.14). Associated with each subtask
command sent to the level below is a set of failure constraints. If these
constraints cannot be met, then the subtask fails. Examples of failure
constraints are: (1) achieving the subtask within a time window, (2)
achieving a goal (e.g., aklPyi1gh��aloint in space), and (3) not
deviating more than a certain amount from a given path.
The Subtask Failure Replanning Module has thus far been imple­
mented only at the e-move level to handle imminent collision between
the vehicle and the lake bottom. The module generates a plan in which
the vehicle slowly moves upward, collecting sensory information, until it
has determined that there is room to continue forward.
Both of the Cyclic Replanning and the Subtask Failure Replanning
modules access the Plan Schema Database to generate plans. Plan
schemas are used to define the input task commands and will be de­
A plan schema is used to define a subtask command. It provides all
possible sequences of actions that define the command. In order to
determine the best sequence in a given situation, it allows the application
of a cost function and provides the ability to perform a search which is
driven by the plan schema. As shown in figure 18. 15, the plan schema
is represented as a graph. The nodes of the graph represent actions and
the arcs represent events in the world or internal events in the system.
The plan schema is converted into a specific plan by an interpreter which
steps through the plan schema graph and outputs a plan graph. When
the interpreter reaches a node in the plan schema graph, it adds the
action associated with the node to the output plan. It then queries the
world model about the world events associated with the node to the
output plan. It then queries the world model about the world events
associated with the arcs leading out of the node. The queries relate to
a hypothetical future world formed by starting with the current model
of the world and simulating all the hypothetical actions in the output
plan. The interpreter follows the arc whose world event is true, and then
processes the next node in the plan schema.
Each node of the plan schema is divided into two components, the
component and the context subroutine component.
The alternative action component contains a function that generates all
possible alternative actions that can be considered when the node is
reached. These alternative actions represent the possible operators that
can be applied to the state space at a given point in the state space
search. In figure 18. 15, for example, the GO-STRAIGHT node contains
a function that returns all permissible directions for a GO-STRAIGHT
action. Since the stat�gbteltiM&t8rri. a three-dimensional grid,
Vehicle level plan schema for "Rendezvous at point P."
all GO-STRAIGHT actions, when executed, will lead to some adjacent
The context subroutine component of a plan schema node contains
a subroutine that sets the context (i.e. , sets certain variables) for the
alternative action component. This context is also assumed for all future
nodes of the plan schema that will be traversed by the interpreter. These
future nodes also have their own context subroutine components which
The plan schema contains two types of arcs. The first type is a world
arc. This arc contains a predicate that queries the world model
about a hypothetical future world. A function is then applied to the
result of this query, and the predicate returns true or false depending on
the value of the function. In figure 1 8. 1 5 , for example, the arc out of the
GO-STRAIGHT node et1A\m.,h��i is a predicate that queries
a hypothetical future worrJ: 'fusult'ingu/rom the hypothetical execution
of a set of GO-STRAIGHTs, about whether the vehicle is at point P. If
it is, then the interpreter will step to the HOVER node.
The kind of predicate just described is a plan time predicate. Also
associated with each world event arc is an execution time predicate.
This is the predicate that is actually placed in the plan graph, and this
predicate will query the most current world model at execution time.
The second type of arc in the plan schema is the else arc. This arc
also contains plan time and execution time predicates. The plan time
predicate returns true if the node that it leads out of has been processed
and the predicates of all other arcs leading out of the mode return false.
In figure 18.15, for example, there is an else arc and a world event arc
leading out of the GO-STRAIGHT node. If the node has been processed
and the predicate of the world event arc ( i.e. , whether the vehicle is at
point P) ret u rn s false, then the predicate of the else arc will return true
and the node will be revisited. The execution time predicate of the else
arc returns true if the node that it leads out of in the plan graph has
successfu lly completed execution and the predicates of other arcs leading
The world modeling component serves to accumulate and store infor­
mation obtained from sensory processing, and to make this information
available to the planners and executors. The executors query the world
model about the current state of the world so that they can monitor
the execution of plans. The planners query the world model about the
current state of the world and about hypothetical future states of the
world. The world model also provides expectations and predictions to
The world model data base is updated from sensory data obtained
Obstacle avoidance sonars-provide range to obstacles ahead of
Flux-gate compass-provides vehicle orientation.
Altitude sonar-provides altitude of vehicle above bottom.
Depth sonar-provides depth of vehicle beneath surface of water
( vehicle z positior/fDpyrighted Material
provides depth of vehicle beneath surface of wa­
In this section, we focus on representing and maintaining the bot­
tom terrain map, with an emphasis on confidence-based mapping in
an underwater environment from a sequence of data acquired by six
sonar sensors ( five forward-looking obstacle avoidance sonars and one
downward-looking depth sonar ) . As the vehicle moves, the information
gained from the sonars is used to build an understanding of the environ­
ment. Each sonar reading is modeled as a cone, and the positions of the
The world model has two types of data for its mapping scheme: a set
of global maps, each of which contains data for the vehicle's operational
domain, and region-of-interest maps, which only store a localized area
around the vehicle's current location. The global maps include underwa­
ter terrain elevation data and several overlay feature maps which include
data on soil, vegetation, ridges, ravines, landmarks, obstacles, defense
points, and transponders. The local maps include terrain elevation and
The region quadtree ( Samet 1984) is used to represent terrain elevation
in the global maps. The advantages of using a quadtree are that large
uniform areas in the map can be described compactly by a small number
of large quadrants and that information retrieval is fast since the number
of levels in the quad tree is related logarithmically to the resolution of
the tree. In addition to the quadtrees used to represent elevation, point­
and line-storing quadtrees have been implemented to provide locations
of known objects and topographic features of the lake bottom used in
high-level planning. Because the local maps are updated every time new
sensor data are obtained, we represent them as grid structures, which
The environment for the MAUV project is Lake Winnipesaukee in New
Hampshire. A priori data from a survey of the lake bottom were collected
and converted to quadtree format. Figure 18. 16 shows the a priori data
A separate sensor quadtree is used to store higher-resolution depth
values collected frotilotiJWg/IIi1fBd MfJIIIJeri8'I during vehicle runs. Both
downward- and forward-looking sonars are used in refining the sensor
map. A third quadtree stores a depth confidence value for each node
in the tree. The confidence map supports the function of distinguish­
ing spurious sonar readings caused by debris or signal inconsistencies
from actual obstacles that must be detected and avoided. The region
quadtree is particularly efficient for sensor and confidence map represen­
tation, since unexplored portions of those maps are empty. Such areas
can be represented by a small number of nodes in the tree
Point- and line-storing quadtrees (Samet 1984) provide locations of
known objects and topographic features of the lake bottom. These sim­
plify tasks such as locating the nearest other vehicle to a given location
or plotting a course along linear topographic features like ravines or
Different levels of the control hierarchy require different local map reso­
lutions. Also different types of data may be needed at each level. Gener­
ally, the resolution of the map at each level is about an order of magni­
tude less than the level below. All local maps are implemented as array
data structures and only the lowest level (highest resolution) local map
updates the global quadCepyn§lgett MBlUridtows the mapping hierarchy
for a generalized data set. Arrays are used for their fast , constant ac­
cess and update time and for ease of implementation. Local maps are
generated from the global quadtree database, first by extracting a priori
map data for the region, then overlaying the data stored in the sensor
and confidence quadtrees , which are presumed to be more accurate than
the lake survey information. In fusing the three sets of data, all three
quadtrees are traversed over the local map region. Because the updat­
ing algorithm only stores data in the sensor quadtree if the confidence
measure is above the level assigned to the a priori data, any node for
which there are sensor data uses the sensed value. The local map uses
a priori knowledge only if insufficient sensor data have been collected
for that node. Confidence quadtree values are also copied into the local
In the current implementation, the mission-level map divides the area
into a coarse grid of approximately 25 x 25 pixels, each pixel storing
the average depth of the corresponding area. The next two levels in the
hierarchy, the group and vehicle levels respectively, share the same local
map for this data set. Each pixel of the local map stores the minimum
and maximum known depths over a 4 x 4 meter area. It serves the
purpose of providing information for high-level navigation tasks, such
as determining the probability that an area is traversable by one or
more vehicles. This map is updated as new information is added to
the lowest-level map, the e-move map. The e-move local map has the
highest resolution (each grid square represents a 0.5 x 0.5 meter area) ,
and is used in determining the traversability of a path between two
specified points. The world model returns a probability that the path
is traversable based on the information in this map. For example, the
output may be a percentage of pixels for which the vehicle clears the
lake bottom over the hypothesized path. In the simplest case, the world
model can provide a probability of 1 if all of the pixels are traversable,
or 0 if any are obstructed. Typically, the e-move pilot planner will query
the world model for the traversability of several paths, using A. search
to choose the best path. The e-move map is also the level updated
directly by sensor readings; its modifications are propagated up through
At the beginning of a mission, the MAUV control system initializes
the global and local ma]?s, re�ding availab �e a priori knowledge from a
secondary storage devi�PArfJ9!:MnQ#li!J.�/ee is initially composed of
a single, empty node, though it could also contain sensor data stored
from previous missions if available. Likewise, the confidence quadtree
is initialized as a single node containing a base confidence value, unless
there is confidence data from a previous mission. In general, the world
model starts up in a state of total dependence on a priori knowledge,
gradually becoming more reliant on the current sensor map as data are
In updating the map from downward-looking sonar data, the algo­
rithm first computes an approximate neighborhood size of pixels to be
updated around the current vehicle location which depends on the width
of the sonar beam and the distance to the lake bottom. Given that the
beam width is fixed and the range is returned by the sensor, a closed­
form trigonometric solution can be performed using a lookup table. Al­
though the 2-D projection would be best represented as a circular region,
for our purposes, a square neighborhood is sufficiently accurate and more
efficient to update. The depth stored at each pixel of the neighborhood
in the local map is compared to the observed sonar reading. If the two
values are not within an acceptable margin of error, the conflicting data
cause the pixel's confidence to be lowered. If the two depth values are
in agreement, the confidence value is incremented unless it has already
reached the maximum allowed. Whenever a pixel's confidence value
drops below the predefined threshold, it takes on the new depth reading
and is assigned a base confidence value (Figure 15. 18) . For the depth
sonar, all information is classifiable as either conflicting or agreeing with
the knowledge already in the model. None of the data are irrelevant in
The obstacle avoidance (forward-looking) sonar mapping algorithm
is more complicated. Here the projection of the cone into the two­
dimensional plane approximates a triangular region. The cone itself is
approximated by two planar surfaces representing the top and bottom
surfaces of the cone. Due to the relatively coarse resolution used in the
obstacle avoidance algorithm (O.5m3 per pixel) and the narrow width
of a sonar beam, this does not introduce significant error into the cal­
culations. As with the depth sonar algorithm, each pixel in the two
dimensional projection is examined and updated if its confidence value
drops below the threshold. Forward-looking sonar readings provide two
types of information: a given pixel may be clear, or it may be obstructed
by an obstacle. When the vehicle detects an obstacle, the mapping al­
gorithm adds the information to the local map by raising the modeled
bottom of the lake at that location (i.e. , making it shallower, see figure
Three stages of a map update from depth sonar:
It is also an essential function of the world model to be able to re­
move hypothesized obstacles in the local map as weB as add them. For
each pixel in the triangular projection, if the three-dimensional distance
(measured along the cone trajectory) from the sonar source to the cur­
rent pixel being examined is less than the range returned by the sensor,
the pixel is assumed to be clear. No obstacle was detected there, so the
depth at that location in the local map should reflect this information.
Its value should be greater than or equal to the depth of the bottom sur­
face of the sonar cone at the location, since any object obstructing the
beam would presumably cause the sensor to return the range to that ob­
ject. If the local map value is shallower than the beam, it conflicts with
the new sensor data and the confidence value is decremented. If this re­
sults in a confidence lower than the threshold, the pixel is reassigned the
depth value of the bottom surface of the cone and a new base confidence
value. Note however that a local map value in agreement with sonar in­
formation does not necessarily increase its confidence. The sonar beam
may be projected in front of the vehicle when it is near the surface, and
a clear reading near the surface would not yield any information about
the depth of the lake bottom if we already have some a priori knowledge
that the lake is approximately N meters deep. In this case it would be
The same is not true for pixels in the projection whose distance from
the sonar source is greater than or equal to the range returned by the
sensor. These pixels corresp<?nd to detec�ed obstacles and the depth
values in the local mafRRYQ9lltp�Malf¥ii:He top surface of the cone.
Updates from obstacle avoidance sonars remove false obstacles i n the world model
by increasing depth values in the map . Obstacles are added by decreasing the
depth, in effect , raising the bottom of the lake model.
Here the local map data should be at lease as shallow as the top surface
of the beam to be in agreement with the sensor reading . If the map
data does agree, it represents confi rmation of an existing obstacle and
the confidence value should be incremented. It should be noted that this
confirmation only supports the hypothesis that there is an obstacle at
the depth it was detected; no concl usions can be drawn as to the true
height of the object or whether it extends all the way to the lake bottom.
In a similar manner, if the model continually disagrees with the sensor
reading, the confidence is decremented until the depth value is reassigned
to the depth of the top surface of the cone, making the model shallower.
Its confidence is again initialized to a base value. Further details about
the world model may be found in Orser and Roche (1987) and Oskard,
An important issue for real-time control is timing of processes. In dis­
cussing the timing in the MAUV system, we consider the following fac­
tors at each level of the hierarchy: executor cycle period, input command
update interval, replanning interval, and planning horizon.
The input command update interval is the rate at which new com­
mands are input into a give!l level from t.he level above. The replanning
interval is how often f:ffiP�m Ma��n level do cyclic replanning.
The planning horizon is the amount of time into t he future covered by a
plan at a given level. The executor cycle period at each level is t he rate
at which the executor checks to see whether a new output command is
to be sent to the level below. This cycle period is relatively fast. Table
shows t hese values for each level of the hierarchy.
The executor cycle period at each level is the same-600 msec. This is
the rate at which new sensor data are collected. Therefore , the executor
need not cycle faster t han this since it will not determine that there can
be a new output command unless new information about t he world is
known. The input command update interval increases by about a factor
above represent approximate average times .
which new input commands can be received can be as fast as 600 msec
cycle period ) at any level. However, we do not expect this
The replanning interval at a given level is the same as the output
command update interval at that level. In this way, the planners attempt
to replan before each next command is determined.
The planning horizon at a given level is about twice the input com­
mand update interval at that level. Each planner therefore generates a
plan that represents a decomposition of the current input command as
The control system was implemented on the computing systems shown
in figure 1B.20. In each vehicle, a VME bus supports high bandwidth
communication between sensory processing, world modeling, planning,
and execution modules at each level of the hierarchy. These modules
are partitioned among three separate single-board Ironies computers so
as to maximize the use of parallel computation. A two-megabyte com­
mon memory board is used for communication between processes, and
an BOO-megabyte optical disk is used for mass storage. The real-time
multiprocessor, multitasking operating system used is pSOS .
Also shown i n figure 18.20 i s the software development and simula­
tion environment. A variety of computers, including Sun workstations,
a VAX 1 1/785, a micro-VAX, IRIS graphics systems, PCs, D uals, and
Ironies development systems are tied into the development environment
for code development and simulation. Once the software has been trans­
lated to run on the Ironies Unix-based development system, it can be
compiled to run under pSOS and downloaded into the 68020 target hard­
This section describes some initial experimental results on lake tests
performed with one of the MAUV vehicles. These tests were performed
during October 1 987. Due to lack of continued funding, the MAUV
project was terminated in December 19B7. We were therefore unable to
perform all of the demonstration scenarios described in the Introduction.
The lake tests were performed at Lake Winnipesaukee and were run
using code at the servo, primitive, and e-move levels. The first exper­
iment involved local obstacle avoidance. Figure 1B.21 shows the path
executed by the vehicle during a test run in which an obstacle was man­
ually entered into the wb1taJ!fi98"�W�int C, and the vehicle was
On the left i s the target hardware for t h e t w o M A U V veh icles. On t h e right is the
MAUV software development and simulation environment.
commanded to go from point A to point B. The control system success­
fully planned and executed a path around the obstacle at point C.
The second experiment involved following along a predefined path.
Figure 18.22 shows a raster-scan path from point A to point B. The
vehicle determined its x, y position from on board acoustic navigation
transponders which receive signals from navigation buoys placed in the
water. The actual path executed by the vehicle during this run is shown
in figure 18.23. One of the obvious problems brought out by this run
is that the vehicle tends to overshoot when it makes turns. This is a
problem with the current low-level control, which allows position control
but not velocity control. Because the velocity is at maximum value when
it takes a turn, it will always overshoot. Also, there is considerable error
in the position measuring transponders, which largely accounts for the
The third experiment involved updating the internal model of the
lake bottom with altitude information obtained from the downward­
looking depth sonar. Figure 18.24 shows three graphs. The top and
middle graphs display the x and y positions, respectively, of the vehicle
path. The bottom graph shows the lake depth values obtained from the
world model along this path after the world model is updated from the
The achievement of real-time intelligent control for autonomous vehicles
will require a system that integrates artificial intelligence with modern
control theory, and that can be implemented on parallel, possibly special­
purpose hardware. This chapter has presented the basic components of
such a system, and has presented a hierarchical control system archi­
tecture that can Serve to integrate the various components. A first cut
at the algorithms and software for many of these components has been
developed and is presented here. However, many of these algorithms
need to be improved to handle more complex scenarios.
A major problem is the achievement of real-time performance. Al­
though the multiprocessor computing system described here can serve
as a good basis for achieving real-time performance, we believe that
it must be augmented with special-purpose hardware such as real-time
sensory processing devices (e.g. , PIPE; Kent, Shneier, and Lumia 1985)
and massively parallel device!! (e;!;.J neur� networks) . Generally, such
special-purpose hardwa�martlt'd�{ he functions of one or two
I II 1 1 ' 1 ' 1 ' 1 ' 1 ' 1 ' 1 ' 1 1 1 1 1 ' 1 ' 1 ' 1 ' 1 ' 1 ' 1 ' 1 ' 1 1 1 , I
modules in the hierarchical control system architecture, and can thus fit
very elegantly into a system that implements this architecture.
Another maj or problem of intelligent control is that of learning.
Learning and the ability to generalize are very important for autonomous
systems that must operate effi ciently in a wide variety of situations in a
complex real-world environment. We feel that neural network systems
offer promising approaches to this problem. Again, these systems can
fit very nicely i nto our hierarchical control system architecture.
The followi ng people at Che Na1A�ncf��titule of Standards and Tech­
nology made signi fi c ant (�WJt1lmul1ons ct;�'l�e MAUV project: Hoosh
Abrishamian, Mike Ali, Shu-jen Chang, Ken Goodwin, Hui-Min Huang,
Maris Juberts, Steve Legowik, Peter Mansbach, John Michaloski, Don
Orser, Dave Oskard, Rick Quintero, Marty Roche, Mark Rosol, Scott
Swetz, Tsung-Ming Tsai, Barry Warsaw, Tom Wheatley.
The following people at the University of New Hampshire contributed
significantly to this project: Richard Blidberg, Jim Jalbert, Steve Chap­
pell, Mike Shevenell, Dennis Stamulis, Bob Welsh, Rod Haywood.
Significant contributions were also provided by George Pugh and Joe
Krupp of Decision Science Applications, and Bob Finkelstein of Robotic
The MAUV project was funded by the Defense Advanced Research
Projects Agency (DARPA) Naval Technology Office.
Identification of commercial equipment in this paper is only for ade­
quate description of our work. It does not imply recommendation by the
National Institute of Standards and Technology, nor that this equipment
was necessarily the best available for the purpose.
Albus, J. S. ( 1 987) . A Control System Architecture for Intelligent Ma­
chine Systems. Paper presented at IEEE Conference on Systems,
Man, and Cybernetics, Arlington, VA, October.
Albus, J. S . ( 1988) . System description and design architecture for mul­
tiple autonomous undersea vehicles. NIST Technical Note 1 25 1 ,
National Institute o f Standards and Technology, Gaithersburg, MD,
Blidberg, D. R. , and Chappel, S. G. ( 1 986) Guidance and control archi­
tecture for the EAVE vehicle. IEEE Journal of Oceanic Engineering.
Kent, E . W . , Shneier, M . O . , and Lumia, R . ( 1985 ) . PIPE (pipelined
image processing engine) . Journal of Parallel and Distributed Com­
Nilsson, N. J. ( 1 971 ) . Problem-Solving Methods in A rtificial Intelligence.
Orser, D . J . , and Roche, M. (1987) . The extraction of topographic
features in support of automnomous underwater vehicle navigation.
In Proceedings of the Fifth International Symposium on Unmanned
Untethered Submersible Technology, University of New Hampshire,
Oskard, D . N . , Hong, T. -H., and Shaffer, C. A. ( 1 988) . Spatial mapping
system for autonomous underwater vehicles. In Proceedings of the
SPIE Conference on Sensor Fus ion: Spatial Reasoning and Scene
Pugh, G . E. , and Krupp, J. C. ( 1 987) . A value-driven control system
for the coordination of autonomous cooperating underwater vehicles.
Paper presented at Fourteenth A nnual Symposium of the Association
for Unmanned Vehicle Sys tems . Washington, DC, July.
Samet, H. (1984) . The quadtree and related hierarchical data structures.
A CM Computing Surveys , 16(2 ) : 187-260.
Simpson, J. A . , Hocken, R. J., and Albus, J. S. ( 1 983) . The automated
manufacturing research facility of the National Bureau of Standards.
Journal of Manufacturing Systems, 1 ( 1 ) : 1 7-32.
Charles W. Anderson and W. Thomas Miller, III
In this appendix, we present a number of control problems as challenges
to experimenters wishing to explore new ideas for building automatic
controllers that improve their performance by learning from experience.
The problems were chosen for the simplicity with which they can be
stated and modeled and for their relevance to difficulties encountered in
real control situations. The difficulties addressed by this collection of
problems include incomplete system knowledge, nonlinearity, noise, and
Two of the problems are taken from previous chapters-Ungar's biore­
actor ( chapter 16) and Jorgensen and Schley's autolander ( chapter 17)
These problems are repeated here in a concise, consistent, and com­
plete format for the purpose of providing all of the details that would
be needed by anyone wishing to test the performance of a controller on
these problems in simulation. Particular constraints, parameter values,
and control objectives are suggested to encourage researchers to per­
form the same experiments and thus facilitate quantitative performance
comparisons of different control techniques.
Additional problems provided in this appendix concern balancing a
pole, steering a tractor-trailer truck, steering a ship, and moving a
robotic manipulator. These problems have been the subjects of research
in neural network control methods or in other forms of learning control.
Two more problems are from a comparative demonstration of adaptive
control design techniques presented at the 1988 American Control Con­
Each problem description includes a short introduction, the definition
of the plant to be controlled, controller input and output variables, and
discussions of the problem's objective, relevance, and possible exten­
sions. Previous results are referenced if they exist. A plant is specified
by its state variables, constraints, equations of motion, and parameters.
Note that the plant details are for the sake of its simulation; ideally,
a learning controller would function even when this knowledge of the
plant is incomplete or incorrect. The problems are expressed in discrete
time to ease the task of simulating the plants on a digital computer. The
definitions of some of the plants are simplified by the use of intermediate
variables that do not represent part of the plant's state. Intermediate
variables are recognized by the lack of a time index; only state variables
are indexed by time. T�ftd :lIM��nted in the order in which
they should be evaluated, with the exception of the parameter values
We hope that these problems will prove useful as test beds for com­
paring the performance of neural network techniques for learning control
with other approaches to control design. The best reported performance
on these and similar control problems will serve as challenges to others
and as benchmarks against which further results can be compared.
The bioreactor (see chapter 16) is a tank containing water, nutrients,
and biological cells as shown in figure A. 1. Nutrients and cells are intro­
duced into the tank where the cells mix with the nutrients. The state
of this process is characterized by the number of cells and the amount
of nutrients. The volume in the tank is maintained at a constant level
by removing tank contents at a rate equal to the incoming rate. This
rate is called the flow rate and is the variable by which the bioreactor
is controlled. The bioreactor control problem is to maintain the amount
a random variable from uniform distribution
C2 [O] a random variable from uniform distribution
r[O] a random variable from uniform distribution
The bioreactor is a tank of a liquid mixture of cells and nutrients. The objective is
to control the amount of cells by adjusting the flow rate.
The objective is to achieve and maintain a desired cell amount, ei[t], by
altering the flow rate throughout a learning trial. If T is the number of
time steps in a trial, then the objective is to minimize the cumulative
T should be on the order of 5000, which is equivalent to 50 seconds.
After T time steps have elapsed, the state of the bioreacter is reset in
the manner used to generate the initial conditions and the controller
again attempts to maintain the desired cell amount. This procedure is
repeated for some number of trials. The cumulative errors for each trial
can be averaged, perhaps weighting the errors for the most recent trials
more heavily, to form a final performance measure.
Ungar defined three kinds of bioreactor control problems. In the
first problem, the bioreactor is started in a region of the state space
from which a stable state is easily achieved. Let ei, C;, and r*
be desired values of the state and flow rate. For the first problem,
(ci,ei) (0.1207,0.8801) and r* 0.75. For these values, (ci,c;) is a
stable state. Recall that the initial conditions specify that cdO), C2[O),
For the second problem, the desired state is (ci, C;) (0.2107, 0.7226)
1.25. The fact that state (0.2107,0.7226) is unstable makes
The third problem is a combination of the first two. The desired state
is first set to a stable value, (ci,c;) (0.1237,0.8760) with r* 1/1.3.
Then, after 100 control intervals, which equals 50 seconds, 0.05 is added
to ei, giving ei = 0.1737CiJi:fYf;gMfHl _�em from one of controlling
about a stable desired state to one involving an unstable desired state.
One increment in the value of ci is sufficient.
The bioreactor is a challenging problem for neural network controllers
for several reasons. Although the task involves few variables and is eas­
ily simulated, its nonlinearity makes it difficult to control. For example,
small changes in parameters value can cause the bioreactor to become
unstable. The issues of delay, nonlinearty, and instability can be stud­
ied with the bioreactor control problem. Significant delays exist between
changes in flow rate and the response in cell concentration. Nonlinear­
ities in the bioreactor's dynamics present a challenge to networks for
learning nonlinear models. Neural networks that learn to compensate
for deficiencies in the performance of conventional controllers for this
task can be tested. This is also a good problem for investigating com­
binations of methods for predicting future states with controllers that
learn to avoid unstable regions of the state space.
The bioreactor easily satisfies our goals of relevance to real-world prob­
lems. Improvements in bioreactor control techniques can result in sig­
nificant savings to the biochemical industries.
In real bioreactors additional controls are available. The contents of a
bioreactor are often heated and cooled to maintain the temperature at a
level most conducive to cell growth. Also, since cell metabolism depends
on contact between cells and nutrients, the contents are often stirred.
Models incorporating temperature and stirring controls would result in
more challenging and realistic multiple-control problem.
Other extensions involve more realistic models of concentration mea­
surements. The accuracy of these measurements depends on the accu­
racy of the measurement device and on how well mixed the bioreactor's
contents are. As a first step, noise can be added to the concentration
Agrawal, P., Lee, C., Lim, H. C., and Ramkrishna, D. (1982). theo­
retical investigations of dynamic behavior of isothermal Continuous
Stirred Tank Biological Reactors. Chemical Engineering Science, 37
Ungar, L. H., Powell, B. A., and Kamens, S. (1990). adaptive networks
for fault diagnosis and process control. Computers and Chemical
The autolander problem of Jorgensen and Schley (chapter 17) concerns
the landing of an aircraft as it is subjected to wind disturbances. The air­
craft is represented by a linearized model with parameter values chosen
to match the model to a commercial aircraft. The aircraft model includes
two feedback controllers typically found in commercial aircraft-an au­
tothrottle and a pitch autopilot. The aircraft's descent is controlled by
specifying the desired elevator angle to the pitch autopilot. The speed
of the aircraft is maintained at a constant value by the autothrottle.
Input to an autoland controller includes the aircraft's altitude and
vertical speed and the desired values of these variables obtained from an
Instrument Landing System (ILS). The ILS determines a trajectory such
as the one shown in figure A.2. The controller must generate a sequence
of desired elevator angles that result in the aircraft touching down on
the runway within the given ranges of horizontal position, speed, and
horizontal position as negative of ground distance to
An autolander must adjust the elevators of the aircraft in order to guide the plane
along a trajectory that is as close as possible to that specified by the Instrument
(Zq - Uo1l' /180)q[tj + 9 sin ,0[t]1l'/180+
current altitude rate of change (defined above)
he desired altitude rate of change (defined below)
The desired altitude and altitude rate of change are determined by
Equations of motion for plant must be calculated first to
-1.5 ftls desired altitude rate of change on
Let T be the time step at which the airplane either lands or crashes,
i.e. , h[T] :s: O. The landing is judged by four performance measures.
Listed in their order of importance, they are the plane's vertical speed,
horizontal position, pitch, and horizontal speed. They are defined as
horizontal position: - 300:S: x[T] :s: 1000
Flying aircraft are subject to wind disturbances that can be fatal when
they occur close to the ground while landing. Autoland systems that are
routinely employed as commercial aircraft are not designed to handle
large wind gusts that do ocassionally occur. Systems that could learn to
improve the performance of a current autoland controller in large wind
conditions stand to increase the reliability and safety of landing.
Holley, W. E. (1975) . Wind modeling and lateral control for automatic
landing. Ph.D. diss.c;�lI�, Stanford, CA.
Neuman, F., Foster, J. D. ( 1 970). Investigation of a digital automatic
aircraft landing system in turbulence. NASA Technical Note TN
D-6066, Ames Research Center, NASA/Moffet Field, CA. October.
Pallett, E. H. J. ( 1983). Autopilot logic for flare maneuver of stol air­
craft. in Automatic Flight Control. 2nd ed . London: Grenada.
The pole-balancing problem is the problem of learning to balance an
upright pole, sometimes called an inverted pendulum. The bottom of
the pole is attached by a pivot to a cart that travels along a track as
shown in figure A.3. Movement of both cart and pole is constrained to
the vertical plane. The state of this system is given by the pole's angle
and angular velocity and the cart's horizontal position and velocity. The
only available control actions are to exert forces of fixed magnitude on
the cart that push it to the left or right.
The event of the pole falling past a certain angle or the cart running
into the bounds of its track is called a failure. A sequence of forces must
be applied that avoid failure as much as possible by balancing the pole in
the center of the track. A naive controller, before learning much about
this task, will be unable to avoid failures. The pole and cart system is
reset to its initial state after each failure and the controller must learn
to balance the pole for as long as possible.
angle of pole from upright position (degrees)
The objective of the pole-balancing problem is to keep the pole upright and the
-2.4 < x < 2.4 m Center of cart cannot exceed limits
f -10 or 10 N fixed-magnitude force (bang-bang
J[t] + mpl ( (0 [t] 1r /I80? sin O[t] - e[t]1r /180 cos O[t]
0.02 s ( equal to the sampling interval, �)
mg sin O[t] - cos O[t] (/[t] + mpl (O[t]1r /180)2 sin O[t]
The objective of this problem is to avoid failures. Note that this ob­
jective is equally satisfied by balancing the pole within a very narrow
or a wide range in angle about the upright position as long as failure
is avoided. This objective can be formalized by defining a failure signal
over the length of an experiment of T time steps.
A measure of progress during an experiment is the number of steps
between failures, i.e., the balancing time until failure. The level of per­
formance achieved at the conclusion of a run is the balancing time since
The inverted pendulum is one of the simplest inherently unstable sys­
tems. It has been used to demonstrate a number of conventional control
techniques (Cannon 1967, Cheok and Loh 1987, Eastwood 1968, Roberge
1960). The relatively large set of conventional controllers for this prob­
lem can be the basis for comparisons to neural network control methods
and for the development of hybrid schemes incorporating conventional
There are also considerable precedents for the application of neural
networks to this problem, referred to in the following Results section.
Results already exist with which novel approaches can be compared.
This is a failure avoidance task that involves an unstable system and is
therefore related to a wide class of real problems requiring the avoidance
of costly or harmful conditions, as in the control of power generation and
flight control. Another advantage of this task is that many extensions
can be made that facilitate the exploration of a number of different
This problem can be extended in many ways. Friction can be added to
the pole's pivot and to the cart's wheels. Other disturbances, like wind
effects and inclinations of the track, can also be added. Such extensions
can be used to compare conventional adaptive control techniques for
dealing with unknown disturbances with neural network controllers.
Other extensions result from different contraints and objectives. The
fixed-magnitude force can be replaced by a real-valued force bounded
within a realistic range. Linear control laws can be used to balance the
pole when restricted to small angles as described here. A pole allowed
to swing through 360 degrees would require a nonlinear control law.
Multilayer networks for learning nonlinear control laws could be tested
on this extension to the problem. More than one pole can be either
mounted on the cart or stacked one upon the other. Another extension
is to use different actuaE&p� jfafjeRllile
velocity of a wheel motor can be controlled to move the cart back and
The balancing problem becomes much more challenging when multiple
tasks are defined. For example, if the pole is allowed to swing in a
complete circle, the controller can be given the tasks of either balancing
the pole or spinning it at a given velocity, depending on the value of a
command input specifying which goal is desired.
Finally, a major extension is to move out of the world of simulation
into the real world. Many physical pole and cart systems have been
constructed and interfaced to computer control as part of educational
courses on real-time control. The real-world system will undoubtedly
exhibit difficulties not captured in simulation.
A number of results have been obtained using additional knowledge not
included in the above problem description. Guez and Selinsky ( 1 988),
Tolat and Widrow ( 1987), and Widrow and Smith ( 1 964) applied su­
pervised learning methods to either learn to mimic a human controller
or a given control law. Widrow ( 1 988) showed how supervised learn­
ing can be used when the requirement of an existing controller is re­
placed with the information that the desired state of the pole and cart
Methods for learning solely from the relatively un-informative failure
signal have been applied to the pole balancing problem by Michie and
Chambers (1968), Barto, Sutton, and Anderson (1983), Sutton (1984),
Selfridge, Sutton, and Barto (1985), and Anderson (1987, 1989). In
Barto, Sutton, and Anderson (1983) a neural network reliably learned
to balance a simulated pole for at least 30 minutes after experiencing
Anderson, C. W. ( 1987). Strategy learning with multilayer connection­
ist representations. Technical Report TR87-509.3, GTE Laborato­
ries, Inc. Waltham, MA. (Corrected version of report published
in Proceedings of the Fourth International Workshop on Machine
Learning: 103-1 14. ODpJI(igIiUJQ Material
Anderson, C. W. ( 1989 ) . Learning to control an inverted pendulum
using neural networks. IEEE Control Systems Magazine, 9 ( 3 ) : 3137.
Barto, A. G., Sutton, R. S . , and Anderson, C. W. ( 1 983 ) . Neuronlike
adaptive elements that can solve difficult learning control problems.
IEEE Transactions on Systems, Man, Cybernetics, 1 3 (5): 834-846.
Bridge,!. D. ( 1988 ) . Learning to control a dynamically unstable sys­
tem. Master's thesis, College of Manufacturing, Cranfield Institute
Cannon, R. H . , Jr. ( 1967) . Dynamics of physical systems, New York:
Cheok, K. C. and Loh, N. K . ( 1987 ) . A ball-balancing demonstration
of optimal and disturbance-accommodating control. IEEE Control
Connell, M. E. and Utgoff, P. E. ( 1987) . Learning to control a dynamic
physical system. In Proc. AAAI-87 2: 456-460. American Associa­
tion for Artificial Intelligence, Seattle, WA.
Eastwood, E. ( 1968 ) . Control theory and the engineer. Proceedings
Guez, A . and Selinsky, J . ( 1988 ) . A trainable neuromorphic controller.
Journal of Robotic Systems, 5 (4): 363-388.
Higdon, D. T. and Cannon, R. H., Jr. ( 1 963 ) . On the control of unstable
multiple output mechanical systems. Paper presented at the ASME
J0rgensen, V. ( 1974 ) . A ball-balancing system for demonstration of basic
concepts in the state-space control theory. International Journal of
Electrical Engineering Education, 11: 367-376.
Michie, D. and Chambers, R. A. ( 1968 ) . BOXES: An experiment in
adaptive control. E. Dale and D . Michie, eds. , Machine Intelligence,
2: 137-152. EdinburEh, Scotland: Oliver and Boyd.
sachusetts Institute of Technology, Cambridge, MA.
Rosen , B. E., Goodwin , J. M. , and Vidal, J .
learning. Paper presented at First Annual International Neural Net­
work Society Meeting, Boston, MA, September.
Experimental results from an evaluation of algo­
Fifth International Conference on Machine Learning: 437-443. Ann
rithms that learn to control dynamic systems. In
Arbor, MI., June. San Mateo, CA: Morgan Kaufman.
tional Joint Conference on Artificial Intelligence , Los Angeles, CA.,
(1984) . Temporal credit assignment in reinforcement learn­
Ph.D. diss., COINS Technical Report 84-02, University of Mas­
Never at rest: learning to control a dynamically un­
stable system . Master's thesis, College of Manufacturing, Cranfield
(1987). The original adaptive neural net broom-balancer. In
International Symposium on Circuits and Systems, 35 1 -357 , May.
( 1 964) . Pattern-recognizing control sys­
Computer and Information Sciences (COINS) Sym­
posium Proceedings. Washington, DC: Spartan.
The tractor-trailer truck backs up at a constant speed. The objective is to position
Nguyen and Widrow (1989) demonstrated the application of a neural
network to the problem of learning to steer a tractor-trailer truck back­
ing up at a constant speed. Their problem definition, with minor modi­
fications, is repeated here. The cab's front wheels move a fixed distance
backward with each step. Steering is accomplished by changing the an­
gle of the front tires with respect to the orientation of the cab. The goal
is to guide the back of the trailer to a p oint on a loading dock with the
trailer perpendicular to the dock. Refer to the diagram in figure AA to
relate the following state variables to the physical layout of the problem.
x, y coordinates of center of rear of trailer ( m )
angle of trailer, measured from positive x with
counterclockwise being positive ( degrees )
angle of cab, measured from positive x with coun­
u steering angle of front wheels relative to cab orienta­
tion, counterclockwise positive ( degrees )
0.2 m distance front tires move in one time step
de 6.0 m length of cab, from pivot to front axle
ds 14.0 m length of trailer, trailer rear to pivot
the cab. The cab starts in a random position
and orientation relative to the dock, with a random angle between the
cab and trailer. Each trial terminates when any corner of the trailer or
trial is terminated. The squared error in the positioning of the truck is
respectively. Additionally, the desire for a minimum time solution can
The truck backer-upper is representative of many sequential decision
problems. The control decisions made early in the backing up process
have substantial effects upon final results. Early moves may not always
to reduce error, but they position the truck and trailer
In many respects, this truck steering problem re­
control strat egy that is like a dynamic programming problem
would then terminate either by contacting the loading dock or
of the obstacles. The same set of fixed obstacles could be utilized for
New positions for the stationary obstacles could be determined for each
The obstacles could also be allowed to move dur ing each trial, providing
The objective can be modified to incorporate the desire for particular
truck paths over others in addition to the goal of reaching the loading
dock. The results of learning can be biased toward shorter paths or
paths requiring smaller steering angles by adding appropriate terms to
Computer simulations of the truck and a neural network controller by
Nguyen and Widrow (1989 and Chapter 12) have demonstrated work­
ability, although no mathematical proof yet exists. Their approach in­
volved the combination of a neural network that learned to generate
good steering commands and a second neural network that learned to
predict the next state of the truck. After much experience, the truck
could be initially "jackknifed" and aimed in many different directions,
toward and away from the dock, but as long as there was sufficient
clearance the controller appeared to be capable of finding a solution.
Nguyen, D. and Widrow, B. (1989). The Truck Backer-Upper: An Ex­
ample of Self-Learning in Neural Networks. In Proceedings of the
International Joint Conference on Neural Networks. Washington,
Figure A.5a shows an overhead view of a ship cruising on an ocean.
The ship's state is given by its position, orientation, and turning rate.
The ship starts at a particular position and orientation and is to be
maneuvered at a constant speed through a sequence of gates. In a real
ship, the turning rate would be indirectly controlled by changing the
rudder angle. Here, a desired turning rate is specified directly by the
controller. There is a time lag between changes in the desired turning
rate and the actual rate, modeling the effects of a real ship's inertia and
The ship must be steered through a sequence
the actual turning rate of the ship (degrees/s)
the desired turning rate of the ship (degrees/s)
15 degrees/s ship cannot turn faster than
0.2 s (equal to the sampling interval, .:l)
The ship's center starts at coordinate (x, y)
O). The goal is to generate sequences of r values that
steer the center of the ship through a number of gates in a particular
A simple problem having a single gate should be attempted first. Place
the sides of the gate at coordinates (0.2,1) and (0.3,1). The ship must
be steered through this gate as quickly as possible. A typical trajectory
The problem becomes more challenging with additional gates. For
example, using the same final gate as before, we can add four gates with
sides at (0.8,0.3) and (0.9,0.3), (0.3,0.6) and (004,0.6), (0.5,0.7) and
(0.6,0.7), and (0.3,0.8) and (004,0.8), as shown in figure A.5c. This
sequence of gates is similar to the one used by Anzai (1984).
This problem is a good test of a controller's ability to deal with long
delays and to plan for future consequences. The delays involved in the
steering of mobile vehicles depend on the interactions between a vehicle
and its supporting medium. Nautical ships exhibit particularly long time
delays in response to rudder lI!0vements. P!anning for future encounters
with gates should be p�Q(llENll��t'Q8htrol decision, because the
ship's position and orientation as it moves through one gate can greatly
affect the ease of navigating through successive gates.
used this model of a ship steering problem to study
the development of cognitive strategies in the control of systems with
The evolution of ship trajectories during learning can be
compared to those published by Anzai that were exhibited by humans
as they learned to control Anzai's simulation.
As formulated, this problem requires a controller to learn to guide the
tiple sets of gates is realized by uniquely labeling each set of gates
providing the label for the current set of gates as an additional input
to the controller. Alternatively, one can attempt a solution to the more
general problem of deciding how to steer the ship when given some rep­
resentation of the position of the next gate and train on a wide variety
of gate sets. One representation that is similar to what the controller of
a ship might experience is the distance from the ship to the next gate
and the gate's direction relative to the ship's orientation. Including rel­
ative distances and directions to subsequent gates as input wou1d allow
a controller to optimize current actions for future encounters with those
Other simulated effects, such as water currents and wind gusts, can
be added to the model and treated either as unknown disturbances or
as environmental variables that are sensed by the ship. Other additions
to the model can simulate more accurately the interaction between the
ship's motion and its rudder, requiring a controller to set the rudder
rather than specify a desired turning rate.
Y. (1984). Cognitive control of real-time event-driven systems.
The control of this three-joint robotic manipulator must deal with complex
a typical five or six axis industrial robot is too
complicated to qualify as a straightforward control task for comparative
studies of alternative control techniques (Neuman and Murray 1987b),
though considerable effort has been applied to the problem of efficient
representation of manipulator dynamics, for both simulation and real­
time control applications (Luh, Walker, and Paul 1980, Neuman and
Murray 1987a). The three-axis robot described in this section (Morgan
and Ozguner, 1985) and shown in figure A.6 is a reasonable compromise
between system complexity (and thus realism) and ease of implementa­
tion. It is similar to the three major axes (base, upper arm, and forearm)
of typical industrial robots. The model is complete, in that all joint cou­
pling terms (centripetal and Coriolis torques, variable effective moments
of inertia, etc.) are included. It is still an idealized model, however, in
that all masses are assumed to be lumped at discrete points and effects
such as drive train stiction are not modeled.
fh angular position of robot base axis (radians)
02 angular elevation of upper arm above horizontal
01 angular velocity of robot base axis (radians/s)
O2 angular velocity of upper arm (radians/s)
Oa angular velocity of forearm (radians/s)
-B38� [tj - B4 8� [t] - K82 [tj + GI9 + T2 [tj
B38� [t] - B58� [t] - K83 [t] + G2 9 + T3 [t]
5 kg -+ 20kg point mass at end of arm ( including
0.01 s (10 times as long as the sampling interval, �)
els the limited resolution of joint-angle sensors. Let the
resolution be 0.0002 radians, so f(9i} E { ±0.0002nln
0, 1 , 2, . . . } . There are no other inputs. Sensors for joint
velocities and accelerations are assumed to be unavailable.
Ti [t] , i 1 , 2, 3, for t 0, 10, . . . and
Ti [t] Ti [t - 1 ] for other values of t.
Trajectory Following Task: The trajectory following task involves track­
ing predetermined trajectories (Oi tt], O2 ttl , OJ [t]) representing high-speed
movements of the arm. The trajectories used should traverse a wide
range of arm configurations with high velocities and accelerations rel­
ative to the maximum values possible given the torque limits on the
actuators. The controller should be able to accommodate variable pay­
load masses, M2 , within the specified range, either by including the mass
as an explicit network input or by adapting to each mass individually.
Performance should be evaluated both in terms of root-mean-square
position error, computed over each test trajectory, and in terms of max­
imum instantaneous position error during each trajectory. Let time be
indexed from 0 to T over a test trajectory and let N be the number of
control intervals within this period. Then the root-mean-square position
and the maximum instantaneous position error by
Tests should be designed to evaluate speed of convergence and absolute
performance using repetitions of a single trajectory, to evaluate learning
interference using multiple trajectories, and to evaluate generalization
Trajectory Planning Task: The same dynamic model can be used to
study optimal trajectory planning. The goal is to move the arm from an
initial position at time step t 0 to a final position at a specified time
step t = T, minimizing the objective function:
This is the minimum torque change criteria suggested by Uno,
Kawato, and Suzuki ( 1989; and see chapter 9 ) . Performance should
be evaluated both in terms of the objective function above and in terms
of the position error ;G,OI/If�flrMBtIJrial
The three-axis articulated manipulator model contains many of the same
characteristics as real-world control problems involving mechanical sys­
tems. T he parameter values have been chosen such that the nonlinear
joint coupling effects will be important and the control characteristics
will be sensitive to payload over the range specified. The problem dy­
namics are dependent on velocity and acceleration, but these quantities
cannot be measured in the model. Only serial position measurements are
available. Additional real world difficulties are included in the torque
limits and the fixed position measurement resolution.
involved in learning to control the dynamics of such a system with high
accuracy, over a reasonable subset of the operating space, with good
generalization to new movements and with good resistance to learning
The problem can be extended in many ways.
The actuator model can be changed to represent
the more complicated dynamics of pneumatic actuators.
the hand position can be measured in a Cartesian frame of reference
( with a limited measurement resolution such as 0 . 1 mm ) .
following and trajectory planning tasks can then be implemented in hand
coordinates rather than joint coordinates.
Different approaches to the control of simulated robot dynamics using
neural networks can be found in several places ( Kawato, Furukawa, and
vary, but are sufficiently similar to that described in t h is section to be
useful for obtaining insight into possible control architectures.
learn the dynamics of the cmu direct-drive arm II. CMU Robotics
Institute Report , Pittsburgh, PA, August.
Guez, A. and Selinsky, J. ( 1988) . A trainable neuromorphic controller.
Kawato, M . , FUrukawa, K . , and Suzuki, R. ( 1 987) . A hierarchical neural­
network model for control and learning of voluntary movement,"
Luh, J . Y. S., Walker, M. W. , and Paul, R. P. (1980) . On-line compu­
tational scheme for mechanical manipulators. Transactions of the
ASME Journal of Dynamic Systems, Measurement and Control, 1 20:
Miller, III, W.T . , Glanz, F.H . , Kraft, L.G . ( 1987) . Application of a gen­
eral learning algorithm to the control of robotic manipulators," In­
ternational Journal of Robotics Research, 6 (2): 84-98.
Morgan, R. G. and Ozguner, U. ( 1985). A decentralized variable struc­
ture control algorithm for robotic manipulators. IEEE Journal of
Neuman, C. P. and Murray, J. J. (1987a) . Customized computational
dynamics. Journal of Robotic Systems, 4: 503-526.
Neuman, C. P. and Murray, J. J. (1987b) . The complete dynamic model
and customized algorithms of the PUMA robot. IEEE Transactions
on Systems, Man and Cybernetics, 17(JulyjAugust): 635-644.
Uno, Y. , Kawato, M. and Suzuki, R. ( 1989) . Formulation and control of
optimal trajectory in human multijoint arm movement: Minimum
torque-change model," Biological Cybernetics, 6 1 : 89-101 .
Problems from the ACC Showcase of Adaptive
the 1988 American Control Conference in Atlanta, leading researchers
adaptive control assembled to present a "Showcase of Adaptive Con­
troller Designs" (Astrom 1987, Goodwin, Salgado, and Middleton 1981,
Huang and Morse 1 987, Johnson 1987, M'Saad, Landau, Samaan, and
Duque 1987, Masten anft��W7�Mlmflra and Duarte 1987) . The
purpose was to highlight current adaptive control techniques and to
enable comparisons of methodology and capabilities. Each group of re­
searchers applied their technique to two simple control problems. Results
of this comparitive study are presented in Masten and Cohen ( 1 989) .
The comparison of neural network control methods on these problems
to the results published in t he ACC showcase could initiate a series of
benchmark studies spanning the neural network and adaptive control
fields. Additional showcases are planned for future ACC conferences
and would be excellent forums for introducing neural network control
approaches to an audience of researchers active in the control field.
The ACC problem definitions are tailored to the application of adap­
tive control techniques. The structures of the plants to be controlled are
provided, along with the parameters of the plant. Control designers can
assume knowledge of the plant's structure and particular values of the
parameters. A command signal is provided as input to the controller .
The adaptive controllers are to respond to the command with control
actions that force the plant to follow a given "idealized" response, in
spite of variations in the plant parameters and unknown disturbances
To make these problems more suitable for the application of learning
methods, we make the following changes. The a priori knowledge of
plant structure and parameter value ranges are replaced with a phase of
learning during which the learning controller interacts with the simulated
plant operating in various parameter value regimes. The controller must
learn a control law that performs well under the experienced parameter
values and command and disturbance signals. The learning phase is
followed by a testing phase that is identical to the testing conditions
specified in the ACC problem definitions, including the parameter values
Our reformulation of the ACC problems are stated below. The two
problems differ only in their plant equations and ideal responses-one
problem involves a first-order plant arid the other a second-order plant.
d unknown disturbance added to controller output
d unknown disturbance added to controller out­
x[t] and x[t] (for the second-order plant)
The objective of this control problem, as stated in the ACC Showcase,
is to for ce the output of the plant to follow an ideal response, r , defined
for second-order problem: r[t + 1] r[t] + b. itt] ,
followed for the given command and disturbance trajectories spanning a
testing phase of 20 seconds. Let to be the initial time step of the testing
phase and tf be the final time step, i.e., tf to + 20/�. The goal, then,
These signals are piecewise constantj c changes value at 8 seconds and
In the ACC showcase, the goal was to design controllers that behave
well for a range of parameter values. In order for a learning controller
to converge on a control law that results in good performance, it must
have experience with the plant when it is operating under a variety of
The parameter values can be changed in many ways. For example,
their ranges can be divided into a number of equally spaced values. The
parameter values can remain constant for short periods of time and then
one parameter value can be changed to its next higher or lower value.
The set of possible parameter values are stepped through until every
combination of values has been experienced, after which the cycle is
repeated. A random sequence of parameter values can be generated by
periodically setting the parameters to values selected from a uniform
probability distribution over their allowable ranges. An alternative is
for the parameter values to take uniform random walks. It is important
to specify in any report of experimental results the method used to vary
In addition to various parameter values, the controller should also
be exposed to a large variety of commands and disturbances during
learning. The set of p<fSQp}41glJtMlitfSteti8J disturbance signals is not
constrained by the ACC showcase problem definitions, except for the
particular trajectories to be used for testing the controller. One rea­
sonable assumption is that they should not exceed the values used for
testing. Thus, commands can be limited to the range [-1, 1] and distur­
One strategy for varying commands and disturbances is to change
their values at random times, setting them to values selected randomly
from the above ranges. Additional assumptions can be made about the
maximum frequency of changes, since the command and disturbance
vary only once and twice, respectively, during the 20 second testing
There are two philosophies of adaptation to unknown variations in a
plant. The conventional philosophy is to adjust the parameters of a
control law in order to produce control actions more appropriate for the
current plant parameters and disturbances. This adaptation must take
place whenever parameters and disturbances change. The learning phi­
losophy is to remember these adaptations so that when similar changes
in the plant occur in the future, good control actions are produced with­
out waiting for further adaptation. This presupposes that changes in
the plant can be represented by the input to the controller. The con­
troller must be given more than just the current state and error in the
plant's output. For some problems, providing a recent history of val­
ues taken by available plant outputs might be sufficient to represent the
plant changes. For example, a large disturbance might be indicated by a
sudden change in the trajectory of the plant's outputs. Neural networks
could learn associations from the possibly large input vectors of previous
plant outputs to appropriate control actions.
The ACC showcase problems, with their obvious relevance to adaptive
control, would provide a good testbed for comparing these two philoso­
phies of adaptation. Results from testing neural network controllers on
these problems can be directly compared with the existing results from
the application of conventional adaptive control designs. Such compar­
isons might even lead to the inclusion of neural network controllers in
J. (1987) . Robust and adaptive pole placement . In Proceed­
ings of the 1988 American Control Conference, 2423-2428. Atlanta,
Goodwin , G. C . , Salgado, M. E. and Middleton, R. H.
adaptive control: An integrated approach. In In
1988 American Control Conference, 2440-2445. Atlanta, GA, June.
Huang , J . and Morse, A . S . ( 1 987) . A computer study of adaptive control
systems. In Proceedings of the 1 988 American Control Conference,
( 1 987) . Applications of a New approach to adaptive
Proceedings of the 1988 American Control Conference,
Control Conference, 2418-2422 . Atlanta, GA, June.
International Journal of A daptive Control and
M'Saad , M . , Landau, 1. D . , Samaan, M . , and Duque, M.
formance oriented robust adaptive controller. In
1 988 American Control Conference, 2434-2439. Atlanta, GA, June.
using combined direct and indirect methods. In
1988 American Control Conference, 2429-2433. Atlanta, GA, June.
Details of the bioreactor problem were provided by Lyle Ungar. Charles
Jorgensen and Charles Schley helped clarify the description of the au­
tolander problem. Melvyn White and Ian Bridge added several items
to the list of references for the pole-balancing problem. Derrick Nguyen
contributed details of his implementation of the truck-steering problem.
Richard Sutton provided useful suggestions on the form and content of
Adaptive prediction, training information sources, 30-32
Biology, adaptive critic design parallels,
Control. See also Designs and capabilities
Control loops, sensory input incorporation into, 353
Cost function, adaptive critic design, 70
Group level tasks, MAUV, 437, 447448, 465
Nonlinear networks, system identification procedure, 33
Nonlinear plant, position control of, 352
On-line parameter estimation, 12, 1516, 43--44
Parameter estimation. See also Largescale optimization
critics for evaluation of, 42-44 (see also
Persistent internal states, cerebellumbased model, 337-340
