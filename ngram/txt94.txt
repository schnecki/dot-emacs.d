As is required in the university regulations, this thesis is all my own work,
and no part of it is the resultof collaborativework with anybodyelse.
I would like to thank all the people who have given'me encouragement
and advice, and the many people I have had conversations with about this
work. Other people’s interest and encouragementhas helped me enormously.
In the early stages of my research,Doug Frye introduced me to the work
of Piaget, and I had a number of conversations with David Spiegelhalter on
I would like to thank Richard Young, my supervisor, for patiently giving
me good advice over a number of years, and for spending a lot of time talking
' to me aboutideasthat led up to this work.
Alex Kacelnik has encouraged me greatly, as well as providing many fascinating examples from animal behaviour and foraging theory.
Rich Sutton gave me his technical reports, and pointed out a difficulty
I have had a number of helpful conversations with Andy Barto, who has
raiseda numberof issuesand possibilitiesthat I have not beenable to discuss
I have also had interesting conversations with, and comments
from, Graeme Mitchison, and John McNamara.
Alastair Houston, and Robert Zimmer have read and made comments on early
drafts of some parts of the thesis, and the ■nal version is considerably clearer
I am most grateful for the ■nancial support I have received from King’s
College. I would also like to thank Tony Weaver, my group leader at Philips
Research Laboratories, for his unfailing support which has enabled me to complete this work.
Finally, I would like to dedicate this thesis to my wife Lesley,~becauseshe
In behavioural ecology, stochastic dynamic programming may be used as a
general method for calculating animals’ optimal behavioural policies. But. how
might the animals themselves learn optimal policies from their experience?The
aim of the thesis is to give a systematic analysis of possible computational
First, it is argued that it does follow from the optimality assumption that
animals should learn optimal policies, even though they may' not always follow
them. Next, it is argued that Markov decision processesare a general formal
model of an animal’s behavioural choices in its environment. The conventional
methods of determining optimal policies by dynamic programming are then
described. It is not plausible that animals carry out calculations of
However, there is a range of alternative methods of organising the
dynamic programming calculation, in ways that are plausible computational
modelsof animal learning. In particular, there is an incrementalMonte-Carlo
method that enablesthe optimal values ( or ‘canonical costs’) of actions to be
learned directly, without any requirement for the animal to model its environment, or to remember situations and actions for more than a short period of
time. A proof is given that this learning method works. Learning methods of
this type are also possible for hierarchical policies. Previously suggestedlearning methods are reviewed, and some even simpler learning methods are
presentedwithout proof. Demonstration implementations of some of the learning methods are described.
Page 90, lines 13-21 should be replaced by:
A learningmethodcan be implementedby, at eachtime step,addingappropriatefractionsof the
current prediction difference to previously visited states.
The total change in U(x) that results from a visit to x at time t is ‘
x andr. If U is exactlycorrect,thentheaverage
■rst term' on the RHS abovewill be zero; however,if thereis any error in U, then the second
term on the RHS above will become negligible forsuf■ciently small a.
For each observation, it, a, and a may be chosen with knowledge of previous observations,
but r and y are sampled,independentlyof otherobservations,from a joint distributionthat
For each state-actionpair x, a, the subsequence
form [x a y,‘ r‘] is monotonicallydecreasing,tendsto zero, and sumsto in■nity.
of observationsin which action a is performedin
sequence of observations. The replay probabilities when performing in state <x,m.->in the ARP
may be written explicitly as follows. Let Bi; be the probability that the m,th observation will be
replayed when action a is perforrmd in <x,m,->.Then '
That is, if d(<x,n’>.a)— d(<x,n>,a)= D then if action a is performed' in <x,ri'> then the probabil- V
ity that theobservationreplayedhasindex lessthann is lessthan1—
Hence, the depth construction given previously on page 228 shows that. for any chosen
numberof replaysk and any chosenvalue 6 of the learningfactor andfor any small probability6.
it is possible to choose a level n in the ARP so large that, starting at any state <x,n> and following any sequenceof actions, the number of replays will be greater than k and the maximal value
of or encounteredduring any of the ■rstIt replayedmoveswill be less than a, with probability
greaterthan 1—8.It and8 may be chosenso that theexpectedk-steptruncatedreturnsof the ARP
are as close as desired to the expected returns of the ARP.
It remainsto show that the transitionprobabilitiesandexpectedrewardsin the ARP converge to those of the RP. There is a delicate problem here: the ARP is constructedfrom a
sequenceof observations,and some observationsequenceswill be unrepresentativesamples.
and a. may all be chosen with knowledge of the previous observations 1
a convergence result, it is necessaryto regard the observed rewards and transi-
tions as randomvariables,and to arguethat, for any RP, the transitionprobabilitiesand expected
rewardsin the ARP will convergeto those'in the RP with probability 1, the probability being
of observationsof action a in statex, and let the ith
observation in this subsequencebe observation m, in the main sequence,and let R,-be the random
variable denoting the reward observed on this mith observation. Let the states be numbered from
state, then T‘-‘-= l and Ti: 0 for J atIt. Note that E[ T’ ]- —P,’(a) and
The expected reward and the transition probabilities in the ARP (which are now random vari-
ables,sincetheydependon the observationsequence)are:
-) 0 as i—) co. Since, by hypothesis. the means and variances of all rewards are
■nite, and the T’-‘ are bounded, the strong law of large numbers implies that as i—)°°,
>(a)-) 1”,,(a),bothwith probability1. Sincethereis
only a ■nitenumberof state-actionpairs, all transitionprobabilitiesand expectedrewardsin the
ARP converge uniformly with probability 1 to the correSpondingvalues in the RP as the level in
theARP tendsto in■nity.This completestheproof.
4: Policy Optimisation by Dynamic Programming
................................................................
......................................................
Appendix 1: Convergenceof One-StepQ-Learning
Leaming-toexitin waysthat arerewardedis aisign‘of intelligence.It is,
for enample, natural to train a dog by rewarding it when it responds appr0pri' ately to icomrnands.That animals can leam'to 'obtain reWards and to avoid punintelligence has been
ishments is generally accepted,and this aspect of animal
studied extensively in experimental psychology. But it is strange that this type
of learning has been largely neglected in cognitive science, and I do not know
of a single paper on animal learning published in the main stream of
This thesis will present a general computational
has been studied, as well as to many other types of
will present systematically a family of algorithms that could in principle be
applicaused by animals to optimise their behaviour, and which
I tions in arti■cial intelligence and adaptive control systems.
In this introduction I will discuss how animal
1. Classical and Instrumental Conditioning
the experimental study of animal learning. Instead I will describe the essential
research, the nature of the phenomena studied, and
Animals’ ability to learn has been stuing, as described by Mackintosh
died by keeping them in controlled arti■cial environments, in which events and
contingencies are under the control of the experimenter.
is the Skinner box, in which an animal may be con-
fronted with stimuli, such as the sound of a buzzer
lever in the case of a rat, or pecking at a light in the case of a pigeon. The
animal may be automatically provided with reinforcers. In behavioural terms, a
positive reinforcer is something that may
reSpo-nset positive reinforcer might‘be a morsel
for instance, or a sip of water for a thirsty animal. Conversely, a negativereinforcer, such as an electric shock, is something that may reduce the probabilty of
forcers may be made contingent upon the stimuli
animal. The events and contingencies that
artificial environment are known as the reinforcement schedule.
Two principal types of experimental procedure have been used: instrumental and classical conditioning schedules.
dependson what it does. Instrumental learning is learning to perform actions to
obtain rewards and to avoid punishments: the animal learns to behavein acer-
tain way because behaving in that way leads to positive reinforcement. The
adaptive function of insu'umentalconditioning in animals is clear: blue titswill
obtain more food in winter if theycanlearn to visit well stocked bird-tables.
events, not on the animal’s own behaviour: a rat may be exposed to a light, and
then given an electric shock regardlessvof what it does,‘for example. Experiments of this type are often preferred
between events and reinforcersmaybe controlled by the experimenter, whereas
Classical conditioning experiments depend on the fact that an animal may
naturally respond to certain stimuli without any previous learning: a man will
The stimulus that elicits the response is termed the unconditioned stimulus or
in an environment in which another Stimulus—the
stimulus or CS—tends to occur before the US, so that the
occurrenceof the CS is correlated with the occurrenceof the US, then an
animal may producethe responseafter the CS only. It is as if the animallearns
to expect the US as a result of the CS, and respondsin anticipation.
Whether there are in fact two types of learning in instrumental and classiissues
cal conditioning is much disputed, and complex and dif■cult
involved in attempting to settle this question by experiment. However, I will be
therefore give only a brief discussion of one interpretation of the experimental
evidence from animal experiments, as part of the argument in favour of the
type of learning algorithm I will develop later.
Mackintosh (1983) discussesthe relationship between classical and instrumental conditioning at length.
First, it might seem tempting to regard classical conditioning as a form of
might a dog not learn to salivate in anticipation of
food because the dog found that if it salivated the food was more palatable?
Mackintosh argues that classical conditioning cannot be explained as instrumenOne-of
the neatest experimentsis that of (Browne
A1976) in which animals were initially'prevented from giving any response while
they observed a correlation between a stimulus and a subsequent reward.
removed, they immediately gave the response; there was no possibility of any
instrumental learning becausethe animals had been preventedfrom responding.
Mackintosh also notes that, perhaps more supn’singly, much
instrumental conditioning may be explainable as classical conditioning. In an
instrumental experiment in which animals learn to perform some action in
to a conditionedstimulus, the animal must inevitably observea correbetween
of its action. This correlation, produced by the
’ classically conditioned response:‘if this classically conditioned response is the
corresame as the instrumental response, then each responsewill
lation between the CS and the reward, thus strengthening the conditioning of
the CS. Learning would thus be a positive feedback process: the more reliably
' the animal responds,the greater the correlation it observes,and the greater the
correlation it observes,the more reliably the animal will respond.
But, as Mackintosh argues, not all instrumental learning may be explained
in this way.’A direct and conclusive argument that not all instrumental learning
is explainable as classical conditioning is the common observation that animals
can learn to-perforrn different responses in response to the same stimuli
sequence of responses,that results in reward—why does the animal produce
learning consists of attempting to repeat
explanation. One possibility. is that an animal performs a totally random
exploration of its environment: but this cannot be accepted as a complete explaclassical
behaviour releasedby classical conditioning may then be
by instrumental learning. The question of the relationship between
funclassical and instrumental conditioning is, therefore, one aspect of a more
What ways does this innate knowledge contribute to learning?
explain, for example, just how the time interval between a responseand a reinforcer' affects the rate at which the response is learned. As a consequenceof
this level of detail, conditioning theory cannot readily bejuse'd to explain or to
predict animal learning under more natural conditions. the relationships between
complex for models of instrustimuli, responses, and reinforcers
mental conditioning to make predictions. It is clear in a general way that
an animal to learn to obtain what it needs,but
conditioning theory cannot in practice he used to predict the results of learning
quantitatively under most natural conditions. The dif■culty is that conditioning
theory has tended to be deveIOpedto explain the results of
experiment, rather than to predict the effect of learning on behaviour overall.
The optimality argument as used in behavioural ecology can
which this type of instrumental learning may be achieved.
Behavioural ecologists seek to explain animal behaviour by starting from a
animals need to behave ef■ciently if they
their ecological niches, should not evolution also lead to similarly exquisite
behaviour? On this view, it should be possibie to explain natural
animal behaviour in terms of its contribution to reproductive success. This
of animal behaviour is known as the optimality argument.
(1986) give an extendeddisattack its uncritical use, and Stephensand
that there are both practical andtheoreticaldif■cultieswith the optimality argument.
One dif■culty that Gould and Lewontin point out is that the optimality
argument must be applied to an animal and its behaviour as a whole, and not to
each aspect of the animal separately.Further, optimality can only be assessed
relative to the animal’s ‘choice' of overall design (or ‘bauplanf) and mode of
life. One cannot recommend a whale to become a butter■y.
* A potential weaknessof the optimality argument is that evolution is not a
therefore, that there are some aspects of
behaviour which have no adaptive value, just as there are some parts of the
from being re-designed. Nevertheless, there are many
arguments can be .applied convincingly to explain
And, of course, optimality arguments may often be dif■cult to provide in
practice becauseit may be dif■cult to establish what the optimal behavioural
strategy actually is for an animal in the wild. The dif■culty may be either that
to determine what an animal’s intermediate goals should be if it is
to leave as many surviving descendantsas possible, or else the dif■culty may
be that although the goals of optimal behaviour are reasonably clear, it is
dif■cult for a behavioural ecologist to know how animals could best go about
achieving them. What is the best way for a squirrel to look for nuts?
To apply the optimality argumentto any particular example of animal
behaviour is fraught with subtle dif■culties, and a substantial amount of investigation of the animal’s behaviour and habitat is necessary.But I am not going to
do thiseeall_l need to assume is. a rather limited form of the optimality argu-
The optimality argument as applied to behaviour suggests that the function
‘of instrumental learning is to learn to behave optimally. Some aspects of
There is a basic dif■culty: animals cannot learn how to leave as'many
descendants as possible. It is not possible for an animal to live its life many
times and to determine from its experience the optimal strategy for perpetuating
its genes. All that an animal can learn to do is to achieve certain intermediate
objectives. To ensure ‘maximal reproductive success, animals may need to
of intermediategoals: ■ndingfood andWater,■ndingshelter,
defending territory, attracting a mate, raising offspring, avoiding predators, resting, grooming, and so on. Animals may learn to achieve these goals, but they
It is often possible to identify certain skills that animals need to have—
one such skill that many animals need is the ability to
at the maximum possible rate. To describe an intermediate objective
quantitatively, it is necessary to specify a performance criterion, which can be
used to ‘score’ different possible behavioural strategies.The maximally e■icienr
score according to the perone that leads to
formance criterion. If animals can represent suitable performance criteria inter-
nally, so that they can score their current behaviour, then it becomes possible
for them to learn ef■cient behaviour. This is the type of learning Iwill
But an animal will not always need to achieveall its intermediateobjectives with maximal ef■ciency.A plausible view is that, forjeach speciesof
animal, there are certain critical objectives, in that the levels of performance an
animal achieves in these areas strongly affect its reproductive ■tness.In other
minimum level of ef■ciency, further improvements do not greatly affect ■tness.
For example, a bird may have ample leisure during the autumn when food is
plentiful and it has ■nishedrearing its young, butits survival in the winter may
depend critically on its ability to forage ef■ciently. There is, therefore, an
to choose to devote appropriate amounts of
activities; but it is likely that it is more usual that animals need to learn certain
Speci■cskills, such as how to hunt ef■ciently. It IS unlikely that an animal will
have to learn to seekfood when it is hungry:it is more likely to need to learn
so that it can exercise this skill when it is hungry.
to forage ef■ciently while actually foraging somewhat inef■ciently, so that
the animal’s true level of skill may only become evident when the animal needs
to use it. Furthermore, it is usually necessary to make
learn: animals must necessarily behave inef■ciently sometimes in order to learn
they need to. This view of learning is rather
different from traditional views of reinforcement learning, as presented by, for
__4’ Learning and the Optimality Argument
(The optimality argumentndoespredict that animals should have the ability
to learn—to adapt their behaviour to the environment they ■nd. The reason for
The same genotype may encounter circumstances in which different
encounter different circumstances.This is not
point: one reason why we do not learn to beat our hearts is that the design of
the heart, the circulatory system, and the metabolism is encoded in the genes,
so that the optimal strategy.for beating the heart may-be. genetically coded as
dwell. It'is possible, however, that there could be mechanism for ■ne-tuning
the control system for the heart-beatinresponse to experience, becausephysi.
cal development is not entirely determined by the genotype.
notions of Optimality. in learning: optimal learning, and learningrof e■‘icient
I Vsrrategievs.‘Optimal learning’ is a; processiof collectingand
during learning in an optimal manner, so that the learner makes the best possible'decisions‘ at all stages of learning: learning itself is regarded as a multilearning; is' optimal-if the learner adOpts
i that will yield the highest possible return from actions over the whole course of
suf■cient experience, the learner will eventually acquire the ability to follow
The difference between thesemonotions may be made clear by considerplayer
ing the ‘two-armed bandit' problem. In this problem, a
levers. On each turn, the player may pull either lever A or lever B, but not
both. After pulling a lever, the’player receives a"reward. Let us supposethat,
distribution." Successiverewards are independentof each other, given the
choice of lever. The average rewards given by the two levers are different.
The reward the player obtains, therefore, depends only on the lever he pulls.
Now, suppose that the player is allowed only 10 turns; at each turn, the player
may decide which lever to pull based on the rewards he has received so far in
If the player knows that lever A gives a higher reward than lever B, then
ef■cientstrategyis to pull lever A 10 times. But if the
player is uncertain about the relative mean rewards offered by the two levers,
and his aim is to maximise his total reward over n turns, then the problem
becomesinteresting.[Thepoint is that the player should try pulling both levers
alternately at ■rst, to determine which lever appears to give higher rewards:
. once the player has sampled enough from both levers, he may choose to pull
one of the levers for the rest of the session.Other sampling s■'ategiesare possible.
The difference between optimal learning and learning an ef■cient strategy
- is clear for this problem. Learning an ef■cient strategy is learning which lever
gives the higher rewards on average;a learning method learns the ef■cient strategy if it always eventually ■nds out which. lever gives the higher rewards.
.'However, a learning method is optimal for sessionof length if it results in
the playerobtaining the highest possible expected reward over the It turns,
‘highest possible’_taking into account the player’s initial uncertainty about the
use of information to inform behaviour. It
is learning that is optimal‘when considered,over the whole course of learning,
taking into account both. early mistakes and 1atersuccesses._
sense refers to the learning method itself, not to the ■nalbehaviour attained. In
problem, for example, if only a few turns are allowed, it
may be optimal for the player to perform very little initial sampling before
choosing one lever to pull for the rest of the session. If the player does not
perform enough sampling, then he may easily choose the Wrong lever: if many
are allowed, therefore, the optimal strategy may be to
Note that optimal learning does not necessarily lead to the acquisition Of the
maximally ef■cient strategy: if learning the maximally ef■cient ‘skill is costly, it
may not be worthwhile for the animal to learn it.
The two-armedbanditproblemis perhaps’thesimplestlearning.problem
which involves ’a trade-off between; exploration of the [possibilities of the
environment, and exploitation of the strategies that have been discovered so far.
This is a dilemma that arises in almost any instrumental learning problem. If an
animal performs too much exploration, it may not spend enough time in
exploiting to advantage what it has learned: conversely, if an animal is incurious and does too little exploration, it may miss discovering some alternative
behaviours that would bring much higher returns, and it may spend all its time
exploitation trade-015“.During its life time, an animal must continually choose
whether to explore or whether to exploit what it knows already. One prediction
of optimality theory, therefore, is that an animal should make an optimal choice
in the exploration-exploitation trade-off. It may happen that, in following the
optimal strategy,the animal will not necessarilyperform enoughexplorationto
achieve maximally efficient performance: it may be better to be incurious and
so avoid making too many mistakes during exploration.
Houston and McNamara (1988) and Mangel' and Clark (1988), propose
explaining natural animal learning as optimal learning in this sense. This
approach is surely correct for learning in the sense of collecting and using
information,but it is in practiceimpossibleto applyfor learning in the senseof
dynamic programming literature (for example, de Groot (1970) or Dreyfus and
Law (1977)) to refer to the short-term collection of
use. This is the sense of ‘learning’ as in ‘In the darkness of the room there
sound and.then the sound of the sofa being pushed across
the ■oor, so I'leamed I was dealing with a snake of remarkable size." as
opposed to the sense of learning in ‘It took him three months of continuous
practice to learn to ride that unicycle.’ The short-term collection and use of
information (learning in the ■rst sense) is a skill that can itself be gradually
improved by practice (learning in the second sense).
Krebs, Kacelnik, and Taylor (1978) performed one of the ■rst experiments
to determine whether animals could learn to collect and use information in an
optimal. way—indeed, one of the ■rst experiments to determine whether
animals could learn an optimal strategy,where the optimality of the strategy
in an arti■cial environment in which they were fed 1na series of short sessions
For the duration of each feeding session, the bird was
feeders, one of which would yield food more readily than the other. The birds
could only tell which feeder was better by trial, and error, so that each session
optimal—is to sample from both feeders for a short time
at the start of each session, and then to feed for the
exclusivelyfrom the feederthat gaveout food most readily during the sampling
period. Over many. sessions,the birds did indeed acquire near optimal
But the type of learning thatI will be interested in is the improvement in
performance over many feeding Sessions.In this example, the birds gradually
learned a maximally ef■cient strategy for the problem. Was this gradual leaming also optimal? That is a very dif■cult question to answer, for two reasons.
‘ The ■rst reason is that it is exceedingly dif■cult to devise demonstrably
for the two-armed bandit problem, ■nding the optimal strategy is a formidable
computation; There is“ a7straightforwardgeneral method, as explainedin
de Groot (l970), for constructing optimal learning strategies, but the strategies
and the computation become impractically complex for any but small problems,
complex for it to be possible to determine the optimal learning strategy.
and more fundamental dif■culty is that an optimal learning
respectto some prior assumptionsconcerning
probabilities of encounteringvariouspossible environments. In an experiment
After the great tits have experienced many feeding
the'statisticaldistribution Aof yields from eachfeeder,
and it makes sense ask whether they can acquire the optimal strategy for the
distribution of yields that they have experienced. But to ask whether the birds’
learningis optimal over the whole experimentis a different matter:theoptimal
from.the birds”pointof view is depends the birds’prior expectaon
tions, and we have no means of knowing what these expectations 'are‘or what
In other words, to show that some particular learning method is optimal, it
is necessary to specify a probability distribution over the environments that the
animal may encounter, as noted by McNamara and Houston (1985). In praclikely
to be an insuperable-dif■culty in providing convincing quantitative optimalityexplanations for any type of skill acquisition.
But although quantitative arguments on Optimal learning may be dif■cult
some qualitative explanations involving optimal learning are comthey
mon sense. Animals that are physically specialised so that
particular way of life, for example, should in general be less curious and
exploratory than animals that are physically adapted to eat many different
foods. The reason for this is that a highly specialised animal is unlikely to disof food, while
its behaviour to exploit whatever is most available.
computational models of optimal learning, both
-because of the technical dif■culty of constructing optimal
and becauseof the need to introduce explicit'assumptions about a probability
distribution over possible environments. In any case, Optimal learning will seldom be apractical quantitative method of explaining animal learning
. tegies. By the learning of ef■cient strategies, I
mean the acquisition of the ability to follow astrategy that is maximally ef■cient according to an intermediate
strategy. an animal with this ability need not
ef■cient strategy, but it can do so if it chooses to.
theory would predict that an animal should
a prey animal should learn how to return to its
burrow as fast as possible from any point in its territory. Of course, the animal
it is vitally necesneednot alwaysreturnto its burrowasfast as‘lpossible—but
important that an animal shouldfollow an ef■cient strategy in searching for
food if it has just eaten, but it is advantageousfor an animal to be able to follow an ef■cient strategy'in searching for food if it needs to.
Optimal learning will require the learning of an ef■cient strategy if the
The capacityfor maximally e■icientperformance is
The time taken to learn the behaviour is short compared to the period of
time during 'which the behaviour will be used.
The third condition implies that the ■nal level of performance reached is more
important than the time taken to learn it—hence optimal learning will consist of
Animals need to be able to survive adverseconditions that are more
extreme than those they usually encounter. It is likely, therefore, that under normal circumstances most animals have some leisure for exploration: in other
words, the opportunity cost of exploration is usually small. Animals may, thereefficiency in
fore, norrnally perform with slightly less than maximum
Even if theseassumptionsare not entirely satis■ed,it is still plausiblethat
animals should learn ef■cient strategies. The point is that
entail learning an ef■cient strategy unless learning is expensive. Learning may
“be expensive if mistakes are costly: prey animals would be unwise to attempt to
for example, the prey animal may lose some of its fear,
time and energy in avoiding the predators.Animals may have innate knowledge
or behaviours that prevent them from making
progressively modi■ed to become maximally
behaviours through instrumental learning. In
I In conclusion, the optimality assumption leads
ef■cient behavioural strategies. That is, after
suf■cient experience in an environment, an anirnal should acquire the ability to
exploit that environment with maximal ef■ciency.‘Most of the.thesis is devoted
" to investigating what algorithms‘animals might Useto learn in this way.
4.1. Learning Ef■cientStrategiesand Conditioning
. Instrumentalconditioningandthe learningof ef■cientstrategies related
concepts, but they are not at'all the same. The motivation for studying instru‘ mental conditioning is that it is possible mechanism for a type’of learning that
could be useful to an animal in the wild. However, operant conditioning theory
does not explicitly consideref■ciencyof strategy,and many aspectsof instru' mental conditioning
viewpoint of optimality theory.Conversely,many experimentsthat test whether
animals can learn an ef■cient behavioural strategy are not easy to interpret as
conditioning experithey are super■cially similar to
of ef■ciency’ experiment should give animals both incentive and opportunity to
imenter can determine the maximally ef■cient behavioural strategy.
opportunity Ofroptimising-their strategies.
environmentshouldnot bechangedduring this time.
The animals should have an adequate motivation to acquire the optimal
strategy, but the incentive should not be so severe that
Control groups shouldlbe placed in arti■cial environments that differ in
chosen respects from the environment of the}experimental group. Control
groups should be given the same opportunities of Optimising their
Experiments designed,in this way have two considerable advantages.First, it is
possible to devise experimentsthat simulatedirectly certain aspectsof natural
conditions. Second, Optimality theory 'can be used tojmakequantitative predictions about what strategy the animals will eventually learn.
Some conditioning experiments satisfy these design'requirements; others
do not. For example,the phenomenonknown as ‘extinction’ inconditioning
theory, in which a learned responsegradually extinguishes when the'stimulus is
repeatedly presentedwithout the reinforcer, is not directly interpretablein terms
of Optimality. This is becausein a typical instrumental conditioning experiment,
the purpose of testing animals under extinction is to determine the persistence
or ‘strength’ of the animal’s expectation of a reward following the stimulus.
This concept of ‘strength’ is dif■cult to interpret in terms of optimality. There
is often no _‘correct’behaviourduring extinction; whetheran animal should
continue to respond for a long time or not dependsentirely upon what types of
should expect tor■nd in its environment. Since the extinction conVditionsoccurs only once during the experiment, the animal is not given enough
data for it to work out what it ought to do.
If, on the other hand, extinction were to occur repeatedly in the course of
an experiment, the animal has the chance to learn how to react in an optimal
way. Kacelnik and Cuthill (1988) report an experiment
repeatedly obtain food from a feeder. Each time it is used, the feeder will sup.
ply only a limited amount of food, so that as the birds continue to peek at the
no more food at all. To obtain more food, they must then leave the feeder and
their cage until a light goes on that indicates that
feeder has been reset. In'tenns of conditioning theory, this experiment is
(roughly) a sequenceof repeatedextinctions of reinforcement that is contingent
upon peeking at the feeder: however, becausethe birds have the
accumulate suf■cient experience over many” days, they ‘have the necessary
pretable; they ask different questions and, perhaps, provide some more detailed
answers than optimality experiments do. However, the optimality approach is
both quantitative?and strongly motivated,
possible to classify and to implement a range of
McNamara and Houston (1980) describe how decision theory may be used
‘ to analysethe choicesthat animalsface in somesimple tasksand to calculate
the ef■cientstrategy.‘ They point out that it is in principle possible that animals
estimation of probabilities, and 'then use decision
animals learn by special purpose, ad'hoc methods. McNamara and Houston
give two main arguments in favour of this conclusion.
complex even for simple problems, and that, in
would not otherwise need. Their second argument is that animals do not face
7the problem of determining Optimal strategiesin general: each species of animal
environments. "Animals, therefore, should'only need simple, special-purpose
heuristic learning methods for tuning their behaviour to the
different from those in which the animals evolved.
heuristic, special-purpose, fallible learning
captivity, the ducklings may become imprinted on their keeper rather than on
their mother. There can be no doubt that many other special-purpose learning
methods exist, of exactly the type that McNamara and Houston describe.
Houston'sarguments convincthat McNamaraand
some'innate Special-purpose'adaptive mechanisms
that all animal learning can be described in
this way. Many species such as rats or starlings are opportunists, and can learn
to invade many different habitats and to exploit novel sources of food. Animals
can be trained to perform many different tasks in conditioning
different speciesappearto learn in broadlysimilar ways.‘Is it not more plausible that there are generally applicable learning mechanisms, common to many
.enableanimals to learn patterns of behaviour that their ancestors never needed?
The next section presents a speculative argument that
learning methods may sometimes evolve from general learning methods applied
to particular tasks. But the most convincing argument against the hypothesis of
special-purpose learning methods will be to show that simple general learning
methods are possible, which I will attempt to do later on.
6. Learning Optimal Strategies and Evolution
Evolution may speed up learning. If the learning of a critical skill is slow
7and expensive, then there will be selective pressure to increase the ef■ciencyof
7learning. The ef■ciency of learning may be
be called ‘innate knowledge'. By this, I do not necessarily mean knowledge in
the ordinary sense of knowing how to perform atask, or of knowing facts or
information. Instead, I mean.by ‘innate knowledge’ any innate behavioural tendency, desire, aversion, or area of curiosity, or anything else that in■uencesthe
; normal senseof the word, but in normal circumstancesit is able to learn that
skill more quickly than another animal without this innate knowledge.
A plausible hypothesis, therefore, is that useful behaviours and skills are
' initially learnt by some individuals at some high cost: if that behaviour or skill
useful, the effect, of selective pressure will
learning of it quicker, less costly, and more reliable. One origin of Specialpurpose learning methods, therefore, may be as innate characteristicsthat have
evolved to speedup leaming’bya generalpurposemethod.
dif■cult to 'pin down the sensein which a learning mechanism can be general,
because all learning must start from some innate structure. It has become a
that learning from a tabula ram is necessarily
impossible. Any form of learning or empirical induction consists of combining
fromiexperience with some prior structureLNo learning
method, therefore, can be completely general, in the sensethat it dependson no
'method can be ‘general’. An animal has sensory abilities that enable it to distinguish certain aspects‘of its surroundings, it can remember a certain amount
about the recent past, and it has a certain range-of desires, needs, and internal
sensations that it can experience.-It can perform a variety of physical actions.
A behavioural strategy is a‘method of deciding what action to take on the basis
of the surroundings, of the recent past, and of the animal’s internal sensations
and needs. A Strategy might be viewed as a set of situation-action rules. or as a
set of stimulus-responseassociations,where the situations or
the appearance of the surroundings, memories of the recent past, and internal
sensationsand desires, and the ‘responseS’are the actions the animal can take. I
do not wish to imply that 'a strategy is actually represented as a set of
stimulus-response associations,although some strategies can be: the point is
merely that a strategy is a method of choosing an action in any situation.
_Learning is a process of ■nding better strategies.Now, a given animal will be
able to distinguish a certain set of situations, and
actions, and it will have the potential ability to construct a certain range of
situation-action strategies. A general learning
experience to improve the current strategy, and, ideally, to ■nd the
is the best one for the current environment, given the situations the animal can
recognise and the actions the animal can perform. As will be shown, there are
and optimising behaviourai strategies in this
cannot directly learn to‘optimise their ■tness,becausethey
cannot live their lives many times and learn to perpetuate their genesas much
may learn critical skills that improve their ■tness.
a skill for which improvementsin performance
in ■tness.For example, the rate at which a bird can
directly affects the number of chicks it can raise. In
not alwaysperform its critical skills with maximal
ef■ciency: it is the capacity to perform with maximal ef■ciency when necessary
that is valuable. A bird may need to obtain food with maximal ef■ciency all the
time during the breeding season, but at other times it may have leisure.
One role of instrumental learning, therefore, is in acquiring the ability to
perform critical skills as ef■ciently as possible. This learning may be to some
extent incidental, in that performance does not always have to be maximally
“ef■cient during learning: indeed, sub-optimal performance
without disaster; then the animal should ultimately acquire a capacity for maximally ef■cientperformance.
In the next chapter, I will describe how a wide range of
Vmay be posed as problems of learning how to obtain delayed revvards. I will
argue that it is plausible that animals may represent tasks subjectively in this
way. After that, I will describe the established method for calculating an
optimal strategy, assuming that complete knowledge of
consider systematically What learning methods are possiThen I will
performing dynamic programming. After that, I will describe computer implemenmethods.
‘animals, becausethe discussionwill not be related
animal learning. The learning algorithms are strong
may also have practical applications in the construction of learning machines.
In this chapter I will describe several problems to which the learning
methods are applicable, and I will indicate how the problems are related.
and Barto, Sutton, and Anderson (1983), is the
and forth on the track between the two endjoined
to the cart by a hinge, and is free to move in the
vertical plane aligned with the track. There are two possible control actions,
a constant force to the cart, pushing it either to the right or
be acquiredis that of pushingthe cart to left
and right so as to keep the pole balanced more or
and.to keep the cart from bumping againstthe ends of the track. This skill
or to the left, the decision being made on the basis of the state of the cart-pole
There are severalways of posingthis as a learningproblem.If an ‘expert’
is available, who knows how to push the cart to balance the pole, then one
approach would be to train an automatic system to imitate the expert’s
behaviour. If the learner is told which action would be correct at each time, the
learning problem becomesone of constructing a mapping from states of the cart
At each time step, the controller may push the cart either tor
The task is to keep the pole balanced, and to’ keep the cart from
and pole to actions. The problem of learning a procedural skill is reduced to the
problem of learning a single functional mapping from examples: The disadvantage of this ‘imitate the teacher’ method is that a teacher may not be available:
indeed, if there is a machine teacher, there is rarely any point in having a
A more interesting and general formulation of the learning problem is that
the learner receives occasional rewards and penalties, and that the learner’s aim
is to ■nd a policy that maximises the rewards it receives. The pole-balancing
problem, for example, can be formulated as follows. The learner may repeatedly set up the cart and pole in any position, and it may then push the cart to
the pole balanced.The information that the learner can
use consists of the sequenceof states of the cart-pole system
that the learner itself performs; the learner is informed when the pole is deemed
to have fallen. The falling of the pole may be thoughtof as a punishmentor
‘penalty’, and the learner may be viewed as having the goal of avoiding these
penalties.Note that the learneris not given the aim of ‘keepingthe pole nearly
vertical and the cart away from the ends of the track’: it is just given the aim
of avoidingpenalties,and it must work out for itself how to do this. The
learner does not know beforehandwhen penaltieswill occur or what sort of
strategy it might follow to avOid them. The learning method of Barto, Sutton,
andAnderson(1983)learnsundertheseconditions.
In the ‘imitate the teacher’ formulation of
is told whether each action it performs is correct; in the reward/punishment formulation, the rewards or punishments may occur several steps after the actions
that caused them. For example, it may be impossible for the learner to prevent
the pole from falling for some time before the pole actually falls: the ■nal
actions thelearner took may have been correct in that by these actions the
falling of the pole was delayed as long as possible, and the actual mistaken
actions may have occurred some time earlier.
The pole-balancing problem has a particularly clear structure. What needs
to be learned is a method for deciding whether to push right or left. At any
time, all the information that is needed to make this decision can be summed
up in the values of four state-variables:
the angular position of the pole relative to the cart
These variables describe the physical state of the system completely for the
purposesof pole-balancing:thereis no point in consideringany more information than this when deciding in which direction
available for the current time, then it is not necessary to know anything more
The space of possible combinations of values of these state variables is the
State-spaceof the system—the set of possible situations that the agent may
face. The purpose of learning is to ■nd some
agenthasmasteredthe skill if it can decidewhat
to do in any situation it may face. There need not necessarilybe a single
prescribedaction in any state—theremay be somestatesin which either action
is equally good, and a skilful agent will know that it can perform either.
The state-spacemay be described in alternative ways. For example, suppose that the agent cannot perceive the rates of change of the position of the
cart or of the angle of the pole. It cannot, therefore, perceive the state of the
system directly. However, if it can remember previous positions and angles and
the actions it has recently taken, then it can describe the state in terms of its
memories of previous positions and previous actions. If the information that the
agent uses to describe the current state would be suf■cient to determine approximate values; of all four state variables, then it is in principle suf■cient informa-
tion for the agent to decide what to do. In the cart and pole problem, it is easy
to de■ne one adequate state-space:many other systems of descriptions of the
state are possible. The criterion for whether a method of description of state is
adequate is that distinct points in the adequatestate space should correSpondto
Stephens and Krebs (1987) review a number of decision-making problems
that animals face during foraging, and describe formal models of these prob-
lems that have beendevelopedfor predicting what animalbehaviourshouldbe
One ubiquitous problem that animals face is that food
distributed—it occurs in ‘patches’: foraging consists of searching for a"patch’
of food, exploiting the patch until food becomes harder to obtain there. and
then leaving to searchfor a new patch.While searchingfor a patch,the animal
intake of food becomes high initially, and!then declines, as the obtainable food
in the patch becomes progressively exhausted. Eventually, the animal has to
make an unpleasant decision to leave the current patch and search for a new
patch to exploit. This decision is ‘unpleasant’ in that the animal must leave
behind some food to go and search for more, and the initial effect of leaving a
patch is to reduce the rate of intake of food.
This dilemma is known as the patch-leaving problem. If the animal stays
in patches too long, it will waste time searching for the last vestigesof food in
exhausted patches. If, on the other hand, the animal leaves patches too soon,
then it will leave behind food that it could pro■tably have eaten.
The animal gains energy from food, and spends energy in looking for and
in acquiring it. It is reasonableto suppose that the optimal foraging Strategyfor
an animal is to behave so as to maximise some average rate of net energy gain.
A common assumption is that an animal will maximise its long-term average
rate of energy gain; this is not the only assumption that is possible, but it is one
that is frequently used by foraging theorists.
Chapter 3, consideredthis and similar problems, and pr0posedthe marginal
value theorem. This applies in circumstances where
At any time, an animal may be either searching for opportunities (e.g.
prey, patches), or else engaged in consumption (e.g. eating prey, foraging
The animal may abandonconsumption at any time, and return to searching
for a new opportunity. The animal may only stop searching once it has
The results of searcheson different occasions—the values of the opportunity discovered, the time taken to ■nd the opportunities, etc—are statistically independent.
New opportunities are not encountered during consumption.
The rate of energy intake during consumption declines monotonically.
That is, ‘patch depression’ is monotonic.
The decision problem for the animal is that of when to stop consumption
and return to searching. An animal’has the option of not starting consumption at all if an opportunity is not suf■ciently promising.
If all these assumptions are satis■ed, then there is a simple optimal decision
the following form. Suppose that the best possible long-term
average rate of energy gain is L. Then the optimal decision rule is to abandon
consumption and return to search when the instantaneous rate of energy gain
falls to L or below. This de■nition might appear circular but it is not: L is a
well de■ned quantity that could in principle be found by calculating the long
term average returns of all possible strategies, and then choosing the largest of
the results. The marginal value theorem states that the optimal strategy consists
of leaving a patch when the rate of return drops below L, and this fact can
often be used as a short-cut in ■nding L.
This decision rule, however, requires an animal to assessthe instantaneous
rate of energy gain during consumption. There are some circumstances where
it is reasonable for animals to be able to do this (e.g. a continuous feeder such
as a caterpillar), but it also often happens that energy gain in a patch is a stochastic process, in which food comes in chunks within a patch (such as a bird
eating berries in a bush). In this case, the animal cannot directly measure its
instantaneousrate of energy gain, but the marginal value theorem still applies if
the instantaneousexpected rate of energy gain is used. To use a decision rule
based on the expected instantaneous rate of return, the animal must estimate
this on the basis of other information, such as the appearanceof the patch, the
history of finding food during residence in the patch, and whether it has found
McNamara (1982) has proposed a more general type of decision rule,
based on the idea of potential. This formulation is more general than that
required for the marginal value theorem in that it is no longer necessary to
assume that the rate of energy gain declines monotonically as an animal
exploits a patch: the other assumptionsremain the same.
The potential is rather cumbersome to de■ne in words, but a de■nition
An animal continually estimates the potential of a patch on
knows about the patch. The potential is the
estimated maximum achievable ratio of energy gain to residence time in the
patch, the maximum being taken over all possible exploitation strategies that
the animal might adopt. A bird might, for example, estimate the potential of a
particular tree on the basis of the type of tree, the season, how long it has been
searchingin the tree, and how many berries it has found recently. The decision
rule is to leave the patch if the potential drOpsbelow L, and to stay otherwise.
considerably-more complex analysis than Chamov and Orians'
original presentation of the marginal value theorem. All that is left of the simplifying assumptions for the marginal value theorem is that the searchesare statistically independent, and this assumptionitself may not always be plausible. If
this assumption too is dropped, a still more general method of determining
optimal strategies may be used: dynamic programming.
The point is that, to calculate optimal strategies and how they depend on
certain environmental variables, is necessaryto construct a simpli■ed formal
model of the foraging problem that the animal faces. It is sometimes possible to
justify a very simple type of model, such as the type of model needed to apply
the marginal value theorem, and the optimal strategy can then be derived by
and a little algebra. However, thesestrong
The most general form of foraging model that it is reasonable to construct
is a dynamic model, which can be described in the following terms. The foraging problem is described abstractly, in such a way that the foraging could in
principle be simulated on a computer. At any time, the animal and the environment can be in any of a certain set of objective States, the objective state being
the information about the state of the animal and its
that is necessary for continuing the simulation. The objective state may contain information
that would not be available to the animal, such as how much food is left in the
Let us suppose that the animal does not make decisions‘continuously, but
that decision points occur at intervals during the simulation. The foraging problem faced by the animal is that of taking decisions as to what to do next: at
each decision point, there is a certain range of alternative actions that the
animal can choose between. The action the (simulated)
affect the amount of food that it ■ndsin the time up to the next decision point,
and it will affect the objective state at the next decision point.
The book by Mangel and Clark (1988) is an extended description of this
approach to the modelling of the behavioural choices that animals face.
One constraint on foraging is that an animal can take decisions only on
the basis of the information available to it. The animal may not be able to
know the objective state, but the information that the animal uses to decide
what to do might be called the subjective state. The subjective state may con-
sist of information about the appearanceof the environment, about recent
events in the past, and about the animal’s internal state, and about its current
goals. if it has any. I will suppose that an animal has an internal subjective
model of the foraging problem, different from, but corresponding to the objective model. The animal forms subjective descriptions of the situations it faces,
the actions ititakes,‘ and of the bene■ts or costs of the actions it takes (the
What are the subjective rewards and costs? A behavioural ecologist may
determine what an animal's short term goals ought, objectively, to be, but need
animal’s subjective reward system correspond directly to the objective
reward system? If animals learn to achieve subjective rewards rather than
objective rewards, then all that can be deduced from the optimality argument is
that the optimal strategy according to the subjective reward system should be
the same as the Optimal strategy according to the objective reward system. This
does not imply that the objective and subjective rewards are the same.
A plausible example where there is a difference between subjective and
' objectiverewardsis that of an innatefear of predators.If a bird feedsin a certain spot, sees a kestrel, and escapes,it has suffered no objective penalty
becauseit has survived. The only objective penalties from predation occur
when animals get eaten, after which they can no longer learn. In order for
animals to avoid predators, it is plausible that sights and sounds of predators
should be subjectively undesirable experiences that the animals should seek to
Within the framework of a subjective dynamic model, the sight of a predator should therefore be a subjective penalty. In the bird’s natural environment, a policy of avoiding situations in which predators are seen may be a
To suggest possible learning methods, I do need to assumethat the animal
or agent’ssubjectiverepresentationof the problem is suf■cientlydetailedthat
the subjective problem is that of controlling a Markov decision process. This is
a large assumption to have to make, but it is unavoidable.The only consolation
is that the learning may often succeed even if the assumptiondoes not hold. As
I will not mention this assumption for the rest of the thesis, I will give two
examples of how things can go wrong if the agent does not encode enough
information about states for the results of its actions to be predictable.
The ■rst example is that of ■nding one’s way about in central London. It
is easy enough to build up a mental map of the streets and the junctions, so
that one can mentally plan a route; the ‘states’ in this case are the junctions,
and the ‘actions’ are the decisions as to which street to turn into at each junction. Now, the street layout and even the one way system are easy enough to
information is not suf■cient,becausethere are‘heavy restricremember,
tions on which way one is allowed to turn at each junction. That is, a descrip-
tion of which junction one is at is not a descriptionof the statethat is suf■cient
for determining what action to take—it is also necessaryto specify which street
one is in at the junction. If one does not succeedin remembering the turning
restrictions, one cannot plan ef■cient routes.
As a secondexampleof a non-Markoviansubjectiveproblem,consideran
animal undergoing some conditioning experiment in which the reinforcements
depend in a complicated way on the Sequenceof recent events and actions. If
the animal does not remember enough information aboutthe recent past to be
able to distinguish aspects that are relevant to the reinforcement it will receive,
then it may not be able to learn the most ef■cient strategy for exploiting that
environment. Furthermore, the environment could be contrived so as to frustrate
the animal's attempts to learn a simple strategy that made use only of the information
In both of these examples, the problem is that the agent does not encode
enough relevant information about the situation it is in to be able to have
effective situation action rules. In both cases the remedy is clear: the agent
should base its decisions on more information: it is a question of determining
Autonomous learning agents will surely need to have some methods, pos-
sibly heuristic,of detectingwhethertheir current encodingof stateis adequate.
I have not considered this problem, and I think that it is unlikely
any single general approach to it. From now on, I will assume that the
subjective formulation of the problem is indeed a Markov decision process.
To supposethat animalsformulate subjectiveproblems in this way is to
make a psychological hypothesis. I think that this hypothesis is.both plausible
and fully consistent with long established assumptions about associative learning.
The formal model that will be usedfor theseand other problemsis the
are a number of books that treat Markov deci-
sion processes. A clear and concise account of discrete Markov processesis
Ross (1983); other books are Bellman and Dreyfus (1962),
Denardo (1975), Bertsekas (1976), Dreyfus and Law
Yushkevich (1976). To make this document self-contained, I will give a brief
account of the main methods and results for ■nite-stateproblems here.
A Markov decision process, or controlled Markov chain, consists of four
parts: a state-space S, a function A that gives the possible actions for each
state, a transition function T, and a reward function R.
The state-Space S is the set of possible states of the system to be controlled. In the case of the cart-pole system, the state-space is the set of 4vectors of values of the position and velocity of the cart, and of the angular
position and angular velocity of the pole.
In each state, the controller of the system may. perform any of a set of
possible actions. The set of actions possible in state x is denoted by A(x). In the
case of the cart-pole system, every state allows the same two actions: to push
To avoid the complications of systemswhich have continuous state-spaces,
continuousaction sets, or which operatein continuoustime, I will consider
only ■nite, discrete-time Markov decision processes.That is,
A(x) is a ■nite set of discrete actions for all x in S
states are observed, actions taken, and rewards received at discrete times
1,2,3, . - Any non-pathological continuous Markov decision process may be approx-
imated adequately for present purposes by a ■nite, discrete-time Markov deci- .
sion process. From now on, I will discussonly ■nite Markov decision processes
The random variable denoting the state at time t is X,, and the actual state
at time t is x,. The state at time t+1 dependsupon the state at time t and upon
the action a, performed at time t. This dependenceis described by the transition
tions may be probabilistic, so that T(x,a) may return a state sampled from a
Since there is only a ■nite number of states, we may de■ne ny(a) to be
the probability that performing action a in state x will transform x into state y.
For ■nite systems,T is fully speci■edby the numbers Pym) for all x, y, and a.
Finally, at each observation, the controller receives a reward that depends
upon the state and the action performed. The random variable denoting the
reward at time t is R,, and the actualrewardat time t is r,. That is, the reward
In the case of the pole-balancing problem, the
rewards might be de■nedas -1 at the time-step during which the pole falls, and
O at all other times. Rewards may be probabiliisric: the
sampled from a probability distribution determinedby x and a.
Usually, we need not consider the reward function itself, but only its
This completesthe de■nition of a Markov decision process.
Note that although both transitions and rewards may be probabilistic, they
depend only upon the current state and the current. action: there
dependence on previous states, actions, or rewards. This is the Markov property. This property is crucial: it means that the current state of the system'is
all the information that is needed to decide what action to take—knowledge of
the current state makes it unnecessaryto know about the system’s past.
It is important to note that the Markov propertiesfor transitions and for
rewards are not intrinsic properties of a real process: they are properties of the
state-spaceof the model of the real process. Any processcan .be modelled as a
processif the state-spaceis made detailedenough to ensure that a
' description of the current state captures those aSpectsof the world that are
relevant to predicting state-transitions and rewards.
A policy is a mapping from statesto actions—in other words, a policy is a
rule for deciding what to do given knowledge of the current state. A policy
should be de■ned over the entire state-space:the policy should specify what to
A policy that speci■es the same action each time a state is visited is
termed a stationary policy (Ross 1983). A policy that specifies that an action
be independently chosen from the same probability distribution over the possible actions each time a state isivisited is termed a stochastic policy. During
be neither stationlearning, the leamer’s behaviour will change, so that it
ary nor stochastic; however, the optimal policies that the learner seeks to construct will be stationary.
If a stochastic policy f is followed in state x, the probability that the next
It will be convenientto write the transitionprobability from x to y when following policy f as
be the next state, the reward, and the expected reward respectively when following policy f in Statex.
Broadly, the aim of the agent is to maximise the rewards it receives. The
agent does not merely wish to maximise the immediate reward in the current
state, but wishes to maximise the rewards it will receive over a period of future
There are three main methods of assessingfuture rewards that have been
studied: total reward; average reward; and total discounted reward. I will
assume that the agent seeks to maximise total discounted rewards, becausethis
is the simplest case. The learning methods can, however, be modi■ed for leam-
ing to maximise total rewards, or averagerewards under certain conditions.
The total discounted reward from time t is de■ned to be
where rk is the reward received at time k and y is a number between0 and 1
(usually slightly less than 1). y is termed.the
reward will be called the return. The effect of 7 is to determine the
con-present value of future rewards: if ”yis set to zero, a reward at time 2+1 is
sidered to be worth nothing at time t, and the return is the same as the immediate reward. If Y is set to be slightly less than one, then the expected return from
the current state will takeinto accountexpectedrewardsfor sometime into the
future. Nevertheless, for any value of 'Y strictly less than one, the value of
future reWards will eventually becomenegligible.
The aim of the leamer will be to construct a policy that is optimal in the
sense that, starting from any state, following the policy yields the maximum
possible expected return that can be achieved starting from that state. That is,
an optimal policy indicates the ‘best’ action to take in any possible situation in
the sense that continuing to follow the policy will lead to the highest possible
It may not be obvious that the highest possible expected return can be
a stationary policy, or even that there is a single policy
that will be optimal over all states. Nevertheless,it can be proved that for every
Markov decision process as described above, there will be a stationary optimal
policy, and the proof may be found in Ross (1983). The essential reason for
this is that, in a Markov process, a description of the current state contains all
information that is needed to decide what to do next; hence the same decision
will always be Optimal each time a state is visited.
It is not immediately obvious how to compute the Optimal policy, let alone
how to learn it. The problem is that some judicious actions now may enable
high rewardsto be achievedlater; eachof a sequenceof actions may be essential to achieving alreward, even though not all of the actions are followed by
immediate rewards. Conversely, in the pole-balancing problem, the cart and
pole may enter a ‘doomed’ state from which it is impossible to
from eventually falling—but the pole actually falls some time later. It would be
wrong to blame the decisions taken immediately before the pole fell, for these
decisions may have been the best that could be taken in the
actual mistake may have been made some time previously.
Because of this dif■culty of determining which decisions were right and
which were wrong, it may be dif■cult to decide what changes should be made
to a suboptimal policy. In arti■cial intelligence, this problem of assigning credit
or blame to one of a set of interacting decisions is known as the ‘credit
assignment problem’. For ef■cient learning, it is necessary to have some
ef■cient way of ■nding changes to a policy that improve it, because the sheer
numberof different possiblepolicies in any signi■cantproblem makes a strategy of policy optimisation by trial and error hopelesslyinef■cient.
Policy Optimisation by Dynamic Programming
Dynamic programming is a method of solving the credit-assignment problem in multi—stagedecision processes.The scope of dynamic programming is
often misrepresented in the computer science literature—the true variety of its
applications is perhaps best explained in Bellman and Dreyfus (1962) or
Dreyfus and Law (1977). The basic principle of dynamic programming is to
solve the credit assignment problem by constructing an evaluation function,
also known as a valuefunction or a return function, on the state-space.
In discussingMarkov decisionprocesses,it is necessaryto be able to refer
to random quantities such as ‘the state that results after starting at state x and
following policy f for ■ve time steps’. A notation for this is the following,
where x is a state,f is a policy, and n is a non-negative integer:
are the random variables denoting the state reached and the immediate reward
obtained, after starting at state x, and following policy f for n steps. Clearly,
as the state that results from performing action a in state x.
Chapter 4 — Policy Optimisation by Dynamic Programming
In a Markov decision process, the future evolution of the process—in par—
ticular, the expected return—depends only upon the current state and on the
policy that will be followed. ’If the process is in state x and the policy f is followed, the
Note that the value function could in principle be estimated by repeatedlysimulating the process under the policy f, starting from state x, and averaging the
discounted sums of the rewards that follow, the sums of rewards being taken
over a suf■cient period of time for Y‘ to become negligible. But becausevalue
discounting is exponential, Vf also satis■esthe following equation for all x:
For a ■nite-state problem, the evaluation function is known if its value is
known for each state. The evaluation function may, therefore, be speci■edby
Thus in a ■nite-state problem, if p and P are known, the evaluation function
calculation of the evaluation function for‘a policy is, therefore, Straightforward
2. Using the Evaluation Function in Improving a Sub-Optimal Policy
The point of constructing the evaluation function Vf for a policy f is that,
once the evaluation function is known, it
improve the policy f if it is sub-optimal, or else to establish that f is in fact
Chapter 4 — Policy Optimisation by Dynamic Programming
Suppose one wishes to know whether some proposed policy g will yield
than the existing policy f, for which the
function Vf is known. This question might be phrased as ‘Is g unif?’
One way of determining whether g is uniformly better
than f would be to compute Vg and then to compare V‘gwith Vf over the entire
state space. But calculating the evaluation function for a policy g is computationally expensive: to do this whole calculation for each proposed modi■cation
of the policy f would be extremely wasteful.
Asimpler method of comparing f and g using Vf only is as follows.
sider the expected returns from following policy g for one step, and then folrecommends
lowing policy f thereafter. Suppose that the policy g
state x, while policy f recommends action a. The
policy g for one step (Le. taking action b) and then following
This is much simpler to calculate than V for to calculate Qf (x,g(x)) it is only
necessaryto look one step aheadfrom state x, rather than calculating the whole
evaluation function of g. I will call the quantity Qf(x,a) the acrion-value‘ of
acrion a at state 1: under policy f. Note that Q (x,f(x)) = V}(x), by de■nition.
To allow for the possibility that g may be a stochastic policy, I will de■ne
That is, Q,(x,g) is the expected return from starting at x, following policy g for
one step, and then following policy f thereafter.
Chapter 4 — Policy Optimisation by Dynamic Programming
Action-values are useful for the following reason. Suppose that the
expCCtedreturn from performing one step of g and then switching to f is uniformly as good as or better than the expectedreturn from f itself, that is
for all states x. One can then argue inductively that g is uniformly as good as
or better thanf. Starting at any state x, it is (by assumption) better to follow g
for one step and then to follow f, than it is to start off by following f. However,
by the sameargument, it is better to follow g for one further step from the state
just reached. The same argument applies at the next state, and the next. Hence
it is always better to follow g than it is to follow f. The proof is given in detail
in Bellman and Dreyfus (1962), and in Ross (1983). The result is
Let f and g be policies, and let g be chosenso that
Then it follows that g is uniformly better than1“,Le.
The signi■cance of the policy improvement theorem is that it is possible
to find uniformly better policies thanf, if such exist, in a computationally
ef■cient way. If the starting policy is f, then an improved policy is found by
■rst calculating Vf, and then calculating the action-values
for each state x and each possible action a at x. A new policy f is de■nedby
choosing at each state the action with the largest action-value. That is,
Chapter 4 — Policy Optimisation by Dynamic Programming
According to the policy-improvement theorem, f
better than f. This process repeats: the evaluation function and action-values for
f' may be computed,and a new policy f’ obtained,andf" will be uniformly as
good as or betterthan f'. With a finite Markov decision process, this processof
policy improvement will terminate after a ■nite number of steps when the ■nal
In other words, no improvement can be found over f“ or V]. using the policy
still be sub-optimal? 'I'he'answer is no,
theorem, proved in e.g. Bellman and Dreyfus
funcLet a policy f“ have associated value function V*
for all x e S, then V“ and Q* are the unique, uniformly optimal value and
action-value functions respectively, and f“ is an optimal policy. The
optimal policy f“‘ is unique unless there are States at which there are
several actions with maximal action-value, in which case any policy that
recommends actions with maximal action-value according to Q*
Policy Optimisation by Dynamic Programming
As a consequence of these two theorems, the following algorithm is
guaranteed to ■nd the Optimalpolicy in a ■nite Markov decisionproblem:
f(x) := a such that Q (x,a) = max Qf(x,a)
This method of calculating an optimal policy is the policy-improvement algorithm. Note that the entire evaluation function has to be recalculated at each
stage, which is expensive. Even though the new evaluation function may be
similar to the old, there is no dramatic short cut for this calculation. There is,
however, another method of ■nding the optimal policy that avoids the repeated
calculation of the evaluation function. This method is known as value iteration.
■nding the optimal evaluation function and policy. The principle is to solve the
optimality equation directly for each of a sequence of ■nite-horizon problems.
horizon is made more distant, the evaluation function of the
■nite-horizonproblem convergesuniformly to the evaluationfunction for the
A ■nite-horizonproblem is a problem in which some ■nite numberof
actions are taken, which may have immediate rewards as in the in■nite horizon
problem, and then a ■nal reward is given; the ■nal reward dependsonly on the
Chapter 4 — Policy Optimisation by Dynamic Programming
Let V°(x) be the ■nal reward for state x. V0 is the optimal return after no
stages:if there is an initial estimate of the optimal evaluation function, then this
may be used as V0, or alternatively V0 may be an arbitrary guess. The only
restriction that needs to be placed on V0 is that it should be bounded: in ■nitestate problems, any V0 is necessarily bounded. Let V"(x) be the optimal
expected return achievable in n stages, starting in state x.
Once V0 has been choSen, it is then possible to calculate V1, V2, - - - as
over all states. This proof is given in Ross (1983), and it is generalised in
appendix 1 where is is used to prove the convergence of a learning method.
until the di■'erencesbetween V and V“1 are small for all x.
The method of value iteration that has just been described requires that,
after V0 has been de■ned, V1 is calculated for all states, then V2 is calculated
for all states using the values of V1, and then V3 is calculated using the values
of V2, and so on. This is a simple and ef■cient way to organise the calculation
Chapter 4 — Policy Optimisationby Dynamic Programming
in a computer:for instance,the valuesof V" and V"'1 may be heldin. two
arrays, and once V" has been computed, the values of V’“1 are no longer
needed,and the array used to hold them may be re-used for the values of V’“'1
as they are computed in turn. In addition to the computational simplicity, the
time-horizon argument is intuitively clear.
learning processesI will consider, the learner may not be
able to consider all the possible states in turn systematically, and so ■ll in the
values for a new time-horizon. The value iteration method need not be carried
out in a systematic way in which V0, V1, - - - are computed in sequence.Provided that the values of all states are updated often enough, the value iteration
method also converges if the values of individual states are updated in an arbitrary order. As with policy iteration, the computation may be less ef■cient if
the states are updated in arbitrary order, but it remains valid.
A different argument for the convergence of .the value-iterau'on method
runs as follows. At some intermediate stage in the calculation, let the approximate value function be U, and let the (as yet unknown) optimal value function
be V. Let M be the maximal absolute difference between U and V, that is
If some state y is chosen, the value of U0) may be updated according to
where U’Cy) is the updated estimate of the value of y. It is possible to show
If I U(x) - V(x) l S M for all states x, then for any state y
Chapter 4 —'-Policy Optimisation by Dynamic Programming
for some action a’, which is the optimal action with respect 'to U. Observe
Note that U' will not necessarily be uniformly more accurate than U. However,
since the maximal error of U’ is guaranteed to be geometrically smaller than
the maximal error of U, it follows that the value iteration method is guaranteed
to converge, and that the maximum error of the estimatedevaluation function is
guaranteed to decline geometrically, the rate of decline depending upon the
Chapter 4 — Policy Optimisation by Dynamic Programming
Note also that the updating of the estimated value function may be done
one state at a time, in any arbitrary order—it is not necessary to. perform the
updates systematically over the state-spaceas in the value-iteration algorithm.
The estimated value function will converge to the true value function as the
total number of updates tends to in■nity provided that all states.are updated
The Local Improvement Theorem has an immediate corollary:
This is a most useful result, for it yields an ef■cient method of determining whether some function U is a good approximation to the Optimal evaluation
function V. Given an approximatevalue function U, one can obtain an upper
This has been a very brief account of the principle of dynamic programming applied to Markov decision problems. The main point I wish to convey is
that the computation consists of three processes: computing the evaluation
Chapter 4 — Policy Optimisation by Dynamic Programming
function for the current policy; computing action-values with respect to the
current evaluation function; and improving the current policy by choosing
actions with optimal current action-values.These three procedures may be carried out repeatedly in succession,as in the policy-improvement method, or else
they may be carried out concurrently, state by state, as in
method. In either case, the policy and the evaluation function will eventually
converge to optimal solutions. In this Optimisation process, there are no local
improvements can be found at every step. Furthermore,
the minimum improvement theorem shows that convergence of either method is '
These results together paint a picture of an optimisation
benign as it is possible for an optimisation problem to be; and small, ■nite
problems are indeed benign. However, in practical problems, the state-space
may be extremely large, and it may be impossible ever to examine all parts
the state-space. Although it is not necessary to examine the whole state-space
to ■nd guaranteed improvements to the current policy, it is‘ necessaryto examine the entire state-space to be sure of ■nding the optimal policy. The
minimum improvement theorem states only that there is some state for which
an improvement of a certain size may be found, not that such improvements
can be found for any state. Hence, if it is not possible to
state space, it is not in general possible
An animal or agent chooseswhat actions to perform by doing some computations on internally stored information. The type of computation done and
the type of information used constitute the mode of control of behaviour.
Different modesof control,require different learning algorithms, so it is
modesof■control of behaviour before considering
In classifying modes of control, the ■rst distinction to make is between
rewards that would result from various courses of action, and modes in which
considers the effects of different courses of action, it may be said to use a
‘look-ahead’ mode of control, and if it considers only the
currentstate,it may be said to usea ‘primitive’ mode of control. Hierarchical
In controlling its actions by look-ahead, an agent uses an internal model of
its world to simulate various courses of action mentally before it performs one
of possible courses of action, and chooses
best. For example a hill-walker may need to consider how best to cross a
mountain stream with stepping stones without getting his feet wet. He may do '
Chapter 5 — Modes of Control of Behaviour
this by tracing out various possible routes from rock to rock across the stream,
and for each route he should consider how likely he is to fall in, taking into
account the distancesbetween the rocks, and how slippery they appear.
The method of deciding what to do by ‘look-ahead’ is important because
it is much used in'arti■cial intelligence. For example, chess-playing programs
decide what to do at each turn by tracing out many
sequencesof further moves, and considering the desirabilities of the positions
In abstract terms, the essential abilities that an agent must have to use
look-ahead control are: a transition model, a reward model, and an ability to
consider statesother than the current state. An agent with these abilities can in
principle compute the best course of action for any ■nite number of time-steps
may be laid out in the form of a tree, as illus-
trated as the upper tree in the diagram overleaf. Let us assume for the moment
that actions have deterministic effects, so that a unique state results from applybranch
ing any action. Each node represents a state, and each
action, leading from one state to another.
state in the diagram—is the current state. Paths from the root through the tree
represent possible sequencesof actions and states leading into the future. Each
action is labelled with the expected reward that would result from it.
If the tree is extended fully to a depth of n, so that it represents all possicourses
of action for n time-steps into the future, then it is possible to
the sequenceof moves that will lead to the greatest expected reward
steps.The path from root to leaf in the tree that hasmaxi-
mal total discounted reward represents the optimal sequence of actions. This
path may be found ef■ciently as follows. The nodes in the tree are labelled
The action at each state that is optimal in the look—aheadtree is marked in bold.
Chapter 5 — Modes of Control of Behaviour
with their values, the value of a node being the maximal possible expected
return obtainable by starting in that node and following a course of action
represented in the tree. The values may be computed ef■ciently by starting with
the leaves and working back to the root, labelling each node of the tree with its
value. The node labels for the previous tree are shown on the lower tree in the
diagram, assuming, for convenience, a discount factor of 0.5
The optimal path from each node—that is, the optimal path within the tree
of possibilities—is drawn as a thicker line. The node values are calculated as
follows. Leaf nodes are given zero value, since no actions for them are
represented in the tree. If x is an interior node of the tree then the estimated
where T(x,a) ranges over all the successor nodes of x as a varies.
With this mode of control, the action the agent chooses to perform is the
■rstaction on the path with maximalexpectedreturn. Once the agenthasdone
this, it chooses the next action in the same way as before, by extending a new
tree of possibilities to the same depth as before, and then recomputing the path
This mode of control is equivalentto following the policy that is optimal
over a time-horizonof n steps—thenth policy calculatedin the value-iteration
If the state-transitions are not deterministic, then the tree of possibilities
becomes much larger. This is becauseeach action may lead to any of a number
of possible states, so that at each level, many more possible transitions have to
be considered. An example of a (small) tree of possibilities for actions with
probabilistic effects is shown in the diagram overleaf. In this case, actions are
Each action may have several possible results, so that the size of the tree
grows rapidly as more actions are considered.
The actions need to be labelled both with the expected reward
and with the probability of each result; only one action has
Chapter 5 — Modes of Control of Behaviour
representedby multi-headed arrows, becauseeach action may lead to any one
of several states. Not only must each action be labelled with the expected
immediate reward—each possible transition between a state x and a successor
It is still possible to calculate the optimal course of action over the next It
moves, with the difference that future actions will depend on which states are
reached. With probabilistic actions, a ‘course of action’ itself has the form of a
tree, a tree that Speci■esa single action for each state reached, but which
branches to take account of the possibility of reaching different states.
The bestcourse of action may be found by a procedure that is similar to
that for the deterministic case. The value of a node is still defined as the maxi-
mal expectedretum that can be obtainedby startingfrom that node and following a courseof action representedin the tree,but V(x) is given by
That is, the agent must take into account all possible states that could
result when computing the expected return from performing the action.
It is only computationally possible to extend a tree of possible courses of
optimal total discounted rewards, then it must extend the tree of possibilities far
However, the number of nodes in the tree will in general grow exponentially with the depth of the tree. It is therefore usually impractical to consider
courses of action that extend for more than a short time into the future—the
number of possible sequencesof state transitions becomes far too large. This is
the so called ‘combinatorial explosion’ of arti■cial intelligence.
Chapter 5 — Modes of Control of Behaviour
If the agent can extend the tree of possible courses of action far enough
into the future, then it can compute the optimal policy and value function.
However, it is just this vast calculation that dynamic programming avoids, if
The amount of computation required for look-ahead control may be
greatly reduced if the value function, or even an approximate value function, is
evaluation function, then it may choose its
actions by considering a much smaller tree ofpossibilities
needed if the naive look-ahead method of the previous section were used.
Recall that in computing the values, the leaf states
in the naive look-ahead method. If, instead, the leaf nodes are given estimated
values, the values computed for interior nodes of the tree may be more accurate.
In fact, if an accurate optimal value function is available, the agent need
only look‘one step ahead to compute the optimal action to take. That is, the
agent selectsthe action to perform by ■nding an action for which
is maximal. Note that a policy determined in this way from V will be optimal,
becausethis is just the optimality condition of dynamic programming. Even if
the estimatedvalue function is not optimal, the size of tree neededto compute
an adequatepolicy may be substantially reduced.
Chapter 5 — Modes of Control of Behaviour
The special case of one-step look-ahead control is important because the
agent only needs to consider possible actions to take in the current state: the
agent needs to imagine what new states it might reach as a result of actions in
the current state, and it must estimate the values of these new states,but it need
not consider what actions to take in the new states.
'It is possible that there are examples of agents that can consider the conse'
quences of actions in the current state, but which cannot consider systematically
the positions that would be reached after playing various sequencesof moves:
the longer the sequence of moves, the more different the appearanceof the
board would become,and the more dif■cult it is
moves would be possible, and what the values of
Much research in arti■cial intelligence has been devoted to ■nding
methods of using an evaluation function to reduce the size of the tree of possi-
bilities that it is necessaryto constructto determinethe optimal aetion to take.
The formulation of the problem is, however, usually in terms of minimising
total distance to a goal‘state, rather than of maximising total discountedreward.
Pearl (1984) is a standard reference on this tepic.
In conclusion, it is only feasible for an agent to choose its actions using
The agent has accuratetransition and reward models.
The effects of actions are deterministic or nearly so.
Chapter 5 — Modes of Control of Behaviour
The number of alternative actions at each stage is small.
The agent seeks to ■nd an optimal course of action for only a few steps
The agent has enough time to consider the tree of possible courses of
If the agent has can compute a value function or an approximate value function, the amount of computation required may be greatly reduced, and the
method of look-ahead becomesmore feasible. The limiting case is that of onestep look-ahead, which requires an accurate value function.
In primitive control, the agent does not consider future states.
that primitive control methods are suitable for simple animals in stochastic
environments. Although higher animals are capable of considering the consequences of their actions, this certainly does not mean that they govern all of
environment is stochastic, or if the effects of actions are poorly understood or
dif■cult to predict, then it is likely that higher animals may also use these
‘ There are three basic types of primitive control: by policy, by action- '
values, and by value function alone. The amount of information that the agent
may need to store in order to represent any of these three functions may on
occasion be far smaller than the amount of information that would be neededto
representa transitionor rewardmodel.Furthermore,eachtype of primitive control may be learned, by methods that will be described in the following
Chapter 5 — Modes of Control of Behaviour
The most direct way for the agentto choosewhat actions to perform is for
it to store a policy f explicitly. In effect, the agent then has a ‘situation-action
rule’: in any state x it performs the actionf(x).
However, if an agent only has a representationof a policy, then it is not
able to compute values of states or action-values without considerable further
computation. As a result, ef■cient unsupervisedlearning of a policy alone may
be dif■cult, and some additional internal representation either of action-values
or of a value function may be helpful for learning.
3.2. Explicit Representation of Action Values
In choosing actions by one-step look-ahead with a value function, the
agent computes the action value for each possible action, and then chooses an
action with maximal action value. That is, the action value of an action is
and the agent chooses an action a for which Q(x,a)_is maximal. If the agent
were to store the valuesof the function Q explicitly, insteadof computingthem
by a one step look ahead,then it could choosethe action a for which the stored
value of Q(x,a) was maximal. This is ,pn'mitive control according to stored
' This type of primitive control has the advantage that the agent represents
the costs of choosing sub-optimal actions. If special circumstances arise, so that
exceptional rewards or penalties attach to some actions, then the agent may
choose sub-optimal actions with action values that are as high as possible.
Note that the agent does not need to store the actual values: any function
increasing in Q(x,a) at x will serve as well,
Chapter 5 — Modes of Control of Behaviour
becausethe agent just chooses the action with maximal Q(x,a). It is, therefore,
still possible to represent a policy using an inaccurate action-value function
which has the same maxima at each state as the value function.
3.3. Control Using a Value Function Alone,
Selfridge (1983) describes a mode of control of behaviour that he terms
run and twiddle. Loosely, this may be de■nedas
If things are improving, then carry on with the some action.
If things are getting worse, then try doing something—anything—«Ise.
Perhaps the classic example of control of this type is that of the motion of
cilia, and they can move in two ways: they may move roughly in a straight
line; or they can ‘tumble’ in place, so that they do not change position but they
do change direction. A bacterium alternatesbetweenthese two types of motion.
The bacterium seeks to move from areaswith low concentrations of nutrients to
areas with high concentrations.It does this by moving in the direction of
increasing concentration of nutrients. Too small to sense changes in concenn'ation along its length, the bacterium can neverthelesssensethe time variation of
concentration as it swims along in a straight line.
A bacterium will continue to swim in its straight-line mode as long as the
concentration of nutrients continues to increase. However, if the concentration
begins to fall, the bacterium will stop swimming in a straight line, and start
tumbling in place. After a short time, it will set off again, but in a new direction that is almost uncorrelated with the direction in which it was swimming
before. The result of this behaviour is that the bacteria tend to climb concentration gradients of desirable substances,and cluster around sources of nutrients.
Chapter 5 — Modes of Control of Behaviour
Although this is perhaps the most elegant example of run and twiddle
behaviour—and particularly impressive because it is so effective a strategy for
such a simple organism—run and twiddle may also be used by higher animals.
Selfn'dge (1983?)describesthe behaviour of moths following concentration gradients of pheromones to ■nd their mates as a form of ‘run and twiddle’ with
rules. But even humans may use run and twiddle
sometimes: who has not tried a form of ‘run and twiddle’ while, say, trying to
work the key. in a dif■cult lock, or in trying to ■y a steerablekite for the ■rst
time, when one does not know which actions affect the motion of the kite?
Run and twiddle is a suitable control strategy if the agent cannot predict the
effects of its actions, or, perhaps,even represent adequately the actions that it is
It is necessary to distinguish between run and twiddle control by immediand immediate rewards.
ate rewards from control according to a value function
Consider some hypothetical bacteria that control their movement in the manner
described above. A bacterium’s aim is to absorb as much nutrient as possible:
the amount of nutrient absorbedper unit time is the ‘immediate reward’ for that
time. If the bacterium decides whether to tumble or to continue to move in a
straight line on the basisof the changein the concentrationof nutrients,then it
is controlling its actions according to immediate rewards. In contrast, consider a
■y looking for rotten meat. The ■y could ■nd rotting meat by following concentration gradients of the smell of rotting meat, in a similar way to that in
which the bacteria followed the concentration gradients of nutrients. The meat,
however, is the reward—not the smell: the current smell may be used in the
de■nition of the current state, on which a value function may be de■ned.(In this
case, therefore, the state might be de■ned as the current smell.
Chapter 5 — Modes of Control of Behaviour
One method of run and twiddle control using a value function works as
follows. The value of the current state is an estimate of the expected return.
Suppose an action a is performed on a state x, resulting in a new state y and an
immediate reward r. Let the value function be V. Then the return expectedfrom
following the control policy at x is V(x). The return that results from performing a may be estimated as
The action is deemed successful if the resulting return is better than expected,
and it is deemed unsuccessfulif return is less than expected. That is, a is suc-
A control rule is ‘If the last action was successful, then repeat that action, oth-
This is a very simple method of control. More complex control rules are
possible. The essential feature of ‘run and twiddle’ control is that
The returns estimated using the evaluation function are used in choosing
As in action-value control, run and twiddle need not necessarily use the value
function itself: it may also use functions that are monotonically related to the
value function. Such a function of states might be called a desirability function.
An alternative description of these control methods is as desirability gradient
is a form of stochastic hill-climbing of a
desirability function. Many such control methods are possible.
Chapter5 — Modes of Control of Behaviour
A published example of an arti■cial learning system which chooses its
actions by a desirability-gradient method is that of Connell and Utgoff (1987),
who describe a learning system for the cart and pole problem
chapter 2. This problem is particularly simple in that there are only two possible actions in each state, so that it is suitable for desirability gradient control.
is not a value function. It is constructed by using ad
hoe heuristic rules to identify a certain number of ‘desirable' and ‘undesirable’
points in the state space during performance.The desirability function is constructed assigning a desirability of l to each desirable point, and of -1 to each
smoothly interpolating between the desirable and
undesirable points. The undesirable points are the states of the'system after a
are statesafter which balancingcontinued for at least 50 time steps, and which satisfy certain other conditions. In
Connell and Utgoff’s system, the controller continues to perform the same
action until the desirabilities of successivestatesstart to decline; when this hap-
pens,it switchesto the other action. Their methodwas highly effectivefor the
cart and pole problem, but it is of course based on an ad hoc method of constructing a desirability function.
It is possible to learn value functions for run and twiddle methods by
incremental dynamic programming; I 'do not know
methods for Ieaming desirability functions of other kinds. It is possible that
Connell and Utgoff’s method of identifying desirable and undesirablestates
during performance and then interpolatingbetween them can be applied more
Chapter 5 — Modes of Control of Behaviour
A mode of control is simply a method of choosing an action on the basis
of a knowledge of the current state: given the current state, one or more alternative actions are recommended for performance. There is, therefore, no reason
of control may not be used as alternatives to one another.
An agent may have alternative modes of control that it can use, or else it may
use different modes of control in different parts of the state space.
‘For example, an agent may be able to predict the effects of some actions,
but be unable to predict the effects of others. It may, therefore, evaluate some
actions with one-step look-ahead, and it compare these values with stored
This is an appropriate point to describe one type of learning which I am
not going to consider further. This is the learning of a fast mode of
using a slow mode.of control to provide training data. This type of learning is
analogousto the ‘imitate the teacher’learningconsideredin chapter2.
Suppose, for example, that an agent can use look-ahead to control its
actions, but that this mode of control is inconveniently slow, or that it conresources
activity. It would, therefore, be advantageousfor the agent to acquire a faster or
computationally less expensive form of control.
While using the slow mode of control, the learner
situation-action pairs that may be used as training data for learning a faster
mode of control. The situation-action pairs, laboriously calculated by lookahead, for example, may be used to learn situation-action rules inductively.
Learning of this type has been studied in arti■cial intelligence, by Mitchell,
Chapter 5 — Modes of Control of Behaviour
approach has been suggested by Laird. Rosenbloom, and
Newell (1986) who propose ,‘chunking’ as the general learning mechanism.
‘Chunking' is the process of cacheing action sequencesperformed, and then
treating these store" action sequences as single actions. Once an action
sequence has been .. -nd by look-ahead search, and successfully applied, the
state, generalisedif possible, and then stored, ready to be applied again in the same situation.
The particular method of chunking has the limitation that as more and
more action-sequences are stored in this way, the search for an appropriate
action sequence may become more and more lengthy, so that the actual speed
of the mode of control may sometimes get worse. An abstract model of this
phenomenon has recently been given by Shrager a1(1988).
Chunking appears to be a possible general learning mechanism—but is is
not plausible to claim that it is the general learning mechanism.
Learning a fast control mode using by (internal) observation of the performance of a slow control mode may have considerable importance in learning
many skills. In skill-leaming, slow closed-loop control is gradually altered to
There are, therefore, a number of possible modes of control of action, each
based on the use of a different type of internally representedknowledge. For
each control mode, there is at least one method of learning.
The possible modesof control of action can be divided according to
whether or not the agent can predict state-transitionsand rewards—thatis,
according to whether the agent can look ahead. From now on I will concentrate
Chapter 5 — Modes of Control of Behaviour
on those modes of control of action in which the agent cannot predict the
effects of its actions in the sense of predicting state-transitions or rewards.
0 by using an explicitly representedpolicy, and choosing the action
0 by using explicitly representedaction-evaluations, and choosing
o by following the gradient of a desirability function, by
preferentially performing those actions that currently
These modes of control are in a sensemore primitive than those in which the
agent uses an internal model of 'its world to look ahead. One of the main con-
tributions of this researchis to show that it is still possibleto optimise these
primitive modes of control through experience, without the agent ever needing
Learning in "which the agent uses a transition and a reward model will be
termed ‘model-based’ learning. There are essentially two possibilities: either the
learner knows the transition and reward models at the start, or else it acquires
1. Learning with Given Transition and Reward Models
Consider a learning problem in which an agent initially
but does not know the optimal value function or
policy. In other words, suppose the agent has all the information it needs to caloptimal
solution by one of the standard dynamic programming
not done so. This might not appear to be a learning
problem at all: in principle, the agent could use its initial knowledge to calcu-
latetheOptimalsolution. But this may not be possible.
One reasonis that the agentmay not havethe leisure or the computational
ability to consider all states systematically in carrying out one of the conventional optimisation methods. An animal might know the layout of its territory,
and, if located in any spot, it might know how to reach various nearby places:
however, it might be unable to use this knowledge to plan ahead becauseit
might be unable to consider alternative possible routes from a place different
from its current location. A speci■creason an agent might not be able to compute an optimal policy or course of action is that it may not be able to systematically consider alternative courses of action in states different from its
Chapter 6 — Model-Based Learning Methods
current state: the ability to use the transition and reward models might be tied
In other problems, it is completely impractical to compute the optimal policy at all, because the state-Spaceis too large. The classic examples of such
problems are board games, in which the rules of the game are easy to state but
is. hard to ■nd. The whole problem of learning to play a
game such as solitaire, draughts, or chessis not in understanding the rules but
In learning from experience, the agent will not be surprised by any
itrewards it receives, or by any state-transitions it observes, since
possessestransition and reward models. The new information
states that are visited. One method of improving an initial
a value-iteration operation at each state
visited. The value-iteration can be neatly combined with look-ahead control of
action, since many of the same computationsneed to be performed for both.
Let the agent’s approximate current value function be U, and suppose the
agent controls its actions by one-step look-ahead. If the current state is x, then
the action chosen by one-step look-ahead according to U will be an action a
among actions a,-possible in x; that is, an action with maximal estimated retum
according to U. But the value iteration at x is to set U(x) to be equal to the
maximum estimated return according to U, that is:
So the improvement of U using value-iteration can be done as a by-product of
Chapter 6 — Model-Based Learning Methods
If the agent performs a multi-step look-ahead search,so that it constructs a
large tree of possibilities, then it is also valid—indeed usually better—to combine many-step look-aheadcontrol with value iteration in the same way.
This type of ‘learning’ is just value—iteration
agent happens to_visit or to consider, rather than value iteration carried out systematically over the whole state Space.If the agent does not repeatedly visit or
consider all states, then the learning may not
function becauseU may remain perpetually in error on statesthat the agent
does not visit. There is also no guarantee that each value iteration will be an
actual improvement of U towards the optimal value function: if U is correct for
some state x but in error for the successors of x, then the
worsen U at x. But according to the local improvement theorem, U'(x) cannot
be in error by more than ‘Y times the maximal error of U. If there is a subsetof
the state-spacethat the agentcovers repeatedly, then the agent must develop an
optimal value function for the problem restricted to that set of states.
The classic implementation of this method of learning is Samuel’s (1963,
1967) program for playing checkers. This program re■ned its positionevaluation heuristic during play by what was essentially value-iteration. Two
One method was to cache certain board positions encountered together
with their values estimatedfrom a look-ahead search.The store of cached posi-
tions and valuescan be viewed as a partial function from states(board positions) to their estimatedvalues—apartial implementationof U. If a cached
position is encountered during a look-ahead search, the search need not go
beyond the cached position—the cached value may be used as an estimate of
Chapter 6 — Model-Based Learning Methods
the value of the look—aheadtree from the cached position, so that the effective
size of the look-ahead searchis increased. Note that the values associatedwith
the cached positions should in principle be updated periodically because the
look-ahead search used to compute the cached value did not take advantageof
subsequently: it is unclear whether Samuel’s
program did this.' Cacheing and the recomputation of cached values is thus a
form of value iteration. The effect of the cachcing may be described either in
terms of increasingthe effectivesize of look-aheadsearch,or equally in terms
of storing and improving an evaluation function by value iteration.
impractical to cache more than a tiny fraction of all positionsl The second
learning method that Samuel’s program used was method of developing and
improving a value function that could be applied to any position, not just to
cached positions. This was a parametrised function of certain features of board
positions, and the parameters could be altered by a gradient method to ■t the
function to revised values. The parameters were incrementally adjusted during
play according to a value-iteration method.
This method of learning is similar to methods I will consider later. The
danger in adjusting a parametrisedvalue function is that in changing the parameters, the value function changesfor many positions other than the current position. There is, therefore, the possibility that different adjustmentswill work in
opposite directions and the overall quality of the value function will deteriorate.
Samuel reports that he found that this method of adjustment of the evaluation
was far from reliable, and that occasional manual interventions were
A further danger of the value-iteration learning method is that in checkers,
where the payoff occurs at the end of the game only, values must be adjusted
Chapter 6 -— Model-Based Learning Methods
to be consistent over very long chains of moves. Although an approximate
value function might be locally near-consistent according to value-iteration, it
might nevertheless be largely wrong. This simply re■ects the fact that value
iteration is not always guaranteed to improve the value function at every stage.
of checkers did not use discounted rewards; in this case,
although value iteration would in principle be guaranteed to converge to the
optimal value function if the iteration could be carried out repeatedly over all
states, a single value-iteration, even over all states,
guaranteedto reduce the maximum error of the value function.
Samuel's program did not really need to play games of checkers to learn
to improve its evaluation function: in principle, the learning could have been
done' by performing value-iteration on an arbitrary collection of checkers posiencountered
tions, instead of doing it on the positions
program played. As far as I know, Samuel did not try the experiment of comparing learning from an arbitrary collection of positions to learning from the
positions encountered during play, and it seems probable that learning from
arbitrary positions would have been worse. On the other hand, if the arbitrary
positions were taken from games between human expert players, the learning
The reason why it may often be useful to perform value iteration at the
statesencounteredduring performance that many arbitrarily generatedstates
might never be reachedin actual performance.In chess,for example,only a
tiny proportion of random-con■gurationsof pieces on the board could plausibly
occur as game positions. Even if a chess-playerdeveloped an ability to choose
good movesfrom randompositions,this hard-wonskill might not be applicable
in actual play since the type of position encountered in games
In spite of the limitations I have mentioned, Samuel’s program worked
very well. In fact, it may still claim to be the most impressive ‘learning program’ produced in the ■eld of arti■cial intelligence, as it achieved near-expert
levels of performance at a non-trivial game. I have argued that both of the
that Samuel used may be regarded as forms of value‘iteration
applied at the states the program visits during play. In the case of checkers,this
type of learning cannot be guaranteed to improve the value function, but the
analysis in terms of incremental dynamic programming provides a framework
for explaining both the program’s successesand its limitations.
1.1. Learning With Adaptive Transition and Reward Models A different type of learning problem arises when the learner does not possess accurate transition and reward models initially, and the learning task is
both to learn transition and reward models and to optimise the value function.
A learner may have initial approximate transition and reward models, and
an initial approximate value function; it may improve both its models and its
The problem of improving the transition and reward models is a problem
of inductive inference. In a ■nite system, the most general way of inferring the,
transitionand reward modelsis to visit all statesrepeatedlyand to try out all
possibleactions repeatedlyin each state;it is then possibleto keep counts,for
each state-action pair, of the numbers of transitions to each other state, and to
record the rewards received. The relative frequency of each transition may be
used to estimate its probability, and the records of rewards may be used to estimate the expected reward as a function of state and action. The construction of
“the transition and reward models is a problem of system identi■cation, which
may be described as a problem of statistical estimation complicated by the need
Chapter 6 — Model-Based Learning Methods
to visit a suf■cient variety of statesto obtain the necessaryempirical data.
An obvious and straightforward approach to combining model estimation,
action, and learning is for an agent to maintain current estimated transition and
agent also maintains a current estimated value function, and the agent
current estimated models and its current estimated value function to choose its
actions by look-ahead. Because both the models and the value function may be
in error, the problem of choosing an appropriate look-ahead method in the early
However, as learning goes on, the transition and reward models will
become progressively more accurate, so that the learner’s policy and value
function approach optimality asymptotically.
Many self-tuning control problems are of this type. In‘ self-tuning control
theory, it is usual to assume that the structure of a model of the process is
known, and that what remains to be done is to estimate the values of a (relativelyvsmall) number of initially unknown parameters. .It is talcen for granted
that once the parameter values of the model have been estimated, an appropriimmediately—the computation of a polate policy to follow may be computed
icy or value function from the estimated model is not regarded as a part of the
adaptiveprocess.A commonassumptionis that an appropriatepolicy to follow
given uncertain estimates of the parameter values is a policy that would be
optimal if the estimated parameter values were correct; this is known as a ‘cer5
tainty equivalence’ assumption. If the estimation process is consistent so that
do eventually converge to their true values, then ‘certhe parameter
equivalencc control’ will ultimatel Y converge to an
Chapter 6 — Model-Based Learning Methods
policy, even if a certainty-equivalent policy is not optimal while the parameter
This method of empirical model identi■cation combined with certainty
equivalent control is conceptually simple, but there is a snag that may sometimes arise. The problem is that the early estimates of the model parameters
error, and if the agent (i.e. the controller) follows a certainty-
equivalent policy for the erroneous parameter estimates, then it may limit its
subsequent experience, so that the wrong parameter estimates are never
corrected, and performance never improves. To ensure that the learning agent
does obtain suf■ciently varied experience for it to be sure of estimating the
to perparameter values correctly eventually, it may be necessaryfor
form experiments as part of its learning strategy.
A number of papers have been published in the control literature on selftuning control of Markov decision processes, mainly considering the average
reward criterion rather than the discounted reward criterion. Mandl (1974)
proved that a certainty-equivalence approach to self-tuning control of ■nite
Markov decision processeswould converge under certain restrictive conditions.
Kumar and Becker (1982) criticise Mandl’s approach as requiring too restrictive
conditions, and they propose a method based upon intermittent experimentation,
with the intervals between experiments growing progressively longer. The
experiments they suggest consist of following the certainty equivalent control
policy for a randomly chosen set of parameter values until the starting state is
revisited (one of their requirements is that the chain should be recurrent for all
certainty equivalent policies). They prove that the average performance of their
self-tuning controller taken from the start of the run will tend to the optimal
possible value, although their controller will, of course, continue to behave
sub-Optimally during its intermittent experiments.
Chapter 6 — Model-Based Learning Methods
There are two differences between mainstream self-tuning control theory
and the learning methods I have described in this section. The ■rst. and most
important, is that I regard the computation of the optimal value function as a
part of the learning process, rather than as something which can be done instantaneously. The second difference is one of emphasis: most self~tuning control
theory is concerned with linear systemsor with non—linearsystems for which it
is assumed that the form of the model is known, and that the values of only a
relatively small number of parametersneed to be determined.»In the problems
I wish to consider, the uncertainty about the form of the model may be much
greater. In problems with continuous state-spaces, the models will be constructed from large families of explicitly representedfunctions, and it will also
be possible to consider other problems in which the state-spaceis discrete.
By ‘primitiv‘e’ learning, I mean learning in which the agent does not have
and does not estimate transition or reward models. Instead, the agent develops a
policy and evaluation function directly. In
optimal policy and value function without having to remember more than one
past observation, and without being able to predict the state-transitions or the
immediaterewards that result from its actions. Although such an agent has
only ‘primitive’ abilities, it may still be able to learn complex and effective pat.
may be described as incremental dynamic programming
by a Monte-Carlo method: the agent’s experience—the state-transitions and the
rewards that the agent observes—are used in place of transition and reward
learning, the agent does not perform mental
it has no internal models. The agent performs actual
Methods of primitive learning are described and discussed from section 3
to the end'of the chaptertthese are the main results of the thesis.
Primitive learning is important becauseit requires only simple computations and,because the transition and reward models are often dif■cult to construct and to represent if the environment is complex.
Transition models, in particular, may be complex. If actions do not have
deterministic effects—if performing an action may lead to any of a number of
possible states—then the transition model is a mapping from state-action pairs
to probability distributions over states. A large amount of information may need
to be stored to represent {his mapping, and a large number of experiments
might need to be performed to acquire it inductively.
For a ■nite Markov decision process, the reward model may have ISHAI
parameters, and the transition model may be larger with as many as
parameters, where lSl and |A| are the numbers of possible states and actions
respectively. In contrast, a value function requires
a stochastic policy requires at most ISllAl parameters. Thus, as
remarked, the practical applicability of the conventional approaches to policy
optimisation in ■nite Markov decision processesis severely limited byptheneed
transition model. Even if ‘ other methodsof representingthe
transition model are used, it will still often be the most complex data object in
the learning system, since it must represent a mapping from one
S x A into another large set—the set of probability distributions over S.
A more subtle reason why it may often be dif■cult to construct a suitable
completely—in constructing any model, it is
to represent. However, consider an agent that
how can the agent know which aspects of the world are relevant for constructing the Optimal policy, and which are not? If the agent models the world in
unnecessarydetail then it will waste resources. On the
models the world in too little detail, and so ignores some relevant aspectsof
the state of the world, then a policy that is optimal according to its model may
The trouble is that, in model-based learning, the agent in effect replaces
the real world with an internal model, and constructs an optimal policy by performing mental experiments withits internal model. If the agent considers the
model with the world and the optimality of its policy with
an adequate representation of the world for the purposes of constructing an
To determine what aspects of the world are relevant to the value of a
course of action, the agent must experiment in the world and keep track of the
returns that result from different states.The agent must
must choose to distinguish states according
to whether they lead to differing returns. As will become apparent, this is what
The learner's task is to ■nd an Optimal policy after trying out various possible sequences of actions, and observing the rewards it receives and the
changes of state that occur. Thus an episode of the leamer’s experience consists of a sequence of triples of states, actions, and rewards:
The xk are observedby the learner;the at are chosenby the learner;andthe r,‘
aremeasuresof whetherthe learneris achievingits goals.
For successful learning, the agent must obtain suf■cient experience. In a
■nite-stateproblem, the learner must try out each possible action in each state
repeatedly to be sure of ■nding the optimal policy.
The constraints on what experience the learner can easily obtain depend
learning problem. For example, solitaire player
in any position he chooses,and return to any previous position to study it, and
he may try out many different sequencesof moves from the same board position. A lion learning how to stalk and kill gazelles does not have this luxury:
if the lion charges too early so that the gazelle escapes,then it must start again
from the beginning, and ■nd another gazelle and stalk it, until it faces a similar
situation and it can try creeping just a little closer before starting to charge.
2. Methods of Estimating Values and Action-Values
All methods of primitive learning rely upon estimating values and action
values directly from experience. At all times, the agent will have a current policy: values and action values are estimated according to this policy. The ■rst
is the various methods of forming estimatesof
expected returns. In this section of the chapter, I will develop a notation for
describing a family of estimators of returns; this notation will be convenient for
the concisedescriptionof learningmethodsin later parts of the chapter.
The simplest way to estimate expected returns is just to keep track of the
sum of discountedrewardswhile following the policy. Let us de■netheactual
That is, the actual return r, is the actual sum of discounted rewards obtained
after time t. The optimal policy is that policy which, if followed in perpetuity,
will optimise the expected actual return from every state. For any given policy
f, Vf(x) is the expected value of the actual return that would be received after
starting in state x and following policy f thereafter.
Because‘y < 1, Y‘ will approach zero as n becomes large, and becauseall
rewards are assumed to be bounded, for each value of 7 there will be some
n after which the remaining part of the actual return will
be negligible. Hence the agent may calculate an acceptably accurate value of
the actual return from time t at time t+n—1.The n-step truncated return is
If the rewards are bounded, then the maximal difference between the truncated
return and the actual return is bounded, and tends to zero as the number of
Clearly, one method of estimating the value function for a given policy f
is to follow f and, for each state, to average its n-step truncated returns, for
There are three disadvantagesof this method of estimating the value func.
The value of the truncated return is only available to the agent after a time
delay of n steps. To calculate the actual return from each state visited, the
are necessary to obtain accurate estimates.
To use the truncated return from time t to estimate the value of state x,
according to policy f, the agent must continue to follow policy f for at
These problems may be avoided by using a different class of estimators; some
of these have been described by Sutton (1988), and he terms them ‘temporal
difference’ methods. These methods rely upon the agent maintaining a current
approximation;to the value function. This approximate value function is
denoted by U. The agent seeks to make U7approximate the value function
The n-steptruncatedreturnr,+yr,+1+ ~- ' + Y“ 1r,+,,.1does not take
into account the discounted rewards Y'rm, + Y'+1r,+,,+1+ - - - that would be
r,M + Y":+n+i+ - - ' can be approximated by the agent as U,+,,(x,+,,).This can
be used as a correctionfor the n-step truncated return. The corrected n-step
1"")E rt + Vt+1+ ' ' ‘ + YHrMn-l + T Ut+n(xl+n)
If U Wereequal to Vf then the corrected truncated returns would be unbiased
The reason that corrected truncated returns are useful estimators is that the
expected value of the corrected truncated return tends to be closer to Vf than U
is. Let K be de■ned as the maximum absolute error of U, that is
This might be called the error-reduction property of corrected truncated
V returns. For rm, the proof is a specialcaseof the local improvementtheorem
proved in the chapter on dynamic programming; for ri") the proof is essentially
the same. Note that E[r(")(x)] is not necessarilycloser to Vf (x) than U(x) is for
error of E[r"‘)(x)] is less than the maximum error of
One is not restricted to using ri") for just a single value of n: it is possible
to use weighted averagesof r(") for different values of n. These weighted averages of corrected truncated returns will still have the error-reduction property in
the following sense.If riw) is a weighted sum of corrected truncated returns
Note that Zyilwil S 1 provided that all the w; are between 0 and 1.
An importantcaseis to usea weightedsumin whichtheweightof ri") is
proportional to 7k” for some K between0 and 1: this weighted sum will be
denoted by r A. This estimator has been investigated by Sutton (1988); I will
What Sutton terms the TDO.) return from time t is
Because rl' is a weighted average of corrected truncated returns, it has the
error-reduction property. Note that U is time-indexed; in value estimation, U
may change slightly at each time step. However, the changesin U will be small
The expression above may be written recursively as
and if 7Lis set to 1, the expression for the TD(1) return is
‘ All these different estimators of the expected
what use are they? In particular, how should one choose the values 1.? If the
agent follows the policy Vf for long'periods of time, then should it not use the
obvious method of truncated returns for some suf■ciently large n? Although I
have not developed any rigorous argument, the following considerations should
The choice of valuesof it dependson a trade-off betweenbias and variance. If the values of U are close to those of V, then it is easy to show that the
variancewill be lowest for A = O, and highest for A = 1. However,low variance is obtained
truncating the sequenceof discounted returns and adding a
value of U as a correction. If these corrections are wrong, then the estimates
will be biased. The optimal choice of 1., therefore, dependsupon how close the
current values of U are to Vf. It seems plausible that, to estimate Vf by following f and observing rewards, the fastest method is to use A = l to start with,
and then reduce it to zero as U becomes more accurate.
have not done any quantitative analysis of this problem, but Sut-
ton (1988) reports some computational experiments for a related problem in
which A. was kept constant throughout estimation, and he found that the most
rapid convergence during the time course of his experiment was obtained with
2.2. Implementation Using Prediction Differences
Sutton (1988) de■nesthe prediction difference at time t as
The motivation for the term ‘prediction difference’ is that at time t the agent
might predict the return from time t to be U,( x, ); at time t+1 the agent has
more information to go on in predicting the return from time t, as it has
It may, therefore, make a new prediction of the return
from time t as r,+yU,(x,+1 ). The prediction difference is the difference
betweenthese two predictions. If U is equal to Vf for all states,then the
expected value of all prediction differences is zero. (The individual prediction
differences actually observed will not be zero if the process is random so that
Now, as Sutton suggests, the difference between the TDOQ return and the
If the learning factor is small, so that U is adjusted slowly, then the second
summation on the right hand side above will be small.
The usefulnessof this way of calculating r} —‘U,( x, )
terms of prediction differences is that the agent can calculate the prediction difference for time
k at time k+1—a delay of only one time-step. Furthermore, if U is close to V,
the average value of the prediction differences will be small. Now, the natural
way to use rl to improve the estimated value function U is to use an update
Thus the update rule can be implemented by, at each time step, adding
appropriate fractions of the current prediction difference to previously visited
One way of doing this is to maintain an ‘activity trace’ for each state
visited (Barto et al (1983) describe this as an ‘eligibility trace'—a trace of how
the estimated value of a state is to be modi■ed).
visited, the activity becomes high; the activity then declines gradually afterwards. The amount by which the estimated value of the state is adjusted is ac
times the current activity of the state times the current prediction difference.
Let the ‘activity’ of a state x at time t be C(x,t). The levels of activity of all
That is, l is added to the activity of the current state, and the activities of all
states decay exponentially with a time constant of 7?». This form of the algorithm
memory of a recent visit to a state might take the form of ‘traces of activity’ in
a distributed representation of the state. This method is
algorithm in Barto,_Sutton, and Anderson (1983).
The value updating rule suggestedby Sutton is
This rule may be most suitable for connectionist implementations of incremental updating of value functions, where it may be natural to update all statesat
eachtime step,but for implementationson sequentialcomputers,it is inef■cient
to update all states at each time step, and a more ef■cient way is to keep track
of the last It statesvisited for sufficiently large n. The size of n requiredwill
dependon the value of 3»used:the smallerthe valueof k, the smallerthe value
Sutton (1988) describes the use of thesetemporal difference estimators for
estimating value functions in Markov processes.with rewards. He considers a
more general formulation of the problem, in which the states of the Markov
process are described by linearly independent vectors, and the value function is
represented by a vector of adjustable weights. The value of any state is the
inner productof the statevector with the weight vector.
Sutton also suggests that these methods can be used in procedural learning:
connectionist-style implementation of this method of estimating expected
returns. Sutton (1988) suggests widespread application of the method in
behavioural learning. Sutton and Barto (1988) suggest a model of classical conditioning in terms of the animal learning to estimate the expected total
of a clasdiscounted unconditioned stimulus that it will receive. The
sical responsewas assumedto be proportional to this estimate. In this model,
during experience, rather than occurring at the end of
each ‘trial’; this is an advantage, since the animal might not divide its ■ow of
experience into ‘trials’ in the same way that the'experimenter would.-
2.3. Estimating Values and Action Values in a Markov Decision Process
So far, a range of methods have been introduced for estimating expected
returns in a Markov process with rewards. In a decision process, in which
different actions are possible at each state, there is a complication:it is only
' meaningful to estimate a return relative to some policy.
In the learning methods to be described, the agent seeks to estimate
some methods of learning the estimation policy is stochastic, in others, it is sta-
tionary. In some methods, the agent always follows its estimation policy; in
other methods, the agent may deviate from its estimation policy.
If the agent always follows its estimation policy, then the estimation problem is the same.as the previous problem for Markov processeswith rewards.
policy g, or else does not follow a consistent
Yes it can, by using different values of l at different time steps, with the
value of 2. depending on whether the current action is a policy action or not.
r? = r, + 7(1"Xt+l) Ul(xt+l) + 7114-1rill
=(1—l,+1)r§l) + EMU-1H2)"; 2) + lt+llt+2(l_)‘t+3)rs3) +
time I. Since 1"" 1sa weighted average corrected truncated returns, it has the
The point of this de■nitionis that by a suitablechoice
estimate the retums'under- one policy while behaving quite differently. Let the
agent divide the. steps of its experience into two classes: ‘policy steps’. and
‘experimental steps’. If the agent’sestimation policy is stationary—that is,
there is a unique policy action for each state—then all steps in which the agent
If the estimationpolicy is stochastic,so that the policy is to chooseactions
by. sampling from a. probability distribution at each state, then the question of
the agent can use is to accept or reject steps in such a way that, for each state,
of acceptanceof actions are kept approximatelythe sameas
the actionprobabilitiesaccordingto the policy.
the effects of experimental actions. If the
experimental action at time t, then it cannot use any of its experience after time
t in an estimate of the return from an action taken before time I. To do this, the
where 1* is a chosen value between 0 and 1. The essentialpoint is that if step
7» must be zero. if A is de■nedin this way, then I“?
be an estimate of Q,(x,, a,), uncontaminated by the effects of subsequent
To see this, consider the recursive de■nition of r"' in terms of r111:
If step 1+1 is experimental then km = 0 so that the term on the right hand
sidecontainingthe subsequentreturnrf‘ is multiplied by 0. ii“ is, therefore.an
from any further policy steps from t+1 onwards. If some step t+n is an experiment, 1”,, = O, and no further time steps contribute to the estimate. rA has the
is always taken into account in forming the estimate; But if a, is experimental,
then it is not possible to use r, to estimate Vf(x,) Let us
The agent:may estimate Vf(x,) only if the action at time t— a,
action: the agent cannot use the rewards following a non-policy step to estimate
the value of the state according to the policy.
methods used for variance reduction in Monte-Carlo
The problem of ■nding the optimal value function for a decision problem
is more complex than that 'of merely estimating values and returns: the optimal
policy is initially unknown, so that initially it is not possible to estimate the
optimal value function directly. Instead, leaming'is a process of improving a
One method of representing a policy and value function, as described in
values of Q at time t are denoted by Q,. Frbm its values of Q, the agent may
Q in U9 is to indicatethat U9 is calculated
method is onemep Q-learm‘ng, in which the values of Q are adjusted according
where a is a ‘leaming factor’, a small positive number. The values of Q for all
other combinations of x and a are left unchanged.
Note that the actionsthat the agentshouldtake are not Speci■ed.In fact,
the agent is free to take whatever actions it likes, but for it to be sure of ■nding
the optimal action value function eventually, it must tryout
Does this learning method work? It does indeed, because it is a form of
value iteration, one of the conventional dynamic programming algorithms
described in chapter 4. As is explained.in appendix 1, one-step giQ-learningvcan
Monte-Carlovalue iteration: QM is estimatedfrom
Appendix 1 presents a proof that this learning method does work for ■nite
Markov decision processes.The proof also shows that the learning method will
simpleidea,it hasnot, asfar asI know,beensuggested
it must be said that ■nite Markov decisionprocessesand stochasticdynamic
programming have been extensively studied for use in several different ■elds
this Montefor over thirty years,andit is unlikelythatnobodyhasconsidered
3.1. LearningAction ValuesusingGeneralEstimatesof Returns
,In fact, there is a family of methods of Q-learning, which use different
estimators of expected returns. The principle of all the methods is that the
values of Q are updated using estimates r? of the action values. As before, 7L,
With 1., de■nedin this way, returns are estimated only over sequencesof policy
of 7L,,which is used in the second demonstration
program in chapter 11, is' to make A, depend
estimated action value of the action performed and the estimated value of the
state.If Q,(x,,a,)is muchlessthanU,Q(x,)thenit shouldbesmall,whereas
Q,(x,,a,) is nearly aslarge as U‘,9(x,) then A should be nearly as large as N".
One method of achieving this is to calculate 9» thus:
where the parameter1] is non-negativereal number._If TI is zero, then A, = 7V“
for all t, whereas if 11is large then I. will be small if Q,(x, ,a,) is even slightly
Note that the value of the estimatedreturn r? will only become■available
at some later time. As a result, Q cannot be updated immediately according to
rf‘ —the update must be made later on, at time t+T, say:
using the method of predpotential problem
iction differences described in section 2.3 above. Theneis a
used in calculating the estimates of returns rA on which the updates.are based.
That is, changesin Q may affect r“, which then affect the changes in Q, and so
on: however, these effects are proportional to (12,and so will be negligible. for
Thisis a family of learningmethodsthat'appearplausible—do
not been able to show that they will always world-for the
following reason. The dif■culty is that if the values of Q differ from the
Optimal values Q*, then the implicit policy 1‘9 may differ from the optimal policy f*. The problem is that if Q is perturbed away from the optimal action
values Q* by only a small amount, so that the implicit policy fQ differs from
the optimal policy, the value function for f9 may differ from the optimal value
function V by a larger amount. That is, a small perturbation of Q away from
under some circumstances lead to instability. These instabilities nccd not necessarily occur, but I have notgbeenable to ■nd useful condithey
tations to be described later, the instabilities were not signi■cant.
policy has been represented by action-weights, or action probabilities, rather
than by action-values. The values of states were represented explicitly, but
action-values were not. ' Methods 90f this type have been described by
ten (1977), Barto, Sutton, and Anderson (1983) and Sutton (1984), Wheeler and
Narendra (1986),_and Anderson (1987), and also by Liepinset a1(1989), and.
Theseprevious methodsdivide naturallyinto two types: learning a policy
alone, and learning a policy together with a value function.
Theragent maintains and adjusts a representationof a policy during learn‘
ing. The policy is stochastic. Different actions may be performed on different
visits to the same state, so that the agent has the opportunity to compare the
effects of different actions; After an action has been performed, the agent uses
to form an estimate of the expected return from that
action that results from following the current policy. The agent then gradually
increasesthe probabilities of those actions that lead to high estimated returns,
of actions that lead to lower expected returns.
ing process can be viewed as a form of incremental, Monte-Carlo policy
Wheeler and Narendra(1986) proposean interesting method for the case
.of a ■nite Markov decision process in which the aim is to maximise long-run
reward, rather than expected discounted reward. Their estimateof the
expected return wasobtained by a recurrencemethod. If the state at time t is x,
and action a is performed, and the next time atvwhich the process returns to
as an unbiasedestimateof the expectedaveragereturn that would result from a
policy of performing a in x, and of following the current policy elsewhere. Of
course, the policy is changing slowly at all states during the estimation of
average returns, but Wheeler and Narendra give a proof that the system can be made to converge to an optimal policy with as high a probability as
They represent the policy directly as a set of action-probabilities at each
state. They assume that the Markov processes for all possible policies are
recurrent, and that the estimated returns lie between 0 and 1. They update the
state probabilities by the LR_I rule (Narendra and Thathachar (1974) or Laksh- '
mivarahan (1981)‘a're surveys of stochastic learning automata).
Widrow et al (1972) considered a sequential task (playing Blackjack) that
agent received reward depending on the outcome. The
maximised by an optimal policy was the expected terminal reward. The terminal reward actually obtained was used as the. estimate of expected terminal
reward for each action taken during each bidding sequence.The action probabilities were adjusted so that actions taken became more probable if the reward
was high, and the probabilities becamelower if the reward was low.
Barto, Sutton, and Anderson (1983) implemented a method of this type
with discounted returns for the pole-balancing problem as a method to compare with their adaptive heuristic critic algorithm. The results were disappointing. However, their formulation of the pole-balancing task had a larger number
of states than either blackjack bidding or the demonstration'problerns used by
Wheeler and Narendra—and it is possible that for some choice of very small
learning parameters, the method would work after a large number of training
runs. In addition, the formulation of the pole-balancingproblem used was not
actually a Markov process, since the cart and pole system moves deterministi~
cally in responseto actions, and the state-spacewas partitioned into quite
4.2. Learning a Policy with a Value Function
In the learning methods described by Witten (1977), Barto, Sutton, and
Anderson (1983) and Sutton (1984), and Anderson (1987),, the agent acquires
both a policy'and value function. The estimated value function is used to pro~
TDOL) returns that are of lower variance than observed returns, and which
a shorter time. A possible drawback, however, is that these
estimatesmay be biasedif the estimatedvaluefunctionhappensto be in error.
In Witten’s method,there are two concurrent adaptive processes: improvement of the policy and estimation of the value function for the current policy.
At each time-step, the value function is adjusted by
U,‘+1(x,)= (l—a)U,(x,) + 0t(r, + yU,(x,+1))
Witten proposes ”that the policy at each state Should be adjusted by an
unspeci■edlearning automaton, using r, + yU,(x,+1)as the reward. Witten
thatthelearningrate the valuefunctionshouldbemuch higher
than that for the policy, so that on the time scaleof the policy adjustment,the
mean value of U at each state can be assumedto be
thereis a uniqueoptimalvaluefunctionandclass
of optimalpolicies,buthe doesnot pointout anyconnection
and Witten’s method: the AHC algorithm uses TDO.) returns, rather than just
policy is the dzference between the TDO.) estimated return from the state x and
Sutton (1984) showed in a number of simulation
experiments that learning automata‘that used the difference between the reinforcement from the environment and an estimate of the expected reinforcement
under the current policy appearedto converge considerably faster than conventional learning automata.
I have not been able to prove that either of these policy learning methods
will necessarily work, nor have I been able to construct any problem for which,
using stochastic approximation rather than learning automata, the methods
would fail.VI think it very probable that it is possible to give conditions under
' which these methods could be guaranteed to work, but, the proof techniques
used for action-value estimation cannot be applied in this case, because the
action-values themselves are not represented.
The reason for the dif■culty is that there are two concurrent adaptive
processes—valueestimation and policy improvement—and there is a possibility
that these may interact during learning to prevent convergence.
4.3. Representing a Policy by a Single Action at Each State
A still simpler way to represent a policy
is then stored as a function from states to actions. In this
case, the agent does not need to determine which action has the highest
strength in the current state—it simply uses its stored policy to compute an
Note that this representationof the policy may require very
much less information than either the action strength or the action-value
This is a genuinely simpler learning method: if many actions
are possiblein each state, this methodof representingthe policy could have
considerable advantages.As far as I know, the following learning method has
, If the possible actions at each state themselves form a vector space, the
choice of action may be improved by a gradient method, in the following way.
Let the current policy be f. Suppose the agent is in some state x, and it
performs the action a, different from the policy action f(x). One method of
adjusting the policy is according to the rule
where B is a‘ learning factor, a small number greater than zero; The effect of
rule is thatif r? U,+7('x,') positive,sothat
return from performing a, was greater than the estimated'value of x,—in other
words, if d, was ‘unexpectedly good’—then the policy action for x, is adjustedtowards a,.
This learning rule may be extended to cover some stochastic policies. If
the stochastic policy is to perform an action
where L; is a (vector) random variable of zero mean, then the same adjustment
A modi■cationof this adjustmentrule is to have two learning factors [3"
and B‘. Ifr" > U,(x,), so that a, is betterthan expected,then [3* IS used'm the
adjustment rule; if the action is worse than expected, the learning factor [3‘ is
used. Both [5* and [3' are greater than 0, and,
t+1<xr) +B+(r{\ _ UHTC‘:)X aV-fa■xg) ) :if "14> Ur+7<irt)
will cause f(x) to change: experiments will, therefore, cause random perturba-
tions of ■x) about the optimum.If B“ is madesmall in comparison‘to[3* then
the random perturbations caused by sub-optimal experiments will be smaller
and more quickly corrected. This technique of asymmetric learning factors has
been widely used' for stochastic learning automata
and the review by Lakshmivarahan 1981). Widrow (1972) uses asymmetric
learning factors for analogous reasons,and reports a greatly increased speedof
The estimated value function, U may be modi■ed using the estimator for
f" (l—a)Ut+T(xt) fi' q[(1-7L.)U.+7<x:)+ KW? ]
Once again, A may be de■nedso that u" is an estimated return according to f.
Note that the processes‘of adjusting U and f ‘are quite separate, and
different estimators for the returns may be used in each.
Should this method work? One limitation to note immediately is that the
policy is updated by a gradient method at each state. If, therefore; at any time
there is more than one maximum in the action-value function at a state, then it
is possible for the policy action at that state to converge to a sub-optimal local
same instabilities as the action-strength methods of the previous section: I do
not know under what conditions it can be guaranteed to converge to the
Later on, however, I will describe an implementation of this learning method, and, for that example, it appears to work
Finally, perhaps the simplest learning method of all is'to learn a Value
functions alone. under desirability-gradient control of action. once again, a possible learning rule is
style. For this method to be valid, In, must be
policy. There is no adjustment to the policy
becausethe agent does not have one: U itself 15 used 1n the control of action.
Whether this learning process will converge will, I believe, depend on the particular desirability gradient control method used.
learning. For learning of this simple type, the disinstrumental
not. For the learning to be instrumental, it must only
following its desirabilityhappen while the agent is
leaming. Hence if the agent learns from temporal correlations that
occur when the agent is not following its control policy, then it is classical
This is a simple method of learning that could be used by' simple organisms, in the following way, Suppose‘that some ■y had the ability to follow
certain odour gradients in the air, using a desirability-gradient method. Some '
odours mightfbe innately attractive, and generally useful for the ■y to follow.
However, for any"particular habitat, there might be some other odours that it
would be useful for the ■y to learn about: these
odours would provide indications of the presence of the innately attractive odours. This type of
learning, therefore, might more conventionally be described as a form of classical conditioning of odours.
6. RestrictedExperienceand,Meta-StablePolicies
in chapter 4 on dynamic programming, one of the conclusions was that
there is only one optimal value function V, and the optimisation processwill
agent should repeatedly.tryiall possible actions in each possible state. But at
intermediate stages of learning, the agent’s policy may‘leadit to visit only
improve‘it. Metathe experiencenecessary to
stable policies occur frequently in everyday life: sitting in a corner at parties,
for example, 18a strategy that may prevent people from learning to enjoy them.
The agent always behavesaccording to its current estimation policy.
Each action the agent takes is used to adjust the estimated value function, and
the policy is stochastic so that the agent necessarilyexperiments with different
actions in following the policy. This approach of combining experiment with
value estimation might be called the ‘random policy trick’—a ‘trick’ becauseit
simpli■es the learning algorithm, in that it is not necessary to specify any
further what the agent should do. This ‘trick’, however, may severely limit the
The random policy trick restricts the amount of experimentation that
the later stagesof learning,and it governsthe natureof the
policy, the agent will perform fewer and fewer experimental actions, and
further improvement of the policy will become slow. Worse still, at intermedideterministic
ate stagesof learning the policy may be sub--optimal and almost
space. In such‘regionsof the state-space,
low level of experiment, so that changesin the policy in these regions will be
slow. To see how this may happen, consider the following simple example
problem, illustrated in the diagram overleaf.
The dots labelled with letters represent states, which are named A to F.
arrows from A to C and from A to B signify that, in state A, the agent has the
choice of two actions: to move to state C or to move to state B. Whenever the
reachesi'stateB it receives a‘ reward of 1, and whenever it reaches state F
it receives a reward of 2.I'The; agent wishes to ■nd a. policy that optimisesexpected return according to a discount factor of 0.9; otherwise, the agent
In this decision process, there are two loops of states that the agent can
traverse. The loop on the right, passing from state A to state B and back again,
gives low level of return. The loop on the left, from A to C to D to E to F to
A, yields a high reward at F. This path yields a high level of return provided
that the agent goes all the way round it. The optimal policy is to follow the
The optimal policy is to follow the loop ACDEFACDEFAQ.
either action with equal probability, then the probability
of AB. will initially increase, since F is seldom reached“,
If the agent must follow its estimation policy, then acquiring
the optimal policy may take a very long time.
path ACDEF repeatedly. However, at states C, D, and E the agent has a choice
of either continuing along the optimal path, or of returning to A with a small
' Consider the course of;learning according
ity. On this initialpolicy, the action value of going from A to B is higher than
the action value of going from A to C, becauseif
it is likely to return to A from either C, D, or E with no reward, whereas if it
goes from A to B, then it will always receive the sure but sub-optimal return.
Initially, therefore, any primitive learning system will adjust its estimation polbecomes
E, that it is much better to go to F than to go to A, and it will adjust its
estimate for E upwards. When the value estimate for E is high, the agent will
■nd, when it visits D, that'it obtains a better return from going to E than from‘
going to A, and ittwill adjust its policy accordingly. In this way,'the estimation
the Optimal policy and the values will be
However, during a substantial initial part of this learning process,
C, and the estimation policy will be adjusted to
its estimation policy, it will visit C less and less often. Provided that
the probability of going from ‘A to C does not decreasetoo quickly, the agent
will eventually'obtain enough experience of the path from C to recognise
of B, the probability ‘of going-from A to C will start to’rise. Ultimately, the
inde■nitely long time if the agent always follows its estimation policy
of learning dependscritdemonstratesis that the Speed
ically on the agent’5 pattern of experience during the course of
This small, arti■cial example is not a contrived or exceptional case: it is a
simple example of a general dif■culty. In learning, the agent needs to improve
its policy, to estimate expected returns, and, depending on the learning method,
an agent can only improve its current policy by
if it ■nds actions that yield higher returns. Yet, in most
of the learning methods, this requirement to experiment is at odds with the
requirement to follow the policy. The policy may become almost deterministic
while it is still suband further learning is then very
Not only must an agent try out a suf■cient
the possible states. In justifyingIItheir learning
Wheeler and Narendra (1986) and WittenI (1977) both need to assume
that the agentwill repeatedlyvisit all stateswhile following any policy: but this
assumptionis restrictive, and in many problemsit is simply not true. If, under
some policies, the agent does not visit certain areas of the state space, then the
agent cannot improve its policy in those areas. It is then possible for the
agent’s policy to be optimal for the decision problem restricted to the regions
of. state space that it frequently visits; in this case, policy improvements could
only be made in the areas that the agent does not visit. The learning system can
settleinto a metastablestate,at a sub-optimalpolicy. '
follow its estimationpolicy, then this problem
of meta-stability will be particularly acute, becausethe leamer’s behaviour is so
restricted. If the learner uses a method that allows it to make experiments, then
the learner can potentially get the experiencethat it needs to improve its policy,
: Another, perhaps clearer, example of meta-stability
‘secret tunnel’ problem in route ■nding. When I' drive to work, I travel South
across London and down to Surrey. I know the alternative routes, and in South
LondonandNorth Surrey,I can follow an optimalpolicy.My Ichoiceof route
is to some extent stochastic, but the areas I may visit during the journey are
quite limited, being restricted to a narrow ellipse surrounding the optimal route.
this route through experience: when I ■rst started making the jourI have
ney, I tried alternative routes over a wider area, but now I have narrowed it'
down. There are, therefore, vast areas of England that I never visit during my
journeys to work. It is possible that, if I were to travel a few miles North, I
might, if I turned down some unpromising side-street, find the entrance to a
secret tunnel beneath London, that would take me directly, unimpeded by
traf■c, to work. If such a tunnel existed, my present policy would be suboptimal and meta-stable:if I continued to follow it, I should never ■nd the
secret tunnel. To guarantee to ■nd the optimal route, I would have to make
experiments, so that eventually, over a long period of time, I would explore
every byway and eventually ■ndthe tunnel.
If such a tunnel existed, my current policy would be. meta-stable in the -
sensethat I would never ■ndthe tunnel during my current explorationsnear the
route I now take: however, if I ever found the tunnel (or if I was led to it), I
would change my current policy. Note that the optimal policy may be quite
different from the meta-stablepolicy even on those parts of the state space that
the meta-stablepolicy: for example,if the secret
tunnel existed, it would be better for me to turn back and head for the secret
tunnel from up to half of my current route to work.
Meta-stability is a general phenomenon in learning by animals, machines,
and people. The dif■culty may be overcome to some extent by allowing the
agent to perform actions inconsistent with its current policy.
agent to reach parts of the state space that it might not otherwise experience,
and to perform more experimental actions, and so be able to adapt its policy
more quickly. However, there is no general method for obtaining suitable experience in an ef■cient way. As I will argue in the next chapter, the experimental strategy is an important type of prior knowledge for a learning agent;
this chapter, I have discussed a number of different plausible learning
methods for Markov decision processes; but only for one of them—one-step
Q-learning—haveI beenable to give a proof (in appendix1) that it will converge to the optimal value function and policy. I have not been able to ■nd
for the other learningmethods:I believethat the
methods will generally workrbut I have not been able to ■nd conditions on the
policy duringleaming'that would ensurethat the methodsare
stable. The problem in‘proving that a learning method will converge is that, for
the proof to be useful, the behaviour of the agent must be allowed to vary
almost arbitrarily, and some patterns of behaviour might cause inStabilities that
Finally, the problem. of meta-stable policies affects all of these learning
methods. Unless the agent tries all aetions in all states, no primitive learning
method can be guaranteed to converge to an optimal solution. No local experimental strategy can succeed in eliminating this problem in general. One of the
roles of prior knowledge and of advice is to induce the agentuto try out useful
actions, and to visit states of high value.
It is notorioirsthat it is dif■cult to learnsomethingunlessthere is a sense
in which one almostknows it already.Empirical observationsneed to be combined with prior knowledge, and the practical usefulness of any learning
methods will be in proportion to how much use they can make of innate
knowledge,and to how easily innateknowledgecan be provided.
In the study of animal learning, one of the most important Questionsis that
of what innate knowledge animals have, of what form it is encoded in, and of
how it affects learning. In the introduction I arguedthat once membersof a
Speciesrely on acquiring some valuable skill by learning, there will be selective
advantagein that skill becoming innate. That is, animals that have some innate
characteristics that enable them to learn the skill faster or more reliably will
have an advantage over those individuals that do not learn the skill as fast or as
surely. What types of innate characteristics, or ‘knowledge’, might individuals
be provided with that would enable them to learn a behavioural strategy faster?
Any candidate theory of animal learning should provide a variety of ways in
which innate knowledge could be encodedand used.
1. Types of Innate Knowledge in Incremental Dynamic Programming
There are, perhaps, six types of innate ‘knowledge’ that may affect the
speedof learning byincremental dynamic programming. They are
Chapter8 — PossibleForms of InnateKnowledge
methods of representationand approximation of functions
initial policies, value functions, and models
o ‘ bounds and constraints on policies, value functions, and models
I will consider these types of innate characteristics in turn. ‘Innate knowledge’
is, perhaps, an inapposite term: ‘innate characteristic that affects learning’
Appropriatephysicalcapacitiesmay help learningin the sameway that
good tools can help carpentry. For example, if a bird has an appropriately
necessaryactions must be performed to open a seed might be‘much less than
with the expenditure of the same amount'of energy:l however, the bird with the
well adapted beak might be able to learn the skill
ways—and one of these ways is in making a
‘knowledge’ a little far, but the effect of a more suitable bill might be the same
So far, I haveusuallyimpliedthat the immediaterewardsthat an animal
receives for its actions are the primary reinforcers that directly affect its”
chances of survival: food, water, expenditure of energy, avoidance of danger,
However, an animal may have innate subjective reward systems to guide
its learning. Certainly, to predict an animal’s natural behaviour according to the
optimality argument, a behavioural ecologist will wish to argue that a certain
pattern of behaviour optimisesan animal’s eventual reproductive success: the
behaviour may do this by optimising some necessary intermediate criterion,
such as the rate of energy intake. However, although the
the animal itself may not be trying to optimise the behavioural ecologist’ s criterion. The animal may have innate "likes-and dislikes’, and it may award itself
and punishments according to this innate scheme, and it '
behave so as to maximise. the expected discounted sum of
these subjective rewards and punishments.
behaviour, but it might be possible to construct
As a hypotheticalexampleof how a subjectivereward system might aid
learning, supposethat there were animals constructedjust like the cart and pole
mentioned in chapter 2. Supposethat theseanimals experienced pain and possible injury whenever they fell over. Then the primary reinforcers that would
Chapter 8 — Possible Forms of Innate Knowledge
motivate the animalsto keep balancingare the primary punishmentsthat occur
when they fallover. However, these primary reinforcers would provide rather
slowly, and which awarded itself a subjective reward whenever it 'was in
position, would learn to balance more quicklyvand with
An innate subjective reward system, then, can
useful behavioural strategiesz-anencodingzthat
learning in an appropriate environment. Animals must have innate subjective
reward systems that enable them to recognise the primary reinforces—food,
injury, sex—when'they occur. The development of
1.3. Methods of Representation and Approximation of Functions
The aspectsof the stateof the environmentthat the agent
the rangeof actionsthat it can encode,and the typesof functional relationship
that it can construct, are all innate characteristics that can affect the rate of '
learning.In particular,the methodof approximationof functionsthat the
policy and value.function;incrementallywill affect the
generalisationsthat it makes statesthat it has not'previously encountered.
use, what class of function to construct to ■t
Chapter8 4 PossibleFormsof InnateKnowledge
1.4. Initial Policies, Value Functions, and Models
dynamic programming, the agent’s starting
described consist 'of the gradual modi■cation of an initial strategy. Clearly, if h
In the same way, if an agent were to construct a model of the world
i‘knowledge’■should carefully distinguished
1.5. Boundsand Constraintson Policies,ValueFunctions,and Models
The difference betweena constraintand an initial stateis that a constraint
applies throughout the whole course of learning, whereas the initial state is
and may eventually be entirely lost. A constraint, in
contrast, may in■uence any stage,or all stagesof learning.
One possible type of constraint that incremental»dynamic programming
can take advantage of is a constraint on the value function. For example, suppose one of the hypothetical pole-balanéing animals happened to fall over, or
erroneously, assign a low value to that state, which
very safe. One type of innate knowlege that could help to prevent such a misbounds on the value functhelearning is
states from ever falling below some limit value.
Chapter 8 — Possible Forms of Innate Knowledge
The strongest form of innate constraint on the policy is that the policy, or
a part of the policy, may be innately ■xed, 'so■that the agent has innate
A crucial difference between a constraint and an initial state is that a poor
initial state need not prevent an agent from attaining optimality in the end,
whereasif the agent has innate constraintsthat are inconsistent with the optimal.
policy and value function then it can never reach optimality.
Finally, an agent may have innate knowledge in the form of tendenciesto
experiment. These tendencies may be general, in the form of curiosity or the
Speci■cinnate knowledge may be encoded as speci■c tendencies to experiment. These tendencies are quite a different form of innate behaviour than an
innate policy: they are tendenciesonly, not constraints. These behavioural ten-
denciesmay lead the agent to performpotentially useful actions that it may
then incorporate into its policy, or they may simply be actions that lead the
agentto regionsof statespacewhereit will obtain useful experience.
A tendency to experiment might be very dif■cult to‘distinguish experimen-
tally from an innate behaviour—butfrom the point of view .of the learning
algorithm, an innate behaviour is anfunchangeableinheritance, while if experiments do not work the animal’s policy is unaffected, and the experiments may
eventually be abandoned if their estimated action value becomes too low.
startingvalues is delicate but important.A tendencyto experiment may affect
Ieaming at any stage, and may speedup or slow down
in principle affect the capacity to learn the Optimal policy m the end. A starting
policy or value function may greatly affect the course of learning, but its effect
will decline as time goes on. Innate constraintshave a permanenteffect, and
may aid learning in environments to which the constraints are appropriate, but
may prevent learning in inappropriate environments.
1.6.1. Particular Importance of Recommendations of Actions
Tendencies to experiment may be a particularly/important type of prior
knowledge for learning systems becausethey provide a particularly useful type
of inforrnation—information as to which actions it might be pro■table to perform in the current situation.
The point is that in primitive learning, an agent can
consequencesof an action by performing it. If many actions are possible in a
particular state, the agent must revisit the state many times and try out the
many different actions before it can be con■dentthat it knows which action is
best. This is a slow process if there are many possible actions, because an
animal may only observe the results of one action at a time.
A tendency to experiment might be encodedin the form of recommendations for action, perhaps as an ‘if-then rule':
Chapter 8 —- PossibleForms of Innate Knowledge
the current state has been found to be correlated
Innate knowledge in this form might be useful because an agent cannot help
about cbrrelan’ons of events'with statesthan it can"
collect about the e■‘ectsof actions in states, becausethe agent must revisit a
state many times before it can learn the effects of all actions, whereas on each
visit it may observe subsequentevents, and so improve its knowledge of correlations.
The crucial point is that it is possible for an agent to learn
types of correlation in parallel, but the agent can only perform one action at a
time. Thus prior information in the form of recommendationsfor action based
upon correlations of events with states could be an important type of innate
This discussionwould not be completewithout consideringhow an agent
could make use of observationsof other agentsor advicefrom a teacher.The
ability to experimentwithout corrupting the existing policy and value function
allows an agent to try out advice, or to imitate another agent during learning.
The actions advised, or the actions the agent performs while imitating another
agent more skilled than itself, may be performedas experiments,so that the
advice or imitation may be incorporatedinto the policy if it turnsout a success,
or it may be ignored and rejected if it fails.
This raisesthe possibility of automaticcontrollerswith a manualoverride
such that a human operator could take control and try out a strategy of his own
devising. The automatic controller could observe the actions of the human controller, and would analyse the action sequence as if it were a sequence of
experiments that the automatic controller had performed itself. If the human
operator managed to perform better than the automatic controller’s current policy during the period of manual control, the automatic controller coulduse its
observations to improve its internally representedpolicy and value function.
3. Restrictive and Advisory Innate Knowledge
In this chapter, I have described various ways in which innate ‘knowledge’
Such knowledge can be provided in a much greater variety of
than is possible in simpler learning processes, such as supervised learning in pattern
There are two points that I wish to emphasisemost. First, the main limitation of the primitive learning methods is likely to be the amount of experimenting that is required: the main use of prior knowledge may be to reduce the
amount of experiment neededby recommending actions. -'
Second, behaviour need not be either innate or leamed—it can be innazely
learned, in that the agent learns the behaviour with "the help of innate
knowledge. This innate knowledge maybe either restricn've
or advisory. Restrictive innate knowledge limits the rangeof behaviours that the agent can ever
learn, while advisory knowledge may‘affect the rate of learning, but it does not
in principle affect the range of behavioural strategies that the agent'could ulti-'
learning methods so far described are suitable only for small prob-
lems. The methods rely for their successon visiting a suf■cient variety of
states and trying out sufficient actions: if the state-Spaceor the number of possible actions is large,
then the learning methods will take an impractical amount
which learning is practical. The limit is not so much
time—the action replay argument of appendix 1 shows that
the amount of experience necessaryis proportional to the number of steps that
the actionvaluesof OptimalandIsubactionsisproportional to 0—7).
value the discount factor 7 should have I have done this becausethere is no '
interrupted, it must start foraging anew,
it left off' 1n the previous session.In this
Chapter 9 — Learning in a Hierarchy of Control
further n time steps provided that the foraging sessioncontinues until then, then
it should value that future’reward less than 'a reward of the same size that could
be obtained immediately, becausethe animal might not obtain the future reward
becausethe foraging period might be cut off by an interruption. If the probability of continuing the session until the next time step is y< 1, then the animal
It should, therefore, value a reward r that could be obtained n steps hence at
This optimality argument for seeking to optimise discounted rewardst‘is
a probability of interruption per unit time. However, the
discount factor per decision that the agent makes. Although
random interruptions, so that a time-based discount '
factor is appropriate, it may still need to make far too many decisions between
feasible using'the timebased discount factor.
policies relative to a short time horizon, even though a
longer time horizon (a larger discount factor) would be
their needs.But this is not a suf■cient argument.
be so great that it would not be plausible to explain
terms of incremental dynamic programming in the one decision process.
Chapter 9 — Learning in a Hierarchy of Control
One way of reducing the apparent complexity of behaviour is to analyse it
in terms of a hierarchy of strategies. In this chapter, I will present
of constructinghierarchiesof Markov decisionprocesses,and I will describe
how it is possible for the controller at each level of the hierarchy to learn
to a lower level of control; in reSponseto the top--level instruction, the lower
level of control may carry out one or more actions, and each action at the
lower level may be an instruction to a still lower level of control, and
the language I have been using, once an ‘action’ has been chosen by the top
level, it then forms part of the state for the lower level of control. The lower
level of control chooses actions on the basis of its current state, which consists
of the [action speci■ed by the top ’level and other information, such
‘ Supposeith‘at navigator and a helmsman are sailing ship
navigator knows the intended destination, and he is responsible for deciding
to get there. To do this, he plots possible routes on a chart, estimates
expenses and times, and, by look- ahead, he choosesan initial course to take.
The navigator then tells the helmsman which direction to
is to steer the ship in the direction chosen by the
in order to change the direction in which the ship is
has to perform a number of actions such as moving the
rudder and adjusting the sails, and that the helmsman may have to perform '
several such actions over a period of time to achieve a change of direction; the
Chapter 9 '— Learning in a Hierarchy of Control
heading of the ship may drift, so that the helmsman may have to take intermittent actions to keep the ship on the speci■edCourse., The navigator is the top
and the helmsman is the lower level controller.
This example can be described in temts of two interacting Markov decision processes, one at the top level, and one at the lower level. The navigator’s
The state consistsof the position of the ship, and the direction of the
The actions consist of sailing in a chosen direction for a certain (given)
There is a ■xed cost during each time interval, and a large reward is
received when the ship arrives at its port of destination.
The navigator makes decisions at ■xed intervals of time, and each decision is a
choice of a direction in which to sail. The information that is relevant to the
for this determines what directions are feasible, and how fast the ship will sail
on each possible course. If the intervals at which the navigator makes his decisions are much longer than the time neededifor the helmsman to changecourse,
deciding in which direction to sail next. One might imagine, for example, an
navigator in mid-ocean, who would take his position laboriously
he would retire to his cabin. If, onthe other hand, the ship were sailing in a
to give commands at short intervals, then he
current direction and Speedof the ship into his
Chapter9 -_—Leaming in a Hierarchy of Control
The helmsman’s environment may also be described as a Markov decision
The current stateconsistsof the currentdirection of the ship, the direction
of the wind, the con■gurationof the rudder and sails, and the navigator’s
The actions are to alter the con■gurationof the rudder and sails.
The rewards are given by the navigator: the navigator punishes the helmsman when the ship is going in the wrong direction and rewards him when
discounted rewards. The rewards may come at every time step—if the navigator is sitting on the ship’s bridge monitoring progress, for example. Even in
this case, the helmsman’s optimal strategy may sometimes be to' take a punishment now for more rewards later on: if the helmsman is changing course, it
may sometimes be more ef■cient to turn the ship through more than 180
degrees. Alternatively the rewards may come at longer intervals—for instance,
if the navigator emerges from his cabin‘at random intervals and either praises
or berates the helmsman according to the current heading of the ship. If both
the navigator and the helmsman follow optimal policies in their respectiveMarkov decision processes,the progress of the ship will besmooth.
As I have describedthis example,the navigator at the top level is using a
sophisticated model-based mode of control. The helmsman might be using a
an important point is that any mode of. control may be used at any level.
I For example, the navigator could be completely
■nd land by following the concentration gradient of ■otsam using the search
strategy of E. Coli. In this case the mode of control at the top level would be
that at the lower level. But in “general one might expect
model based methods to be used at the higher levels of control,
sons. First, at higher levels decisions are made at a lower rate, so that
time for more computation for each decision. Second, at higher levels, obser—
vational data on the effects of actions is acquired more slowly, so that primitive
learning methods are less suitable as they require more observational data.
The point of formulating a control hierarchy in this way is that the control
problem at each level is that of controlling a. Markov decision process. The
control system should be designed in such a way that simultaneous optimal performance at each level results in good performanceof the whole system.
The coupling between the levels is achievedby links between the decision
processes,rather than.by direct links betweenthe controllers. An action in the
higher process is a command for the lower process, which causes the lower
controller to pursue a certain policy. This command, however, does not go
directly to the lower controller—it affects the decision problem at the lower
First, the higher action causes a change in the state of the lower decision
process.This changein state affects the actionthat the lower controller takes.Seeond, the change in state does not have meaning by itself: the commands
from the higher controller are given meaning by alterations in the reward system for the lower process.
It is worth discussing one distinction that marks a striking difference
between natural and arti■cial systems: the difference between delegarory and
supervisory hierarchical control. In delegatory control, the top level passesa
command to the lower level, and the lower level then seeks to carry out the
command. The lower level then has.the responsibility for transferring control
level has successfully carried out the command, or perhaps when the lower
level has decided either that it has failed, or that the command is impossible to
complete. The point is that once the command has been given, the lower level
is in control until it passescontrol back to the top level. In this sense the toplevel delegatescontrol to the lower level.
control is natural for sequential programming languages.It has, however, the'drawback that the lower level may not correctly,
back tothe' top level: the top level of a program may (all too
behaviour, so common in sequential machines, is totally uncharacteristic of
In supervisory hierarchical control, the top
‘current top--level command is part of the state for the lower level; if the toplevel changesits command, the state for the lower level changes, and the lower
a different action. On the ship, the navigator may
his point of view—which is the current position—
and he may then issue new instructions when the ship reachesapprOpriatepositions. The navigator may change his current command at any time in response
navigator has given his command, and then back to the navigator
the navigator retains his initiative and supervises
While learning is possible,I believe,in both types of control
in arti■cial intelligence hierarchical control
' Chapter9 4- Learningin a Hierarchyof Control
concept of ‘■ow of control’ in a computer program is one that is alien to '
natural systems, which appear to be inherently concurrent.
the result will be that all conaollers will
decisionproblems at their levels of control: this. optimality at each level
from. global optimality of the entire control
In this section, I will consider how optimality at each level can be achieved by
addicess to control, learning of optimality at each level is possiblewith no
tional constraints;but two typesof dif■culty can arise.Both
result of the freedom of behaviour that learning by incremental dynamic probut
it need not always behave optimally. The. de■nition of optimality at each leVel
A hierarchical control system is optimal at
of this de■nition is that the effects of the
level action consists of changing the state and
the effect of a particular performance of a high level action is what
the low level controller actually doesin responseto the changedstate—butif
low level controllers are to be able to learn they must be free to experiment.
Chapter 9 — Learning in a Hierarchy of Control
for eachpossiblecommandof the navigator,the helmsmanhasacquireda
In terms of the ship example, the system is optimal at each level if
policy that is Optimal with respectto obtaining the navigator’s rewards.
the navigator’s policy is Optimal, provided that the helmsman always follows his optimal policy for the current command.
The simple vision is that eachlevel of the control hierarchy should be a Markov decision process; if this is so, then each controller may learn to control its
ownwdecision process independently of the other controllers. Butltherc are two
potential complicationswhich may destroythe Markov propertiesof the deci,
sion problems—lower controllers may experiment. instead of following
orders, and higher levels may directly and arbitrarily affect the state transitions
at the lower levels. Both of theseproblemsmay be solved.
3.1. Lower Controllers may Behave Sub-Optimally
If the helmsman does not follow his optimal policy for. the current cornmand, but persists in experimenting with different directions, then the navigator
may be disappointed if he follows a policy that would be optimal on the
assumption that the helmsman always obeyed orders. But the helmsmanneeds
to be free to experiment if he is to be able to learn; and if the helmsman
chooses a wild series of experiments so that he does not follow the same policy
each time the navigator issues a command, then he may prevent the navigator
from experiencing a Markov decision problem.
How can this dif■culty be overcome? One approach is to stipulate that lower levels of control must always follow their orders to the best of their ability; but this would make it more dif■cult for the lower levels to learn optimal
policies. A less severe restriction would be to allow the lower levels some lim-
ited freedom to experiment:this requirementmight be formalisedby allowing
’ Chapter 9 4 Learning in a Hierarchy of Control
the lower levels to pursue a stochastic policy with the probabilities dependent
upon currently estimated action values. But this
■avour, and does not permit the arbitrary experimentation or advice-following
that incremental dynamic programming should allow. Another possibility is that higher levels of control should be able to com-
mandlower levels of control to follow their currentestimationpolicies if necessary, but that they should sometimes allow the lower’levels some ‘time off" inwhich to improve their policies by experiment. In a hierarchy of several levels
of control, if the nth controller from the top wanted to learn by experiment,
thenall controllersaboveit would be switchedoff, and all controllersbelow it
A more ■exible idea would be for the lower controller-to pass a message
saying "I am experimenting now" to the higher controller whenever it deviated
from its policy by more than a certain amount.‘ When the upper controller
received such a message, it would simply not use the current stage of experience as one of its training observations. The lower controller would still have
the freedom to behave as it liked—but it would have to declare its experiments
to the controller above. This system has the virtue that experiments are
allowed at any time during normal performanceof the whole system. It would
also be compatible with allowing occasional directives from higher levels to
lower levels saying "Follow your optimal policies!". In this way the experience
of the higher controller may be made Markovian in that the effects of its
Chapter 9 — Learning in a Hierarchy of Control
3.2. Higher Controllers may give Non-Markovian Commands
The secondproblem is more subtle.Recall that the effect of a high level
action is to alter the state of the low level controller. If the low level controller
is simultaneously performing actions of its own, it might .‘conclude’ that the
state was'caused by its own actions, and not by the high level action.
It might indeed,becausethe high level controller has com-
plete freedom of action, and it is therefore possible that it might issue commands in such a way that the effects of actions in the lower decision process
appearednon-Markovian. The entire discussion of learning methods has relied
heavily upon the Markov property, and if state-transitions and rewards are
non-Markovian, then the learning algorithms can no longeribe guaranteed to
A simple solution to this problem is to require that when the higher level
controller initiates a new action, it should send a message to the lower controller saying "High, levelaction
force, the lower controller should not use observationsof its state-transitions,
actions, andrewards as training data in optimising its policy, because the high
level action changes may be arbin'ary and need not be Markovian with respect
Provided that high levelaction changesare occurring for only a small proportion of the time, this solution is valid, in that it prevents the lower level con-
troller from corrupting its learning process with non-Markovian data.
But there is a much more interesting solution. The lower level controller
seeks to protect. its learning from non-Markovian data that is caused by arbitrary actions of the higher level controller.
Just suppose,for a moment, that the high level actions were in fact proba- ‘
bilistically chosen, and dependent only upon the current lower level state
(which includes the current higher level action); and the currentlower level»
action.- If this were so, then the higher level actions would appear to be caused
by the lower level states and actions, and higher level action’s effect on the
state transitions and reward would appear to be caused by the lower level
actions. If the lower level actions and states drove the higher level controller in
this way, then the effects of the higher level actions would be Markovian with
respect to the lower level process, and there would be no need to protect the
lower level learning from’the effects of the higher level actions.
This odd situation in which the lower level processcontrolledthe higher
level process should not actually occurin
troller can observe correlations of the high level actions with its current state
and its choice of action, and the lower level controller may then use the correlations to predict the higher level actions from the lower level state and action.
If the higher level controller always chose the actions the lower level process
predicted, then the lower level process would seem to control the higher level '
controller, and all the lower level data could be used in learning.
If the lower level controller can make strong predictionsaboutthe higher
level actions, then it can, in effect, alter the definition of its Markov decision "
process so that the effects of lower level actions include the predicted higher I
level actions: to ensurethat its learning.datais Markovian, the lower level controller then only needs to reject those observations in'which the high level '
The lower level controller may, therefore,‘learn to‘ predict the upper
controller’s actions,and then optimise its policy' relative to its predictions. I
have not yet determined under what circumstances this method of learning
Chapter 9 — Learning in a Hierarchy of Control
Initially, neither the top-level controller—or ‘master’—nor‘ the lower level
controller—or ‘slave’—possessesan optimal policy, The ■rst stage of learning
is that the master may perform arbitrary actions, and the slave learns to obey
some of the master’s commands.During this ■rst stage, the master may possi—_
learn anything, since the slave does not know how to obey
the commands.Even if the masterdoesalter its policy, the slave’spolicieswill
be changing, so that there is no guarantee that the master’s initial, policy
changeswill produce any lasting improvements.
,In the second stage, the slave has learned to obey enough commandswell
enough for the master to start to improve its policy; as the master improves its
policy, it will tend to give the commands it ■nds useful more often than the
experience with the commandsthat the master ■nds useful, and its performance
Ultimately, the master and slave will acquire stable or meta-stablepolicies.
One reason why the slave’s policy may be meta-stable is that it may not gain '
enough experience to learn to obey commands that the master did not initially
■nd useful; the mastermay ceaseto give these commandsfrequently,so that
the slave continuesto havelittle experiencewith them.Note that the slavecannot correct this de■cit in its experience by initiating its own experiments,
becausethe state in which it may gain the relevant experience is that in which
the master has given theappropriate command; hence it is partly the master’s
responsibility to ensure that the slave receives a suf■cient variety of opportunities of experience.
Note also that the master’s policy may be optimal with respect to the
slave’s current abilities, but that the slave’s policy may be meta-stablein, that it
Chapter 9 — Learning in a Hierarchy of Control
cannot obey certain potentially useful commands. Thus, even though from the
master’s point of view, its policy is optimal, the masters policy may be meta5tableif the slave has not yet learned certain abilities.
As the master’s policy ceases to change rapidly, the slave may, if it has
the ability to do so, learn to predict the master's commands on the basis of its
own state (and, perhaps, action). In doing this, the slave is altering its own
decision problem, and it may adapt its policy accordingly. This changes the
way in which it obeys commands, and so causesthe master to adapt its policy ’
also.I do not lcnowunder what Conditionsthis mutualadaptationof masterand
slave is convergent, bene■cial, or stable. -’
In this chapter, I have presented informally a method of formulating
as coupled Markov decision problems, with autonomous controllers at each level. Provided that the controllers are able to com-
municatein some simple ways, it is in principlepossiblefor all the controllers
to converge to optimal policies for their own decision problems. This hierarchical learning provides a mechanism for developmental self-organisation of a
hierarchy of skills, using the method of incremental dynamic programming at .
each level. The rewards at each level may be determined both by the level
above, and also by the environment: at each level of the hierarchy, therefore,
the development of the skills may take account of environmental constraints.
There are also possibilities for mutual adaptation of the controllers in coupled decision
con■gurationsof coupled Markov decision problems besides a simple hierarchy. There are fascinating possibilities for furtherresearch here. Unfortunately
I have not yet implemented any examplesof thesehierarchical control systems.
I have only discussedproblemsin which there are.■nite numbers
This is because,in a ■niteproblem, each action value
in principle, problems with continuous sets of states or
need to contain a large number of adjustable parameters—one for each stateof
action pair, perhaps. At each step of learning, only one or a few
parametersmay be adjusted signi■cantly. Learning will,
approximation. The statea region in a Euclidean space, and
representedexplicitly, by storing values at each of
of locations over the state space. (Action
for each state, but, once the maximal action
With this method of representation, the amount of storage required is proportional to the number of points-one
For a high-dimensional state-space,one is'faced with the choice of
either having a coarse-grid which may represent the function inaccurately, or ‘
of grid points with a correspondingly large
and also for calculation, since repeated calculations are =
necessaryto obtain the value at each grid point."
dimensionality’ that limits the applicability of dynamic programming in practice
(Bellman and Dreyfus 1962). For learning by incremental dynamic program-
this corresponds roughly to the constraint on the amount of calculation in con-
The natural and straightforward representation of a function as values at
points on a grid gives, for most practical applications, a representationwith far
representtoo many independently adjustable parameters.
ing a function on a high dimensional space may often be ■nessedby choosing
a more compact form of representation than
on multi-dimensional .spacesin which standard
science techniquesare used to store the function economically. Howof representing functions that have recently attracted enor«
mous interest are ‘arti■cial neural networks'.
is a (usually simulated) collection of small
two-way.nected to many other cells; these connections may be
Each connection has a numeric weight, which may be
(Part of) the current state of each cell consiSts of a numeric ‘level of
other cells from which it receivesinputs, and upon the weight attachedto each
of these inputs. A cell’s level of activation may affect other cells through its
In a neural network, there are two forms of computation, short-term, and
long-term. The short term computationsare the changesin the levelsof activation of the cells that followla changein the inputs to the net. The long term
computations consist of slow adjustment of the weights to improve the agree-_
ment of the short term computationswith training examples.
methodsfor neural nets havebeenpublished; a numberof theseare described
in, for example, Rumelhart and McClelland (1986); a good, concise review of
various types of neural net is in Lippman‘ (1987).. What all of these methods
have in common is that they can all be viewed as devices for approximating
functions; the approximation is achieved by presenting the network with train-
ing exbmples,eachof which is an exampleof an input pairedwith the desired
corresponding output. ‘In the majority of the current work on ‘neural networks’,
it is assumedthat a neuralnetworkis a device that ‘learns’ to computea func_tionfrom training examples.
_Thepoint I want to make here is that it is widely assumedthat at the level
above individual neurons, the computational modules of the nervous system are
collections of neuronsthat learn functional mappingsfrom training examples.
This assumptionmay be right, half right, or plain wrong—nobodycan know,
yet. But, in primitive learning, the basic computational modules
are modulesto learnthe actionvalue function,or‘to learn a value
function and a policy, or a value function alone, from ‘training examples’ that
are extractedfrom»observationsof experience.In this rathershallow sense,the
learning methodsI have describedappearto be fully compatiblewith current
assumptions about neural computation. There is, however, a problem. All methods of representing functions that
require less storage than is required by grids must necessarily also have fewer
adjustable parameters, for it is the adjustable
The effect of this is that each adjustment dun'ng learning will affect a whole
region in state-space, and not the value stored for just one point. If an adjustment is made; for example, to a stored estimated value function as a result of
an observation of 'an action taken from point x, the adjustment will not just
value for point x but also the estimated values for many
other points y. Which other points have their estimated values adjusted, and
how, will depend on the particular approximation and adjustment method used.
In incremental dynamic programming, the overall effect of the adjustmentsmay
be either to speed up or else to hinder or prevent convergence.
So the problem is: what types of parameter estimation and function
representationwill work with the types of incremental adjustments that are provided by the learning methods? And for what classes of Markov decision
function estimation method be used?This is not
single problem as a ■eld of research.I have not attempted to tackle this prob-
lem theoretically,but I have implementeda demonstrationusing one particular
function approximation method—the ‘CMAC’.
2. Function Representation Using the CMAC -
The CMAC (or "CerebellarModelArticulationComputer’)was
by Albus (1981), partly as a speculative model of the mode of information
storagein the cerebellum,and partly as a practicallyuseful methodfor the
approximation of functions, for use in robot control. I
will consider'theCMAC purely as a‘methodfor the local approximation
for which it is undeniably useful, rather than as a neural model, in
which role its status is questionable. Considered
of approximating scalar or vector functions over a multi-dimensional space
using pre-defined step functions. The advantageof the method is computational
on a space: that 15, for each point x in the space,
space is partitioned into rectangular regions, or tiles. A tiling of the Spaceis a.
covering of the space by non-overlapping tiles, such that each point in the
spaceis contained in exactly one tile. The tiles may be denoted £1 t2, - ' - If ‘
then it is very easy to compute which tile any given pointx is in.
a numberof tilings of the space,so that eachpoint in the spaceis contained in a number of overlapping tiles.
an array u[l] , - ' - , u[n] of scalar or vector values, which are adjusted
0 - a hash function hash which mapseach tile to an
The tilings overlap, so that each point in the space will then be in exactly ten
tiles, one tile from each tiling. The method given by Albus, and the method I
have used, is to choose similar tilings, but to displace them relative to each
other, so that no two tilings have their tile boundaries in the same place. A
one region for each combination of overlapping tiles.
u[n] and their values are stored in an array. Now, each
tiling is in■nite in extent, so that it is not possible to store a
for each tile: the solution is that each tile is
■red when the CMAC is constructed. The effect is-that each array element—
each adjustable parameter—is associated with
pseudo-random fashion throughout the space. To visualise this
space, consider some parameter u[i]; if the tiles mapped to um were
coloured in, then on looking at the space the visual effect would be that a pattern of tiles would be coloured in, approximately one in n tiles being coloured,
' and this pattern would stretch as far as the eye could see. If the tiles mapped to
some other array element were also coloured in, a second such pattern of
coloured tiles would appear, and noutile would be coloured twice, since each
tile is mapped to exactly one array element. Let us represent the mapping from
tiles to array elements as hash. If the tile ti]-is mapped to the kth element of the
array, then this is denoted by hash(tij) = k.
To compute the function represented by the CMAC ata given point x in
the space, the following steps are carried out. (Assume that the CMAC has
The array elementscorresponding to each Vofthe tiles are determined.
The values of the array elements are then averaged, to yield the
Note once more that the array u may store either scalar or vector values. That
is, U(x) is equal to the averageof the array values associatedwith the tiles'that
contain 2:. This methodof storing a"function is an exampleof the methodof
as described by Sejnowski in Rumelhart and McClel-
The method of adjustment that Albus (1981) recommends, and which I
have used, is the follovving. Supposethat a ‘training example’ is presented,
consisting of a point x and a desired value v. The parametervalues of the
The value U(x), and the array'indices k1,
If [U(x) - ..v|< e ,l‘wherea is a predeterminedsmall positive constant,then
no further adjustment is made, otherwise adjust each array element according to
where a is a predeterminedpositive learning factor, less than 1. If the
desired values v are subject to noise, then at should be small.
The effect of this adjustment method can be more clearly seen if we consider
the set of tiles that contain a point x, as illustrated overleaf. The values associated with all these tiles are adjusted by, the same amount, so that the values for
points near x and contained in all 10 tiles will change by a(v-U(x)),
values for points further from x' and contained in only one of the tiles contain_
ing it will change by an amount -1%-(v-U(x)).
This notation is intended to indicate that there is a process of adjustment that
will affect U in a region surrounding x; the CMAC function after the adjust-
How large must the u array be? That is, how many adjustable parameters
are necessary? It will (usually) only be desired to approximate the function
This diagram shows the collection of CMAC squares that contain P.
Each CMAC square is associated with a stored value.
The value of the function at point P is the average of the values stored
When the CMAC value is adjusted for point P, the values stored for all
The value for point Q, which is contained in only two of the
CMAC squares, will only be slightly affected by the adjustment.
over a ■nite region A of the space. The number. of array elements needed
depends on the size of the region A, and on the variability of the function in
this region. If the array is too small, then many tiles within A will
represent the function accurately over A. If, on the
array element, then there are (almost) enough degrees of freedom to adjust the value
associatedwith each tile in A separately.
One practical advantageof the CMAC is that it is not necessaryto know
beforehand the region A of the spaceon which the approximation will be made.
The training examples presented de■ne the region over which the function is
the array u is large enough, the'CMAC can be viewed as a
local approximation method, in which a training‘example for a point x will
affect the values only of other points near x.‘ From a purely practical point of
view, the great advantageof using a CMAC is that, unlike most other proposed
‘arti■cial neural networks’, it works fast and reliably.-
chapter, Ibis/ill Edescribetwo demonstration implementations of
learning methods: The‘point of the demonstrations is ■rst to;show that the
problems,and secondto demonstratesome qualitative characteristicsof incremental dynamic programming. The ■rst demonstration is of a'synthetic ‘route- ’
■nding’ problem, and the second demonstration is analogous to a Skinner box
with various simple reinforcement schedules.
The programs have been written in Pascal, and run on a Sun 3 worksta-
tion. The randomnumbergenerator‘usedis not the systemsuppliedroutine, but
described in chapter 7, section 4.3, is that of learning
with policy improvementby a gradientmethodat
each state. This method is interesting becausethe policy representationis
‘ economical—onlyone action need be stored for eachstate, rather than the
storageof a value for eachstate-actionpair.
The aim of the ■rst demonstrationis to display the functioning of the
learning method as clearly as possible; it is Inotintendcd to model any particular real problem.
The state Spacechosen is a square in the} Euclidean plane, with sides of
enables the estimated value function and policy to be readily displayed: the
as a 3D plot, and the policy is displayed by drawing
the action vectors at each of a grid of points over the state space.
At any time, the current state of the agent is a point in this square.
At any point in the square, the set of possible actions is.the set of two
where Ax and Ay areAarbitrary real ~numbers.
arethe actions the agent can choose to perform. Broadly,_the effect of an action
taken at a point (x,y) is to move to a point (x + Ax, y + Ay), with the pro-
viso that the agent must always remain inside the square of the state-space.
If the agent makes a move that would take it out of the. square, it is
stopped at the edges Suppose the agent is at state (x,y), and it chooses to make
(x+ Ax, y + AY); for brevity, supposethis point is (x’,y’). The agent cannot
leavethe square:its movesare curtailedby the rule
The effect of this rule can be seen in the diagram overleaf.
There are two sources of rewards and penalties.
it is in a speci■edregion in the state space. In the
coming example there is a ‘target’—a small rectangle in the upper right hand
receivesa large reward at time : whateveractionit performs.
Second, each move the agent takes has an immediate cost. The cost of a
move is a function of its length. If the length is d, then the cost C(d) is
wherec1 and c2 are positive numbersthat are kept constantfor the durationof
The cost is a function of the length of the intended move, rather than of
the actual distance travelled. The reason for this choice is
greater than the cost of a shorter move in the same direction, so that the agent
moves, even though the actual moves might all be cut off at the boundary of
the statespace,so that the actualdistancestravelledwould be the same.
The cost function was chosen to be a combined quadratic and linear func—
tion of the length of the move for the following reasons. The quadratic term
was included to penalise long moves, so that to reach a target a long way away,
the optimal strategy wouldvbe to take a sequenceof short moves rather than a
single long move. If the cost of a move were simply a multiple of the square
of its length, the cost of small moves would be very small, and there would be
The agent cannot leave the square: if it attempts to move '
outside the square, its move is curtailed, as shown. ~ 3
no incentive for the agent to reduce the size of small moves on grounds of cost.
an incentive to reduce the size of even small moves, part of
cost was made proportional to the length of the move.
2. Demonstration of Action-Gradient Learning
The learning problem is set up as follows" The
placed in the ‘top right hand corner’ of the state space,and it is drawn
small rectangle in the policy diagrams. If the agent performs
the action the agent takes does not have its normal effect: instead, the
effect is that the agent moves to a randomly chosen position in the
. space. The landing position is chosen from a uniform distribution over the
The point of re-positioning the agent in a randomly chosen position in the
' space is to ensure that it gets experienceof all parts of the state space.
Both the policy f and the value function U are representedusing CMACs,
described in chapter 10. These functionsare,
in the state Space. It is not possible to assign new values
individually; instead, the CMACs must be adjusted according
All cells inpthe CMAC initially contain zero for the value function and the
zero vector for the policy, so that the initial policy of the agent is the zero vector throughout the state space,and the initial estimated value is also everywhere
’ Actions are chosen by calculating the policy action for the current position, and then adding a randomly generated deviation vector. That is, the
wheref,(x,) is‘ the policy action,and b, is a randomly generatedvector of uni~
formly distributed direction. The mean length of the deviation vectors b, is
denoted by b. The value of b is supplied to the program atthe start of a run.
The lengths of the deviation vectors b, are independent, and exponentially dis-
tributed,so that the probabilitydensityfor b havinglengthl is
l > 0. The point of adding the deviation vectors is that in
policy, the agent must perform actions that are different from those recommended by the policy: this mechanism for adding deviation vectors to policy
actions is just one simple way of achieving this. The exponential distribution
was chosen simply because it is convenient, but I have also implemented the
method with deviation vectors of constant length andrandom direction, and the
behaviour of the learning system is similar. There is no reason to supposethat
other distributions should not also give qualitatively similar behaviour, provided
that the frequency of large deviations is not too great.
Estimatedreturns are computedas follows. The value of )1 dependsupon
the length of b,, which is the randomvector addedto the policy action. That
where n is the rejection facror, as describedin chapter 7, section 3.1. n is kept
constant throughout a'learning run. The effeetrof n is to control the extent to
which large deviations are rejected in estimating'returns. If n is zero, then 7»,is
always equal to l, and all sequencesof actions count equally in estimating
The estimate of return used in policy-adaptation is r(t), and
return used in value-adaptation is u(t). r(t) and u(r) are de■ned in a way that is
similar, but; not identical, to r? and uf.
of x, a, and U(t) is an estimate of the value of x,. The difference between
them is that if a, deviates greatly from the policy action, 1, will be close to
zero, and u(t) will be close to the existing estimated value of x, , so it will not
causeany changein the estimatedvaluefunctionU. The de■nitionsare:
where M is the ‘learning period’ for and N is the ‘learning period’ for
point of these ‘Iearning periods’ is that they put an upper limit on the number
of previousstatesit is necessaryfor the agentto store. The estimatesr(:) and
u(t) can be computed at times r+M and z+N respectively. A second difference
betweenthe two methods of calculation was causedby
of calculation makes a signi■cant difference to the results.
The learning rules are, in the notation of chapter 10, page 144:
f1+M+l = adjust( ft+M x, [3(r(t)-U,+M(x, )) a, )
- where [3= [3*if r(r) 2 U,+M(x,), and B = B“ if r(t) < U,+M(x,).
The values of n, a, [3, M, and N, and of all the other parameters mentioned below, may be supplied to the program at the start of each run. ‘
The behaviour of the program is consistent and reliable over a wide range
I will show results from one run, and comment on them in
some detail; results obtained using other parameter values and
seeds are qualitatively similar. In informal experimentation with this and other
learning problems, I have found that the changes in the parameters have the
following qualitative effects, if the other parametersare kept the same:
If the size of the CMAC patches is increased, learning is faster, but the
■nal result becomesworse, since the representationis coarser.
If the discount factor is made too close to l, the policy 1may become
Using a low policy learning factor for negative prediction. differences
If the learning factors for the policy are made too large, the policy
The parametervaluesusedin the run for which results are presentedwere
Note that the returns used for adjusting actions are computed over only one
time step: the reason for this is that the adjustment of the actions is a gradient
descent process for ■nding a maximum, whereas the adjustment of the values is
a process of ■nding a mean value. The adjustment of actions, therefore,is more
sensitive to noise in the estimated returns than is the value adjustment.
The run for which results are presented was selected arbitrarily, and the
learning method will work over a wide range of parameter values. The main
limitation is that if the learningfactorfor actions'is too large, or if the discount
factor is chosen to be too close to 1, the process becomes unstable.
The CMAC table may appear large for such a simple problem: in fact, the
table may be reduced to little more than 1% of that size without undue degradation of the results.
At the start, the policy is to stay still everywhere, and the estimatedvalue
function is everywhere 0. The agent’s initial performance, therefore, takes the
form of a random walk in the state space, each step being just the deviation
vector b,. The program displays the path of the agent in the state space with
randomperformanceis overleaf.It is followed byl‘plots of the policy andvalue
function after 3, 10, 100, and 1000 successes,a ‘success’ being what happens
whenthe agent landsin the target. In the screenprintout, the stippledrectangle
in the top right hand corner of the graphics area is the target; the black disc is
the starting position of the agent c .1 that trial, and the sequence of jumps‘that
the agent has made is shown as a line. Note that the agent may jump over the
The course of learning may be described as follows. Since experimental
facactions are imperfectly separatedfrom policy actions—that is, the
tor is not equal to in■nity—the agent’s estimation policy is in effect stochastic.
It therefore finds that it accumulates costs during its initial random walk, and in
those parts of the state Space that it has visited during its random walk, the
value function is reduced below zero, and slight modi■cations may be made to
Eventually, the agent lands in the target. On the next move, it experiences
a large reward, and it lands in a randomly chosenpoint in the state space. The
initial effect is that a peak in U develops over the target. The ■rst actions that
receive rewards are those taken in theltarget,
becauseall actions in the tarare reinforced equally in all directions
actions that lead the agent to ‘climb the
so that the actions now turn in the direction of the gradient of U The U
‘mountain’ gradually spreads across the state-space, and the policy adjustments
in the target), the hill of U has Spread across most of the state Space. An '
at this stage. The estimated value function as
it spreadsis not a symmetrical cone with its peak over the target; instead, it is
shapedmore like. a mountain, with Spursradiating out from it. This is no
it is something that occurs consistently in
every run, often to a more marked extent than is visible in the results shown.
The reason that it happens can be seen from the diagram showing the policy
after 100 successes:this shows that there are ‘well wom paths’ leading into the
target. These are ‘paths’ in the state space along with the policy leads the agent
to the target relatively ef■ciently; because the policy is relatively ef■cient on
the paths, the value of U on the paths is relatively high. The policy adjustment
,method tends to direct the policy arrows towards regions of state space with
relatively high estimated value. The result is that the policy tends to change so
as to direct the-agent towards the existing well-wom paths. the effect of this is
to cause the agent to travel along the well worn paths still more often, so that
the policy on the well worn paths becomesyet more ef■cient.
After 1000 successes,the policy is to move towards the target from all
parts of the state space. I then introduced an ‘obstacle?into the problem. The
obstacle consisted of a rectangle in the middle of the state space, with the property that if the agent was in the obstacle, then the lengths 6f its moves were
reduced by a factor of 10, but the cost of a move remained the same. That is, if
and chose to perform an action that would normally cause it to travel a distance of 1 unit, then it would pay the cost of travelling 1 unit, while in fact travelling only 0.1 units. This penalty applies only to
The obstaclehas two effectson theproblem. first, it
the agent more to travel through the obstacle
should alter its policy so as to either travel round it or else to jump over it.
Note that the agent cannot ‘sense’ that it is in an obstacle: all it knows is
its position in the state space. Nor does the agent notice that it travels a shorter
distance than before when it is in the obstacle. All it notices is the difference in
Plots of the policy and value function 500 and 5000 successesafter"the
introduction of the obstacle are overleaf.
500 successes after the introduction of the obstacle, the'value function in
still travels straight into it. The estimated value function for states to the left. of
the obstacle is, as a result, negative. The value function for the extreme bottom
left hand corner still appearshigh, possibly because it has not yet been visited
'often enough to be reduced to a level appropriate to the changed problem:
But 5000 steps after the introducu'on of Vthe obstacle, the estimated value
of states at the bottom left has recoveredsomewhat, as the policy now leads the
agent around or over the obstaclefrom virtually all points in the state-space
outside'the obstacle. ‘ Within the obstacle itself, however,jthe value is still low
' becauseof the time taken and cost paid 1n escaping.
agent isrepeatedly placed' randomly and uniformly chosen positions
agent will, therefore, gain experience of
state space eventually It will, of course, visit those parts of the state spacenear
to the target more often than it visits the edges of the state Space,becausethe
4. A Learning Problem Analogousto Conditioning
The second computational demonstrationis analogous to a simple form of
The problem chosen is_that at each time step an agent may
chooseeither to perform or not to performa certain action (such as ‘pressinga
a key’). If the agent performs the action, there is a small
cost; if the agent does not perform it, the cost is zero. This is a reasonable
assumption, sinceperforming a responsesuch as a lever press,or a peck will
cost an animal more energy thansitting still. I will call the actionv‘pecking’.
in a Skinner box' with no prior knowledge
reinforcement schedules. The following schedulesof reinforcement
reward is available, the agent receives the reward only if it peeks, and
when the reward is unavailable, the agent does not receive a reward
whether it peeks or not. If a reward is made available at the rth time step,
the agent will receive it if it peeks at time t.
The fixed interval schedule may be described as follows: for the 11-1
steps following each reward the agent receives, the reward is unavailable.
On the nth step, the reward is made available again, and the reward
remains available until the agent peeks, and so receives it; the cycle then
begins again. The agent may, therefore,receive a reward on at most one in
’ - In this and subsequentschedules,the agentcannot perceivewhether
,. the reward is” available or not, and there is no
than the passageof time to show that a reward has become available.
If the agent receives a reward at time t, the reward becomes unavail-
able. On each subsequentstep, from time t+1"onwards, if the'reward is
reward has already become available, then it
peeks-and receives it. The effect of this schedule is that the lengths of the
unavailable are exponentially distriduring which the rewards
The agent receives a reward on every nth step, whatever it does.
On each step, the agent receives a reward with probability % whatI
The agent receives a reward on every nth peck, regardless of how
These are the objective problems that the agent may face. What remains to be
doneis to de■nethe agent’ssubjectiveproblem.
mentation of Q-learning for conditioning. I do not want to claim that the implementation that follows is a realistic model of animal learning in conditioning
experiments;animal learning is likely to be considerablymore sophisticated
It is reasonableto suppose that, since rewards come at infrequent intervals,
receiving a reward is a salient event for the agent. One dimensionof the
agent’s state space, therefore, is a measure of the elapsed time since the last
reward. The only other information the agent has is
the second dimension of the state-spaceis a measure of the’number of peeks
:An intuitively reasonable restriction is that the region of state space the
agent may .‘visit’ is bounded, since one would not expect an agent to distin-
Let the agent’s measure of elapsed time since the’last reward be m. At
time I, the value of m is m, If the agent receivesa reward at time t, then on
the time step t+l immediately following, mm is zero. On each subsequenttime
step until the next reward, m is increased, but it never gets larger than 1:
if the agent does not receive the reward at time I. That is, the difference
between )1: and 1 is reduced by a factor of 0.95 at each time step, until the
agent receives a reward, in which casem is reset to zero.
Let p be the measureof the amount of pecking since the last reward. p is
1-0.95(1—p,) if the agent peeks at time t
with vertices at (0,0), (1,0), and (1,1); the time axis is horizontal, and the peck
axis is vertical. Each time the agent receives a reward, it returns to the origin,
is receivedat time t, (0,0) is the stateat time 1+1.If the last
rewardwas at.time t, andthe timeis now t+i, andthe agenthasperformedj
peeks since the last reward, then the state is H
In the program, I have also provided that the state changesmay be subject
to small random perturbations. On each state transition, both m and of p may
have small randomquantitiesaddedto them: the randomquantitiesare drawn
independently from uniform distributions of zero mean, and the widths of the
uniform distributions may be set as required for m and for p separately. The
of this that m and p accumulate random error, and this reduces the
amount of information the agent retains about the past, since the. agent’s
‘memory’ is its current position in the state space. Adding small random perturbations ensures that the accuracy of this memory declines gradually with
For this problem, I have used the method of learning action values, both
to demonstrate it, and becausethe considerations in appendix 1 suggest that
one-step Q-learning should be reliable. For each action, the action-value function Q is approximated over the state space using a CMAC, just as the value
function was approximated in the previous demonstration.
The method of one-step Q-learning was used, as described in section 4 of
I‘ where <x, ,‘a,> is the state-actionpair for which Q, is adjusted. and is
i ' learning'factor. Only the action values
of a state is taken to be the maximum of the estimated action. values That is.
where UQ(x) is the estimatedvalueof the statex. Taking the maximumof estimates as an estimate of the maximum is dubious statistical practice, but the
argument in Appendix 1 shows that this method of estimating the value can be
used in a convergent learning algorithm, although it is likely that a less biased
the action valueswould give betterperformance.
In addition, results are presented for two demonstrations in which the
with r(:) de■ned as in the previous demonstration. and with the learning period
M equal to 3. In calculatingr(t), 7L,was calculatedaccordingto
The agent chooses its actions as follows. At each
the agent compecking; then withproputes the estimated action values of pecking and of net
bability 1-1:, it choosesthe action withhigher estimated value, and with probability 1:,
‘experiments’, and chooses the action with lower estimated value.
The probability of experimenting—ut—depends on the number of times that the
agent has previously visited the current region of state space. If the agent has
many times, the probability of experimenting will be
low; if the region near x, is relatively unexplored, the
The program keepstrack of thenumber of visits to eachpart of the state
space with a CMAC function Y—the occupancy—which is incremented by a
■xed amount at the current state on each time step. That is, Y is initially zero
where c is an amount kept constant during each run. That is, c is added to the
value of Y stored for each CMAC patch containing1:,
Y,(x)will be proportional to the numberof visits to points in statespacenearx
The occupancy Y is used to control both the probability of experiment It,
parts of state-space,1t and 0t should be small. In the
program, this is arrangedby calculating 1t and a at each time step according to
(10are parametersthat set the initial valuesof it
positive number called the rate parameter which determines how rapidly 1t and
occupancy. If T small, and will decline rapidly
with increasing occupancy; if T is large, they will decline more slowly. T, Ito,
and do are parametersthat are passedto the program at the start of a run.
Note that the probability of experiment does not depend on the difference
between the estimated action values for the current state. Better methods of
setting 1: might take into account both the current occupancy and the difference
In the demonstrations, I have selected the following parameter values as
the ‘default’ values; these are the values the parametershave unless it is
Costof apeckin FLCFI,vi, cvr schedules ' -o.5
That is, rewards are once every 10 time steps in the FI schedule; the reward is
set with probability 0.1 in the VI schedule; rewards are given every 10 time
steps in the CPI schedule; the chance of receiving a reward is 1 in 10 in the
CVI schedule;and 10 peeksare neededto obtain a rewardin the PR schedule.
cost to be less in the FR schedule because
ether schedulesa reward can be had for just one peck, or for no peeks at all.
patch size (below), and of the parameters
state transitions are arranged so that the highest levels of occupancy (near the origin) are approximately equal to the number.of trials.
All the results presented were obtained by performing 50 runs with the
same parameter values but starting with a different random seed in each run.
Each run consisted of a number of trials: a trial is a sequenceof actions starting from the state (0,0), and ending either when the agent receives a reward, or
else after 100 time steps, whichever comes soonest. The number of trials on
which learning took place in each run was 20T, so that when T=100, each run
had 2000 learning trials. The number of learning trials was made proportional
to T in this way so that values of It and onwould
end of runs with different values of the rate parameter T.
took place.Then the statetransitionsfor eachof twenty ‘test trials’ were
recorded. The point of this is that in these test trials, the agent followed the
estimate of the Optimal policy that it had constructed during the learning trials;
the agent adopts its ‘subjectively optimal’ strategy.
values used are presented as graphs of the average cumulative number of peeks
plotted against the elapsedtime since the previous reward. The averagecumulative number of peeks was obtained byaveraging the records of all 20 test trials
for all 50 runs for each combination of parametervvalues.The average cumulative number of pecks after n time-steps was calculated by averaging the cumunot
yet ended after n time steps; trilative number of peeks in all trials that had
als that ended before n time steps did not contribute to the average at n time
and the difference between the estimated action
values of pecking and of not pecking Q(x, peck) - Q(x, no peck) were also
recorded for a grid of points over the state Spaceat the end of the learning nials in each run. These data are presentedas contour plots over the state space.
Each contour plot depicts the average of 50 surfaces, one from each run with
The contour plots of the differences in eStimatedaction values enable one
the policies found: if the'value is negative, then not pecking is
the preferred action, whereas if the value is positive, then pecking is preferred.
Contour lines for negative values are dotted; those for positive values are solid;
this enables areas of state space in which different actions are preferred to be
For Peck Cost set at ‘0.125, ‘0.5, and ‘2
Plots 1 to 4 each show the averagedperformancefor all ■ve schedules
underdifferent conditions.Plot 1 showsperformancewith the “default paramePlot. shows performance
with the default parametersexcept that no
noise is addedto state transitions.Plot 3 showsperformancewith the default
parameters (and with noisy state transitions) except that the learning period M
is set to 3, and the rejection factor 11is set to zero. The parameters for plot 4
are the sameas those for plot 3, except that the rejection factor n is set to 20.
With the default parameters, performance is not optimal under any
schedule,but there is some adaptation to each.
striking differences from the ■rst plot. The increased
phenomenon is the very consistent learning in the ■xed interval and classical
identical ■nal performance. In both the FI and the CFI schedules, the agent
always peeked on the ■rst step, which is sub-optimal: I am at a loss to explain
this. After the ■rst step, the performance’was optimal under both schedules.
1 in all schedules except the classical variable
after 10 time stepsis over 9, so that in most trials the agentpeekscontinuously
until it receivesa reward, which is the optimal policy with these parameters
(see plot 11). In the VI schedule, the agent makes on average approximately 5
peeks in 15 time steps, which is the optimal policy (I will discuss the optimal
VI policy below). In both the FI and CFI schedules, there is a small
improvement over plot 1, in that there fewer peeks on the ■rst time step. Only
with the CVI schedule is performance approximately the sameas in plot 1.
A plausible explanationfor the superiority of the condition M=3 is that
underthis conditionvaluesmay be propagatedbackthroughthe statespacefaster than is posSible with M=l. That is, the ■ansfer of information about state
values in one-step Q is slower than in three-stepQ-learning.
, Plots Band 4 are very similar: the rejection factor n appearsto have little
effect on learning under any of the reinforcement schedules.One reason for this
is that the performance is nearly optimal on all except
variable interval schedule, so that there isperhaps little scope
for any effects of n on performance to emerge. However, performance in the
CVI schedule is similar for all four conditions;
,The pronounced F1 ‘scallop’ is apparently the result of state transition
noise making it dif■cult for the agent to judge the passageof time; under these
circumstances, the agent may mis-judge when to peek for the reward. In
the agent will sometimes peck too soon and
less than the cost of pecking one step too late, so we may
expect the agent to start pecking earlier than later, and this is what apparently
happens.This“ explanation is supported by the fact that there is no ‘scallop’ in
learninghad little effect. One reasonfor this may be that exceptfor the initial
the performance achieved may have been the best possible given the state
space and the level of state transition noise. No differences will be observed if
near-optimal performance is reachedunder all conditions.
The VI schedule deserves a special discussion for two reasons: ■rst, the
determination of _the optimal policy is more complex than for the other
schedules;and second,the statespaceusedin _theexperimentsis not adequate
for the VT schedule,’yet neverthelesspassableperformance has beenachieved.
First, what is the optimal policy under the VI reinforcement schedule? It
is easy to show that the form of the optimal policy is to peck periodically, with
schedule is a measure of how much time has elapsed since the last peck. This
information, however, is not carried in the state space used by the learning
algorithm. The learning algorithm cannot, therefore, necessarily be expected to
The optimal interval between peeks will depend on the probability that the
rewardis set duringeachtime step,the cost of a peckpandthe valueof 'a
reward. Let the reward be r, the cost of a peck be c, the probability of the
reward being set on each time step be p, and let the expected return on the ■rst
Plot 11 shows the theoretical values of r calculated according to this equation
are plotted againstn for r=10, p=0.1, 7:0. 95,
the default parameters, with peck-cost equal to -0.5, the optimal inter-peck
interval is 3 time steps,with an interval of 2 time steps being very nearly
expected return from a policy of pecking on every step is over
Plot 12 [shows the effect of different values of learning rate T; slower
Ieaming, with T=300, has the effect of leading the agent to peck on almost
every turn. Final performance appearsto get worse with increasing T.
- Plot 13 shows the average performance with three different values of the
cost of. a peck, and the rest of the parametersat their default values. Although
optimal performance is not achievedin any condition, in each case performance
is in the region of 25% below optimal; this is encouraging, considering that the
to -2—shows how this behaviour was achieved. The agent ‘tracks' the zero
contour; if it is on the left hand side of the zero contour, not pecking is pre-
takethe agenthorizontallyto theright at each
action, and the 'state transition takes the agent upwards and to the right, so that
‘it may find itself on the left hand side of "the zero-contour again. The resultis
that the agent will tend to peck intermittently. How can the agent develop such ‘
a policy, when the state-space does not make the necessary distinctions? The
answer, I believe, is that when the experiment-choice parameter has become
small,the agentdoesnot takejust anyrouteto a givenpointin statespacezlthc
agenthas a habitual path in the statespace,so that its position on the pathindicatesnot only the information carriedby the state itself, but also the fact that i
thepagentfollowed its current policy to get where it is.
interval schedule,the agent shouldoptimally not
peek at all, and it should merely accept the rewards that are intermittently given
to it. Why, then, does the agentin theseexperimentskeep on pecking,albeit at
a lower rate than in the operant' variable interval schedule? A
explanation—and one which is con■rmedby examining the contour plots of the
value function and the difference of action values below—is that the optimal
policy is extreme: not to peek at all. In the early stages of learning, the agent
will experiment by pecking on about half of all steps; the states in which it
receivesthe rewards will be states in which it has already peeked several times.
It is in these areas of state space that the value function will at ■rst increase;
and it is to these areas of state Spacethat the agent will tend to return. When
the agent experiments by pecking less, it enters regions of state space where the
estimated value function is still low; when this happens, the one-step ahead
estimate of eventual return is low, so that not pecking appears to be a bad
choice. Once a ‘path habit’ in state space has formed, the associated ridge in
the value function will move only slowly. The initial adventitious correlation of
past pecking with rewards will thus cause the agent to develop a ‘superstitionl
(Skinner 1948) that the pecking leads to the rewards. Superstitions of this type
are dif■cult for an agent to get rid of if it is restricted to making short term
The‘appearance of the performance for the FR schedule in plot 21 'is
deceptive because of the averaging: theFR curve is the result of averaging
many curves that start off with few peeks and ■nish with a peek on every time
step. The initial periods of procrastination under the FR schedule are of
different lengths, so that the average of the curves has a sigmoid shape.
The optimal policy in the FR scheduleis either not to peek at all, or else
So why should the agent procrastinate in this
way? A plausible explanation is that the agent starts off pecking in 50% of the
time'steps, so that it collects its reward after 10 peeks and,
steps. A peak inhthe‘value function starts to develop in this area of state
space—thispeak is visible in plot 22. A ‘mountain’ of the value function will
space; the ‘well worn path’ on which a ‘spur’ of the
develop will consist of procrastinating for a time, and then of
■rst path that consistently leads to rewards tends to
persist. With M=l, it takes a considerable time for the good
peeks there will be a large reward to pereolate back to the states corresponding
to one or two peeks. In these states, peeks appear to be costly, so that in the
initial stages of learning, the agent learns not to peek at early times, and
peek continuously at a later time in the trial. This can be seen'by examining
the zero contour in plot 23—the shape of the zero contour is similar to that for
The Q-Iearning algorithm is capable of acquiring near-optimal policies
under a variety of reinforcement schedules. The most signi■cant ■ndings
■rst that a longer learning period (M=3) gives a considerable improvement in
performance.Second, even though the state space is inadequate for the VI reinforcement schedule, the average performance is not grossly sub-optimal, and it
But, I believe, the main message of these results is that the
learning is strongly affected by adventitious correlations and by the initial policy and value function. The state-action combinations that happen to precede
rewards will tend to be repeated, whether the actions caused the reward
Becausethere is no way for the agent to ■nd out which actions
and which are unnecessaryother than by systematictrial and
agent’s actions will appear ‘superstitious’ to a more knowledgeable observer,
but the agent cannot avoid this during the early stagesof learning. -
should be close to 1 for small ‘Y, and k should be smaller at stateswith large Y.
in this way might combine the best features of one-step
A family of simple algorithms for associative learning has been described
systematically; these algorithms may be implemented as computer programs, I
and they can be applied to models of problems that have been given to animals.
The algorithmsthemselveshavebeendevelopedby an argument-from■rstprinciples, and not with the intention of explaining particular experimental results. They can be motivated, and in one 'case justi■ed, as forms of incremental
dynamic programming; and they can be viewed as methods for'optimising'short
to medium term averages of rewards, and costs that result from action. The
algorithms can also be viewed as direct implementations of associativelearning
according to the law of effect. They can be applied to a wide range of simple
tasks, and not just to the tasks that have been used in the study of animal learn-
provides a framework according to which a variety of learning algorithms'may '
be described in common terms. The framework makes clear both the potential
scope and also the limitations of the learning methods.
The ■rst area in which further work istneeded is in enabling autonomous
agentsto tune the values of the parametersfor their learning algorithms. If, for
example, an agent chooses a learning factor that is too small, then it will make
insuf■cient use of its experience; but if the learning factor is too large, the policy learning may become unstable. An autonomous learning agent needs to
have some method of choosing its suitable values for its learning parameters
A second area that requires further work is that of methods of representation and approximation of functions. I have used only one method—the
CMAC—which is crude and simple. An important question is that of what
other representations may be used in conjunction with these learning methods.
In particular, are there connectionist methods of learning functional mappings
Third, the life of even a simple animal cannot be treated”as a single Markov decision problem, because the learning problem becomes too complex. I
feel that the most interesting possibility for further work is that of linking Markov decision problems together, into hierarchies and other con■gurations, in
such a way that the control of eachdecisionprocesscan be learnedindividually. One attraction of this approach is that there are likely to be mathematical
methods_for studying the interactions between linked decision problems.
Although I have not yet implemented any such program, the approach appears
3. Computational Theories of Intelligence
As I said at the beginning of the introduction, the ■elds of cognitive science andarti■cial intelligence have been concernedmainly with analysing and
modelling the.abilities of humans, rather than those of animals. Many examples of human performance in performing various cognitivetasks have been
analysed, and computer programs have been constructedthat can solve some of
the same problems that people can solve, in apparently similar ways. Has this
led us much closer to an understanding of human intelligence?
I do not think so, because there is an insidious methodological problem
with this approach. The problem is that when researchers set out to study
higher thought processes, they set their subjects tasks to perform, and
then study their subjects' performance on those tasks.The result of the work is
a computational theory of how people perform those tasks. The computational
theory consists of a description of hypothetical ‘cognitive operations’ that people perform in accomplishing the tasks; ideally, this description should take the
program, in which certain blocks of code or de■ned procedures correspond to and perform the same role as the cognitive operations
that thesubjects perform. In other words, the ,computer‘program serves as a
descriptionof an algorithm that the subjectsfollow in performingthe task.
The methodological problem is that what has been achieved is to describe
the algorithm that the subjects have chosen to use: what has not been done is to
explain how the subjectsdecided.what mentalalgorithm to use.The mental
algorithm is merely a product of the subjects' intelligence.
A classic example is also one of the clearest: after'reading Newell and
Simon’s (1972) study of cryptarithmetic, one feels one has learned some useful
tips on how to solve cryptarithmetic problems, but very little about people. I
Newell and Simon devised an elegantnotationto describe‘whatpeople do in
subgoals, and searching, more or less systematically, for assignmentsof digits
to letters that are consistent with the sum givenf This is an excellent description of what people do while they are tackling a cryptan'thmetic problem, and i
the computational model can no doubt be used to predict which cryptarithmetic
problems should be easy and which should be' dif■cult, what errors people usually make, how long people should take to ■nd solutions on different problems,
and so on. But the computational theory has absolutely nothing to say about
why people choose to proceed in that way, or why they believe that the procedure, they use should lead to a'solution
describe what their subjectsdid, but they do not describe how or-why their subjects approached the problem in that way, which is a much more interesting
In later work, such as that of Laird, Rosenbloom, and Newell (1986), there
is an attemptto constructa systemthat is able to formulate.problem descriptions of this type by solving a higher level problemof the sameform. This is
perhaps the only signi■cant attempt so far in the ■eld of AI to give a general
theory of intelligence, but I do not think that the attempt comes close to sue-'problem formulations that the
■ndshave to be built into the program beforehand in a rather speci■cway.
Thebasicproblemin thecomputermodellingof humanthoughtis thatthe
researcher faces a dilemma: the model he or she constructs needs to be simple
enough to be supportedby the experimental evidence that
yet a model can only be plausible as an explanation if it can be presented‘as a
part of, or as a product of, some vastly more complex and unknown system.
But animal cognition is likely to be simpler, and that of very simple
animals is much simpler. In this thesis I have set out systematically a range of
ways in which simple agents might control their behaviour, and a range of
learning algorithms that such agents might use to optimise their behaviour
according to certain plausible criteria. It should be feasible to construct computer simulations of autonomous agents that learn to fend for themselves in
Indeed, Wilson (1987) has already attempted to do
this. Although considerable work would be needed, it seems by no means an
impossible objective to construct a relatively simple, autonomous,learning program that shows, in simulation, a similar rangeof
The agent learns using an initial estimateof Q, and data from experience,
which consists of observations of the form
which are reSpectively the state, the action taken at the state, the immediate
The observations are assumed to be independent observations of statetransitions and rewards in a Markov process. There is no assumption that the
observations come from a connected sequenceof actions—le
to be the same as y,,. The observations,therefore, can be collected from short
disconnected sequences of behaviour, and the choices of actions at states may
be arbitrary. The only constraint on the sequenceof observations is that there
must be suf■cient observations of each action at each state: this will be made
To show that the learning method converges,I will ■rst show how to con-
structa notional Markov decision processfrom the data: this notional decision
process is a kind of ‘action replay’ of the data. Next, I will show that the Q
values produced by the one-step Q-learning method after n training examples
have been used are the exact optimal action values for the start of the actionreplay process for n training examples. Finally, I will show that, as more data
is used, the optimalaction-value function at the start of the ‘action replay’ process converges to the optimal action-value function of the real process. For
brevity, I will refer to the action-replay process as ARP and to the real process
1. Using the Observations to Adjust Q During Q-Learning
Recall that the one-step Q-learning method is as follows. The initial values
of Q, before any adjustments have been made, are Qo(x,a) for each state x and
action a. After the nth observation has been used to update'Q, the values of Q
are written Q” . The estimated value of a state x at the nth stage.is
and it is used to calculate Q" from Qn_l by
-. The learning factor an may depend on x" and an. I will discuss the requirements that the learning factors must satisfy later.
The action replay processis 'a purely notional Markov decision process,
"which is usedas a proof device; This processis constructedprogressivelyfrom
M the sequence of observations. The ARP consists of layers of states, numbered
0, l, 2, - - ' n,'° ‘ - In each layer—in the kth layer, say—thereis a state
<x,k> in the ARP corresponding to each state x in the RP. That is, the state in
the kth layeriof the ARP Correspondingto state x in the RP is <x,k>. The kt
layer of states of the ARP is constructed when the kth observation is processed.
At the state <x,k>, the same actions are possible as at state x in the RP but
Actions in the ARP are de■ned1n the following way. The essential idea' 15
that an action in the ARP is a "replay of an
‘model’ of the RP, in which performing an action a in state x is
recalling an observation [x a ry] of performing a in x, and then using the
y as the simulated reward and new state respectively.
and one wishes to perform action a. To do this,
observations to ‘replay’; an observation is eligible for replaying if it was
before observation k and if it is of the form [x a .1, where the x
perform respectively. If the 1th observation, which is [x a'r, y, ], is eligible.
and is selected for replay, the reward obtained is r,, and the next state in the
cess: one may choose an action to perform— b,
for a suitable observation to replay. This
however, only the ■rst I observations are eligible. With each actiontaken in the ARP, the list of observations
that are eligible for replay becomesshorter, until ■nally there is no observation
eligible for replay. When there is no eligible action left, as must eventually
happen, a ■nal payoff is given, which is Qo(z,c), where z and c are the state
one is at and the action one is trying to perform when one runs out of actions
to replay. Starting atany statein the ARP, it is, therefore,only possibleto perform a ■nite number of actions before running out of observations to replay.
I have not yet explained exactly how observations are selected for replay.
this is done according to the following (randomised). algorithm, which I will
a in <x, k>, one ■rst checks whether the kth observation is eligible. If not, one
k—lth observation, then the k—Zth, and so on, until one ■nds a an
eligible observation—numberI, say.‘ Let a, be the learning factor that was
used when Q was adjusted when observation I was processed. Then, one ,takes
a randomchoice: with probability a, one ‘replays’
goesto (Yb 1-1> and one takesan immediatereward r,. If the randomdecision
is found, and then one repeats the random
choice. If one reaches the beginning of the list of observations, and then one
To put this another way, let the current state be <x,k> and let the action to
be performedbe a, andlet the eligible observationsbenumberedn, n2
of replaying observation n,- is ca ; the probability of
replaying observation fit-.4 is (l—ai)a;_l
.replaying any of the eligible actions is ’
If no eligible‘action is selectedfor replay, the ARP terminates,with a ■nal
Procedural instructions for performing action a in state <x,k> may be
terminate the ARP with an immediate reward of Q0(x,a),
The ARP is a decision process: if performing a in <x,k> leads to the state
<y,k-m>, then one may ’choose to perform any of the actions possible in the RP
at y in <y,k—m>. It is not possible to perform an in■nite sequence of actions in
what actions are chosen, if one starts at level k, each
action will lead to a new state at a lower level, until ■nally one reaches level 0
The return of a sequenceof replaysof observationsk1 ,‘ k2
where a_is the action chosenin the last state (”a
It is straightforwardto show that Q",de■nesthe optimal action valuesfor
ARP at stagen.' Let Quin; be the optimal action-valuefunction for the ARP;
is the optimal action value for action a at state<x,n> of
the ARP, andlet U*ARPbe the optimal valuefunction of the ARP.
' Ieaming rule, are the Optimal action values for the ARP
for all x, a. This implies-that Un_l(x) are the optimal values at the n-Ith
Recall that Q" is calculated from Qn_1in the following way. Except
for the value for the state-action pair x", an at stage )1, Q is unaltered by
the learning procedure, so that Qn(x,a) = Qn_l(x,a) for all x, a not equal to
Now, consider the nth stage of the ARP. For all x, a not equal to x," an
performing a in <x,n> in the ARP gives exactly the same results as per-
for all x, a not equal to x", an respectively.
Now, consider the Optimal action value of <x,,,n>, an in the ARP.
Performing a,I in <x,,,n> has the effect of
for all x, a, and n 2 0, which was to be proved. '
conditions will the optimal action values for the action replay
process at the nth stage converge to the optimal action values for the real processasn—->oo?
Suf■cientconditionsarethatfor eachstate-action
There is an in■nite number of observations of the form [x
The learning factors an forobservations of the form [‘x
a r,, yn ] are positive, decrease monotonically
with increasing :1, and tend to zero as n —>
The sum of the learningfactors0thfor observationsof the form [x a r,I y"
Note that it is' required that the learning factors decrease monotonically for
the form [ x a. .] for each x, a—the learning factors need not
7 that if one startsfrom the nth layer of the replay process,
processto any given degree of accuracy for any
■nite number of stages, provided that n is chosen to be large enough.
the real process in the sense that, for any k,
the state <x,k> 1n the replay process corresponds to the state in the real
process; and actions and rewards in the replay process correspond directly with
Let the depth of a state-actionpair d(<x,k>,' a) in the replay process be the
sum of the learning factors for all observationsof the form [ x a r, y; ] with
l S k. If one follows the procedure for performing a in <x,k> in the replay process, the probability of reaching <x,0> becomes arbitrarily small as d(<x,k>, a)
There are a ■nite number of state-action pairs, and according to the third
assumption above, the d(<x,n>, a) tends to in■nity as n tends to in■nity. For
any given D, and any given a, it is possible to choose :1 such that
Given any such n, his thenpossible to choose n’ such that
For any a and any D, it is possible to choose a sequence of values
n1 , n2 , n3 , - - - such that the depths of all state-action pairs increase by D
between each value of n in the sequence. It is, therefore, possible to choose an
a probability as close to one as desired, and such
factor a is so small that the transition probabilities
are,with a probability as close to 1 as desired,
to choosean n so largethat Q*ARPat the nth level
of the ARP is, with a probability as close to as desired,
action values of the RP; and this is what
Strategy Learning with Multi-Layer Connectionist Representations
Proceedings of the Fourth International Workshop on Machine Learning,
University of California at Irvine, Morgan Kaufman
Landmark Learning: An Illustration of Associative Search
Pattern-Recognising StochasticLearning Automata
IEEE Transactions on Systems,Man, and Cybernetics 15 pp360-374
Barto, A.G., Sutton, RS, and Anderson, CW. (1983)
Neuronlike Adaptive Elements that can Solve Dif■cult Learning Control
IEEE Trans. Systems,Man, and Cybernetics, Vol. SMC-l3, pp834-846
The Role of Primary Reinforcement and Overt Movements in AutoShaping of the Pigeon
Learning to Control a Dynamic Physical System
DynamicProgramming: Models and Applications
The Art and Theory ‘of Dynamic Programming
The Spandrcls of San Marco and the PanglossianParadigm: a Critique of
Proc. Roy. Soc. London (B) Vol. 205 pp581-598
PhD Thesis, University of California, Irvine, CA, 1983
Houston, A.I., Clark, C., McNamara, J.M., and Mangel, M. (1988)
Dynamic Models in Behavioural and Evolutionary Ecology
Nature, Vol. 332, No. 6159, pp 29-34, 1988
A Framework for the Functional Analysis of Behaviour
Behavioural and Brain Sciences11 1 pp117-163
LearningRules,Matching,andFrequencyDependence
Ioumal of Theoretical Biology, (1987), 126, 289-308
A Model Regulatory System: Bacterial Chemotaxis
Krebs, J.R., Kacelnik, A., Taylor, P. (1978)
,Test of Optimal Sampling by Foraging Great Tits
A New Family of Optimal Adaptive Controllers for Markov Chains
IEEE Trans. Automatic Control, Vol. AC-27, No. 1, February 1982
Laird, 1.,Rosenbloom,P., Newell, A. (1986)
Learning Algorithms: Theory and Applications
Liepins, G.E., Hilliard, MR, andPalmer,M. (1989)
Credit Assignment and Discovery in Classi■erSystems
An Introduction to Computing with Neural Nets
Advances in Applied Probability 6, pp40-60
Optimal PatchUse in a StochasticEnvironment
The Application of Statistical Decision Theory to Animal Behaviour
Journal of Theoretical Biology, (1980), 85, pp673-690
Journal of Theoretical Biology, (1985), 117, pp23l-249
Memory and the Ef■cient Use of Information
Journal of Theoretical Biology, (1987), 125, pp385-395
Machine Intelligence 2, pp137-152, Oliver and Boyd 1968
Michalski, R.S., Carbonell, 1.6., and Mitchell (Eds.) (1983)
Machine Learning: an Arti■cial Intelligence Approach
Mitchell, T.M., Utgoff, P., andBanerji,R. (1983)
Learning by Experimentation: Formulating and Generalising Plans from
in ‘Machine Learning: an Arti■cial Intelligence Approach’, Michalski,
R.S., Carbonell, J.G., and Mitchell, T.M. (eds.) Tioga Publishing Company
Narendra, KS, and Thathachar, M.A.L. (1974)
IEEE Trans. Systems, Man, and Cybernetics, Vol. SMC-4, No.4, July
Ef■cient Algorithms with Neural Network Behaviour
Heuristics: Intelligent SearchStrategiesfor Computer Problem Solving
Press,W.H., Flannery, B.P., Teukolsky, S.A., and Vetterling, W.T. (1986)
Rumelhart, D.E'., and McClelland, LL. (1986)
ParallelDistributedProcessing:Explorationsin the Microstructureof Cog—
Some Studies in Machine Learning Using the Game of Checkers
Reprinted in Feigenbaum and Feldman (1963)
Some Studies in Machine Learning Using the Game .of Checkers II
SomeThemesandPrimitivesin Ill-De■nedSystems
Selfridge,0.6., Rissland,E.L., andArbib, M.A. (Eds.)(1984)
Shrager, 1., H033, T., and Huberman, RA. (1988)
A Graph-Dynamic Model of the Power Law of Practice and the Problem
Temporal Credit Assignment in Reinforcement Learning
Doctoral Dissertation, Dept. of Computer and Information Science,
University of Massachusettsat Amherst, 1984
Learning to Predict by the Methods of Temporal Differences
A Temporal-Difference Model of Classical Conditioning
The Learning of World Models by Connectionist Networks
Proceedings of the Seventh Annual Conference of the Cognitive Science
Decentralised Learning in Finite Markov Chains
IEEE Trans. Automatic Control Vol. AC-31, No. 6, June 1986
Dynamic Programming, Markov Chains, and the Method of Successive
Ioumal of Mathematical Analysis and Applications 6, pp373-376, (1963)
Widrow, B., Gupta,N.K., andMaitra, S. (1973)
Punish/reward: Learning with a Critic in Adaptive Threshold SyStems
IEEE Transactions on Systems, Man, and Cybernetics, Vol. SMC-3,
An Adaptive Optimal Controller for Discrete-Time Markov Environments
Information and Control, 34, 286-295, 1977
