The principles of cognition are becoming increasingly important in the areas of signal
processing, communications, and control. In this ground-breaking book, Simon Haykin,
a pioneer in the field and an award-winning researcher, educator, and author, sets out the
fundamental ideas of cognitive dynamic systems. Weaving together the various branches
of study involved, he demonstrates the power of cognitive information processing and
highlights a range of future research directions.
The book begins with a discussion of the core topic, cognition, dealing in particular
with the perception–action cycle. Then, the foundational topics, power spectrum estimation for sensing the environment, Bayesian filtering for environmental state estimation,
and dynamic programming for action in the environment, are discussed. Building on
these foundations, detailed coverage of two important applications of cognition, cognitive radar and cognitive radio, is presented.
Blending theory and practice, this insightful book is aimed at all graduate students
and researchers looking for a thorough grounding in this fascinating field.
Simon Haykin is the Director of the Cognitive Systems Laboratory at McMaster University, Canada. He is a pioneer in adaptive signal processing theory and applications
in radar and communications, areas of research that have occupied much of his professional life. For the past 10 years he has focused his entire research interests on cognitive
dynamic systems: cognitive radar, cognitive radio, cognitive control, and cognition
applied to the cocktail party processor for the hearing impaired. He is a Fellow of the
IEEE and the Royal Society of Canada, and is the recipient of the Henry Booker Gold
Medal from URSI (2002), the Honorary Degree of Doctor of Technical Sciences from
ETH Zentrum, Zurich (1999), and many other medals and prizes. In addition to the
seminal journal papers “Cognitive radio” and “Cognitive radar,” he has also written or
co-written nearly 50 books including a number of best-selling textbooks in the fields of
signal processing, communications, and neural networks and learning machines.
Cambridge, New York, Melbourne, Madrid, Cape Town
The Edinburgh Building, Cambridge CB2 8RU, UK
Published in the United States of America by Cambridge University Press, New York
Informatio on this title: www.cambridge.org/9780521114363
This publication is in copyright. Subject to statutory exception
and to the provisions of relevant collective licensing agreements,
no reproduction of any part may take place without the written
permission of Cambridge University Press.
Printed in the United Kingdom at the University Press, Cambridge
A catalogue record of this book is available from the British Library.
Library of Congress Cataloguing in Publication data
Cognitive dynamic systems : perception–action cycle, radar, and radio / Simon Haykin.
Includes bibliographical references and index.
1. Self-organizing systems. 2. Cognitive radio networks. I. Title.
Cambridge University Press has no responsibility for the persistence or
accuracy of URLs for external or third-party internet websites referred to
in this publication, and does not guarantee that any content on such
websites is, or will remain, accurate or appropriate.
1.3 Cognitive dynamic wireless systems: radar and radio
1.4 Illustrative cognitive radar experiment
1.5 Principle of information preservation
2.6 Practical benefits of hierarchy in the perception–action cycle
2.7 Neural networks for parallel distributed cognitive information
2.8 Associative learning process for memory construction
Power-spectrum estimation for sensing the environment
3.7 Harmonic F-test for spectral line components
Bayesian filtering for state estimation of the environment
4.1 Probability, conditional probability, and Bayes’ rule
4.2 Bayesian inference and importance of the posterior
4.3 Parameter estimation and hypothesis testing: the MAP rule
4.8 On the relationship between the cubature and
4.10 Recurrent multilayer perceptrons: an application
Dynamic programming for action in the environment
5.5 Approximate dynamic programming for problems
5.6 Reinforcement learning viewed as approximate
5.9 On the relationships between temporal-difference
5.10 Linear function approximations of dynamic programming
5.11 Linear GQ(λ ) for predictive learning
6.3 Baseband model of radar signal transmission
6.5 Cubature Kalman filter for target-state estimation
6.9 Approximate dynamic programming for optimal control
6.11 Two-dimensional grid for waveform library
6.12 Case study: tracking a falling object in space
6.13 Cognitive radar with single layer of memory
6.14 Intelligence for dealing with environmental uncertainties
6.15 New phenomenon in cognitive radar: chattering
6.16 Cognitive radar with multiscale memory
6.17 The explore–exploit strategy defined
7.1 The spectrum-underutilization problem
7.2 Directed information flow in cognitive radio
7.5 Multitaper method for spectrum sensing
7.6 Case study I: wideband ATSC-DTV signal
7.7 Spectrum sensing in the IEEE 802.22 standard
7.8 Noncooperative and cooperative classes of
7.10 Water-filling in information theory for
7.11 Orthogonal frequency-division multiplexing
7.12 Iterative water-filling controller for cognitive
7.13 Stochastic versus robust optimization
7.14 Transient behavior of cognitive radio networks,
7.15 Case study II: robust IWFC versus classic IWFC
7.16 Self-organized dynamic spectrum management
7.17 Cooperative cognitive radio networks
7.18 Emergent behavior of cognitive radio networks
8.2 Summarizing remarks on cognitive radar and
In my Point-of-View article, entitled “Cognitive dynamic systems,” Proceedings of the
IEEE, November 2006, I included a footnote stating that a new book on this very topic
was under preparation. At long last, here is the book that I promised then, over four
Just as adaptive filtering, going back to the pioneering work done by Professor
Bernard Widrow and his research associates at Stanford University, represents one of
the hallmarks of the twentieth century in signal processing and control, I see cognitive
dynamic systems, exemplified by cognitive radar, cognitive control, and cognitive radio
and other engineering systems, as one of the hallmarks of the twenty-first century.
The key question is: How do we define cognition? In this book of mine, I look to the
human brain as the framework for cognition. As such, cognition embodies four basic
each of which has a specific function of its own. In identifying this list of four processes.
I have left out language, the fifth distinctive characteristic of human cognition, as it is
outside the scope of this book. Simply put, there is no better framework than human cognition, embodying the above four processes, for the study of cognitive dynamic systems,
Putting aside the introductory and epilogue chapters, the remaining six chapters of
the book are organized in three main parts as follows:
• Chapter 2, entitled the “Perception–action cycle,” provides an introductory treatment
of the four basic processes of cognition identified above. Moreover, the latter part of
the chapter presents highlights of neural networks needed for the implementation of
• Chapters 3, 4, and 5 provide the fundamentals of cognitive dynamic systems, viewed
from an engineering perspective. Specifically, Chapter 3 discusses power spectrum
estimation as a basic tool for sensing the environment. Chapter 4 discusses the Bayesian filter as the optimal framework for estimating the state of the environment when it
is hidden. In effect, Chapters 3 and 4 are devoted to how the environment is perceived
by dynamic systems, viewed in two different ways. Chapter 5 deals with dynamic
programming as the mathematical framework for how the system takes action on the
• Chapters 6 and 7 are devoted to two important applications of cognitive dynamic
systems: cognitive radar and cognitive radio respectively; both of them are fast
becoming well understood, paving the way for their practical implementation.
To conclude this Preface, it is my conviction that cognition will play the role of a
software-centric information-processing mechanism that will make a difference to the
theory and design of a new generation of engineering systems aimed at various applications, not just radar control, and radio.
In the course of writing this book, I learned a great deal about human cognition from
the book Cortex and Mind: Unifying Cognition by Professor J. M. Fuster, University
of California at Los Angeles. Just as importantly, I learned a great deal from the many
lectures on cognitive dynamic systems, cognitive radio, and cognitive radar, which I had
the privilege of presenting in different parts of the world.
I would like to extend my special thanks to Professor Richard Sutton and his doctoral
student, Hamid Maei, University of Alberta, Canada, for introducing me to a new generation of approximate dynamic-programming algorithms, the GQ(l) and Greedy-GQ,
and communicating with me by e-mail to write material presented on these two algorithms in the latter part of Chapter 5 on dynamic programming. Moreover, Hamid was
gracious to read over this chapter, for which I am grateful to him.
I am also indebted to Professor Yann LeCun, New York University, and his ex-doctoral student Dr Marc’Aurelio Ranzato for highly insightful and helpful discussion on
sparse coding. In a related context, clarifying concepts made by Dr Bruno Olshausen,
University of California, Berkeley, are much appreciated.
I acknowledge two insightful suggestions:
(1) The notion of “fore-active radar” as the first step towards radar cognition, which was
made by Professor Christopher Baker, Australian National University, Canberra.
(2) The analogy between feedback information in cognitive radar and saccade in vision,
which was made by Professor José Principe, University of Florida.
I thank my ex-graduate students, Dr Ienkaran Arasaratnam, Dr Peyman Setoodeh, and
Dr Yanbo Xue, and current doctoral student, Farhad Khozeimeh, for their contributions
to cognitive radar and cognitive radio. I have also benefited from Dr Amin Zia, who
worked with me on cognitive tracking radar as a post-doctoral fellow. I am grateful to
Dr. Setoodeh for careful proof-reading of the page proofs.
Moreover, I had useful comments from Dr Terrence Sejnowski, Salk Institute,
LaJolla, CA, and my faculty colleague Professor Suzanne Becker, McMaster University,
Turning to publication of the book by Cambridge University Press, I am particularly
grateful to Dr Philip Meyler, Publishing Director, and his two colleagues, Sarah Marsh
and Caroline Mowatt, in overseeing the book through its different stages of production.
In a related context, I also wish to thank Peter Lewis for copy editing the manuscript
The writing of this book has taken me close to four years, in the course of which
I must have gone through 20 revisions of the manuscript. I am truly grateful to my
Administrative Coordinator, Lola Brooks, for typing those many versions. Without her
patience and dedication, completion of the manuscript would not have been possible.
Last but by no means least, I thank my wife, Nancy, for allowing me the time I needed
I see the emergence of a new discipline, called Cognitive Dynamic Systems, which builds on
ideas in statistical signal processing, stochastic control, and information theory, and weaves those
well-developed ideas into new ones drawn from neuroscience, statistical learning theory, and
game theory. The discipline will provide principled tools for the design and development of a new
generation of wireless dynamic systems exemplified by cognitive radio and cognitive radar with
efficiency, effectiveness, and robustness as the hallmarks of performance.
This quotation1 is taken from a point-of-view article that appeared in the Proceedings
of the Institute of Electrical and Electronics Engineers (Haykin, 2006a). In retrospect,
it is perhaps more appropriate to refer to Cognitive Dynamic Systems as an “integrative
By speaking of cognitive dynamic systems as an integrative field, we mean this in
the sense that its study integrates many fields that are rooted in neuroscience, cognitive
science, computer science, mathematics, physics, and engineering, just to name a few.
Clearly, the mixture of fields adopted in the study depends on the application of interest.
However, irrespective of the application, the key question is
What is the frame of reference for justifying that a so-called cognitive dynamic system is indeed
In this book, we adopt human cognition as the frame of reference. As for applications,
the book focuses on cognitive radar and cognitive radio.
With these introductory remarks, the study begins with the next section.
A system, be it linear or nonlinear, is said to be dynamic if time plays a key role in its
input–output behavior. In this book, we are interested in a new class of dynamic systems
called cognitive dynamic systems, the study of which is inspired by the unique neural
computational capability of the human brain2 and the viewpoint that human cognition
To be specific, we say that a dynamic system, operating in an environment to be
explored, is cognitive if it is capable of four fundamental functions (tasks) that are basic
The perception–action cycle implies that a cognitive dynamic system has two functional
parts; namely, perceptor and actuator. The cycle begins with the perceptor perceiving the
environment (world) by processing the incoming stimuli, called observabless or measurements.
In response to feedback information from the perceptor about the environment, the actuator
acts so as to control the perceptor via the environment, and the cycle goes on. In effect, the
perceptor “guides” the actuator by virtue of what it has learned
the actuator “controls” the perceptor by acting in the environment. The benefit resulting from
the perception–action cycle is that of maximizing information gained
Typically, the environment is nonstationary, which means that the underlying behavior
of the environment continually changes with time. Given such an environment to deal with,
we may now go on to say that a cognitive dynamic system must also have memory, desirably of a multiscale variety. This requirement is needed for the system to do the following:
• learn from the environment and store the knowledge so acquired;
• continually update the stored knowledge in light of environmental changes; and
• predictt the consequences of actions taken and/or selections made by the system as
As for attention, a cognitive dynamic system must be equipped with the capability to
focus its information-processing power on a target or footprint in the environment that
is considered to be of special interest or strategic importance; this is done by prioritizing
Turning to intelligence, among the above-mentioned four functions, it is by far the
most difficult one to describe. Nevertheless, intelligence is the single most important
function in a cognitive dynamic system. For the present, it suffices to say that intelligence is based on the perception–action cycle, memory, and attention for its functionality. Most importantly, it is the presence of feedback at multiple levels in the system
that facilitates intelligence, which, in turn, makes it possible for the system to make
intelligent decisions in the face of inevitable uncertainties in the environment. The feedback can itself take one of two forms:
• global feedback, which embodies the environment, and
Typically, the local and global feedback loops are distributed throughout a cognitive
dynamic system. The extent of feedback loops naturally varies from one cognitive
dynamic system to another, depending on the application of interest.
In diagrammatic form, much of what we have just described is captured so illustratively in
Figure 1.1. On the right-hand side of the figure, we have the perceptor of a cognitive dynamic
system that is responsible for perception of the environment. On the left-hand side of the
Cognitive dynamic wireless systems: radar and radio
Figure 1.1. The perception–action cycle of a cognitive dynamic system in its most generic sense.
figure, we have the actuator of the system that is responsible for action in the environment.
And, above all, we have a feedback link connecting the perceptor to the actuator.
Figure 1.1 is commonly referred to as the perception–action cycle of a cognitive
dynamic system. This remarkable operation, in its generic form, stands out in human
cognition, which, as mentioned at the beginning of this introductory chapter, embodies
• a global feedback loop, embodying perception and action so as to maximize information gain about the environment;
• multiscale memory that is organized to predict the consequences of actions;
• a memory-based attentional mechanism that prioritizes the allocation of resources;
• a feedback-based decision-making mechanism that identifies intelligent choices in
We are repeating what we described earlier in order to emphasize that these four distinctive properties of human cognition constitute the ideal framework, against which a
dynamic system should be assessed for it to be cognitive.
Cognitive dynamic wireless systems: radar and radio
A new generation of engineering systems is being inspired by human cognition, the
structures of which vary in details from one system to another, depending on the application of interest. In this context, two dynamic wireless systems stand out:
• cognitive radar, for improved performance in remote-sensing applications for system
• cognitive radio, for solving the underutilized electromagnetic spectrum problem.
There is a remarkable analogy between the visual brain and radar. Indeed, the perception–
action cycle of Figure 1.1 applies equally well to cognitive radar by merely changing the
ways in which the transmitter (actuator) and receiver (perceptor) are actually implemented.
Specifically, the function of the receiver in a radar system is to produce an estimate
of the state of an unknown target located somewhere in the environment by processing
a sequence of observables dependent on the target state. In effect, perception of the
environment takes the form of state estimation. As for the transmitter in the system, its
function is to adaptively select a transmitted waveform that illuminates the environment
in the best manner possible. In target detection, the issue of interest is to decide as reliably as possible whether a target is present or not in the observables. In target tracking,
on the other hand, the issue of interest is to estimate the target parameters (e.g. range and
With radar intended for remote-sensing applications and with its transmitter and
receiver being typically collocated, much can be learned from the human brain to make
The practical use of cognitive radio is motivated by the desire to address the electromagnetic spectrum underutilization problem. In today’s wireless communications world, we
typically find that only a small fraction of the radio spectrum assigned to legacy operators by government agencies is actually employed by primary (licensed) users. The
underutilized subbands of the spectrum are commonly referred to as spectrum holes.
The function of a cognitive radio may then be summarized as follows:
(1) The radio receiver is equipped with a radio scene analyzer, the purpose of which is to
identify where the spectrum holes are located at a particular point in time and space.
(2) Through an external feedback link from the receiver to the transmitter, the information on spectrum holes is then passed to the radio transmitter, which is equipped
with a dynamic spectrum managerr and transmit-power controller. The function of
the transmitter is to allocate the spectrum holes among multiple secondary (cognitive radio) users in accordance with prioritized needs.
Unlike radar, where the transmitter and receiver are ordinarily collocated, in a radio
(wireless) communication system the transmitter and receiver are located in different
places. Accordingly, for the receiver to send the transmitter information about the spectrum holes, we require the use of a low-bandwidth feedback linkk connecting the receiver
With radio intended for wireless communications and with its transmitter and receiver
being separately located, there is still a great deal we can learn from the human brain;
but to make the radio cognitive, we have to use engineering ingenuity.
We will now motivate the power of cognitive information-processing by considering a
The function of the receiver is to estimate the state of a target in space and thereby
track its motion across time, given a set of observables (measurements) obtained on the
target. With this objective in mind, a sequential state estimator suffices to take care of
To elaborate on the sequential state estimator, for the sake of simplicity we assume
that the environment is described by a linear state-space model, comprised of the
where n denotes discrete time. The vector x(n) denotes the state of the environment at
time n; the state evolution across time in (1.1) is called the system equation. The vector
y(n) denotes the measurements recorded digitally by the receiver as input at time n, hence
the reference to (1.2) as the measurement equation. Transition of the state at time n − 1
to that at time n is described by the matrix A. Correspondingly, dependence of the measurements (observables) on the state at time n is described by the measurement matrix B.
Naturally, the imposition of a mathematical model on the environment, as described
in (1.1) and (1.2), gives rise to uncertainties about the physical behavior of the environment. These uncertainties are accounted for by introducing the system noise w (n) in
qn−1) in (1.2). The measurement noise is denoted by n,
the composition of which is dependent on the action of the transmitter. That action is
controlled by a transmit-waveform parameter vector qn−1; the reason for (partially or in
full) assigning time n − 1 to this vector is to account for the propagation delay between
In what follows, we assume that the process noise w (n) and measurement noise
n(qn−1) are both stationary Gaussian processes of zero mean; their respective covariance matrices are denoted by Q and Rq . Invoking these assumptions and recognizing
that the state-space model described in (1.1) and (1.2) is linear, it follows that the solution to the sequential state-estimation problem – that is, estimating the state x(n) given
the sequence of observables { (i )}in=1 – is to be found in the classic Kalman filter. The
issue of sequential state estimation is discussed in Chapter 4. As such, in this introductory chapter, we will proceed on the premise that we know how to formulate the Kalman
filtering algorithm. We may, therefore, go on to say that, with tracking as the issue of
interest, the Kalman filter, formulated on the basis of the state-space model of (1.1) and
(1.2), adequately fulfills the perceptive needs of the receiver.
The target parameters to be estimated are the delay and Doppler shift. The delay t is
defined in terms of the range r (i.e. distance of the target from the radar) by
where c is the speed of electromagnetic wave propagation (i.e. the speed of light). The
Doppler shift fD is defined in terms of the range rate r (i.e. velocity of the target) by
where fc is the transmitted carrier frequency and the dot in r denotes differentiation
The unknown target is located at a distance of 3 km from the radar and it is moving at
The radar is an X-band radar operating at the frequency fc = 10.4 GHz; it is located at
the origin. The 0 dB signal-to-noise ratio (SNR) at the receiver input is defined at 80 km.
The state-space model of (1.1) and (1.2) is parameterized as follows:
where T is the sampling period. The system noise is modeled as the target-acceleration
noise, with its zero-mean covariance defined by (Bar-Shalom et al., 2001)
Figure 1.2 presents the results of Monte Carlo simulations performed to evaluate the
tracking performance of three different radar configurations:4
(1) Traditional active radarr with a fixed transmit waveform, in which case the measurement equation (1.2) simplifies to
It is assumed that the matrices A and B and the covariance matrices Q and R are all
Figure 1.2. Demonstrating the information-processing power of global feedback and cognition
in radar tracking. (a) Root mean-squared-error (RMSE) of target range, measured in meters.
(b) RMSE of range rate, measured in meters per second. TAR: traditional active radar; PAC: the
perception–action cycle, as in the first stage toward cognition in radar, or equivalently, the foreactive radar; CR: cognitive radar.
(2) Perception–action cycle mechanism, which is the first step toward radar cognition;
the same mechanism is applicable to a second class referred to as the fore-action
radar.4 Accordingly, the measurement equation (1.2) holds.
(3) Cognitive radar, the transmitter of which is equipped not only with a transmitwaveform library but also another library in the receiver; the second library
makes it possible for the receiver to select appropriate values for the matrix A and
covariance matrix Q in the system equation (1.1); that is, continually modell the
Figure 1.2a presents the RMSE for the target range and Figure 1.2b the RMSE for
the target range-rate plotted versus time. In both parts of the figure, the top dashed
curves refer to the traditional active radar, the middle dashed bold curves refer to the
perception–action cycle mechanism acting alone (i.e. fore-active radar), and the bottom
full curves refer to the cognitive radar.
The results presented in Figure 1.2 lead us to report two important findings:
(1) The use of global feedback in the perception–action mechanism acting alone makes
a significant difference in the tracking accuracy of the radar, compared with the traditional active radar with no feedback.
(2) The addition of memory to the perception–action mechanism as in the cognitive
radar brings in even more significant improvement to tracking accuracy of the
Having just reported findings (1) and (2) on how the fore-active radar and the cognitive
radar compare with a traditional active radar in terms of tracking accuracy, we may now
Over and above the improvements in tracking accuracy, what else do the results of Figure 1.2
Our first step in answering this fundamental question is to reiterate that the environmental state of the target consists of two parameters:
(1) range r , which defines how far away the target is from the radar;
(2) range rate r , which defines the velocity of the target.
Since both the system and measurement equations, (1.1) and (1.2), are corrupted
by additive noise processes, it follows that both the range r and range rate r are
random variables. According to Shannon’s information theory (Shannon, 1948),
we may, therefore, say that information about the target’s state is contained in the
sequence of measurements { (i )}in=0 . Moreover, in view of this statement, we may
go on to speak of feedback information, defined in terms of the error between the
actual state of the target and its estimate computed by the receiver as the result of
operating on the sequence of measurements. It is by virtue of this feedback information passed to the transmitter by the receiver that the feedback loop around the
1.5 Principle of information preservation
With state estimation as a central issue of interest in cognitive radar, we look to the
Bayesian filter as the optimal recursive data-processing algorithm. Basically, the
Bayesian filter combines all the available measurements (data) plus priorr knowledge
about the system and measuring devices and uses them all to produce the optimal estimate of hidden target parameters. To perform this estimation, the Bayesian filter propagates the posteriorr (i.e. probability density function of the state, conditioned on the
measurements) from one estimation recursion to the other. The rationale for focusing
on the posterior is that it contains all the information about the state that is available in
the measurements. Hence, the optimal estimate is obtained by maximizingg the posterior,
which is the “best” that can be achieved (Ho and Lee, 1964).
Under the combined assumption of linearity and Gaussianity, the Bayesian filter
reduces to the Kalman filter, hence its choice as the functional block for processing the
measurements in the receiver in the motivational experiment of Section 1.4.
Information preservation through cognition
Moving on to the transmitter for action in the environment, the feedback information
highlighted earlier in this section provides a basis of a cost-to-go function that looks
to the future by one time step. Recognizing that this function is also dependent on
the parameters that define the transmitted waveform, a primary function of the transmitter, therefore, is to select a set of transmit-waveform parameters that minimizes
the cost-to-go function. Thus, for every cycle of the radar’s perception–action cycle,
the transmit-waveform parameters are selected such that perception of the radar
environment in the receiver and action in the environment performed in the transmitter are optimized in a sequential manner. This process is repeated on a cycle-bycycle basis.
To assess the overall radar performance, we need a metric that provides an assessment of how close the estimated state of the target is to its actual value. For the experiment, following the traditional approach in statistical signal processing, this metric was
defined simply as the RMSE between the actual state and its estimated value using the
Kalman filter, with both of them defined for one step into the future. What we have just
described here provides the mathematical justification for improved tracking accuracy
through the use of global feedback from the receiver to transmitter, reported previously
under point (i) at the end of Section 1.4.
Next, through the following combination of system additions:
• perceptual memory, reciprocally coupled with the Kalman filter in the receiver;
• executive memory, reciprocally coupled with the transmit-waveform selector in the
• working memory, reciprocally coupling the executive and perceptual memories,
the transmitter and receiver are continuously matched together in their respective operations in an adaptive manner. It is through this adaptive process distributed in different
parts of the radar receiver and transmitter that we are able to explain the additional significant accuracy improvement reported under point (2) at the end of Section 1.4.
Now, we are ready to answer the question that was posed at the beginning of this section. In using the Kalman filter by itself in the receiver, information about the state of
the target contained in the measurements is preserved to some extentt (Kalman, 1960).
In adaptively matching illumination of the environment with the target through the use
of feedback from the receiver to the transmitter (Gjessing, 1986), information about the
state contained in the measurements is preserved even more. Finally, in making the radar
cognitive through the provision of distributed memory, further improvement in information preservation is achieved
What we have just described here is now summed up as follows:
Cognitive information processing provides a powerful tool for fulfilling the principle of information preservation, which is aimed at preserving information about a hidden target state that is
In the statement just made on the principle of information preservation, we have
emphasized the role of cognition. Information preservation may also be viewed as information gain in the following sense: the more we preserve information contained in the
measurements about the target’s state, the closer the estimated state is to its actual value.
This is another way of saying that we are progressively “gaining” information about the
target’s state from one cycle to the next in following the perception–action cycle. By
saying so, we have justified the previous use of “information gain” in describing the
role of a global feedback loop in the perception–action cycle.
To conclude this discussion, it should be noted that successive improvements in information preservation, exemplified by corresponding improvements in state-estimation
accuracy, are achieved at the expense of increased computational complexity. It is
apropos, therefore, that we complement our previous statement by saying:
There is “no free lunch,” in that for every gain we make in practice there is a price to be paid.
The rest of the book is organized in seven chapters, as summarized here.
Chapter 2 is devoted to a detailed discussion of the perception–action cycle, which
is the baseline for the operation of every cognitive dynamic system. This chapter also
• perceptual memory, which is an integral part of the receiver;
• executive memory, which is an integral part of the transmitter; and
• working memory, which reciprocally couples the executive memory to perceptual
These three memories, acting together, enable the radar system to predictt the consequences of action taken by the radar system as a whole. As mentioned previously, unlike
perception and memory, the properties of attention and intelligence are distributed
across the whole dynamic system. Chapter 2 also includes discussion of neural networks
for designing the memory system; learningg from stimuli is an essential requirement of
Chapter 3 addresses power spectrum estimation as a means of sensingg the environment. For reasons discussed therein, much of the material covered in that chapter is
devoted to the multitaper methodd and related issues; namely, space–time processing,
time–frequency analysis, and cyclostationarity. From the brief description just given,
the topics covered in Chapter 3 are directly applicable to cognitive radio discussed later
Chapter 4 addresses Bayesian filtering for estimating the hidden state of the environment in a sequential manner given the observables received from the environment. Of
course, the natural place for this operation is the receiver. Under the assumptions of
linearity and Gaussianity, the Bayesian filter reduces to the celebrated Kalman filter.
However, when the environment is nonlinear, as it is often the case in practice, we have
to be content with approximate forms of the Bayesian filter. The extended Kalman filter
(EKF), suitable for mild forms of nonlinearity, is one such approximation; the EKF is
widely used on account of its computational simplicity. Chapter 4 includes discussion
of the EKF and, most importantly, a new nonlinear filter, named the “cubature Kalman
filter (CKF).” The CKF is a “method of choice” for approximating the optimal Bayesian filterr under the Gaussian assumption in a nonlinear setting. By the nature of it, the
material covered in Chapter 4 is directly applicable to cognitive radar, discussed later
Chapter 5 is devoted to another topic, dynamic programming, which, in its own
way, is also basic to the study of decision-making in cognitive dynamic systems.
Dynamic programmingg addresses problems on policy planning and control. As such,
its implementation resides in the transmitter. Whereas state estimation deals with
hidden states, formulation of dynamic programming theory assumes direct access
to the state of the environment, with the challenge being that of finding an optimal
policy for action in the environment to control the receiver indirectly on a cycle-bycycle basis. The material covered in Chapter 5 is naturally applicable to cognitive
With the fundamentals of cognitive dynamic systems covered in Chapters 2–5, the
stage is set to discuss practical applications. With emphasis being on new wireless
dynamic systems enabled with cognition, the book focuses on two applications: cognitive radar and cognitive radio, in that order.
Cognitive radar is covered in Chapter 6. Much of the material covered in that
chapter deals with the use of cognitive radar for tracking an unknown target in space
(i.e. a Gaussian environment). Computer simulations are presented to demonstrate the
information-processing power of cognition on the performance of a traditional radar
tracker, first using global feedback only and then expanding the perception–action
cycle mechanism by adding memory distributed across the entire radar. The important
message to take from Chapter 6 is that, through cognitive information-processing, we
now have a transformative software technology that is applicable to radar systems, old
and new. Simply put, cognitive radar is a game-changer.
Next, cognitive radio is discussed in Chapter 7 to deal with the underutilized electromagnetic spectrum problem. Unlike cognitive radar, the information-processing
power of cognition is exploited differently in cognitive radio. Here also, computer
simulations are presented to demonstrate how radio-scene analysis in the receiver and
resource allocation in the transmitter can be tackled in computationally efficient and
performance-effective ways. With the electromagnetic radio spectrum being a natural
resource, cognitive radio provides a paradigm shiftt in wireless communications by
virtue of its information-processing power to improve utilization of this important
Chapter 8, the last chapter entitled “Epilogue,” reemphasizes the attributes that bind
cognitive radar and cognitive radio to the human brain, and finishes with brief discussion of unexplored topics, which, in their own respective ways, look to how cognition
can be exploited for innovative practical applications.
1. The predictive article on cognitive dynamic systems
Haykin (2006b) was emboldened to write this predictive article, emboldened by
his two seminal journal papers, the first entitled “Cognitive radio: brain-empowered
wireless communications,” (Haykin, 2005a), and the second entitled “Cognitive
radar: a way of the future” (Haykin, 2006b).
For books on the computational brain, see Churchland and Sejnowski (1992) and
Dayan and Abbott (2001), and Trappenberg (2010).
For books on cognitive science, see Pylyshyn (1984), Posner (1989), and Fuster
In the English edition of his book entitled On The Origins of Science: The Mechanization of the Mind, Dupuy (2009) eloquently articulates those issues that unite
cognitive science and cybernetics (dating back to Wiener, 1948) on the one hand
and those other issues that have led them to move apart on the other hand. With
cybernetics being the older one of the two, Dupuy argues that cybernetics should be
viewed as the “parent” of cognitive science.
In Section 6.2, three classes of radar are defined:
The second class, referred to as fore-active radar, distinguishes itself from the traditional active radar by the use of global feedbackk from the receiver to the transmitter.
In doing so, there is no distinction between a fore-active radar or the perception–
action cycle of a cognitive radar. It is for this reason that we have used the terminology “perception–action cycle mechanism” for the second radar configuration in
simulations described in Chapters 1 and 6.
In Chapter 1 of his book entitled Cortex and Mind: Unifying Cognition, Fuster (2003)
introduces a new term called the cognit, which is used to characterize the cognitive
structure of a cortical network; it is defined as follows:
A cognit is an item of knowledge about the world, the self, or the relations between them. Its
network structure is made up of elementary representations of perception or action that have been
associated with one another by learning or past experience.
Towards the end of Chapter 1 of the book, Fuster goes on to say:
. . . perception is part of the acquisition and retrieval of memory, memory stores information
acquired by perception, language and memory depend on each other; language and reasoning are
special forms of cognitive action; attention serves all other functions; intelligence is served by
These two quotes taken from Fuster’s book provide us with a cohesive statement on the
cortical functions of cognition in the human brain; both are important. In particular, the
second quotation highlights the interrelationships between the five processes involved
in cognition: perception, memory, attention, language, and intelligence. Just as language
plays a distinctive role in the human brain, so it is in a cognitive dynamic system where
language provides the means for effective and efficient communication among the different parts of the system. However, language is outside the scope of this book. The
remaining four processes are the focus of attention henceforth.
With this brief introductory background on cognition, the stage is set in this chapter
for detailed descriptions of the four cognitive processes: perception followed by action,
memory, attention, and intelligence, and their respective roles in the perception–action
cycle. As reiterated in the introductory chapter, the perception–action cycle is at the
very heart of all cognitive dynamic systems; hence the title of this chapter.
For a cognitive dynamic system to perceive the environment in which it operates, the
system must be equipped with an appropriate set of sensors to learn from the environment. For example, in cognitive radar and cognitive radio, antennas are placed at the
front end of the receiver so as to couple it electromagnetically to the environment. By
the same token, another set of antennas is placed at the output of the transmitter so as to
couple it electromagnetically to the environment in its own way.
The sensors feed a functional block in the receiver called the environmental scene
analyzer; its function is to perform perception, described as follows:
Perception is the sensory analysis of incoming streams of stimuli, aimed at learning the underlying physical attributes that characterize the environment in an on-line manner.
Exact composition of the environmental scene analyzer is naturally dependent on the
application of interest. For example, in cognitive radar designed for target tracking, the
environmental scene analyzer consists of a sequential state estimator, the function of
which is to estimate the state of the targett across time in a recursive manner. In cognitive
radio for another example, the function of the environmental scene analyzer is to identify spectrum holes (i.e. underutilized subbands of the electromagnetic spectrum) so
that they can be employed by unserviced secondary users; in effect, the spectrum holes
signify the “state” of the radio environment insofar as cognitive radio is concerned.
Irrespective of the application of interest, a cognitive dynamic system’s perception of
the environment is continually influenced by the current data received by the system, as
well as by cognitive information (i.e. knowledge gained from past experience) already
stored in memory. In other words, every perceptt (i.e. snapshot of the perception process
at every cycle in time) is made up of two components:
(1) The first component of percept refers to recognition and, therefore, retrieval of
relevant information about the environment that is stored in memory for the purpose
(2) The other component of the percept refers to categorization (classification) of a new
d with the memory; in other words, the second component
is an updated version of the first component.
Most importantly, in both cases, perceptual information processing is executed in a selforganized and adaptive manner.
Functional integration-across-time property of cognition
In a cognitive dynamic system, perception of the environment in the perceptor leads to
action in the environment by the environmental scene actuatorr in the actuator. For this
operation to be feasible, however, we require the use of a feedback linkk from the environmental scene analyzer to the environmental scene actuator, as depicted in Figure 2.1.
Thus, whereas the perceptor perceives the environment directly by processing the
incoming streams of stimuli, the actuator gets to know about the environment indirectly through the feedback information fed to it by the perceptor in accordance with the
Indeed, it is through the continuation of this cycle across time that a cognitive
dynamic system acquires its cardinal property, enabling it to adaptt to changes in the
environment by making successive internal changes of its own through lessons learned
Figure 2.1. Directed information-flow diagram in the perception–action cycle of a cognitive
dynamic system with hierarchical memory. (This figure is inspired by Fuster (2003).)
from interactions with the environment. To reemphasize the importance of this capability, we say that the integration of functions involved in the perception–action cycle
across time is a distinctive characteristic of cognition, in that time plays three key roles
(1) Time separates the sensory input signals (stimuli) so as to guide overall system
(2) Time separates the sensory input signals responsible for perception of the environment in the perceptor from actions taken by the actuator in the environment.
(3) Time separates the occurrence of sensory feedback (i.e. feedback information) from
The implication of these three points is profound, prompting us to make the following
Temporal organization of the system behavior of a cognitive dynamic system requires the integration across time of three entities: percepts with other percepts, actions with other actions, and
Hereafter, we refer to this property of cognitive dynamic systems as the functional
integration-across-time property; this property plays the cardinal role behind the temporal organization of a cognitive dynamic system’s behavior.
Before proceeding to discuss the important role of memory in cognitive dynamic
systems, it is instructive that we differentiate between knowledge and memory:
• Knowledge is a memory of certain facts and relationships that exist between them,
none of which changes with time; in other words, knowledge is static in its contents.
• Memory, on the other hand, is dynamic, in that its contents continually change over the
course of time in accordance with changes in the environment.
Stated in another way, the contents of memory are subject to time constraints, whereas
knowledge is timeless and, therefore, free of time constraints.
With a cognitive dynamic system typically consisting of a perceptor and actuator, it
is logical to split the memory into two parts: one residing in the perceptor and the other
residing in the actuator. These two parts of memory are respectively called perceptual
memory and executive memory, which are discussed next in that order.
As the name implies, perceptual memory is an integral part of how, in an overall
sense, the perceptor perceives the environment. To be more specific, perceptual
memory provides the ability for the perceptor of a cognitive dynamic system to
interpret the incoming stimuli so as to recognize their distinctive features and categorize the features learned accordingly in some statistical sense. We may thus make the
Perceptual memory is the experiential knowledge that is gained by the perceptor through a process
of learning from the environment, such that contents of the memory continually change with time
in accordance with changes in the environment; the experiential knowledge so gained through
learning becomes an inextricable part of the perceptual memory.
To satisfy the objective embodied in this statement, we require two provisions:
• First, the perceptual memory is supplied with an internal library, the elements of
which describe different realizations of a prescribed modell of the environment;
in other words, each element represents an item of knowledge about the environment
• Second, the perceptual memory is reciprocally coupled to the environmental scene
As illustrated in Figure 2.1, this reciprocal coupling implies the use of two links:
(1) Bottom-up (feedforward) link. This first link involves two operations:
• retrievall of the old memory representing features learned
• updatingg of the old memory in light of new information about the environment
In other words, new memory is formed on the old memory.
(2) Top-down (feedback) link. The purpose of this second link is that of acquisition of
the new memory by the environmental scene analyzer.
We may, therefore, say that perception in the perceptor consists of adaptive matching
of a particular environmental model retrieved from the internal library to the incoming
Moreover, given a multiscale perceptual memory in the perceptor of a cognitive
dynamic system as depicted in Figure 2.1, the stimuli are processed in the perceptual
memory, level by level. Hence, the perceptual constancy across the hierarchical structure of the memory progressively increases in abstraction. The net result of this abstraction is the ease of perceptual understanding of the environment by the perceptor and,
hence, an improved implementation of the adaptive matching process described above.
Just as perceptual memory relates to perception of the environment in the perceptor,
executive memory relates to the corresponding actuator’s action in the environment. To
be more precise, contents of the executive memory are continually changed through the
transmitter’s action in response to feedback information about the environment that is
fed to it by the perceptor. We may thus make the following statement:
Executive memory is the experiential knowledge gained by the actuator through the lessons
learned from actions taken in the environment by the actuator, with contents of the memory
changing with time in accordance with how the perceptor perceives the environment; here again,
the knowledge so gained through experience becomes an inextricable part of the executive
To satisfy this statement, we require that the actuator be supplied with two provisions:
• First, the executive memory has an internal library of its own, the elements of which
describe different realizations of control signal (i.e., transmit-waveforms used to illuminate the environment); in other words, each element is a cognitt representing an item
of knowledge about possible action in the environment.
• Second, the executive memory is reciprocallyy coupled to the environmental scene actuator.
Here too, the reciprocal coupling illustrated in Figure 2.1 implies the use of two links:
(1) Bottom-up (feedforward link). This first link involves two operations:
• retrievingg old memory learned about decisions made in the past and
g the old memory in light of new information about the environment
obtained in an indirect manner via the feedback link from the perceptor.
(2) Top-down (feedback) link. The purpose of this second link is that of acquisition of
the new memory by the environmental scene actuator.
Moreover, with the perceptual memory made up of multiple layers (levels) as depicted in
Figure 2.1, it is logical that the executive memory should have a corresponding number of
layers (levels) of its own. In a manner similar to that described for the multiscale perceptual
memory, we find that the output produced by the multiscale executive memory assumes the
form of abstract information that helps the environmental scene actuator achieve its goal of
decision-making in the face of environmental uncertainties with relative ease.
Final reciprocal coupling to complete the cognitive information-processing cycle
According to Fuster (2003), the two hierarchies of cortical cognits (i.e. perceptual and
executive) appear to be interlinked at all of their individual layers (levels). In other
words, networks of the human brain dealing with the perceptual representation and processing of incoming stimuli are reciprocally coupled with networks at a corresponding
stage of executive representation and processing of feedback information.
It would, therefore, seem logical that we do the same in a cognitive dynamic system,
especially where the perceptor and actuator are collocated. In other words, we seek a form of
reciprocal coupling between layers of the hierarchical perceptual memory in the perceptor
of the system with the corresponding layers of the hierarchical executive memory in the
actuator of the system, as indicated in Figure 2.1. In so doing, the actuator and perceptor of
the cognitive dynamic system are enabled to continually operate in synchrony.
To be more precise, this final form of reciprocal coupling of the executive and perceptual memories is required to account fully for the functional integration-across-time
property of the cognitive dynamic system in its entirety. These two memories are thereby
enabled to work with each other, so as to select the “best” action that can be taken by
the actuator to control the perceptor via the environment in light of the optimall feedback
information fed to it by the perceptor from one cycle to the next.
With the receiver of a cognitive dynamic system functioned to perceive the environment, which, in turn, prompts the actuator to act in the environment on a cycle-by-cycle
basis, the system needs a mechanism for predicting the consequences of actions taken
by the entire system. This need is all the more essential given that the environment is
nonstationary, which is always practically the case.
As already explained, there is also the need to reciprocally couple the perceptual
memory with the executive memory to satisfy the functional integration-across-time
property of cognition. With both of these two memories having similar multiscale structures, as indicated in Figure 2.1, this reciprocal coupling will also have to be in place on
These two needs are both fulfilled by the use of a multiscale working memory, as
shown in Figure 2.1; this new memory is discussed in the next section.
For now, it suffices for us to summarize the roles of memory in a cognitive dynamic
(1) Through the joint activation of perceptual memory in the perceptor and executive
memory in the actuator, the system acquires organized temporal behavior in accordance with the functional integration-across-time property of cognition.
(2) The memory, viewed in its entirety, predicts the consequences of actions taken by
In the course of describing the roles of memory in cognition, we identified the need
for working memory in a cognitive dynamic system. Simply put, working memory1
refers to active memory that occupies a short span of time. With its function being that
of predicting the consequences of actions taken by the cognitive dynamic system as a
whole, it follows that working memory plays a key role in the attentional mechanism
focused on the internal representation of a recent event associated with a prospective
action; attention is discussed in the next section.
As such, we may think of working memory as having the capacity to temporally hold
a limited amount of information for two purposes: statistical learningg in the perceptor
g in the actuator. The distinctive property of working memory,
• the issue of dealing with a piece of information that pertains to the environment, the
cognitive dynamic system itself, or the relationships between them and
• that the piece of information remains active for a relatively short period of time
required to focus attention on some action to be taken by the system in the next cycle
For example, the perceptor may have made a parametric selection to improve its modeling of the environment, or the actuator may have chosen a certain transmit waveform
for illuminating the environment. In both cases we have an event that may have occurred
at a particular cycle, and the requirement for working memory is to commit a limited
amount of cognitive information-processing power to account for the consequences of
In a fundamental sense, the purpose of attention is to allocate selectively the available
computational resources to realize the execution of a goal-directed action by the transmitter. We may, therefore, think of attention as a mechanism for prioritizing resource
allocation in terms of practical importance, which, from a practical perspective,
makes a great deal of intuitive sense for the following reason. The computational (i.e.
information-processing) resources of a cognitive dynamic system are naturally limited;
Attention is a mechanism that protects both the perceptual-processing power of the perceptor and
the decision-making power of the actuator from the information-overload problem through prioritization of how these computational resources are allocated.
In the context of a cognitive dynamic system, the term “information overload” refers
to the difficulty experienced by the system when the perceptor’s task of sensing the
environment and the actuator’s task of decision-making are compromised by having to
To elaborate, from the perspective of the perceptor in a cognitive dynamic system,
perceptive attention involves focusing the computational processing power of the
perceptor on a specific objective that is of special interest to the application at hand.
Consider, for example, a multi-target tracking application, where there is a target of
special interest that requires focused attention. With perception consisting essentially of
parallel processing and adaptive matching of incoming stimuli to cognits on potential
targets stored in the internal library of the perceptual memory, and with the processing
continued from one cycle to the next, the bottom-up processing performed rapidly may
lead to detection of the target of interest in the perceptor.
In turn, the target detection achieved through the matching process leads to top-down
feedback, hence focusing and subsequent analysis. What we have just described here is
basically a form of the perception–action cycle being carried out locally in the receiver
with target detection as the objective of interest.
Once this objective in the perceptor has been achieved, the next issue is that of preserving information about the target contained in the environmental stimuli; that is,
invoking the principle of information preservation discussed in Chapter 1. With the
actuator linked to the perceptor via feedback information, to protect the transmitter
from the information-overload problem, the objective function to be optimized in
the actuator should be formulated in accordance with the principle of information
Information about the environment fed to the actuator by the perceptor is preserved in its most
Thus, executive attention, also usually viewed as executive control, involves a process
of selecting a number of alternative cognits in the executive memory’s internal library
of transmit waveforms that closely match the incoming feedback information. Once this
is done, the task of optimal control in the transmitter is accomplished by computing the
waveform for which the objective function is optimized for action on the next cycle.
Summarizing the roles of attention in cognition, we may make the following twofold
(1) Based on the perceptual memory and executive memory built into a cognitive dynamic system,
the attentional mechanism of the system allocates the available computational resources,
including prior knowledge, and prioritizes the allocation in their order of importance.
(2) In addition to these two memories, the attentional mechanism looks to the working memory
for information on the consequences of actions taken by the system, with this provision being
Earlier on in the chapter, we identified perception, memory, attention, and intelligence
as four defining processes of cognitive dynamic systems. Among these four processes,
intelligence stands out as the most complex process and the hardest one to define.
We say so because the other three processes, perception, memory, and attention, in their
own individual ways and varying degrees, make contributions to intelligence; hence
the difficulty in defining intelligence. In the Penguin Dictionary of Psychology, Reber
(1995: 379) makes the following point on the lack of consensus, concerning the definition of intelligence:
Few concepts in psychology have received more devoted attention and few have resisted clarification so thoroughly.
Nevertheless, we may go on to offer the following statement on intelligence in the
Intelligence is the ability of a cognitive dynamic system to continually adjust itself through an
adaptive process by making the perceptor respond to new changes in the environment so as to
create new forms of action and behavior in the actuator.
As different as they are, we may say, therefore, that cognition and adaptation work hand
in hand for the overall betterment of cognitive dynamic systems.
To understand the essence of intelligence, we may look to the perception–action cycle
of Figure 2.1 with feedback loops distributed throughout the system. Therein, we see
that feedback control for interactions between the perceptual and executive parts of
the system manifests itself both globally and locally. To this end, we may also go on to
make the following profound statement (Haykin, 2006b):
Feedback is the facilitator of computational intelligence.
Moreover, as it is with attention, intelligence is distributed across the perception–action
cycle in its entirety, which, therefore, means that there is no separate structure or group
of structures dedicated to intelligence as a separate function within the system.
For intelligence to stand for the processing of cognitive information toward the achievement of behavioral goals, the degree of intelligence is measured in terms of the efficiency
with which that information is being processed (Fuster, 2003). The key question is:
How do we measure the efficiency of intelligence?
As observed earlier, the objective of the actuator in a cognitive dynamic system is
optimal action in the environment in light of feedback information sent to the environmental scene actuator in the actuator by the environmental scene analyzer in the
perceptor; the optimization is done in the actuator in some statistical sense. On this
basis, therefore, we may respond to the question just raised above as follows:
Through the use of an optimal control algorithm in the actuator, a cognitive dynamic system
becomes increasingly more intelligent whereby a prescribed cost-to-go function is progressively
reduced and with it, information about the environment is more efficiently utilized from one cycle
In saying so, however, we should not overlook the issue of the overall computational complexity of the system.
2.6 Practical benefits of hierarchy in the perception–action cycle
Synchronized cognitive information processing
Looking at the perception–action cycle of Figure 2.1, we see that we have a highly
complex closed-loop feedback control system, nested within numerous local feedback loops positioned alongside global feedback loops. Accordingly, the perceptor
and actuator of a cognitive dynamic system would have to process information about
the environment in a self-organized, synchronized manner and on a continuous-time
basis. In cognitive radar, for example, the information of interest is about the state
of a target embedded in the environment. In cognitive radio, for another example, it
is about the spectrum holes (i.e. unused or partially used subbands of the radio spectrum). In both of these applications, the environment can be highly nonstationary,
which makes the task of information processing a difficult proposition. It is in highly
nonstationary environments that, in designing the actuator to work in synchrony with
the perceptor and have them jointly reinforce each other on a continuous-time basis
through cognition, we are enabled to make a significant practical difference in overall
In the final analysis, the cognitive role of the actuator is that of decision-making, in
the context of which probabilistic inference plays a key role. The term “inference” or
“reasoning” refers to a process by means of which conclusions to a problem of interest
are reached. Inference may well be the outstanding characteristic of intelligence. We
may, therefore, sum up the role of intelligence in cognition as follows:
The decision-making mechanism in the actuator of a cognitive dynamic system uses probabilistic inference to pick intelligent choices in the face of unavoidable uncertainties in the
The uncertainties are attributed to certain physical characteristics of the environment
that have been overlooked or that are difficult to account for in modeling the environment. Indeed, it may be justifiably argued that the task of decision-making in the face
of environmental uncertainties is the very essence of building a robust dynamic system,
which is where intelligence plays the key role. (The Bayesian perspective of inference
Practical benefits of hierarchy in the perception–action cycle
The information-processing power of the perception–action cycle is enhanced by
building hierarchy in the form of layers or levels into the memory distributed in the
manner shown in Figure 2.1. In effect, each of the three memories depicted therein has
more than one hidden layer of neurons (i.e. computational units) built into its structural
design. To be specific, a layer of nonlinear neurons is said to be “hidden” if that layer is
not reachable from the input or output (Haykin, 2009). Here, we are thinking of a neural
networkk as the building block of the perceptual and executive memories.
Moreover, a neural architecture is said to be deep if it is composed of many hidden
layers of adaptive neurons – adaptive in the sense that their synaptic weights (i.e. free
parameters) are adjustable through training. According to Bengio and LeCun (2007),
a deep architecture permits the representation of a wide family of functions in a more
compact fashion than is possible with a shallow architecture. The rationale behind this
claim is that the use of a deep architecture makes it possible to trade off space for time,
while making the time–space product smaller; this rationale is indeed motivated by what
goes on in the human brain. The space–time tradeoff,
building depth into the design of memory in a cognitive dynamic system.
Moreover, the use of hierarchical depth in the memory distributed across the system
results in an enlarged number of local and global feedback loops in the perception–
action cycle of Figure 2.1; to be more precise, the number of local and global feedback
loops grows exponentially with hierarchical memory depth. An immediate practical
benefit of this exponential growth in feedback loops is enhanced intelligence and, hence,
a more reliable decision-making process in the face of environmental uncertainties.
Last but by no means least, learning features of features, in the sense first defined by
Selfridge (1958), becomes a distinct characteristic of the learning process involved in
designing memory with hierarchical depth. Specifically, the features characterizing the
incoming stimuli in the perceptor or those characterizing the feedback information in
the actuator become increasingly more abstract and, therefore, easier to recognize as the
hierarchical depth of memory is increased.
Therefore, we may summarize the overall benefit of hierarchical memory used in a
In a cognitive dynamic system with hierarchical memory, increased computational complexity is
traded for improved attentive and intelligent capabilities, particularly when the requirement is for
reliable decision-making in the face of environmental uncertainties.
Neural networks for parallel distributed cognitive information processing
Thus far in this chapter, we have focused attention on the perception–action cycle, its
properties, and different structural forms. In a few words, we may summarize the material covered therein as “the underlying principles of cognition in cognitive dynamic
systems.” However, an equally important issue that needs attention, for example, is the
following question that pertains to the hierarchical perception–action cycle of Figure 2.1:
How do we construct a model of a cognitive dynamic system based on the hierarchical
perception–action cycle depicted in Figure 2.1?
Clearly, there is no unique approach to address this practical issue. Nonetheless, we will
follow an approach inspired by the human brain, which is a complex, highly nonlinear,
and distributed information-processing system. The approach we have in mind is that of
To begin, we say: a “developing” nervous system is synonymous with a plastic
brain. Plasticity permits the developing nervous system to adaptt to its surrounding
2.7 Neural networks for parallel distributed cognitive information processing
environment. Just as plasticity appears to be essential to the functioning of neurons as
information-processing units in the human brain, so it is with neural networks made up
of artificial nonlinear neurons; nonlinearity is a desirable property to have in neuronal
processing. In its most general form, a neural networkk is a machine that is designed
to modell the way in which the brain performs a particular task or function of interest;
the network is usually implemented by using electronic components or is simulated in
software on a computer. Later on in the chapter we focus on an important class of neural
networks that perform useful computations through a process of learning. To achieve
good performance, neural networks employ a massive interconnection of “neurons”
(i.e. computational units). We may, thus, offer the following definition of a neural
A neural network is a massively parallel distributed processor made up of simple but nonlinear
processing units that has a natural propensity for storing experiential knowledge and making it
available for use. It resembles the brain in two respects:
(1) Knowledge is acquired by the network from its environment through a learning process.
(2) Interneuron connection strengths, known as synaptic weights, are used to store the acquired
The procedure used to perform the learning process is called a learning algorithm, the
function of which is to modify the synaptic weights (i.e. free parameters) of the network
in an adaptive fashion to attain a desired design objective. The modification of synaptic
weights through training provides the traditional method for the design of neural networks.
It is apparent that a neural network derives its computing power through, first, its massively parallel distributed structure and, second, its ability to learn and, therefore, generalize. Generalization refers to the neural network’s production of reasonable outputs
for inputs not encountered during training (learning). These two information-processing
capabilities make it possible for neural networks to find good approximate realizations
Neural networks offer the following useful properties and capabilities:
(1) Nonlinearity. An artificial neuron is typically (but not always) nonlinear. A neural
network, made up of an interconnection of nonlinear neurons, is itself nonlinear.
Moreover, the nonlinearity is of a special kind, in the sense that it is distributed
throughout the network. Nonlinearity is a highly important property, particularly if
the underlying physics of the function of interest is inherently nonlinear.
(2) Input–output mapping. A popular paradigm of learning, called learning with a
teacher, or supervised learning, involves modification of the synaptic weights of a
neural network by applying a set of labeled training (task) examples. Each example
consists of a unique input signall and a corresponding desired (target) response. The
network is presented with an example picked at random from the training set, and
the synaptic weights (free parameters) of the network are modified to minimize the
difference between the desired response and actual response of the network produced by the input signal in accordance with an appropriate statistical criterion. The
training of the network is repeated for many examples in the set, until the network
reaches a steady state, whereafter there are no further significant changes in the synaptic weights. The previously applied training examples may be reapplied during the
training session, but it should be in a different order. Thus, the network learns from
the examples by constructing an input–output mapping
Such an approach brings to mind the study of nonparametric statistical inference,
which is a branch of statistics dealing with model-free estimation, or, from a biological viewpoint, tabula rasa learning; the term “nonparametric” is used here to
signify the fact that no prior assumptions are made on a statistical model for the
(3) Adaptivity. Neural networks have a built-in capability to adaptt their synaptic weights
to changes in the surrounding environment. In particular, a neural network trained
to operate in a specific environment can be easily retrained
changes in the operating environmental conditions. Moreover, when it is operating
in a nonstationary environment (i.e. one where statistics change with time), a neural
network may be designed to change its synaptic weights in real time. The natural
architecture of a neural network for signal processing and control applications coupled with the adaptive capability of the network makes it a useful tool in adaptive
signal processing and control. As a general rule, it may be said that the more adaptive we make the system, all the time ensuring that the system remains stable, the
more robust its performance will likely be when the system is required to operate in
a nonstationary environment. It should be emphasized, however, that adaptivity does
not always lead to robustness; indeed, it may do the very opposite. For example, an
adaptive system with short-time constants may change rapidly and, therefore, tend to
respond to spurious disturbances, causing a drastic degradation in system performance. To realize the full benefits of adaptivity, the principal time constants of the
system should be just long enough for the system to ignore spurious disturbances,
yet short enough to respond to meaningful changes in the environment; the problem
described here is referred to as the stability–plasticity dilemma (Grossberg, 1988).
(4) Contextual information. Knowledge is represented by the very structure and activation state of a neural network. Every neuron in the network is potentially affected
by the global activity of all other neurons in the network. Consequently, contextual
information is dealt with naturally by a neural network (Rogers and McLelland,
(5) Fault tolerance. A neural network, implemented in hardware form, has the potential
to be inherently fault tolerant, or capable of robust computation, in the sense that its
performance degrades gracefully under adverse operating conditions. For example,
if a neuron or its connecting links are damaged, recall of a stored pattern is impaired
in quality. However, owing to the distributed nature of information stored in the
network, the damage has to be extensive before the overall response of the network
is degraded seriously. Thus, in principle, a neural network exhibits a graceful degradation in performance rather than catastrophic failure.
2.7 Neural networks for parallel distributed cognitive information processing
Figure 2.2. Nonlinear model of a neuron, labeled k; vk is referred to as the induced local field.
A neuron is an information-processing unit that is fundamental to the operation of a
neural network. The block diagram of Figure 2.2 shows the model of a neuron, in which
(1) A set of synapses, or connecting links, each of which is characterized by a weight
or strength of its own. Specifically, a signal xj at the input of synapse j connected to
neuron k is multiplied by the synaptic weight wkj. It is important to make a note of
the manner in which the subscripts of the synaptic weight wkjj are written. The first
subscript in wkjj refers to the neuron in question and the second subscript refers to
the input end of the synapse to which the weight refers. Unlike the weight of a synapse in the brain, the synaptic weight of an artificial neuron may lie in a range that
includes negative as well as positive values.
(2) An adderr for summing the input signals, weighted by the respective synaptic
strengths of the neuron; the operations described here constitute a linear combiner.
(3) An activation function for limiting the amplitude of the output of a neuron. The activation function is also referred to as a squashing function, in that it squashes (limits) the
permissible amplitude range of the output signal to some finite value; hence the nonlinear property of the neuron. Typically, the normalized amplitude range of the output
of a neuron is written as the closed unit interval [0, 1], or alternatively, [−1, 1].
The neural model of Figure 2.2 also includes an externally applied bias, denoted by
bk. The bias bk has the effect of increasing or lowering the net input of the activation
function, depending on whether it is positive (excitatory) or negative (inhibitory) respectively. The overall signal vk produced at the output of the summing junction, including
the bias, is referred to as the induced local field.
An important class of feedforward neural networks distinguishes itself by the presence
of one or more hidden layers, whose computation nodes are correspondingly called
Figure 2.3. Fully connected feedforward network with one hidden layer and one output layer.
hidden neurons or hidden units. As mentioned previously, the term “hidden” refers to
the fact that the hidden neurons are not seen directly from either the input or output
of the network. The function of hidden neurons is to intervene between the external
input and the network output in some useful manner. By adding one or more hidden
layers, the network is enabled to extract higher order statistics from its input. In a rather
loose sense, the network acquires a globall perspective despite its local connectivity, due
to the extra set of synaptic connections and the extra dimension of neural interactions
The source nodes in the input layer of the network supply respective elements of the
input vector, which constitute the input signals applied to the neurons’ computation
nodes in the first hidden layer. The output signals of this layer are used as inputs to the
second hidden layer, and so on for the rest of the network. Typically, the neurons in each
layer of the network have as their inputs the output signals of the preceding layer only;
in other words, the network computes features of features. The set of output signals of
the neurons in the output (final) layer of the network constitutes the overall response of
the network to the input signal vector supplied by the source nodes. The architectural
graph in Figure 2.3 illustrates the layout of a multilayer feedforward neural network for
The neural network in Figure 2.3 is said to be fully connected, in the sense that every
node in each layer of the network is connected to every other node in the adjacent
forward layer. If, however, some of the communication links (synaptic connections)
are purposely missed from the network for the purpose of reduced computational complexity and, quite possibly, improved performance, we say that the network is partially
Associative learning process for memory construction
Associative learning process for memory construction
An associative memory is a brainlike distributed memory that learns by association,
which has been known to be a prominent feature of human memory since the time of
Aristotle. All models of cognition use association in one form or another as the basic
Association takes one of two forms: autoassociation or heteroassociation. In autoassociation, a neural network is required to store a set of patterns (vectors) by repeatedly
presenting them to the network. The network is subsequently presented with a partial
description or distorted (noisy) version of an original pattern stored in it, and the task is
to retrieve (recall) that particular pattern. Heteroassociation differs from autoassociation in that an arbitrary set of input patterns is paired
patterns. Autoassociation involves the use of unsupervised learning, whereas the type of
learning involved in heteroassociation is supervised.
Let xk denote a key pattern (vector) applied to an associative memory and yk denote
a memorized pattern (vector). The pattern association performed by the network is
where q is the number of patterns stored in the network. The key pattern xk acts as a
stimulus that not only determines the storage location of memorized pattern yk, but also
In an autoassociative memory, we have yk = xk; hence, the input and output (data)
spaces of the network have exactly the same dimensionality. In a heteroassociative
memory, on the other hand, we have yk ≠ xk; hence, the dimensionality of the output
space in this second case may or may not equal the dimensionality of the input
There are two phases involved in the operation of an associative memory:
• storage space, which refers to the training of the network in accordance with (2.1), and
• recall phase, which involves the retrieval of a memorized pattern in response to the
presentation of a noisy or distorted version of a key pattern to the network.
Let the stimulus (input) x represent a noisy or distorted version of a key pattern xj. This
stimulus produces a response (output) y, as indicated in Figure 2.4. For perfect recall,
we should find that y = yj, where yj is the memorized pattern associated with the key
pattern xj. When y ≠ yj for x = xj, the associative memory is said to have made an error
Figure 2.4. Input–output relation of pattern associator.
The number of patterns q stored in an associative memory provides a direct measure
of the storage capacity of the network. In designing an associative memory, the challenge is to make the storage capacity q (expressed as a percentage of the total number n
of neurons used to construct the network) as large as possible, yet insist that a large
fraction of the memorized patterns is recalled correctly.
The hidden neurons of a multilayer perceptron (MLP) play a critical role as feature
detectors. A novel way in which this important property of the MLP can be exploited
is in its use as a replicator or identity map (Rumelhart et al., 1986). Figure 2.5
illustrates how this can be accomplished for the case of an MLP using a single
Figure 2.5. (a) Replicator network (identity map) with a single hidden layer used as encoder.
(b) Block diagram for the supervised training of the replicator network. (c) Part of the replicator
hidden layer. The network layout satisfies the following structural requirements, as
• the input and output layers have the same size, m;
• the size of the hidden layer is smaller than m; and
A given pattern x is simultaneously applied to the input layer as the stimulus and to the
output layer as the desired response. The actual response of the output layer x̂ is intended
to be an “estimate” of x. The network is trained using a learning algorithm, with the
estimation error vector, x x̂, treated as the error signal, as illustrated in Figure 2.5b. As
such, the training is performed in an unsupervisedd manner (i.e. without the need for a
teacher). By virtue of the special structure built into the design of the MLP, the network
d to perform identity mapping through its hidden layer. An encodedd version
of the input pattern, denoted by s, is produced at the output of the hidden layer, as indicated in Figure 2.5a. In effect, the fully trained MLP performs the role of an “encoder.”
To reconstruct an estimate x̂ of the original input pattern x (i.e. to perform decoding),
we apply the encoded signal to the hidden layer of the replicator network, as illustrated
in Figure 2.5c. In effect, this latter network performs the role of a “decoder.” The smaller
we make the size of the hidden layer compared with the size m of the input–output
layers, the more effective the configuration of Figure 2.5a will be as a data-compression
There are many learning algorithms that have been devised to train an MLP for a prescribed task of interest (Haykin, 2009). Among these algorithms, the back-propagation
algorithm is simple to implement, yet effective in performance. There are two phases to
(1) In the forward phase, the synaptic weights of the network are fixed and the input
signal is propagated through the network, layer by layer, until it reaches the output.
Thus, in this phase of the learning process, changes are confined to the activation
potentials and outputs of the neurons in the network.
(2) In the backward phase, an error signal is produced by comparing the output of the
network with a desired response. The resulting error signal is propagated through
the network, again layer by layer, but this time the propagation is performed in the
backward direction. In this second phase of the learning process, successive adjustments are made to the synaptic weights of the network. Calculation of the adjustments for the output layer is straightforward, but it is much more challenging for the
Usage of the term “back propagation” appears to have evolved after 1985, when the
term was popularized through the publication of the seminal book entitled Parallel
Distributed Processing (Rumelhart and McClelland, 1986).
Summary of the back-propagation algorithm
Figure 2.6a presents the architectural layout of an MLP. The corresponding signal-flow
graph for back-propagation learning, incorporating both the forward and backward
phases of the computations involved in the learning process, is illustrated in Figure 2.6b.
The top part of the signal-flow graph accounts for the forward pass. The lower part of the
signal-flow graph accounts for the backward pass, which is referred to as a sensitivity
Figure 2.6. (a) Architectural graph of a multilayer perceptron with two hidden layers.
(b) Signal-flow graphical summary of back-propagation learning. Top part of the graph refers
to forward pass and bottom part of the graph refers to backward pass.
graph for computating the local gradients in the back-propagation algorithm (Narendra
d of updating of weights is the preferred method for
on-line implementation of the back-propagation algorithm. For this mode of operation,
the algorithm cycles through the training sample{( ( ), d( n))}nN=1, where x(n) is the
input signal, d(n) is the desired response, and N is the size of the training sample. The
algorithm proceeds as follows (Haykin, 2009):
(1) Initialization. Assuming that no prior information is available, the synaptic weights
and thresholds are picked from a uniform distribution whose mean is zero and
whose variance is chosen to make the standard deviation of the induced local fields
of the neurons lie at the transition between the linear and standard parts of a sigmoid
(2) Presentations of training examples. The network is presented with an epoch of
training examples. For each example in the sample, ordered in some fashion, the
sequence of forward and backward computations described under points (3) and
(3) Forward computation. Let a training example in the epoch be denoted by (x(n),
d(n)), with the input vector x(n) applied to the input layer of sensory nodes and the
desired response vector d(n) presented to the output layer of computation nodes.
The induced local fields and function signals of the network are computed by proceeding forward through the network, layer by layer. The induced local field v (jl ) ( n)
where yi( l ) ( n) is the output (function) signal of neuron i in the previous layer l − 1
at iteration n, and w (jil ) ( n) is the synaptic weight of neuron j in layer l that is fed from
neuron i in layer l − 1. For i = 0, we have y0( l 1) ( n) = +1, and w (jl0) ( n) = b(j l ) ( n) is
the bias applied to neuron j in layer l. Assuming the use of a sigmoid function, the
If neuron j is in the first hidden layer (i.e. l = 1), we set
where xj (n) is the jth element of the input vector x(n). If neuron j is in the output
layer (i.e. l = L, where L is referred to as the depth of the network), we set
where oj (n) denotes the output signal. Correspondingly, the error signal
is computed for all j, where dj(n) is the jth element of the desired response
(4) Backward computation. The local gradients of the network, denoted by dds, are computed, using the definition
⎪ ϕ ′j (v j ( n))∑ d k ( )wk j ( ) for neuron j in hidden layer l < L,
where the prime in ϕ′ j (⋅) denotes differentiation with respect to the argument. The
synaptic weights of the network in layer l are adjusted according to the generalized
) = w (jil ) ( n) + a [ w (jil ) ( n) w (jil ) ( n − 1)] + hd
where h is the learning-rate parameter and a is the momentum constant; the
momentum term is included on the right-hand side of (2.5) to help the algorithm
(5) Iteration. The forward and backward computations under points (3) and (4) are iterated by presenting new epochs of training examples to the network until the chosen
Notes. The order of presentation of training examples should be randomized from epoch
to epoch. The momentum and learning-rate parameter are typically adjusted (and usually decreased) as the number of training iterations increases.
As the name implies, a recurrent multilayer perceptron (RMLP) is a neural network
with one or more hidden layers, and with each computation layer of the network
having feedback around it. Such a network is illustrated in Figure 2.7 for the case of
an RMLP with two hidden layers. For the same reasons that ordinary (static) MLP
are often more effective and parsimonious than neural networks with a single hidden
layer, so it is with an RMLP. Moreover, the use of one or more feedback loops in the
network, as illustrated in Figure 2.7, makes the RMLP not only dynamic, but also
more computationally powerful compared with an ordinary MLP.
Let the vector xI,n denote the output of the first hidden layer, xII,n denote the output of
the second hidden layer, and so on. Let the vector xo,n denote the ultimate output of the
output layer. Then, the general dynamic behavior of the RMLP in response to an input
vector un is described by a system of coupled equations given as
Figure 2.7. RMLP; feedback paths in the network are printed in color.
where f I(.,.), f II (.,.), . . ., fo(.,.) denote the activation functions characterizing the first
hidden layer, second hidden layer, . . ., and output layer of the RMLP respectively, and
K denotes the number of hidden layers in the network; in Figure 2.7, K = 2. Note that
in (2.6) the iteration number n is used as a subscript to simplify the notation.
Supervised learning, exemplified by the back-propagation algorithm, requires the availability of a desired response vector, against which the actual output signal vector of the neural
network is measured. This requirement, as useful as it is, limits the practical application of
supervised-learning algorithms. To overcome this limitation, we look to self-organized or
unsupervised learning procedures. Just as there are several ways of formulating supervised
learning algorithms, so it is with unsupervised learning algorithms (Haykin, 2009). In this
section, we describe an unsupervised learning algorithm called the generalized Hebbian
algorithm (GHA), which was first described in the literature by Sanger (1989).
As the algorithm’s name implies, the GHA is rooted in Hebb’s postulate of learning
(Hebb, 1949), which is the oldest and most famous of all learning rules. In signalprocessing terms, this learning rule is based on a correlational mechanism, described as
(1) If two neurons on either side of a synapse (i.e. connecting link) are activated synchronously,
then the weight of that synapse is significantly increased.
(2) If, on the other hand, the two neurons are activated asynchronously, then that synapse is selectively weakened in strength or eliminated altogether over the course of time.
Mathematically, we may thus formulate Hebb’s postulate of learning simply as follows:
Δ kj ( n) denotes the change in the strength (weight) of the synapse connecting
node k to node j at time n, xj(n) is the input signal, yk(n) is the output signal, and h is the
learning-rate parameter. From (2.7), we find that the repeated application of the input
signal xj(n) leads to an increase in the output signal yk(n) and, therefore, exponential
growth that finally drives the synapse into saturation. At this point, no new information
is stored in the synapse and selectivity is thereby lost. Some mechanism is therefore
needed to stabilize the self-organized behavior of the neurons, which may be achieved
through the incorporation of competition into the learning rule, as described next.
Consider the feedforward neural network of Figure 2.8, for the operation of which the
following two structural assumptions are made:
(1) Each neuron in the output layer of the network is linear.
(2) The network has m inputs and l outputs, with fewer outputs than inputs; that is,
The only aspect of the network that is subject to training is the set of synaptic weights
wji} connecting the source nodes i in the input layer to computational nodes j in the
output layer, where i = 1, 2, . . ., m and j = 1, 2, . . ., l. The output yj(n) of neuron j at time
n produced in response to the set of inputs { i ( n)}im=1 is given as follows:
The synaptic weight wji(n) is adapted in accordance with a generalized form of Hebbian
Δw ji ( n) = h ⎜ y j ( n) xi ( n) − y j ( n)∑ wki ( n) yk ( n)⎟ ,
Figure 2.8. Feedforward network with single layer of computational nodes. The sets { i }im=1 and
{ j }lj =1 constitute the input and output vectors of the network respectively.
where i = 1, 2, . . ., m and j = 1, 2, . . ., l. The term Δji(n) is the change applied to the synaptic weight wji(n) at time n and h is the learning-rate parameter.
Examining (2.9), we see that the term h yj(n)xi(n) on the right-hand side of the equation is attributed to Hebbian learning. As for the second term
we attribute it to a competitive process that goes on among the synapses in the network.
Simply put, as a result of this process, the most vigorously growing (i.e. fittest) synapses
or neurons are selected at the expenses of the weaker ones. Indeed, it is this competitive
process that alleviates the exponential growth in Hebbian learning working by itself.
Note that stabilization of the algorithm through competition requires the use of a minus
Equations (2.8) and (2.9) sum up the GHA. The distinctive feature of this algorithm
is that it operates in a self-organized manner, thereby avoiding the need for a desired
response. It is this important characteristic of the algorithm that befits it for on-line
To develop insight into the behavior of the GHA, we rewrite (2.9) in the form
where x i′ ( n) is a modified version of the ith element of the input vector x(n); it is a
Next, we go one step further and rewrite (2.10) in a form that corresponds to Hebb’s
xi′′ ( n) = xi′ ( n) − w ji ( n) y j ( n).
is the unit-time delay operator, we may construct the signal-flow graph
of Figure 2.9 for the GHA. From this graph, we see that the algorithm lends itself
Figure 2.9. Signal-flow graph of the GHA.
to a local form of implementation, provided that it is formulated as in (2.14). Note
also that yj(n), responsible for feedback in the signal-flow graph in Figure 2.9 is
itself determined by (2.8); signal-flow graph representation of this latter equation is
This chapter presented a detailed description of the many facets of the perception–action
cycle, which is of fundamental importance to the study of cognitive dynamic systems.
There are four basic functions embodied in this cyclic operation:
(1) Perception of the environment in the perceptor, followed by action taken by the
actuator in the environment in response to feedback information computed by the
(2) Perceptual memory in the perceptor and executive memory in the actuator, and the
reciprocal coupling between them via the working memory; the function of memory
is to predict the consequences of actions in the system.
(3) Attention to prioritize the allocation of available resources in the system.
(4) Intelligence, which is best described as the ability of the system to continually adjust
itself through an adaptive process by responding to new changes in the environment,
with new forms of action and behavior being created through a decision-making
mechanism based on intelligent choices in the face of uncertainties in the environment.
With perception in the receiver looking to the environment for stimuli (observables) to
process and the environment itself being stochastic, it follows that, in reality, perception is
a probabilistic process. As such, we may formulate the problem of perception as follows:
Given a set of stimuli received from the environment, estimate the hidden state of the environment
in the environmental scene analyzer as accurately as possible.
To solve this state-estimation problem, we may view perception as an ill-posed
inverse problem. Perception is an “inverse problem” because we are trying to uncover
the underlying physical laws responsible for generation of the stimuli; that is, attempting
to find a mapping from the stimuli to the state. In practice, we typically find that inverse
problems are ill-posedd because, for one reason or another, they violate the conditions for
well-posedness. According to Hadamard, for the problem of “mapping from stimuli to
state” to be well posed, three conditions must be satisfied (Haykin, 2009):
• continuity, also referred to as stability,
which clearly speak for themselves. For example, power-spectrum estimation from a
finite set of observable data, to be discussed in Chapter 3, is an ill-posed inverse problem
because as we shall see in Chapter 3, the condition for uniqueness is violated. The issue
of spectrum is central to the study of cognitive radio, the very purpose of which is to
identify the spectrum holes (i.e. unused or partially used subbands of the radio spectrum) and exploit them for improved utilization of the radio spectrum. In effect, the
spectrum holes define the “state” of the radio environment insofar as cognitive radio
is concerned. In any event, the spectrum can be identified directly from the incoming
radio-frequency (RF) stimuli through the application of a power-spectrum estimation
We may solve the state-estimation problem in yet another way: perception is viewed
as a process of Bayesian inference. In this second viewpoint, information contained
in the stimuli about an object of interest is represented by a conditional probability
density function over the unknown state that is commonly referred to as the posterior.
Through the adoption of Bayesian inference, the posterior is formulated as the product
of two probabilistic functions: the likelihood and the prior. The likelihood is a function
of the unknown state given the stimuli. As for the prior, it provides information already
available about the environment “before” the estimation is performed. The perception
problem is then solved through a recursive estimation procedure aimed at computing
the state for which the posterior attains its maximum value. This viewpoint, adopted
in Chapter 4, is well suited for cognitive radar, where the perception problem involves
estimating the state of an unknown target.
The two views of perception discussed herein, namely
pave the way nicely for the next two chapters: “Power-spectrum estimation for sensing
the environment” and “Bayesian filtering for state estimation of the environment”
The term “working memory” is of recent origin; it was coined by Baddeley (2003).
What we now call working memory was previously referred to as “short-term memory,”
which is the ability to remember stored information over a short period of time.
However, there is no unanimity in the neurophysiological community on working
memory. According to Fuster (2003), the term working memory is a “theoretical”
construct that is essentially long term that has been activated for a short time for the
Nevertheless, we follow the description coined by Baddeley. From an engineering
perspective, Baddeley’s description befits the role described in Section 2.3 for
It appears that, long before the advent of the Internet, the term “information
overload” was mentioned by Gross (1964) in his book on The Managing of Organizations. According to Wikipedia, its popularization is attributed to Alvin Toffler.
For books on neural networks and learning machines, the reader is referred to:
• Neural Networks and Learning Machines by Haykin (2009),
• Pattern Recognition and Machine Learning
• The Elements of Statistical Learningg by Hastie et al. (2001), and
• Parallel Distributed Processing, Volume 1: Foundations by Rumelhart and
This definition of a neural network is adapted from Aleksander and Morton (1990).
Hecht-Nielsen (1995) describes a replicator neural network in the form of an MLP
with an input layer of source nodes, three hidden layers, and an output layer:
• The activation functions of neurons in the first and third hidden layers are defined
where v is the induced local field of a neuron in those layers.
• The activation function for each neuron in the second hidden layer is given by
where a is a gain parameter and v is the induced local field of a neuron in that
layer. The function ϕ ( )( ) describes a smooth staircase activation function with
N treads, thereby essentially quantizing the vector of the respective neural outputs
into K = Nn, where n is the number of neurons in the middle hidden layer.
• The neurons in the output layer are linear, with their activation functions defined
• Based on this neural network structure, Hecht-Nielsen describes a theorem
showing that optimal data compression for arbitrary input data vector can be carried out.
The sigmoid activation function is defined by
where v is the input and ϕ ( ) is the output. Note that
The input v applied to the sigmoid function of a neuron is referred as the induced
Another activation function, representing an alternative to the sigmoid function,
is the hyperbolic tangent function, defined by
Whereas the sigmoid function never assumes a negative value, the hyperbolic tangent function is symmetric about the horizontal axis and may, therefore, assume
Self-organization through the use of Hebbian learning provides one method for the
formulation of unsupervised learning algorithms. Another way in which unsupervised learning can be accomplished is to look to information theory for ideas. In
particular, we find that the idea of mutual information applied to a feedforward neural
network provides a powerful basis for unsupervised learning. (Mutual information is
discussed in Note 8 in Chapter 6.) For example, we may start with the mutual information between the input signal and output signal of the network. By maximizing
this mutual information, viewed as the objective function of interest, the network
is enabled to extract the important features that characterize the input signal. For a
detailed treatment of information-theoretic learning models, the interested reader is
Another important way in which on-line learning can be accomplished is through
reinforcement learning, which builds its formalism on experience gained through
continued interactions of an agentt with its environment; discussion of this important
approach to on-line learning is taken up in Chapter 5.
As discussed in Chapter 2, perception of the environment is the first of four defining properties of cognition, putting aside language. To cater for perception in an engineering context, we not only need to sense the environment, but also focus attention on an underlying
discriminantt of the environment that befits the application of interest. In this chapter,
we focus attention on spectrum estimation, which is of particular interest to sensing the
environment for cognitive radio.1 Moreover, with radar relying on the radio spectrum for
its operation, spectrum sensing is also of potential interest to cognitive radar.
With spectrum estimation viewed as the discriminant for sensing the environment,
there are four related issues that are of practical interest, each in its own way:
(1) Power spectrum, where the requirement is to estimate the average power of the
incoming signal expressed as a function of frequency.
(2) Space–time processing, where the average power of the incoming signal is expressed
as a function of frequency and spatial coordinates.
(3) Time–frequency analysis, in which the average power of the signal is expressed as a
(4) Spectral line components, for which a special test is needed.
Throughout the presentation, the emphasis will be on the use of a finite set of observable
data (measurements), which is how it is in practice.
In Section 2.12 we stated that perception in a cognitive dynamic system may be
viewed as an ill-posed inverse problem. With power-spectrum estimation2 being an
example of perception of the environment, that statement applies equally well to each of
We start the discussion by recognizing that the incoming signal picked up by an environmental sensor(s) is typically the sample function of a stochastic (random) process. As
such, although it is not possible to predict the exact value of the signal at some point of
time in the future, it is possible to describe the signal in terms of statistical parameters,
such as the average power of the signal distributed across frequency. This parameter is
called the power spectrum or power spectral density of the signal; both terms are used
interchangeably in the literature, as well as in this book.
Power-spectrum estimation for sensing the environment
A stochastic process is said to be wide-sense stationary or weakly stationary if the
(1) The mean, defined by the expectation of any random variable sampled from the
(2) The autocorrelation function, defined by the expectation of the product of any
random variable sampled from the process and its delayed version, is dependent
It turns out that, when the stochastic process is weakly stationary, the autocorrelation function and power spectrum of the process form a Fourier-transform pair. In
other words, in accordance with the Wiener–Khintchine relations, the power spectrum is the Fourier transform of the autocorrelation function and, by the same token,
the autocorrelation function is the inverse Fourier transform of the power spectrum (Haykin, 2000). Of these two functions, however, we find that, in practice, the
power spectrum is the more important one of the two. Intuitively, this makes sense,
considering the practical importance of average power expressed as a function of
With the power spectrum representing an important characteristic of stochastic processes, an issue of practical importance is how to estimate it. Unfortunately, this issue is
complicated by the fact that there is a bewildering array of power-spectrum estimation
procedures, with each procedure purported to have or to show some optimum property.
The situation is made worse by the fact that, unless care is taken in the selection of the
right method, we may end up with misleading conclusions.
In parametric methods of spectrum estimation, we begin by postulating a stochastic
model for the situation at hand. Depending on the specific model adopted, we may identify three different parametric approaches for spectrum estimation:
(1) Model-identification procedures. In this class of parametric methods, a rational
π f is assumed for the transfer funcfunction or a polynomial in the exponential e−j
tion of the model and a white-noise source is used to derive the model, as depicted
in Figure 3.1. The power spectrum of the resulting model output provides the desired
spectrum estimate. Depending on how the stochastic model is formulated, there are
(i) autoregressive (AR) modell with an all-pole transfer function;
(ii) moving-average (MA) modell with an all-zero transfer function; and
(iii) autoregressive-moving-average (ARMA) modell with a pole–zero transfer
Figure 3.1. Rationale for model-identification procedure for power spectrum estimation.
Figure 3.2. Rationale of MVDR procedure for power spectrum estimation.
The resulting power spectra measured at the outputs of these models are referred to
as AR, MA, and ARMA spectra respectively. The input–output relation of the model
H j2ππ f ) denotes the frequency response of the model,
Si( f ) denotes the power spectrum of the signal at the model input, and So( f ) denotes
the power spectrum of the model output. With white noise used as the driving force,
the power spectrum Si( f ) of the model input is equal to the white-noise variance
s 2. We then find that the power spectrum So( f ) of the model output is equal to
H j2ππ f )|2 of the model, multiplied by a constant,
namely, s . The problem thus becomes one of estimating the model parameters that
H j2ππ f ), such that the process produced at the
model output provides an acceptable representation (in some statistical sense) of
the stochastic process under study. Such an approach to power-spectrum estimation
may, therefore, be viewed as a problem in model (system) identification.
Among the model-dependent spectra defined herein, the AR spectrum is by far
the most popular. The reason for this popularity is twofold: (1) the linearr form of
the system of simultaneous equations involving the unknown AR model parameters
and (2) the availability of efficient algorithms for computing the solution.
(2) Minimum-variance distortionless response (MVDR) method. To describe this second
parametric approach for power-spectrum estimation, consider the situation depicted
in Figure 3.2. The process, represented by the time series {x(n)}, is applied to a
transversal filter (i.e. a discrete-time filter with an all-zero transfer function). In the
MVDR method, the filter coefficients are chosen to minimize the variance (which is
the same as the expected power for a zero-mean process) of the filter output, subject
to the constraint that the frequency response of the filter is equal to unity at some
desired frequency f0. Under this constraint, the time series {x(n)} is passed through
the filter with no distortion at the frequency f0. Moreover, signals at frequencies
Power-spectrum estimation for sensing the environment
(3) Eigendecomposition-based methods. In this third class of parametric spectrum
estimation methods, the eigendecomposition of the ensemble-average correlation matrix of the time series {x(n)} is used to define two disjoint subspaces:
signal subspace and noise subspace. This form of partitioning is then exploited
to derive an appropriate algorithm for estimating the power spectrum of the
In nonparametric methods of power-spectrum estimation, no assumptions are made with
respect to the stochastic process under study insofar as process modeling is concerned.
In other words, nonparametric methods are model free. Here, we may identify two different nonparametric approaches:
(1) Periodogram-based methods. Given an infinitely long time series { ( n)}nN=−01, its
where XN ( f ) is the Fourier transform of { ( n)}nN=−01 and E is the statistical expectation operator. Mathematical justification for the definition of power spectrum of
a stationary process in (3.1) is presented in Note 2 at the end of the chapter. Note
also that, in (3.1), the expectation is performed first, followed by the limit operation
for the data size N to approach infinity; these two operations cannot, therefore, be
According to this formula, the periodogram, namely | XN ( f )|2/N
starting point for data analysis. However, the periodogram suffers from a serious
limitation, in the sense that it is not a sufficient statistic for the power spectrum. We
say so because phase information about the data {x(n)} is ignored in computing the
periodogram. Consequently, the statistical insufficiency of the periodogram is inherited by any estimator based on or equivalent to the periodogram.
(2) Multitaper method (MTM). A more constructive nonparametric approach is to
adopt a set of special sequences known as Slepian sequences, which are fundamental to the study of time- and frequency-limited systems. The remarkable
property of this family of windows is that the energy distributions of the windows
(i.e. sequences) add up in a very special way that collectively defines an ideal rectangular frequency bin – ideal in the sense that the total in-bin versus out-of-bin
energy concentrations is maximized. This property, in turn, allows us to trade
spectral resolution for improved spectral properties (e.g. reduced variance of the
When the stochastic process of interest has a purely continuous power spectrum and
the underlying physical mechanism responsible for the generation of the process is
unknown, then the recommended procedure is the nonparametric method of multiple
windows (tapers), hereafter referred to as the MTM,
In the older spectrum-estimation literature on nonparametric methods, it was emphasized that the estimation problem is difficult because of the bias–variance dilemma,
which encompasses the interplay between two conflicting statistical issues:
• Bias of the power-spectrum estimate of a time series due to the sidelobe-leakage phenomenon, the effect of which is reduced by taperingg (i.e. windowing) the time series.
• The cost incurred by this improvement is an increase in variance of the estimate, which
is due to the loss of information resulting from a reduction in the effective sample size.
How, then, can we resolve this dilemma by mitigating the loss of information due to
tapering? The answer to this fundamental question lies in the principled use of multiple
orthonormal tapers (windows). Specifically, the procedure linearlyy expands the part of
the time series lying in a fixed bandwidth extending from f − W to f + W (centered on
some frequency f ) in a special family of sequences known as the Slepian sequences;3
these sequences are also referred to in the literature as discrete prolate spheroidal
sequences. The remarkable property of Slepian sequences is that their Fourier transforms
have the maximal energy concentration in the bandwidth 2W
finite sample-size constraint. This property, in turn, permits trading spectral resolution
for improved spectral characteristics; that is, reduced variance of the spectral estimate
without compromising the bias of the estimate. In other words, the old bias–variance
tradeoff is now replaced with a bias–resolution tradeoff,
of, also solves the variance problem. The MTM can, therefore, produce an accurate estimate of the desired power spectrum, which is an important objective in its estimation.
Attributes of multitaper spectral estimation
From a practical perspective, multitaper spectral estimators have several desirable features, summarized herein (Haykin et al., 2009):
(1) In multitaper spectral estimation, the bias is decomposed into two quantifiable
• local bias, due to frequency components residing inside the user-selectable band
• broadband bias, due to frequency components found outside this band.
(2) The resolution of multitaper spectral estimators is naturally defined by width of the
(3) Multitaper spectral estimators offer an easy-to-quantify tradeoff between bias and
(4) Direct spectrum estimation can be performed with more than just two degrees of
freedom (DoFs); typically, the DoFs vary from 6 to 10, depending on the time–
bandwidth product used in the estimation.
(5) Multitaper spectral estimation has a built-in form of regularization, which overcomes
the ill-posed inverse problem associated with the power-spectrum estimation.4
Power-spectrum estimation for sensing the environment
With these highly desirable features built into the composition of a multitaper spectral
estimator, it is not surprising, therefore, to find that it outperforms other well-known
spectral estimators, as discussed later in the section.
Let n denote discrete time, and the time series{ ( n)}n = 0 represent the signal of interest.
Given this time series, the MTM determines the following parameters:
• an orthonormal sequence of Slepian tapers, denoted by υn( k )
• a corresponding set of Fourier transforms, defined by
where k = 0, 1, . . ., K − 1 and K is the total number of Slepian tapers. The energy distributions of the eigenspectra, defined by |X
Xk( f )|2 for varying k, are all concentrated inside
therefore, bounds the number of tapers (windows) as shown by
which, in turn, defines the DoFs available for controlling the variance of the multitaper
spectral estimator. The choice of parameters Co and K ≤ 2C
spectral resolution, bias, and variance. The bias of these estimates is largely controlled
by the largest eigenvalue, denoted by l 0(N
This formula gives directly the fraction of the total sidelobe energy; that is, the total
leakage into frequencies outside the interval (−W,
decreases very rapidly with Co, as may be seen in Table 3.1.
Table 3.1. Leakage properties of the lowest order Slepian sequence as a function of the time–
bandwidth product Co (column 1). Column 2 of the table gives the asymptotic value of 1 − l 0 (C
and column 3 is the total sidelobe energy expressed in decibels (relative to total energy in the signal).
Table reproduced with permission from S. Haykin, D.J. Thomson and J. Reed, 2009, Spectrum sensing
for cognitive radio, Proceedings of the IEEEE, 97, 849–877.
A natural spectral estimate, based on the first few eigenspectra that exhibit the least
sidelobe leakage, is given by the normalized formula
where Xk( f ) is the Fourier transform of x(n), defined in (3.2), and lk is the eigenvalue
associated with the kkth eigenspectrum. It is the denominator in the MTM formula of
(3.4) that makes the spectral estimator Sˆ ( f ) unbiased. This spectral estimator is intuitively appealing in the way it works: as the number of Slepian tapers K increases, the
eigenvalues decrease, causing the eigenspectra to be more contaminated by leakage.
But, the eigenvalues themselves counteract by reducing the weighting applied to higher
Adaptive modification of multitaper spectral estimation
While the lower order eigenspectra have excellent bias properties, there is, unfortunately, some degradation as the order K increases toward the limiting value defined
in (3.3). To mitigate this shortcoming, a set of adaptive weights, denoted by {ddk( f )},
is introduced to downweight the higher order eigenspectra. Using a mean-squareerror optimization procedure, the following formula for the weights is derived
where S( f ) is the true power spectrum, Bk( f ) is the broadband bias of the kkth eigenspectrum, and E is the statistical expectation operator as before. Moreover, we find that
where s 2 is the process variance, defined in terms of the time series x(n) by
In order to compute the adaptive weights dk( f ) using (3.5), we need to know the
true spectrum S( f ). But if we did know it, then there would be no need to perform
any spectrum estimation in the first place. Nevertheless, (3.5) is useful in setting up
an iterative procedure for computing the adaptive spectral estimator, as shown by
Power-spectrum estimation for sensing the environment
Note that if we set |ddk( f )|2 = lk for all k, then the estimator of (3.8) reduces to that of
Next, setting the actual power spectrum S( f ) equal to the spectrum estimate Sˆk ( f )
in (3.5), then substituting the new equation into (3.8) and collecting terms, we get (after
where Bˆk ( f ) is an estimate of the expectation E[Bk( f )]. Using the upper bound of
We now have all the terms that we need to solve for the null condition of (3.10) via the
where j denotes an iteration step; that is, j = 0, 1, 2, . . .. To initialize the recursion of
(3.12), for a reasonably good starting point, we may set Sˆ ( ) ( ) equal to the average of
the two lowest order eigenspectra. Convergence of the recursion described in (3.12) is
usually rapid, with successive spectral estimates differing by less than 5% in 5 to 20 iterations (Drosopoulos and Haykin, 1992). The result obtained from (3.12) is substituted
into (3.5) to obtain the desired weights, dk( f ).
A useful by-product of the adaptive spectral estimation procedure is a stability
which is the approximate DoF for the estimator Sˆk ( f ), expressed as a function of frequency f If ν , denoting the average of v( f ) over frequency ff, is significantly less than
K then the result is an indication that either the bandwidth W is too small or additional
prewhitening of the time series x(n) should be used.
The importance of prewhiteningg cannot be stressed enough, particularly for RF data.
In essence, prewhitening reduces the dynamic range of the spectrum by filtering the
data, prior to processing. The resulting residual spectrum is nearly flat or “white.” In
particular, leakage from strong components is reduced, so that the fine structure of
weaker components is more likely to be resolved. In actual fact, most of the theory
behind spectral estimation is smooth, almost white-like spectra to begin with, hence the
(1) Estimation of the power spectrum based on (3.4) is said to be incoherent, because
the kkth eigenspectrum | k ( f ) |2 ignores phase information for all values of the
(2) For the parameters needed to compute the multitaper spectral estimator (3.4), recommended values (within each section of data) are as follows:
• parameter Co = 4, possibly extending up to 10;
• number of Slepian tapers K = 10, possibly extending up to 16.
These values are needed, especially when the dynamic range of the data being analyzed is large.
(3) If, and when, the number of tapers is increased toward the limiting value 2NW,
the adaptive multitaper spectral estimator should be used.
(4) Whenever possible, prewhitening of the data prior to processing should be applied.
Comparison of the MTM with other spectral estimators
Now that we understand the idea of multitaper spectral estimation, we are ready to compare its performance against other spectral-estimation algorithms. The results described
herein summarize previous experimental work that was originally reported by Drosopoulos and Haykin (1992) and reproduced in Haykin (2007). The test dataset used in that
previous work was Marple’s classic synthetic dataset, the analytic spectrum of which
is known exactly (Marple, 1987). Specifically, the dataset is composed of the following
• two complex sinusoids of fractional frequencies 0.2 and 0.21 that are included to test
the resolution capability of a spectral estimator;
• two weaker complex sinusoids of 20 dB less power at fractional frequencies −0.15
• colored noise process, generated by passing two independently zero-mean real whitenoise processes through identical moving-average filters; each filter has an identical
raised-cosine frequency response centered at ±0.35 and occupying a bandwidth
Following Marple (1987), the experimental study by Drosopoulos and Haykin (1992)
• the periodogram with a 4096-point fast Fourier transform (FFT) and
• a tapered version of the same periodogram, using a Hamming window.
With spectral line components featuring prominently in the dataset, the experimental
study also included two eigendecomposition-based spectral estimators:
• Multiple SIgnal Classification (MUSIC) algorithm (Schmidt, 1981) and
• Modified Forward–Backward Linear Prediction (MFBLP) algorithm (Kumarasan and
Power-spectrum estimation for sensing the environment
The two classical spectral estimators failed in resolving the line components and also
failed in correctly estimating the continuous parts of Marple’s synthetic spectrum. On
the other hand, the two eigendecomposition-based algorithms appeared to resolve the
line components reasonably well, but failed completely to estimate the continuous parts
Next, the MTM formula (3.4) was tested with Marple’s synthetic data, using a time–
bandwidth product Co = 4 and K = 8 Slepian tapers. The resulting composite spectrum
appeared to estimate the continuous parts of the synthetic spectrum reasonably well,
correctly identify the locations of the line components at –0.15 and 9.1, but lumped
the more closely spaced line components at 0.2 and 0.21 into a composite combination
around 0.21. With additional complexity featuring the inclusion of the harmonic F-test
for spectral line components, to be discussed later in Section 3.7, the composite spectrum computed by the MTM did reproduce the synthetic spectrum fully and accurately;
F-test, included with the MTM, is intended to check for an estimate of line components existing in the power spectrum.
In light of the experimental results summarized herein, it can be said that the basic
formula of the MTM in (3.4) did outperform the periodogram and its Hamming-tapered
version, which is not surprising. Moreover, it outperformed the MUSIC and MFBLP
algorithms insofar as the continuous parts of the spectrum are concerned, but did not
perform as well in dealing with the line components. However, when the composite
MTM spectrum was expanded to include the F
F-test for line components, Marple’s synthetic spectrum was reconstructed fully and accurately.
It is also noteworthy that in Drosopoulos and Haykin (1992) and Haykin (2007), a
comparative evaluation of the MTM and the traditional maximum-likelihood estimation
procedure, known for its optimality, is presented for angle-of-arrival estimation in the
presence of multipath using real-life radar data. The results presented therein for low
grazing angles show that these two methods are close in performance.
In another comparative study, Bronez (1992) compared the MTM with weighted overlapped segment averaging (WOSA), which was originally proposed by Welch (1967).
To make this comparison, theoretical measures were derived by Bronez in terms of
leakage, variance, and resolution. The comparison was performed by evaluating each
one of these three measures in turn, while keeping the other two measures equal. The
results of the comparison demonstrated that the MTM always performed better than
WOSA. For example, given the same variance and resolution, it was found that the
MTM had an advantage of 10 to 20 dB over WOSA.
As already discussed, the MTM is theorized to provide a reliable and accurate method
of estimating the power spectrum of environmental stimuli as a function of frequency.
As such, in the MTM we have a reliable method for identifying spectrum holes and
estimating their average-power contents for cognitive radio applications; as mentioned
previously in Chapters 1 and 2, spectrum holes refer to underutilized subbands of the
radio spectrum. In analyzing the radio scene in the local neighborhood of a cognitive
radio receiver, for example, we also need to have a sense of direction, so that the cognitive radio is enabled to listen to incoming interfering signals from unknown directions.
What we are signifying here is the need for space–time processing. For such an application, we may employ a set of sensors to properly “sniff” the RF environment along
To elaborate on the underlying theory of space–time processing, consider an array of
M antennas sensing the environment. For the kkth Slepian taper, let X k( m ) ( f ) denote the
complex-valued Fourier transform of the input signal x(n) computed by the mth sensor
in accordance with (3.2), and m = 0, 1, . . ., M − 1. With k = 0, 1, . . ., K − 1, where K is
the number of Slepian sequences, we may then construct the M-byM
⎢ a( 11)) X ( 1) ( f ) a( 1) X ( M −1) ( f )
where each row of the matrix is produced by environmental stimuli sensed at a different
gridpoint, and each column is computed using a different Slepian taper; the ak( m ) represent coefficients accounting for different localized areas around the gridpoints.
To proceed further, we make two necessary assumptions:
(1) The number of Slepian tapers K is largerr than the number of sensors M
M; this requirement is needed to avoid “spatial undersampling” of the environment.
(2) Except for being synchronously sampled, the M sensors operate independently of
each other; this second requirement is needed to ensure that the rankk of the matrix
A( f ) (i.e. the number of linearly independent rows) is exactly equal to M.
In physical terms, each entry in the matrix A( f ) is produced by two contributions: one
due to additive ambient noise at the front end of the sensor and the other due to the
incoming interfering signals. Insofar as spectrum sensing is concerned, however, the
primary contribution of interest is that due to the environmental stimuli. In this context,
an effective tool for denoising is singular-value decomposition (SVD).
SVD is a generalization of principal-components analysis, or eigendecomposition.
Whereas eigendecomposition involves a single orthonormal matrix, SVD involves a
pair of orthonormal matrices, which we denote as follows:
M-by-M matrix U( f ) defined by the M sensors and
K matrix V( f ) defined by the K Slepian sequences.
Thus, applying SVD to the spatio-temporal matrix A( f ) defined in (3.14), we may
express the resulting decomposition as follows (Golub and Van Loan, 1996):
Power-spectrum estimation for sensing the environment
Figure 3.3. Diagrammatic depiction of SVD applied to a rectangular matrix A; left-hand side of
the figure represents the matrix product U†AV and the right-hand side represents the resulting
M diagonal matrix, the kkth element of which is denoted by
s k( f ), and 0 is a null matrix; the superscript † denotes Hermitian transposition.
Figure 3.3 shows a graphical depiction of this decomposition; to simplify the depiction,
dependence on the frequency f has been ignored in the figure.
Henceforth, the system described by the spatio-temporal matrix A( f ) of (3.14),
involving K Slepian tapers, M sensors, and decomposition of the matrix in (3.15), is
referred to as the MTM-SVD processor. Note that with the spatio-temporal matrix A( f )
being frequency dependent, and likewise for the unitary matrices U( f ) and V( f ), the
MTM-SVD processor is actually performing tensor analysis.
Physical interpretation of the action performed by the MTM-SVD processor
To understand the underlying signal operations embodied in the MTM-SVD processor,
we begin by reminding ourselves of the orthonormal properties of matrices U and V that
K identity matrices respectively. Using this pair
of relations in (3.15), we obtain the following decomposition of the matrix A( f ), after
s m( f ) is the mth singular value of the matrix A( f ), um( f ) is the left-singular vector,
and vm( f ) is the right-singular vector. In analogy with principal-components analysis,
the decomposition of (3.16) may be viewed as one of principal modulations produced
by the incoming time–frequency stimuli. According to this decomposition, the singular
value s m( f ) provides a scaling of the mth principal modulation computed by the MTMSVD processor. Note also that the M singular values, constituting the diagonal matrix
Σ(( f ) in (3.15), are all real numbers. The higher order singular values, namely, s m( f ),
. . ., s K−1( f ), are all zero; they constitute the null matrix 0 in (3.15).
Using (3.16) to form the matrix product A( f )A†( f ) and invoking the orthonormal
property of the unitary matrix V( f ), we have the eigendecomposition
where σ m2 ( f ) is the mth eigenvalue of the eigendecomposition. Similarly, forming the
other matrix product A†( f )A( f ) and invoking the orthonormal property of the unitary
matrix U( f ), we have the alternative eigendecomposition
With s m( f ) = s m+1( f ) = · · · = s M −1( f ) = 0, both eigendecompositions have exactly the
same nonzero eigenvalues, but they have entirely different eigenvectors. Note that the
outer products u m ( f u†m ( f ) and v m ( f )v †m ( f ) are both of rank one.
Recalling that the index m signifies a sensor and the index k signifies a Slepian taper,
we may now make three statements on the multiple operations being performed by the
(1) The mth eigenvalue σ 2m ( f ) is defined by
and dividing σ 2m by this same factor, we get
which is a rewrite of (3.4), specialized for sensor m. We may, therefore, make the
The eigenvalue σ 2m( f ) , except for the constant scaling factor ⎜ ∑ λ (km ) ⎟ , provides the
desired multitaper spectral estimate for the incoming interference picked up by the mth sensor.
(2) Since the index m refers to the mth sensor, we make the second statement:
The left singular vector um( f ) defines the direction of the interfering signal picked up by the
(3) The index k refers to the kkth Slepian taper; moreover, since σ 2k f
0, 1, . . ., M − 1, we may make the third and last statement:
The right singular vectorr vm( f ) defines the multitaper coefficients for the mth interferer’s
Most importantly, with no statistical assumptions on the additive ambient noise in each
sensor or the incoming interferers, we may go on to state that the nonparametric MTMSVD processor is indeed robust.
Power-spectrum estimation for sensing the environment
The enhanced signal-processing capability of the MTM-SVD processor just described
is achieved at the expense of increased computational complexity. To elaborate, with N
W there are N different frequencies with spectral
W/N to be considered. Accordingly, the MTM-SVD processor has to perform a total of N SVDs on matrix A( f ). Note, however, that the size of the wavenumber
spectrum (i.e. the spatial distribution of the interferers) is determined by the number
M, which is considerably smaller than the number of data points N
importantly, the wavenumber is computable in parallel. With the computation being
performed at each frequency ff, each of the M sensors sees the full spectral footprint of
the interferer pointing along its own direction; the footprint is made up of N frequency
Summing up, the MTM-SVD processor has the capability to sense the surrounding
environment both in frequency and direction, the resolutions of which are respectively
determined by the number of data points and the number of sensors deployed. Although
by itself the MTM-SVD processor cannot provide for the attentional requirement of a
cognitive radio, it provides the dynamic-spectrum manager valuable spatial information
about the radio environment to resolve this requirement.
The MTM-SVD processor rests its signal-processing capability on two coordinates of
• frequency, which is necessary for displaying the spectral content of environmental
• space, which provides the means for estimating wavenumber spectra of the
However, for the spectral analysis of environmental data to be complete, there is a third
coordinate that is just as important in its own way; that coordinate is time. Indeed, it
is the temporal characterization of a stochastic process that permits us to describe the
The statistical analysis of nonstationary processes has had a rather mixed history. Although the general second-order theory of spectral analysis was described
by Loève (1946), it has not been applied nearly as extensively as the theory of stationary processes published only slightly earlier by Wiener and Kolmogorov. There
were, at least, four distinct reasons for this neglect, as summarized by Haykin and
(1) Loève’s theory was probabilistic, not statistical, and there do not appear to have
been successful attempts to find a statistical version of the theory until some
(2) At the time of those early publications, over six decades ago, the mathematical
training of most engineers and physicists in signals and stochastic processes was
minimal and, recalling that even Wiener’s delightful book was referred to as “The
Yellow Peril” on account of the color of its covers, it is easy to imagine the
reception that a general nonstationary theory of stochastic processes would have
(3) Even if the theory had been commonly understood at the time and good statistical
estimation procedures had been available, the computational burden would probably have been overwhelming. This was the era when Blackman–Tukey estimates of
the power spectrum of a stationary process were developed. This was not because
they were great estimates, but, primarily, because they were simple to understand
in mathematical terms and before the (re)invention of the FFT algorithm, with the
latter being computationally more efficient than other forms.
(4) Finally, it cannot be denied that the Loève theory of nonstationary processes was
harder to grasp than the Wiener–Kolmogorov theory of stationary processes.
Confronted with the notoriously unreliable nature of a wireless channel for radio
communication, for example, we have to find some way to account for the nonstationary behavior of a signal at the channel output across time (implicitly or explicitly)
in describing the signal picked up by the receiver. Given the desirability of working in
the frequency domain for well-established reasons, we may include the effect of time
by adopting a time–frequency description of the signal. During the last three to four
decades, many papers have been published on various estimates of time–frequency
distributions; see, for example, the book by Cohen (1995) and the references therein.
In most of that early work on time–frequency analysis, the signal is assumed to be
deterministic. In addition, many of the proposed estimators are constrained to match
continuous time t and continuous frequency f marginall density conditions. For a
continuous-time signal, denoted by x(t), the time marginall is required to satisfy the
where D(t, f ) is the time–frequency distribution of the signal. Similarly, if X(
Fourier transform of x(t), then the frequency marginall must satisfy the second condition
Given the large differences observed between waveforms collected on sensors spaced
short distances apart, the time marginal requirement is a rather strange assumption.
Worse, the frequency marginal is, except for the factor 1/N
signal, which is known to be badly biased and inconsistent.5 Thus, we do not consider
matching marginal distributions, as commonly defined in the literature, to be important.
Theoretical background of nonstationarity
Nonstationarity is an inherent characteristic of most, if not all, of the stochastic processes encountered in practice. Yet, despite its highly pervasive nature and practical
importance, not enough attention is paid in the literature to the characterization of nonstationary processes in a mathematically satisfactory manner.
Power-spectrum estimation for sensing the environment
To this end, consider a complex continuous stochastic process, a sample function of
which is denoted by x(t). We assume that the process is harmonizable (Loève, 1946,
1963), so that it permits the Cramér representation, described by
X(v) is the generalized Fourier transform of the process, also referred to as the
increment process; the dummy variable v has the same dimension as frequency. The bandwidth of x(t) has been normalized to unity for convenience of presentation; consequently,
as indicated in (3.23), the integration extends with respect to v over the interval [−1/2,
+1/2]. As before, it is assumed that the process x(t) has zero mean; that is, E[x(t)] = 0
for all time t. Correspondingly, we have E[X
[X(v)] = 0 for all v. Moreover, the power
spectrum S( f ) is defined in terms of the generalized Fourier transform dX
where the asterisk denotes complex conjugation and d ( f ) is the Dirac delta function
in the frequency domain. Note that (3.23) is also the starting point in formulating the
MTM. (See Note 2 at the end of the chapter, described in the discrete-time domain.)
To set the stage for introducing the statistical parameters of interest, we introduce the
covariance function defined in the time domain as follows (Thomson, 2000):
Hereafter, the generalized two-frequency spectrum g L( f1, f2) in the integrand of the
second line in (3.24) is referred to as the Loève spectrum.6 With X(
X f ) denoting the Fourier transform of x(t), the Loève spectrum is itself formally defined by
Equation (3.25) highlights the underlying feature of a nonstationary process by
describing the correlation between the spectral elements X(
at two different frequencies f1 and f2 respectively.
If the process is stationary, then, by definition, the covariance function Γ L ( 1 , t2 )
depends only on the time difference t1 − t2, and the Loève spectrum γ L ( f1 , f 2 ) collapses
where d ( f ) is the Dirac delta function in the frequency domain and
S( f ) is the power spectrum of the process x(t). Similarly, for a white nonstationary process, the covariance function Γ L ( 1 , t2 ) becomes δ ( 1 2 ) P(( 1 ), where d (t) is the Dirac
delta function in the time domain and P(t) is the expected (average) power of the process
at time t. Thus, as both the spectrum and covariance functions include delta-function
discontinuities in simple cases, neither should be expected to be “smooth”; continuity
properties of the process, therefore, depend on direction in the ( f1, f2)-plane or (t1, t2)plane. The continuity problems are more easily dealt with by rotating both the time and
frequency coordinates of the covariance function in (3.24) and Loève spectrum in (3.25)
respectively by 45°. In the time domain, we may now define the new coordinates to be a
Thus, denoting the covariance function in the rotated coordinates by Γ(t0, t ), we may
In a similar manner, we may define new frequency coordinates, f andd g, by writing
The rotated two-frequency spectrum is thus defined in terms of the Loève spectrum as
Substituting the definitions of (3.28) and (3.29) into (3.24) shows that the difference
term (t1 f1 − t2 f2) in the exponent of the Fourier transform becomes the sum term (t0g +
Because f is associated with the time difference tt, it corresponds to the ordinary
frequency of stationary processes; therefore, we may refer to it as the “stationary”
frequency. Similarly, because g is associated with the average time t0, it describes the
behavior of the spectrum over long time spans; hence, we refer to g as the “nonstationary” frequency.
Consider next the continuity of the generalized spectral density gg, reformulated
as a function of f and g. On the line g = 0, the generalized spectral density g is just
the ordinary spectrum with the usual continuity (or lack thereof) conditions normally
applied to stationary spectra. As a function of g, however, we expect to find a d -function
Power-spectrum estimation for sensing the environment
discontinuity at g = 0 if, for no other reason that almost all data contain some stationary
additive noise. Consequently, smoothers in the ( f,
f g) plane or, equivalently, the ( f1, f2)
plane, should not be isotropic, but require much higher resolution along the nonstationary frequency coordinate g than along the stationary frequency coordinate f
A slightly less arbitrary way of handling the g coordinate is to apply the inverse Fourier transform to g ( f, g) with respect to the nonstationary frequency g, obtaining the
as the dynamic spectrum of the process; the D(t0, f ) in (3.37) is nott to be confused with
the time–frequency distributions in (3.21) and (3.22). The motivation behind (3.37) is
to transform very rapid variations expected around g = 0 on the right-hand side of the
equation into a slowly varying function of t0 on the left-hand side of the equation, while,
at the same time we leave the usual dependence on f intact. From Fourier-transform
theory, we know that the Dirac delta function in the frequency domain is transformed
into a constant in the time domain. It follows, therefore that, in a stationary process,
D(t0, f ) does not depend on t0 and assumes the simple form of a power spectrum. Thus,
recalling the definition of Γ L ( 1 , t2 ) in (3.24), we may redefine the dynamic spectrum
∫−−∞ j2 τ f ⎨⎩ x ⎝ t0 2⎠ x * ⎜⎝ t0 − 2⎟⎠ ⎬⎭ dτ
According to (3.38), the dynamic spectrum is an expected value and nonnegative definite, which, being a form of power spectrum, is how it should be for all t0.
Spectral coherences of nonstationary processes based on the Loève transform
From an engineering perspective, we usually like to have estimates of second-order
statistics of the underlying physics responsible for the generation of a nonstationary
process. Moreover, it would be desirable to compute the estimates using the MTM.
With this twofold objective in mind, let Xk( f1) and Xk( f2) denote the multitaper Fourier
transforms of the sample function x(t); these two estimates are based on the kkth Slepian
taper and are defined at two different frequencies, f1 and f2, in accordance with (3.2). To
evaluate the spectral correlation of the process at f1 and f2, the traditional formulation is
to consider the product X k f ) X k∗ ( f 2 ), where, as before, the asterisk in X k∗ f 2 ) denotes
Unfortunately, we often find that such a formulation is insufficientt in capturing the
underlying second-order statistics of the process, particularly so in the case of several
communication signals that are of interest in cognitive-radio applications.7 To complete
the second-order statistical characterization of the process, we need to consider unusual
)Xk( f2), which do nott involve the use of complex conjugation.
However, in the literature on stochastic processes, statistical parameters involving products like Xk( f1)X
)Xk( f2) are frequently not named and, therefore, hardly used; and when
they are used, not only are different terminologies adopted for them, but also some of
To put matters right, in this chapter we follow the terminology first described by
Mooers (1973), and use the subscripts “inner” and “outer” to distinguish between
spectral correlations based on products involving such terms as X k f1 ) X ∗ ( f 2 ) and
X k f1 ) X ( f 2 ) respectively.8 Hereafter, estimates of spectral correlations so defined
are referred to as estimates of the first andd second kinds respectively, and likewise for
With the matter of terminology settled, taking the complex demodulates of a nonstationary process at two different frequencies, f1 and f2, and invoking the inherent orthogonality property of Slepian sequences, we may now formally define the estimate of the
where, as before, K is the total number of Slepien tapers. The estimate of the Loève
spectrum of the second kind is correspondingly defined as
where, on the right-hand side, there is no complex conjugation.
Thus, given a stochastic process with the complex demodulates Xk( f1) and Xk( f2),
the Loève spectral coherences of the first and second kinds are respectively defined as
where, in both equations, the multitaper spectral estimate Sˆ ( f ) is naturally real valued.
Note that these two definitions of Loève spectral coherences share the same denominator but differ in their numerators.
In general, the Loève spectral coherences Cinner( f1, f2) and Couter( f1, f2) are both complexvalued, which means that each one of them will have a magnitude and associated phase of
its own. In practice, we find that a quantity called the two-frequency magnitude-squared
coherencee (TF-MSC) is more useful than the spectral coherence itself. With the two spectral coherences of (3.41) and (3.42) at hand, we have two TF-MSCs to consider, namely
Couter( f1, f2)|2, whose respective definitions follow directly from (3.41)
Two special cases of the dynamic spectrum D (tt0, f )
From the defining equation (3.38), we immediately recognize that9
Power-spectrum estimation for sensing the environment
is the formula for the Wigner–Ville distribution of the sample function x(t). In other
words, we see that the Wigner–Ville distribution is the instantaneous estimate of the
dynamic spectrum of the nonstationary signal x(t). As such, it is simpler to compute than
D(t0, f ) in the classification of signals.
The dynamic spectrum D(t0, f ) also embodies another special case, namely the cyclic
power spectrum of a sample function x(t) that is known to be periodic. Let T0 denote
the period of x(t). Then, replacing the time t0 in (3.38) with T0 + t, we may express the
= ∫ e − j2 πτ f Rx t + T0 + , t T0 − ⎟ dτ .
is the time-varying autocorrelation function of the signal x(t). The stochastic process, represented by the sample function x(t), is said to be cyclostationary in the
second-order sense if this autocorrelation sequence is itself periodic with period T0,
which, as expected, is independent of the period T0. Equation (3.47) is recognized as the
cyclostationary extension of the well-known Wiener–Khintchine relation for stochastic
To be more complete, for a stochastic process to be cyclostationary in the secondorder sense, its mean must also be periodic with the same period, T0. When the mean
of the stochastic process under study is zero for all time t, as assumed here, then this
Instrumentation for computing Loève spectral correlations
Before proceeding to discuss the cyclostationarity characterization of nonstationary processes in the next section on Fourier theory, we find it instructive to have a diagrammatic
instrumentation for computing the Loève spectral correlations using the MTM. To do this,
we look to the defining equations (3.2), (3.41), and (3.42), where t in (3.2) denotes discrete
time and f in all three equations denotes continuous frequency. Let x(t)
N. Then, the inspection of (3.2), (3.41), and (3.42) leads to the basic instrument
diagrammed in Figure 3.4a. In particular, in accordance with (3.2), the identical functional
blocks labeled “multitaper method” in the upper and lower paths of the figure produce the
Fourier transforms Xk( f1) and Xk( f2) respectively. The designation “basic” is intended to signify that the instrument applies to both kinds of the Loève spectral correlation, depending
on how the cross-correlation of the Fourier transforms XT ( f1) and XT ( f2) is computed over
the set of K Slepian tapers. To be specific, we say that, for the overall output:
• the instrument of Figure 3.4a computes the estimate γˆ L, inner ( f1 , f 2 ) of (3.41) if the
cross-correlation is of the first kind, and
• it computes γˆ L, outer ( f1 , f 2 ) of (3.42) if the cross-correlation is of the second kind.
As for Figure 3.4b, it applies to spectral correlations rooted in the classical Fourier
Figure 3.4. Illustrating the one-to-one correspondences between the Loève and Fourier theories
for cyclostationarity. Basic instrument for computing (a) the Loève spectral correlations
of a time series x(t) and (b) the Fourier spectral correlations of cyclostationary signal x(t).
(Reproduced with permission from S. Haykin, D. J. Thomson and J. Reed, 2009, Spectrum
sensing for cognitive radio, Proceedings of the IEEE, 97, 849–877.
Power-spectrum estimation for sensing the environment
When the issue of interest is the characterization of digital modulated signals in
cognitive-radio applications, for example, we find that there is a large body of literature
on the subject, the study of which has been an active area of research for over 50 years.
The prominence of interest in cyclostationarity for signal detection and classification10
is attributed to the works of Gardner (1988, 1994) and the subsequent work of Giannakis
and Serpedin (1998) on alternative views and applications of cyclostationarity. For the
literature on cyclostationarity, see Note 11 at the end of the chapter.
As defined previously, a stochastic process represented by the sample function x(t) is
said to be cyclostationary in the second-order sense if its time-varying autocorrelation
/ 2 ) satisfies the periodicity condition of (3.46). Moreover, if
the mean of the process is nonzero, it would also have to be time varying with the same
period T0. For the present discussion, the mean is assumed to be zero for all time t, and
the focus of attention, therefore, is confined to second-order statistics.
A cyclostationary process may also be described in terms of its power spectrum,
which assumes a periodic form of its own. With interest in this chapter focused on spectral coherence, we now go on to define the inner and outer forms of spectral coherence
of a nonstationary process using Fourier theory.
Let x(t) denote a sample function of a complex-valued cyclostationary process with
period T0. Using the Fourier series, we may characterize the process by its cyclic power
spectrum of the first kind, as shown by the Fourier expansion:
where the new parameter α, in theory, scans an infinite set of frequencies; that is,
α = n / T0 , where n = 0, 1, 2, . . .. The power spectrum of (3.48) is cyclic, in that it satisfies the condition of periodocity, as shown by
The Fourier coefficients in (3.48), namely sinner
The infinitesimally small Δtt is included in (3.49) to realize the continuous-time nature
of the cyclostationary signal x(t) in the limit as Δtt approaches zero. The time-varying
Fourier transform of x(t), denoted by XT (t, f ), is defined by
Most importantly, for prescribed values of f,
(t , f − α / ) ; it follows, therefore, that sinner
inner spectral correlation of the cyclostationary signal x(t) for the two frequencies
In light of the rationale presented in Section 3.5, we say that (3.48) and (3.50) provide
a partial description of the second-order statistics of a complex-valued cyclostationary
process. To complete the statistical description of x(t), we need to introduce the cyclic
power spectrum of the second kind, as shown by
is the time average of the outer product X T (tt f
which does not involve the use of complex conjugation; (3.49), therefore, defines the
outer spectral correlation of the signal x(t).
With (3.48) and (3.51) at hand, we may now define the two Fourier spectral coherences of a cyclostationary process as follows.
(1) Fourier spectral coherence of the first kind:
(2) Fourier spectral coherence of the second kind:
Both spectral coherences have the same denominator, where the Fourier coefficient
s0( f ) corresponds to a = 0; putting a = 0 in the expressions for sinner
As with the Loève spectral coherences, the Fourier spectral coherences Cout
f ) are both complex valued in general, with each one of them having a magnitude
The use of the Fourier spectral coherences of the first and second kinds in
(3.52) and (3.53) can be computationally demanding in practice. To simplify
matters, their cycle frequency-domain profile (CFDP) versions are often used, as
and similarly for the outer spectral coherence Couter
Power-spectrum estimation for sensing the environment
Instrumentation for computing the Fourier spectral correlations
The block diagram of Figure 3.4b depicts the instrument for computing the first and
second kinds of the Fourier spectral correlations at frequencies f1 f + α / 2 and
f 2 f − α / 2 in accordance with (3.52) and (3.53). A cyclostationary signal x(t) is
applied in parallel to the pair of paths in the figure, both of which use a pair of identical
narrowband filters. Both filters have the midband frequency f and bandwidth Δf
Δf is small compared with f but large enough compared with the reciprocal of the
time T that spans the total duration of the input signal x(t). In any event, the Fourier
transform of the input x(t) is shifted due to the multiplying factors exp(±jπ a t), pro/ 2 ) in the upper path and X T ( f
path. Depending on how these two filter outputs are processed by the spectral correlator,
the overall output produced in Figure 3.4b is sinner
Relationship between the Fourier and Loève spectral coherences
Much of the communications literature on cyclostationarity and related topics such as
spectral coherence differ from that on multitaper spectral analysis. Nevertheless, these
two approaches to cyclostationarity characterization of an input signal are actually
related. In particular, examining Figure 3.4a and b, we see that the two basic instruments depicted therein are similar in signal-processing terms, exhibiting the following
− π f2t for the MTM in Figure 3.4a play similar
− π a t for their Fourier counterfrequency-shifting roles as the factors e j2π a t and e −j2
(2) The MTM in Figure 3.4a for a prescribed Slepian taper and the narrowband filter in
Figure 3.4b for prescribed mid-band frequency f and parameter a perform similar
(3) Finally, the cross-correlator operates on the MTM outputs Xk (f
3.4a to produce estimates of the Loève spectral correlations, while the cross-corre/ 2 ) and X T ( f
lator in Figure 3.4b operates on the filter outputs X T ( f
to produce the Fourier spectral correlations with 1
Naturally, the instruments depicted in Figure 3.4a and b differ from each other in the
ways in which their individual components are implemented.
Contrasting the two theories on cyclostationarity
The theory of cyclostationarity presented in this section follows the framework formulated in Gardner (1988, 1994). This framework is rooted in the traditional Fouriertransform theory of stationary processes with an important modification: introduction of
the parameter a (having the same dimension as frequency) in the statistical characterization of cyclostationary processes. Accordingly, the cyclic spectral features computed
3.7 Harmonic F-test for spectral line components
from this framework depend on how well the parameter a matches the underlying statistical periodicity of the original signal x(t).
The other theory on cyclostationarity, discussed previously in Section 3.5, follows the
framework originally formulated by Thomson (2000). This latter framework combines
• the Loève transform for dealing with nonstationary processes; and
• the MTM for resolving the bias–variance dilemma through the use of Slepian
This alternative two-pronged mathematically rigorous framework for the time–frequency
analysis of nonstationary processes has a built-in capability to adaptt to the underlying
statistical periodicity of the signal under study. In other words, it is nonparametric and,
Summing up the basic similarities and differences between the Loève and Fourier
theories of stochastic processes, we say the following:
• both theories perform similar signal-processing operations on their inputs; but
• the Fourier theory assumes that the stochastic process is cyclostationary, whereas the
Loève theory applies to any nonstationary process regardless of whether it is cyclostationary or not.
Harmonic FF-test for spectral line components
The MTM spectra computed using the theory described in Section 3.3 are not expected
to be always good estimates, particularly so when it is known that spectral line components exist in the signal under study. For the MTM, we can apply the statistical F-test
to check for and estimate existing line components in the spectrum. The F
statistical test that assigns a probability value to each of two hypotheses concerning
samples taken from a parent population: one hypothesis assumes that line components
are present and the other hypothesis assumes that they are not. These samples are
assumed to follow a c 2 distribution, which is the case for an unknown mean and variance sample; this distribution consists of a of sum of squares taken from a Gaussian
Suppose that we have a model described by the equation
which is linear with respect to a p-by-1 parameter vector x, and where the n-by-p
- coefficient matrix A and n-by-1 vector y are known or can be estimated from a given data
set. We assume that the error vector e has independent components that come from a
Power-spectrum estimation for sensing the environment
Gaussian distribution of zero mean and variance s 2. Therefore, another way to express
where E is the statistical expectation operator. In order to get the best possible estimate
of our parameter vector x in the least-squares sense, we have to address the minimization
Using the superscript † to denote the Hermitian transposition of a matrix, we may define
which assumes its minimum value at the well-known linear least-squares solution
A† A ) 1A† is the pseudo-inverse of matrix A.
F-test, based on the F-distribution (Percival and Walden, 1993), comes about
from observing that we can break the observed total variance, y†y, of our model into
two components, one due to the regression itself, || Axˆ ||2 , and the other || y - Axˆ ||2 due to
residual errors. Each of these components has associated with it a number of DoFs, v1
and v2, respectively. For N complex data points, the total number of DoFs is
It turns out now that, provided the errors are independent and zero-mean Gaussian
random variables, each of the two variance components follows a χ distribution with
v1 or v2 DoFs respectively. Their ratio follows the F(
which we can make hypothesis testing at a desired level of significance.
We test the hypothesis H0: x = 0 against the alternative hypothesis H1: x ≠ 0 at a significance level a as follow. If H0 is true, then the ratio
F v1, v2) distribution. The value of this distribution for a significance level
of a (not to be confused with a in Section 3.6 on cyclostationarity) is found from statistical tables. If the computed ratio is larger than the table value, then our hypothesis
a % confidence. This means that at least one of the components
of x is different from zero. In practice, of course, we require a computed F ratio that
The material just covered on linear regression provides us with enough background to
3.7 Harmonic F-test for spectral line components
Given a time series {x(n)} with a single line component at frequency f0, the expected
value of the eigencoefficients of this time series, referring back to Section 3.3, is
where K is the number of Slepian sequences. With the { n( ) }nN=1 denoting the kkth Slepian sequence, in terms of which we may define the kkth Fourier transform
At a given frequency ff, the Xk( f ) correspond to y in the linear regression model, the
parameter m corresponds to x, and the Vk( f − f0) correspond to the matrix A. The number
of parameters p = 1 here, and by setting it up as a least-squares problem we have to minimize the residual error ||x − Vm
Vm ||2 with respect to the complex-valued scalar parameter
The term point regression comes about because we consider each frequency
point f as a possible candidate for f0. We set f0 = f and test the model (3.56) for
The residual error at f can also be written as
Following the least-squares solution based on the pseudo-inverse matrix presented earlier in the section, we may write
Power-spectrum estimation for sensing the environment
This solution may also be expressed in the form
where the harmonic data tapers (windows) are defined by
We can now test the hypothesis H0 that when f0 = ff, the model (3.56) is false; that is, the
parameter m = 0, versus the hypothesis H1, where it is not. In other words, the ratio of
the energy explained by the assumption of a line component at the given frequency to
F-ratio as explained in the outline on linear regression
where v is equal to two DoFs (real and imaginary parts of a complex spectral line component). The total number of DoFs, 2K,
K comes about from the set of complex data
points { k ( f )}kK=−01, from which we may draw relevant information. If the F
at a certain frequency ff, then the hypothesis is rejected; that is, a line component does
not exist there; otherwise, it does. The location of the maximum of the F
an estimate of the line frequency that has a resolution within 5–10% of the Cramér–Rao
lower bound (CRLB) (Thomson, 1982); the CRLB is discussed in Chapter 6.
The test works well if the lines considered are isolated; that is, if there is only a single
W). The total number of lines is not important as long as
they occur singly. For lines spaced closer than W
test, which is similar but algebraically a more complicated regression of the eigencoefficients on a matrix of functions Vk( f − f i ) with the simple F
F-tests (Thomson, 1982; Drosopoulos and Haykin, 1992). A cautionary note is in order
in using multiple-line tests: the CRLBs for line parameter estimation tend to degrade
rapidly when the line spacing becomes less than 2/N
F-test is a statistical test. This means that, given a large number
of different realizations of a data sequence, highly significant values can sometimes
be seen which, in reality, are only sampling fluctuations. A good rule of thumb,
as Thomson (1982) points out, is not to get excited by significance levels below
/N). Experience also suggests to try the test for a variety of NW values. Line
components that disappear from one case to the other are almost certainly sampling
Perception of the environment can be viewed from different perspectives, depending on
the application of interest. In this chapter, we focused attention on power-spectrum estimation as the tool for sensing the environment. This perspective is of particular interest
in the study of cognitive radio, which is taken up in Chapter 7.
There are two ways of approaching power-spectrum estimation:
(1) Parametric methods, which rely on the adoption of a model and the issue becomes
one of adjusting the model parameters such that the power spectrum of the model
output provides a reliable estimation of the observables from the environment.
(2) Nonparametric methods, which are model free; in this case, the observable data are
analyzed directly in a methodical way to provide an estimate of the desired power
More specifically, we viewed the power-spectrum estimation problem from a finite
set of observable data as an ill-posed inverse problem. With this viewpoint in mind, the
fundamental equation of power-spectrum estimation assumes the form of a Fredholm
integral equation of the first kind; see Note 2 for detail. To solve this equation, use
is made of prolate spheroidal wave functions that were discussed early on by Slepian
(1965); hence the reference to them in the chapter as Slepian sequences. The remarkable
property of Slepian sequences is that their Fourier transforms have the maximal energy
concentration in a prescribed bandwidth. It is this property that befits the use of Slepian
sequences as the mathematical basis for defining a corresponding set of windows or
tapers in formulation of the MTM for solving the power-spectrum estimation problem,
Basically, the MTM replaces the bias–variance tradeoff (that has plagued the traditional approach for power-spectrum estimation) with bias–resolution tradeofff Once
this tradeoff is taken care of, the variance problem is also solved. Stated in another way,
the MTM provides an analytic basis for computing the best approximation to a desired
power spectrum, which is not possible from the observable data alone.
The MTM also provides the mathematical basis for the following extensions:
• space–time processing, and hence the ability for sensing the environment in both frequency and spatial direction; and
• time–frequency analysis, which, in turn, provides a rigorous mathematical basis for
Cyclostationarity means that second-order statistics of the observable data
exhibit periodicity across time. As such, cyclostationarity is a basic property of
modulated signals, hence the practical interest in the use of cyclostationarity in
Power-spectrum estimation for sensing the environment
cognitive radio applications. We will have more to say on this topic on cognitive
In the latter part of the chapter, we extended the MTM to deal with situations where
the power spectrum contains spectral line components; to this end, we introduced the
In the opening paragraph to this chapter, we pointed out that spectrum estimation is of
particular interest to cognitive radio, which will be studied in Chapter 7. With interest
in this application of cognition growing exponentially, it is apropos that we conclude
Chapter 3 with two messages: one about the mathematical framework and the other
Insofar as cognitive radio is concerned, spectrum sensing plays the role of perception:
the first pillar of cognition. Moreover, by virtue of its physical nature, spectrum sensing
requires a thorough understanding of statistical characterization of the radio environment. In light of this requirement, the first message to take from this chapter is summed
• First, spectrum estimation using MTM is the only method that provides a reliable and accurate
strategy for solving the spectrum-sensing problem.
• Second, given a finite set of observable data, the MTM provides a rigorous mathematical
framework for spatio-temporal processing of the observables in a coordinated manner: temporally, by involving the Loève transform to account for nonstationary character of the radio
environment, and spatially, by incorporating SVD applied to a receiving array of elemental
Naturally, frequency plays the role of a common variable in both the temporal and spatial processing of the observable data.
It is equally important that we pay attention to the computational efficiency of the MTM.
Looking ahead to the material covered in Chapter 7 on cognitive radio, our second message is summed up as follows:
In computational terms, practical implementation of the MTM can be accomplished in a matter
Bearing in mind the mathematical and practical considerations described above, the
MTM is the method that solves the spectrum-sensing problem in a coordinated manner
by accounting for all three variables (frequency, time, and space) that are involved in
statistical characterization of the radio environment, and it does so in a computationally
efficient manner; see Note 2 of Chapter 7.
1. Other methods for spectrum sensing in cognitive radio
In addition to spectrum estimation, there are several other methods that have been
described in the literature for cognitive radio applications. For a comprehensive
overview of other methods for spectrum sensing in cognitive radio, the reader is
referred to the paper by Zeng et al. (2010). However, for reasons described in this
chapter and experimental tests highlighted in Note 6 of Chapter 7, spectrum estimation is viewed as the method of choice.
2. Power-spectrum estimation is an ill-posed inverse problem
Consider a time series { ( n)}nN=−01, where n denotes discrete time. The Cramér representation of this process is written as follows:
X( f ) is the generalized Fourier transform. With x(n) assumed to have
X( f ) will also have zero mean. For its second-order
where d ( f ) is the Dirac delta function in the frequency domain and S(
spectrum of x(n). Note that, with the mainlobe of the power-spectrum estimator occupying a passband of width 2W,
W the frequency in (3.62) has been normalized with
W centered on f = 0; hence the limits of integration in that equation.
Next, by definition, the ordinary discrete-time Fourier transform of the time series
Substituting (3.62) into (3.64), we write
where we have interchanged the order of summation and integration. Now, we introduce the definition
which is called the Dirichlet kernel. Summing the series in (3.66), we may express
Power-spectrum estimation for sensing the environment
N. Thus, referring back to (3.65), we may redefine this equation in
terms of the Dirichlet kernel as the convolution integral
This integral equation, defining a linear input–output relationship with KN ( f −n)
the weigting function is referred to as the fundamental equation of spectral analysis.
In the literature, it is viewed as an example of the Fredholm equation of the first kind
From the perspective of power-spectrum estimation, we are really interested in the
relationship between the expected value of |X
To this end, using (3.63) and (3.68), we obtain the desired relationship, after some
straightforward algebraic manipulations, as shown by
Note that, as the size of the data set N is allowed to approach infinity, the kernel KN ( f )
approaches the Dirac delta function d ( f ), and with it (3.69) reduces to
which is the mathematical definition of the power spectrum. In the general setting
of (3.69), S( f ) is the unknown function and |XN( f )|2 is the known function given a
finite set of observations { ( n)}nN=−01. Hence, we may make the following important
S f ), from the linear input–output relationship described in
the fundamental equation of spectral analysis, we have an inverse problem that is ill-posed
because, given a finite set of observations, we cannot uniquely determine the power spectrum
S f ) defined over an infinite number of points in the frequency domain.
The lack of uniqueness, emphasized in this statement, violates one of the three conditions due to Hadamard for the well-posedness of an inverse problem: existence,
uniqueness, and continuity. The issue of an inverse problem being ill-posed was
discussed previously in Section 2.14. So, indeed, we may say that power-spectrum
estimation is an ill-posed inverse problem.
Detailed information on Slepian sequences is given in Slepian (1978). A method
for computing such sequences, for large data length, is given in the appendix of the
Percival and Walden (1993), building on the fundamental equation of spectral analysis discussed in Note 2 of this chapter, show that the generic formula of (3.69) in
where Xk( f ) is the Fourier transform of the time series { ( n)}nN=−01, Vk( f ) is the Fourier transform of the kkth Slepian sequence { k }kK=−01 and lk is the associated eigencoefficient. S( f ) is the unknown power spectrum to be estimated. Examining the
convolution integral of (3.71), we may view |V
importantly, this smoother is a good one because its smoothing property is confined
entirely to the main lobe of the spectrum that occupies the frequency interval [−1/2,
1/2] and thus avoids sidelobe leakage. This smoothing operation represents a form
of regularization that stabilizes an ill-posed inverse problem (Haykin, 2009). We
may therefore conclude that regularization is mathematically built into the MTM.
5. An inconsistent estimate is one where the variance of the estimate does not decrease
6. Distinction between the Loève spectrum and bispectrum
Care should be exercised in distinguishing the Loève spectrum γ L ( f1 , f 2 ) from the
bispectrum B( f1, f2). Both are functions of two frequencies, f1 and f2, but the Loève
spectrum γ L ( f1 , f 2 ) is a second-moment
process; in contrast, the bispectrum describes the third-moments
process and has an implicit third frequency f3 = f1 + f2.
7. For most complex-valued signals, the expectation E[x(t1)x(tt2)], and therefore
)X( f2)], is zero. For communication signals, however, this expectation is
8. Mooers' approach to complex-valued stochastic processes
In a terminological context, there is confusion in how second-order moments of
complex-valued stochastic processes are defined in the literature:
• Thomson (1982) and Picinbone (1996) use the terms “forward” and “reversed”
to distinguish, for example, the second-order moments E[ X k ( f ) k∗ ( f 2 )] and
• Mooers (1973) uses the terms “inner” and “outer,” borrowed from the notions of
inner products and outer products, to distinguish between these two second-order
• In the cyclostationarity literature on communication signals, the terms “spectral
correlation” and “conjugate spectral correlation” are used to refer to the expectations E[ X ( f1 ) X ∗ ( f 2 )] and E[ X ( f1 ) X ( f 2 )] respectively. This terminology is
misleading: if E[ X ( f1 ) X ∗ ( f 2 )] stands for spectral correlation, then the expression for conjugate spectral correlation would be E[ X ∗ ( f1 ) X ( f 2 )] , which is not
As stated in the text, in this chapter we follow Mooers’ terminology.
The naive implementation of the Wigner–Ville distribution, as defined in this equation using a finite sample size, may result in bias and sampling properties that are
Research interest in cyclostationarity for signal detection and classification
has experienced resurgence with the emergence of cognitive radio, resulting a
Power-spectrum estimation for sensing the environment
signal-detection technique in the draft form of the IEEE 802.22 standard for cognitive radio applications.
11. The literature on cyclostationarity includes the book by Hurd and Miamee (2007)
and the bibliography due to Serpedin et al. (2005) that lists over 1500 papers on the
12. More on Fourier-based cyclostationarity
The Fourier-based approach to cyclostationarity may also acquire an adaptive capability of its own. In many spectrum-sensing applications based on this approach, the
Fourier spectral coherence of the first and second kinds, defined in (3.52) and (3.53),
are computed over the entire spectral domains of interest, and the actuall cycle frequencies (i.e. statistical periodicity of the signal) may thus be accurately estimated.
According to Spooner (personal communication, 2008), applicability of the Fourierbased cyclostationarity approach to spectrum sensing can be extended to a wide
range of situations, from completely blind (no prior knowledge of periodicity) to
highly targeted ones (known periodicities with possible errors).
Perception of the environment, viewed as a problem in spectrum estimation as discussed
in Chapter 3, is most appropriate for applications where spectrum sensing of the environment is crucial to the application at hand; cognitive radio is one such important
application. A distinctive aspect of spectrum estimation is the fact that it works directly
on environmental measurements (i.e. observables). However, in many other environments encountered in the study of cognitive dynamic systems, perception of the environment boils down to state estimation, which is the focus of attention in this chapter.
We defer a formal definition of the state to Section 4.4, where the issue of state estimation is taken up. For now, it suffices to say that estimating the state of a physical
environment is compounded by two practical issues:
(1) The state of the environment is hidden from the observer, with information about the
state being available only indirectly through dependence of the observables (measurements) on the state.
(2) Evolution of the state across time and measurements on the environment are both
corrupted by the unavoidable presence of physical uncertainties in the environment.
To tackle perception problems of the kind just described, the first step is to formulate a probabilistic modell that accounts for the underlying physics of the environment.
Logically, the model consists of a pair of equations:
• a system equation,1 which accounts for evolution of the environmental state across
• a measurement equation, which describes dependence of the measurements on
These two equations constitute the well-known state-space modell of the environment,
on which the state-estimation problem (i.e. perception of the environment) is based. In
particular, we look to the system equation for a value of the state at some prescribed
To solve this problem, we look to the Bayesian frameworkk2 as a general formalism
of the state-space model, with the unknown state (assumed to be multidimensional)
and associated uncertainties all being treated as random vectors. Naturally, in this formalism, the measurement equation characterizes the information available to the perceptor of a cognitive dynamic system under study. However, this is only one side of the
story as to how the environment is being perceived by the perceptor. To characterize
Bayesian filtering for state estimation of the environment
how the perceptor actually makes inferences about the state of the environment, which
is the other side of the story, the perceptor looks to the system equation for exploiting
dependence of the observables on the state. In a way, the two sides of the story briefly
described here sum up how the visual brain perceives its surrounding environment. By
the same token, the story applies equally well to cognitive radar as an important engineering example of cognitive dynamic systems.
Given a probabilistic state-space model of the environment, the aim of this chapter
is to develop algorithms for solving the state-estimation problem in an iterative on-line
manner. The emphasis on iterative computation is for the following practical reason:
The updated estimate of the state builds on its old value; hence, its iterative computation is ideally
The optimal solution to this state-estimation problem is to be found in the Bayesian filter.
When, however, the state-space model is nonlinear, which is frequently the case in practice, the Bayesian filter defies computational feasibility. In situations of this kind, we have
to be content with an approximation to the Bayesian filter. Given this practical reality, we
would like to develop an algorithm that is the “best” approximation to the Bayesian filter
in some sense. This development is indeed an important aim of this chapter.
With the state-space model being probabilistic, however, it is apropos that we
begin the discussion with a brief review of relevant concepts and ideas in probability
Probability, conditional probability, and Bayes’ rule
Let A denote an eventt that describes the outcome of a probabilistic experiment or subset
of such outcomes in a sample space denoted by S. Henceforth, we will use the symbol P
to signify the “operator” for the probability of an event enclosed inside a pair of square
brackets. The probability of event A, denoted by P[A
probability, formulated in the context of set theory.
Axiom I Nonnegativity. This states that the probability of event A is a nonnegative
Axiom II Additivity. This states that if A and B are two disjoint events, then the
probability of their union satisfies the equality
where the symbol ∪ denotes “union.” In general, if the sample space has N elements
and A1, A2, …, AN is a sequence of disjoint events, then the probability of the union of
Probability, conditional probability, and Bayes’ rule
Axiom III Normalization. This states that the probability of the entire sample
These three axioms provide an implicit definition of probability.
When an experiment is performed and we only obtain partial information on the outcome of the experiment, we may reason about that particular outcome by invoking the
notion of conditional probability. Stated the other way round:
Conditional probability provides the premise for probabilistic reasoning orr inference.
To be specific, suppose we perform an experiment that involves a pair of events A and
[ | B] denote the probability of event A given that event B has occurred. The
[ | B] is called the conditional probability of A given B. Assuming that B
has nonzero probability, the conditional probability P[A
[ ∩ B] is the joint probability of events A and B and P[B] is nonzero. The proof
of (4.1) follows directly from the intuitively satisfying relationship
Most importantly, for a fixed event B, the conditional probability P[A
[ | B] is a legitimate probability law, in that it obeys all three axioms of probability in its own way. In
[ | B] captures the partial information that the
occurrence of event B provides about event A. We may, therefore, view the conditional
[ | B] as a probability law concentrated on event B.
Suppose we are confronted with a situation where the conditional probability P[A
[ ] and P[B] are all easily determined directly, but it
is the conditional probability P[B | A] that is desired. To deal with this situation, we first
The left-hand sides of these two relations are identical; therefore, we have
Bayesian filtering for state estimation of the environment
[ ] is nonzero, we may determine the desired conditional probability
As simple as it looks, Bayes’ rule provides the correct language for describing inference; its formulation, however, cannot be done without making assumptions about the
probabilities on the right-hand side of (4.2).
Bayesian inference and importance of the posterior
The brief review material just presented on probabilistic models sets the stage to study
the role of probability theory in probabilistic reasoning
To proceed with the discussion, consider Figure 4.1. This depicts two finitedimensional spaces: a parameter space and an observation space, with the parameter
space being hidden from the observer that only has access to the observation space.
A parameter vector p, drawn from the parameter space Θ, is mapped probabilistically
by the probabilistic transition mechanism (e.g. telephone channel) onto the observation space c
c, producing the observation vector x. The vector x is itself the sample
value of a random vector X. Given the probabilistic scenario depicted in Figure 4.1,
we may identify two different operations that are the dual of each other.5 Specifically,
(1) Probabilistic modeling. The aim of this operation is to formulate the conditional
probability density function pX | Θ(x | p), which provides an adequate description of
the underlying physical behavior of the observation space.
Figure 4.1. Schematic model for the parameter-estimation problem.
Bayesian inference and importance of the posterior
(2) Statistical analysis. The aim of this second operation is the inverse of probabilistic
modeling, for which we need the alternative conditional probability density function
In a fundamental sense, statistical analysis is more profound than probabilistic modeling. We may justify this assertion by viewing the unknown parameter vector p as the
cause for the physical behavior of the probabilistic transition mechanism, and viewing
the observation vector x as the effect. In essence, statistical analysis solves an inverse
problem by retrieving the causes (i.e. the parameter vector p) from the effects (i.e. the
observation vector x). Indeed, we may go on to say that probabilistic modeling
to characterize the future behaviorr of x conditioned on p; on the other hand, statistical
analysis permits us to make inference about the unknown p given x, which makes it
particularly important in probabilistic reasoning.
To formulate the conditional probability density function pX | Θ (x |p), we recast Bayes’
rule of (4.2) in its continuous version by writing
The denominator of the right-hand side of (4.3) is itself defined in terms of the numerator through integration, as shown by
where pX,Θ(x, p) is the joint probability density function of x and p. In effect, the left-hand
side of (4.4) is obtained by integrating out the dependence of the joint probability density function pX,Θ(x, p) on the parameter vector p. Using probabilistic terminology, pX(x)
is said to be a marginal densityy of the joint probability density function pX,Θ(x, p). The
inversion formula of (4.3) is sometimes referred to as the principle of inverse probability.
In light of this principle, we may now introduce four distinct distributions:
(1) Observation distribution. This stands for the conditional probability density function
pX | Θ(x | p) pertaining to the “observation” vector x given the parameter vector p.
(2) Prior distribution. This stands for the probability density function pΘ(p) pertaining
to the parameter vector p “prior” to receiving the observation vector x.
(3) Posterior distribution. This stands for the conditional probability density function
pX | Θ(x | p) pertaining to the unknown parameter vector p “after” receiving the observation vector x.
(4) Evidence. This stands for the probability density function pX(x) referring to the
“information” contained in the observation vector x solely for statistical analysis.
The posterior distribution pΘ | X(p | x) is central to Bayesian inference. In particular, we
may view it as the updating of information available on the parameter vector p in light of
the information about p that is contained in the observation vector x, while the prior distribution pΘ(p ) is the information available on p prior to receiving the observation vector x.
Bayesian filtering for state estimation of the environment
The inversion aspect of statistical analysis manifests itself in the notion of the likelihood
function or just simply likelihood.6 In a formal sense, the likelihood, denoted by l(p | x),
is just the prior density pX | Θ(x | p), but reformulated in a different order, as shown by
The important point to note here is that the likelihood and the observation distribution
are both governed by exactly the same function that involves the parameter vector p
and the observation vector x. There is, however, a subtle difference between them in
probabilistic interpretation: the likelihood function l(p | x) is treated as a function of
the unknown parameter vector p given x, whereas the observation density pX | Θ(x | p)
is treated as a function of the observation vector x given p. However, note that, unlike
pX | Θ(x | p), the likelihood l(p | x) is nott a distribution; rather, it is simply a function of the
In light of the terminologies just introduced, namely the posterior, prior, likelihood,
and evidence, we may now use (4.3) to express the underlying rule for Bayesian statistical analysis (i.e. inference) in words as follows:
For convenience of presentation, henceforth we will let
Then, recognizing that the evidence defined in (4.4) plays merely the role of a normalizing function that is independent of p, we may now sum up the principle of inverse
The Bayesian statistical model is essentially made up of two components: the likelihood l(p | x)
and the prior p (p), where p is an unknown parameter vector and x is the observation vector.
To elaborate on the significance of the defining equation (4.6), consider the two likelihood functions l(p | x1) and l(p | x2) on parameter vector p. If, for a prescribed prior p (p),
these two likelihood functions are scaled versions of each other, then the corresponding
posterior densities of p are essentially identical, the validity of which is a straightforward consequence of Bayes’ rule. In light of this result, we may now formulate the
so-called likelihood principle7 as follows:
If x1 and x2 are two observation vectors depending on an unknown parameter vector p, such
where c is a scaling factor, then these two observation vectors lead to an identical inference on p
Parameter estimation and hypothesis testing: the MAP rule
Consider, next, a model parameterized by the vector p given the observation vector x. In
statistical terms, the model is described by the posterior density pΘ | X(p | x). In this context, we may now introduce a function t(x), which is said to be a sufficient statistic if the
probability density function of the parameter vector p given t(x) satisfies the condition
This condition imposed on t(x), for it to be a sufficient statistic, appears intuitively
appealing, as evidenced by the following statement:
The function t(x) provides a sufficient summary of the whole information about the unknown
parameter vector p, which is contained in the observation vector x.
We may thus view the notion of sufficient statistic as a tool for “data reduction,” the use
of which results in considerable simplification in statistical analysis.8
Parameter estimation and hypothesis testing: the MAP rule
As pointed out previously, the posterior density pΘ | X(p | x) is central to the formulation
of a Bayesian probabilistic model, where p is an unknown parameter vector and x is the
observation vector. It is logical therefore, that we use this probability density function
for parameter estimation. Accordingly, we define the maximum a-posteriori probability
(MAP) estimate of the parameter vector p, assumed to be continuous, as follows:9
where l(p | x) is the likelihood defined in (4.5) and p (p) is the prior defined in (4.6). Note
that, in formulating (4.8), we have ignored the evidence pX(x); this omission is justified,
since the evidence is merely a normalizing function that is independent of the parameter
vector p; as such, it does not affect the maximization in (4.8).
θ̂ MAP , we require availability of the prior p (p). In
words, the right-hand side of (4.8) reads as follows:
Given the observation vector x, the estimate θ
θ̂ MAP is that particular value of the parameter
vector p in the argument of the posterior density pΘ|X(p | x) for which the posterior density attains
We may generalize this statement by going on to say that the posterior pΘ|X(p| x) contains all
the information about the unknown multidimensional parameter vector p given the observation vector x. Recognition of this fact leads us to make the follow-up important statement:
θ̂ MAP of the unknown parameter vector p is the globally optimal solution to
the parameter-estimation problem, in the sense that there is no other estimator that can achieve a
Bayesian filtering for state estimation of the environment
Figure 4.2. Illustrating the MAP rule for two cases of parameter p : (a) continuous parameter;
θ̂ MAP as the MAP estimate, we have made a slight change in our termiIn referring to θ
nology: we have, in effect, referred to pΘ | X(p | x) as the a-posteriori probability density
rather than the posterior density of p for the case of continuous p. We have made this
minor change so as to conform to the MAP terminology that is well and truly embedded
in the literature on Bayesian statistical theory.
The statement just made on the MAP rule applies to the case when the unknown
parameter vector p is continuous. For the case when p is discrete, taking a finite number
of possible values, we modify the statement on the MAP rule to embody the following
important property (Bertsekas and Tsitsiklis, 2008):
The MAP rule maximizes the probability of correct decision for any
observation vector x, because it chooses θ̂ to be the most likely estimate.
Stated in other words, the MAP rule minimizes the probability of making an incorrect
decision for each observation vector x, with the overall probability of error being averaged over all x. Figure 4.2 illustrates the MAP rule for a one-dimensional scenario with
parts (a) and (b) of the figure referring to the continuous and discrete cases respectively.
The discussion presented in this subsection has focused on parameter estimation. In
the next subsection, we study hypothesis testing, which is another way of referring to
Suppose that the posterior-density computer for the MAP rule has been derived, as
depicted in Figure 4.3, which sets the stage for addressing the hypothesis-testing
problem. In a general setting, there are M possible hypotheses to consider, in which
case we speak of M-ary hypothesis testing, with M being typically a small integer
depending on the problem of interest. Binary-hypothesis testing is a special case with
Let the M hypotheses be denoted by the set { m }mM=1. Then, following the MAP
terminology introduced in Figure 4.3, we say that the mth hypothesis refers to the
eventt described by the random variable Θ = pm, where the subscript m takes the value
Parameter estimation and hypothesis testing: the MAP rule
Figure 4.3. Block diagram for illustrating the MAP rule.
In practice, we typically find that the observation vector x is discrete. Then, to solve
M-ary hypothesis-testing problem using the MAP rule, we proceed as follows:
• Given an observation vector x, we use Bayes’ rule to find the posterior probabilities
where X is a random vector and pΘ | X(p m | x) is a discrete probability density function.
• Next, we use the posterior computer in Figure 4.3 to select the particular hypothesis m
for which the posterior probability in (4.10) is the largest.
When there is a tie in the decision-making with two or more hypotheses attaining the
same largest posterior probability, we break the tie by arbitrarily choosing one of them;
here, it is assumed that all the M hypotheses are equiprobable.
Most importantly, the result of applying the MAP rule to the M
problem is optimal, in the sense that the probability of correct decision is maximized
To compute the probability of a correct decision for a specific observation vector x, let
hMAP(x) denote the hypothesis selected by the MAP rule as configured in Figure 4.3. Then,
according to (4.10), the probability of correct decision is described by the expression
Moreover, let Sm denote the set of all x in the observation space, for which the MAP rule
selects hypothesis Hm. Hence, we may define the overall probability of correct decision
M-ary hypothesis-testing problem as follows:
Correspondingly, the overall probability of errorr is described by the expression
The overall probabilities of correct and erroneous decisions, as described herein, add
up to unity, which is exactly how it should be. In effect, having calculated the overall
Bayesian filtering for state estimation of the environment
probability of correct decision using (4.11), the overall probability of error is obtained
simply by subtracting the result of (4.11) from unity.
Summarizing remarks on Bayesian inference
The main purpose of Bayesian statistical inference may be summed up as follows:
Given the observations of a random phenomenon, Bayesian inference provides a coherent methodology for deriving the underlying probability distribution of that phenomenon.
To be more specific, the probability distribution referred to in this statement is the
a-posteriori probability density function or the posteriorr for short. The posterior is
defined as the product of the likelihood and the prior, except for a normalizing factor
called the evidence. For an observation vector denoted by x and a related unknown
parameter vector p, the likelihood is written as l(p | x), which, in reality, is the probability density function of x, conditioned on p; in this context, it is noteworthy that the
Bayesian approach is the only one that allows for conditioning on the observations. As
for the prior, denoted by p (p), it provides for information known about the parameter
vector p prior to collection of the observations. Thus, ignoring the evidence, the posterior, denoted by p(p | x), is defined by the product l(p | x)p (p).
What makes the posterior p(p | x) so very important is the fact that it contains all the
information that we need to know, including the prior about the unknown parameter
Moreover, when it comes to decision-making, we look to the posterior as the distribution to formulate the MAP rule. Basically, the MAP rule maximizes the probability
of correct decision for any observation vector x by virtue of the fact that it chooses the
θ̂ , for which the posterior attains its maximum value. As such, when
considering the issues of parameter estimation and hypothesis testing, the MAP rule is
For parameter estimation, the MAP rule assumes that the unknown parameter vector p
is continuous. When dealing with hypothesis testing, the MAP rule is still applicable,
except that this time the posterior is reformulated as a discrete probability density function compatible with the number of hypotheses under test.
Our primary interest in Bayesian inference in this chapter is motivated by the second
viewpoint on how to deal with perception in the perception–action cycle that was discussed
in Section 2.11. With that viewpoint in mind, our objective is to estimate the environmental state of a dynamic system, given a set of observations received from the environment. With the environmental state being “hidden” from the perceptor, the only course
open to us is to formulate a state-space model for the system, which is done in the next
section. Such a model, in turn, sets the stage for deriving the Bayesian filter that is the best
that we can do for state estimation, at least in a conceptual sense.
Another important point of interest is that state estimation is an example of statistical
analysis. As such, in light of the discussion presented in Section 4.2 on Bayesian inference, state estimation is an inverse problem. Moreover, it is ill-posedd on account of
unavoidable physical uncertainties in the environment.
The material on Bayesian inference presented in Sections 4.2 and 4.3 provides the right
background for Bayesian filtering, which is aimed at estimating the state of a dynamic
system. To this end, we begin the discussion with the following formal definition:
The state of a dynamic system is defined as the minimal amount of information about the effects
of past inputs applied to the system, such that it is sufficient to completely describe the future
Typically, the state is nott measurable directly, as it is hidden from the perceptor. Rather,
in an indirect manner, the state makes its effect on the environment (outside world) to
be estimatable through a set of observables. As such, characterization of the dynamic
system is described by a state-space model, which embodies a pair of equations:10
(1) System equation (model), which, formulated as a first-order Markov chain, describes
the evolution of the state as a function of time, as shown by
where n denotes discrete time, the vector xn denotes the current value of the state,
and xn+1 denotes its immediate future value; the vector vn denotes system noise
and an(.,.) is a vectorial function of its two arguments, representing transition from
state xn to state xn+1. A cautionary note in the context of terminology is in order: in
Sections 4.2 and 4.3, x was used to denote an observation vector. In Section 4.4,
on the other hand, xn is used to denote the state of a dynamic system at discrete
(2) Measurement equation (model), which is formulated as
where the vector yn denotes a set of measurements (observables), the vector mn
denotes measurement noise, and bn(.,.) denotes another vectorial function.
The subscript n in both an and bn is included to cover situations where these two
functions are time varying. For the state-space model to be of practical value, it
must, however, relate to the underlying physics of the system under study. Stated in
a practical way: to formulate the state-space model of a dynamic system, we need
to understand the underlying physics of the system; it is only in this way that we get
Figure 4.4 depicts a signal-flow graph representation of the state-space model defined
by (4.12) and (4.13); and Figure 4.5 depicts its evolution across time as a first-order
Markov chain. In their own individual ways, both of these figures emphasize the hidden
nature of the state, viewed from the perceptor. Most importantly, however, adoption of
the state-space model, described in (4.12) and (4.13), offers certain attributes:
• mathematical and notational convenience;
• a close relationship of the mode to physical reality; and
• a meaningful basis of accounting for the statistical behavior of the dynamic system.
Bayesian filtering for state estimation of the environment
Figure 4.4. Generic state-space model of a time-varying, nonlinear dynamic system, where Z−1
Figure 4.5. Evolution of the state across time, viewed as a first-order Markov chain.
Henceforth, the following assumptions are made:
(1) The initial state x0 is uncorrelated with the system noise vn for all n.
(2) The two sources of noise, vn and mn, are statistically independent of each other,
This equation is a sufficient condition for independence when vn and mn are jointly
Although, indeed, the state is hidden from the perceptor, the environment does provide
information about the state through measurements (observables), which prompts us to
Given a record of measurements, consisting of y1, y2, …, yn, the requirement is to compute an
estimate of the unknown state xk that is optimal in some statistical sense, with the estimation being
In a way, this statement embodies two systems:
• The unknown dynamic system, whose observable yn is a function of the hidden state.
• The sequential state-estimator or filter, which exploits information about the state that
We may, therefore, view state estimation as an encoding–decodingg problem in the
The measurements represent an “encoded” version of the state, and the state estimate produced by
the filter represents a “decoded” version of the measurements.
As a corollary to this statement, we may say state estimation is an inverse problem, reconfirming what we said in “Summarizing remarks on Bayesian inference” in Section 4.3.
The state-estimation problem is commonly referred to as prediction if k > n, filtering
g if k < n. Typically, a smoother is statistically more accurate than both
the predictor and filter, as it uses more observables, past and present. On the other hand,
both prediction and filtering can be performed in real time, whereas smoothing cannot.
The mathematical difficulty of solving the state-estimation problem is highly dependent
on how good the state-space model is a descriptor of the dynamic system under study.
In general, we may identify the following hierarchy of models in increasing complexity:
In this model, which is the simplest of state-space models, (4.12) and (4.13) respectively reduce to
where An+1,n is called the transition matrix and Bn is called the measurement matrix.
The system noise vn and measurement noise mn are both additive and assumed
to be statistically independent zero-mean Gaussian processes, whose covariance
matrices are respectively denoted by Qv,n and Qm,n. The state-space model defined
by (4.15) and (4.16) is indeed the model that was used by Kalman to derive his
classic recursive filter that is mathematically elegant and devoid of any approximation (Kalman, 1960).
In this second model, we still use (4.15) and (4.16), but the system noise vn and
measurement noise mn are now assumed to be additive, statistically independent, nonGaussian processes. The non-Gaussianity of these two processes is, therefore, the only
source of mathematical difficulty. In situations of this kind, we may extend the application of Kalman filter theory by using the Gaussian-sum approximation, summarized as
Any probability density function p(x), describing a multidimensional non-Gaussian vector
represented by the sample value x, can be approximated as closely as desired by the
for some integer N and positive scalers a i with ∑ α i = 1.
Bayesian filtering for state estimation of the environment
N x;xi , ¬i) stands for the Gaussian (normal) density function of x with mean xi and covariance matrix ¬i for i = 1, 2, …, N
Gaussian-sum on the right-hand side of (4.17) converges uniformly to the given
probability density function p(x) as the number of terms N increases and the elemental covariance matrix ¬i approaches zero for all i (Anderson and Moore, 1979).
Note, however, that the terms in a Gaussian-sum model tend to grow exponentially
over the course of time, which may, therefore, require the use of a pruning algorithm in computing (4.17).
The third model in the hierarchy of state-space models of increasing complexity is
where the dynamic noise vn and measurement noise mn are both assumed to be additive and Gaussian. Although this twofold assumption simplifies the state-estimation
procedure, it is here, however, where we start to experience serious mathematical
difficulty in solving the sequential state-estimation problem due to the nonlinear
This last class of state-space models is described by (4.12) and (4.13), where both
the system model and the measurement model are nonlinear, and the system noise
vn and measurement noise mn are not only non-Gaussian but also nonadditive. In
this kind of scenario, the solution narrows down basically to particle filters as the
only viable way that we presently know for solving the sequential state-estimation
problem; more will be said on particle filters later in the next section under the
heading “Indirect numerical approximation of the posterior.”
The adoption of a Bayesian filter to solve the state estimation of a dynamic system,
be it linear or nonlinear, is motivated by the fact that it provides a general unifying
framework for sequential state estimation, at least in a conceptual sense. In any event,
from a theoretical perspective, the state estimation may be viewed as an expanded
form of parameter estimation under the Bayesian paradigm; hence the name “Bayesian filter”: instead of estimating an unknown parameter vector, the new issue of
interest is that of estimating the hidden state of a dynamic state given a sequence of
Naturally, probability theory is central to the Bayesian approach to state estimation.
To simplify the presentation, henceforth we will do two things:
• drop the subscript from the symbol denoting a probability density function and
• use the term “distribution” to refer to a probability density function.
Moreover, referring back to the system model of (4.12) and the measurement model of
(4.13), henceforth we also use the following notations:
sequence of observations, denoting { i }in=1 ;
predictive distribution of the state xn at the current time n, given the
entire sequence of past observations up to and including yn−1;
posterior distribution of the current state xn, given the entire sequence of
observations up to and including the current time n; this distribution is
commonly referred to simply as the “posterior”;
transition-state distribution of the current state xn given the immediate
past state xn−1; this distribution is simply referred to as the “prior”;
likelihood function or simply likelihoodd of the current observation yn
For derivation of the Bayesian filter, the only assumption that we will make is that the
evolution of the state is Markovian, which embodies the combination of two conditions:
(1) Given the sequence of states, x0, x1, …, xn−1, xn, the current state xn depends only on
the immediate past state xn−1 through the state transition distribution p(xn | xn−1). The
initial state x0 is distributed according to
(2) The observations y1, y2, …, yn are conditionally dependent on the corresponding
states x1, x2, …, xn; this assumption implies that the conditional joint likelihood of
the observations (i.e., the joint distribution of all the observations conditioned upon
all the states up to and including time n) factors as
The posterior p(xn | Yn) plays a key role in Bayesian analysis, in that it embodies the
entire information that we have about the state xn at time n afterr having received the
entire observation sequence Yn; hence, the posterior contains all the information necessary for state estimation. What we are saying here is merely a reassertion of statements
Suppose, for example, we wish to determine the filtered estimate of the state xn,
optimized in the minimum mean-squared error (MMSE) sense; the desired solution is
Correspondingly, for a measurement of accuracy of the filtered estimate xˆ n|n, we compute the covariance matrix
Bayesian filtering for state estimation of the environment
With computational efficiency being a compelling practical factor, particularly with
the computer as the machine to do the computation, there is a strong desire to compute
the filtered estimate xˆ n|n and related parameters in a recursive (iterative) manner. To
explain, suppose that we have the posterior distribution of the state xn−1 at time n − 1,
denoted by p(xn−1 | Yn−1). Then, computation of the updated value of the posterior of the
state at time n is governed by two basic time steps:
(1) Time update. The first update involves computingg the predictive distribution of xn
given the observations sequence Yn−1, as shown by
where the integration is performed over the state space, embodying xn−1. Equation (4.23) is justified by the basic laws of probability theory: multiplication of
the old posterior p(xn−1 | Yn−1) by the prior p(xn | xn−1) results in a joint distribution
of the old state xn−1 and the current state xn, conditioned on Yn−1; that is, p(xn,
xn−1 | Yn−1). Then, integrating this conditional joint distribution with respect to xn−1
yields the desired predictive distribution p(xn | Yn−1). The old posterior, p(xn−1 | Yn−1),
is obviously available from the previous filtering recursion at time n − 1. As for the
prior p(xn | xn−1), we proceed as follows:
(i) At the initialization stage, we use two measurements to find an initial value for
(ii) Once the initialization stage is done, we use the state equation in the state-space
model to find the prior; from then on, the prior takes the form of a predictive
(2) Measurement update. This second update exploits information about the current
state xn that is contained in the new observation yn, so as to update the old posterior p(xn−1 | Yn−1). In particular, applying Bayes’ rule to the predictive distribution
The normalizing constant in (4.24) is defined by
It is commonly referred to as the partition function; its inclusion in (4.24) ensures
that the total volume under the multidimensional surface of the posterior distribution p(xn | Yn) is unity, as it should be. The sequence of partition functions { i }in=1
d of the corresponding sequence of observations { i }iN=1 ,
Figure 4.6. Block diagram describing computational recursion of the Bayesian filter, with its
updated posterior p(xn | Yn) as the output of interest.
Equations (4.22)–(4.25) are all consequences of the Markovian assumption described
The time update and measurement update are both carried out at every time step
throughout the computation of the Bayesian filter. In effect, they constitute a computational recursion of the filter, as depicted in Figure 4.6; the factor Zn has been left out in
the figure for convenience of presentation.
The Bayesian filter of Figure 4.6 is optimal, with two important properties:
(1) The filter operates in a recursive manner by propagating
(2) Knowledge about the state xn, extracted from the entire observations process Yn by
the filter, is completely contained in the posterior p(xn | Yn), which is the “best” that
can be achieved, at least in a conceptual sense.
With the posterior as the focus of analytic attention, we now lay down the groundwork
for our filtering objective. To be specific, consider an arbitrary function of the state xn,
denoted by h(xn). In a filtering context, we are interested in the on-line estimation of
signal characteristics of the function h(xn). These characteristics are embodied in the
Bayes estimator, defined by the ensemble average of the function h(xn), namely
where Ep is the expectation operator with respect to the posterior p(xn | Yn) that pertains
to a dynamic system, be it linear or nonlinear. Equation (4.27) includes (4.21) for the
filtered estimate of the state and (4.22) for the covariance matrix of the estimate as two
special cases, illustrating the general unifying framework of the Bayesian model. For
Bayesian filtering for state estimation of the environment
where the arbitrary function now assumes the form of a vectorial function h(.).
For the special case of a dynamic system described by the linear, Gaussian model of
(4.15) and (4.16), the recursive solution of (4.27) is realized exactly through the celebrated Kalman filter. However, when the dynamic system is nonlinear or non-Gaussian
or both, then the product distribution constituting the integrand of (4.27) is no longer
Gaussian, which makes computation of the Bayes estimator hĥn a difficult proposition. In
situations of this latter kind, we have no option but to abandon the notion of optimality
in the Bayesian sense and seek an approximate estimator that is computationally feasible, yet related to the Bayesian filter in some manner. Henceforth, we confine the discussion to a nonlinear model operating under the Gaussian assumption, which may be
justified in practice by invoking the central limit theorem of probability theory.11
Under the Gaussian assumption, we are now ready to state our nonlinear filtering
Given the entire sequence of observations Yn, at time n, pertaining to the nonlinear state-space
model of (4.18) and (4.19), derive an approximate realization of the Bayes estimator h ( n ) ,
defined in (4.27), that is subject to two practical requirements:
To find suboptimal solutions of the nonlinear-filtering problem by approximating the
Bayesian filter, we may follow one of two routes, depending on the way in which the
(1) Direct numerical approximation of the posterior. The rationale behind this direct
approach to nonlinear filtering is summed up as follows:
In general, it is easier to approximate the posterior p (xn|Yn) directly and in a local sense than
it is to approximate the nonlinear function characterizing the system model of the filter.
To be specific, the posterior p(xn|Yn) is approximated locally around the point
x n xˆ n|n, where xˆ n|n is the filtered estimate of the state xn, given all the observables
up to and including time n; the emphasis on locality makes the design of the filter
computationally simple and fast to execute. The objective of the approximation is to
facilitate the subsequent application of Kalman filter theory, for which both linearity
(2) Indirect numerical approximation of the posterior. The rationale behind this second
approach to nonlinear filtering is summed up as follows:
The posterior distribution p (xn | Yn) is approximated indirectly and in a global sense through
the use of Monte Carlo simulations, so as to make the Bayesian framework for nonlinear
Particle filters12 constitute a popular example of this second approach to nonlinear
filtering. To be more specific, particle filters rely on a technique called the sequential
Monte Carlo (SMC) method, which uses a set of randomly chosen samples, referred
to as particles, with associated weights to approximate the posterior distribution
p(xn|Yn). The sampling process is called importance sampling, which is based on the
premise that in certain situations it is more appropriate to sample from a proposal
or instrumental distribution; then, a “change-of-measure” formula is applied to the
filtering procedure to account for the fact that the proposal distribution is naturally
different from the target distribution (Cappé et al., 2005). Now, as the number of
particles used in the simulation becomes larger, the Monte Carlo computation of the
posterior distribution becomes more accurate, which, of course, is the desired objective. However, the increased number of particles makes the use of the SMC method
computationally more expensive. In other words, computational cost is traded for
From this brief discussion, it is apparent that the locally direct approach to approximate Bayesian filtering builds on the well-established Kalman filter theory. On the other
hand, the globally indirect approach charts a path of its own by departing from Kalman
filter theory. Generally speaking, the globally indirect approach to nonlinear filtering is
more demanding in computational terms than the locally direct approach, particularly
when the nonlinear filtering problem is difficult.
The EKF is the simplest locally direct approximation to the Bayesian filter, under the
Gaussian assumption. The basic idea behind derivation of the EKF is summed up as
Linearize the nonlinear equation (i.e., system or measurement equation) about the most recent
estimate of the state; once the new state estimate is computed, a new and better reference state
trajectory is incorporated into the next step of the state-estimation process.
To elaborate, the filtered estimate of the state, xˆ n|n, is used in linearization of the system
equation; and the predicted estimate of the state, xˆ n|n−1, is used in linearization of the
measurement equation. In both cases, linearization is accomplished by using the Taylor
series and retaining only first-order terms in the resulting expansion of the system or
Thus, referring to the state equation (4.18), we write
Similarly, referring to the measurement equation (4.19), we write
Bayesian filtering for state estimation of the environment
Once the transition matrix An+1 | n and measurement matrix Bn have been constructed,
they are respectively used in first-order Taylor series approximations of the nonlinear
functions an(xn), first, and bn(xn) around the estimates xˆ n| n , obtaining
b n ( x n ) ≈ b n ( xˆ n−1 ) B n ( x n − xˆ n| n−1 ).
With this pair of linearized approximations of the nonlinear functions an(xn) and bn(xn)
at hand and assuming that the system noise vn and measurement noise mn are both
Gaussian, the stage is set for the application of Kalman filter theory, yielding the EKF
summary presented in Table 4.1 (Maybeck, 1982; Haykin, 2009).
Nonlinear state-transition vectorial function = an(xn)
Nonlinear measurement vectorial function = bn(xn)
Correlation matrix of process noise vector = Qv,n
Correlation matrix of measurement noise vector = Qm,n
1. The linearized matrices An +1,n and Bn are computed from their nonlinear counterparts
an(xn)and bn(xn) using (4.28) and (4.29) respectively.
2. The values a n ( xˆ n|n ) and b n ( xˆ n|n −1 ) are obtained by substituting the filtered state estimate
xˆ n| n and the predicted state estimate xˆ n|n −1 for the state xn in the nonlinear vectorial
functions an(xn)and bn(xn) respectively, as in (4.30) and (4.31).
3. Examining the order of iterations in this table, we now see the reason for evaluating An +1,n
and Bn in the manner described in (4.28) and (4.29).
where Π0 = d −1I, d is a small positive constant, and I is the identity matrix.
Summarizing remarks on the extended Kalman filter
The EKF is attractive for nonlinear state estimation for two compelling reasons:
(1) It builds on the framework of Kalman filter theory in a principled way.
(2) It is relatively simple to understand and, therefore, straightforward to put into practical use, for which it has established a long track record over the past four decades
However, it has two fundamental drawbacks that tend to limit its practical usefulness:
(1) For the EKF to function satisfactorily, the nonlinearity of the state-space model,
embodying (4.18) and (4.19), has to be of a mildd sort, so as to justify the use of the
first-order Taylor series expansion, upon which its derivation is built.
(2) Its derivation requires knowledge of first-order partial derivatives (i.e. Jacobians) of
the state-space model of the nonlinear dynamic system under study; however, for
many practical applications, the computation of Jacobians is undesirable or simply
not feasible because the state-space model may contain nondifferentiable terms.
There is one other matter of theoretical interest that is noteworthy: the traditional
approach adopted in the literature for deriving the EKF, exemplified by that described in
this section, does not teach us the relationship between the EKF and Bayesian filter. It is
in Elliott and Haykin (2010) where we see that the EKF is indeed an approximate form
of the Bayesian filter under the Gaussian assumption. The theory discussed therein for
establishing this relationship is based on the well-known Zakai equation in nonlinear
For a more powerful approximation of the Bayesian filter, we look to a new nonlinear
filter, named the CKF (Arasaratnam and Haykin, 2009). Derivation of this new filter
exploits the fact that, under the Gaussian assumption, approximation of the Bayesian
filter reduces to computing multidimensional integrals of a special form whose integrand is described, in words, as follows:
(nonlinear function) × (Gaussian function)
Specifically, given an arbitrary nonlinear function f(
Gaussian function, we consider an integral of the form
which is, of course, defined in the Cartesian coordinate system.
For numerical approximation of the nonlinear function h(f)
cubature rule based on monomials (Stroud, 1971; Cools, 1997). In mathematics, the
Bayesian filtering for state estimation of the environment
term “monomials” refers to a product of powers of variables.13 A monomial-based
cubature rule satisfies our numerical approximation needs for the following reasons:
• the provision of a reasonably accurate approximation;
• the requirement of a small number of function evaluations in the approximation; and
• the relative ease of extending the approximation to high dimensions.
Moreover, we will focus the approximation on a third-degree spherical radial cubature rule. The motivation behind considering spherical–radial integration is the fact
that it befits the integral described in (4.33), as shown in what follows. As for the
choice of a third-degree cubature rule, the reader is referred to Note 14 at the end of
To proceed then, we will construct the cubature rule in such a way that the numerical
approximation of (4.33) is built on a set of weighted cubature points that is an even symmetric set. In so doing, the complexity in solving a set of nonlinear equations for a set
of desired weights and cubature points is reduced markedly. Before going into details
about the cubature rule, we introduce a number of notations and definitions.
• Using D to denote the region of integration in (4.33), we say that a weighting function w(x) defined on the region D is fully symmetric if the following two conditions
(1) x ∈ D implies y ∈ D, where y is any point obtainable from x by permutations and
by changes of sign of the coordinates of x.
• In a fully symmetric region, we call a point u as a generatorr if u = (u1, u2, …, ur, 0, … 0) ∈
RM, where ui ≥ ui+i > 0 for i = 1, 2, …, (r − 1).
• We use the notation [u1, u2, …, ur] to represent the complete set of points that can
be obtained by permutating and changing the signs of the generator u in all possible
ways. For the sake of brevity, we suppress the (n - r) zero nodes in the notation. For
example, we write [1] = R2 to represent the following permuted set of points:
• We use the notation [u1, u2, …, ur]i to denote the ith point from the generator u.
Converting to spherical–radial integration
The key step in this conversion is a change of variables from the Cartesian vector x ∈ RM
to a spherical–radial one defined by radius r and direction vector z, as outlined here:
Let x = r z with zTz = 1, so that xTx = r2 forr r ∈ [0, ∞).
Then, the integral of (4.33) can be rewritten in a spherical–radial coordinate system as
where UM is the region defined by UM = {z; zTz = 1} and s (.) is a spherical surface
The integral of (4.35) is computed numerically by the spherical rule. Then, having computed S(r) and using (4.34), the radial integral
is computed numerically by using the Gaussian quadrature; and with it, computation
of (4.33) will have been accomplished. Both of these two rules are described in the
We first derive a third-degree spherical rule that takes the form
The rule in (4.37) entails from the generator [u] a total of 2M cubature points
symmetrically positioned on a unit hypersphere (ellipsoid) centered on the origin; the
cubature points are located at the intersection of an M
axes. To find the unknown parameters u and w, it suffices to consider monomials f (z) = 1
and f (z) = z 12 due to the fully symmetric generators, as shown here:
where M is the dimension of vector x and the surface area of the unit hypersphere is
is the Gamma function. Given the AM just defined, solving (4.38) and (4.39) for w and
Bayesian filtering for state estimation of the environment
For the radial rule, we propose to use a Gaussian quadrature, which is known to be
the most efficient numerical method to compute an integral in a single dimension. An
m-point Gaussian quadrature is exact up to polynomials of degree (2M − 1) and constructed as
( ) denotes a weighting function, and wi = w(x
1990). The xi and the wi are respectively the quadrature points and associated weights
to be determined. Comparison of the integrals in (4.36) and (4.40) yields the weighting
function and the region of integration to be w(x
Thus, using a final change of variables, t = x , we obtain the desired radial integral
where f (t ) = f ( t ) . The integral on the right-hand side of (4.41) is now in the form
of the well-known generalized Gauss–Laguerre formula (Stroud, 1966; Press and
A first-degree Gauss–Laguerre rule is exact for f (x) = 1, x. Correspondingly, the rule
is exact for f (x) = 1, x2; it is not exact for odd-degree polynomials, such as that in f (x) =
x, x3. Fortunately, however, when the radial rule is combined with the spherical rule to
compute the integral (4.33), the resulting spherical–radial rule vanishes for all odddegree polynomials; the reason for this nice result is that the spherical rule vanishes by
virtue of the symmetry for any odd-degree polynomial; see (4.34). Hence, the spherical–radial rule for computing (4.33) is exact for all odd-degree polynomials. Following
this argument, for a spherical–radial rule to be exact for all third-degree polynomials in
x ∈ RM, it suffices to consider the first-degree generalized Gauss–Laguerre rule, which
entails the use of a single point and a single weight. We may thus write
In this final subsection, we describe two useful results that are used: the first is used to
combine the spherical and radial rules and the second is used to extend the spherical–
radial rule for a Gaussian weighted integral. The respective results are presented as two
propositions (Arasaratnam and Haykin, 2009):
Proposition 4.1. Let the radial integral be computed numerically by an mr-point
Let the spherical integral be computed numerically by an ms-point spherical rule:
spherical–radial cubature rule is approximately given by the
Proposition 4.2. Let two weighting functions be denoted by w1(x) = exp(−xTx) and
N x; l, S), where, for a given vector x, the N(
N x; l, S) denotes the Gaussian distribution of x with mean l and covariance matrix S. Then, for every square-root matrix
where the superscript T/2 signifies transpose of the square root of S.
For the third-degree spherical–radial rule, mr = 1 and ms = 2M.
M cubature points. Moreover, the rule is exact for integrands
that can be written as a linear combination of polynomials of degree up to three and
all odd-degree polynomials. Invoking Propositions 4.1 and 4.2, we may now extend
this third-degree spherical–radial rule to numerically compute the standard Gaussianweighted integral
M with M being dimensionality of the state-space,
In effect, the xi are the cubature-point representations of the M-dimensional vector
x. That is, the cubature-point set {ξi , wi }i2=1
standard Gaussian-weighted integral of (4.42).
Bayesian filtering for state estimation of the environment
The formula of (4.42) is the cubature rule we have been seeking for numerical approximation of the moment integral of (4.33). Indeed, the cubature rule is central to the computation of all the moment integrals embodied in the Bayesian framework for nonlinear
filtering. As with the EKF, we assume that the dynamic noise vn and measurement noise
mn are jointly Gaussian. Under this Gaussian assumption, we may now proceed to approximate the Bayesian filter using the cubature rule applied to the traditional pair of updates:
(1) Time update. Suppose that the prior p(xn−1|Yn−1) is approximated by a Gaussian distribution whose mean is the filtered state-estimate xˆ n 1||nn −1 and its covariance matrix
is equal to the filtered-error covariance matrix Pn−1|n−1. Then, using the formula for
the Bayes estimator, we may express the predicted estimate of the state x ∈ RM as
where we have used knowledge of the system model of (4.18) and the fact that the
system noise vn−1 is uncorrelated with the sequence of observations Yn−1; in (4.43),
we have assumed that the vectorial function a(.) is time invariant. Similarly, we
obtain the prediction-error covariance matrix
The pair of (4.43) and (4.44) address time-update aspects of approximating the
(2) Measurement update. Next, to find a formula for the measurement update, suppose
that the joint distribution of the state xn and the observation yn, conditioned on the
sequence Yn−1, is also Gaussian as shown by
y n ⎦ ⎢⎣ yˆ n|n −1 ⎥⎦ ⎢⎣ Pyx , |n 1 Pyyyy , n|n|n−
where xˆ n|n −1 is the predicted state-estimate defined in (4.43) and yˆ n|n −1 is the predicted estimate of the observation yn given the sequence Yn−1, as shown by
The prediction-error covariance matrix of yˆ n|n −1 is defined by
b( x n ) bT ( x n ) N ( x n ; xˆ n| n , Pn| n 1 ) dx n
Lastly, the cross-covariance matrix of the state xn and the observation yn is given by
The four integral formulas, described in (4.43), (4.44), (4.47), and (4.48) address
other aspects of approximating the Bayesian filter. However, as different as these
four formulas are, their integrands have a common form similar to that described in
(4.32): the product of a nonlinear function and a corresponding Gaussian function
of known mean and covariance matrix. To be more precise, all these four integrals
lend themselves to approximation by means of the cubature rule, using the cubature2M
point set {ξi , wi }i =1 x that follows (4.42) for the normalized case of standard Gaussianweighted integral.
Most importantly, once the cubature rule is done with, recursive computation of the
filtered estimate of the state builds on linear Kalman filter theory by proceeding along
(1) Kalman gain. The computation of this is accomplished by using the formula
| −1 is the inverse of the covariance matrix Pyy,n|n−1 defined in (4.47), and
Pxy,n|n−1 is the cross-covariance matrix defined in (4.48).
(2) Updating of the filtered state estimation. Upon receiving the new observation yn,
the filtered estimate of the state xn is computed in accordance with the predictor–
where the predicted estimates xˆ n|n −1 and yˆ n|n −1 are respectively defined in (4.43) and
Bayesian filtering for state estimation of the environment
(3) Filtered-error covariance matrix. The filtered-state estimate of (4.50) is associated with
the filtering-error covariance matrix, which is computed in accordance with the formula:
where the matrices Pn|n-1, Pyy,n|n-1, and Gn are respectively defined in (4.44), (4.47),
Finally, the posterior distribution of the state xn is computed as the Gaussian distribution
where the mean xˆ n|n is defined by (4.50) and the covariance matrix Pn|n is defined by
Thus, having started the computation with the old posterior p(xn−1|Yn−1) under the
time update, the computational recursion of the CKF has moved forward systematically
through the measurement update by propagating the old posterior p(xn−1|Yn−1) as in the
Bayesian filter, culminating in the computation of the updated posterior distribution
p(xn|Yn); and the recursion is then repeated as required. The nth recursion is defined by
(4.43) progressing to (4.52), in that order.
This new nonlinear filter, rooted in Bayesian estimation, has some important properties,
Property 1 Unlike the EKF, the CKF is a derivative-free on-line sequential-state
estimator, relying on integration from one iteration to the next for its operation;
hence, the CKF has a built-in noise-smoothingg capability.
Property 2 Approximations of the moment integrals in the Bayesian filter are all
linear in the number of function evaluations.
Property 3 As with the EKF, computational complexity of the CKF grows as M 3,
Property 4 Regularization is naturally built into the CKF by virtue of the fact that
the priorr in Bayesian filtering is known to play the role of a stabilizer.
Property 5 The CKF inherits well-known properties of the linear Kalman filter,
including square-root filtering that can be employed for improved accuracy and
Property 6 The CKF provides a method of choice for approximating the Bayesian
filter under the Gaussian assumption in a nonlinear setting.
Property 7 By virtue of its uniquely characteristic even set of weighted cubature
points, the CKF has the built-in signal-processing power to ease the curse-ofdimensionality problem in an additive Gaussian noise environment, but in its
present form the curse cannot be mitigated.
The curse-of-dimensionality problem refers to the exponential growth of computational cost with increasing dimensionality of the state-space, measurement space, or
both. More will be said on Property 7 in the next section, where the CKF is contrasted
with another nonlinear filter called the unscented Kalman filter (UKF).
On the relationship between the cubature and unscented Kalman filters
The CKF is a new nonlinear filter that approximates the optimal Bayesian filter by
exploiting the third-degree cubature rule,14 which is well suited for the numerical
approximation of integrals whose integrand is of the form described in (4.33). With
this approximation in place, the stage is set for building on Kalman filter theory, hence
the name “the cubature Kalman filter.” Perhaps the simplest way to describe the CKF
is to say that it is the best-known approximation to the optimal Bayesian filter under
the Gaussian assumption in a nonlinear setting. Simply put, the CKF is an important
addition to the “kit of tools for nonlinear filtering.”
On the relationship between the cubature and unscented Kalman filters
Julier and coworkers (Julier et al., 2000; Julier and Uhlmann, 2004) described a new
nonlinear filter, named the UKF. The UKF is another direct localized method of approximating the Bayesian filter, but, unlike the CKF, it uses a completely different set of
deterministic weighted points for the approximation.
M-dimensional random vector x that has a symmetric prior distribution p (x) with mean l and covariance matrix S, for which the Gaussian
distribution is a special case. Given the dimensionality of the state-space M
of sample points, numbering (2M + 1) and a corresponding set of weights, identified by
{χ i , wi }i2 M0 , are chosen to satisfy the following pair of moment-matching conditions:
Among the many candidate sets of weighted points that can satisfy (4.53) and (4.54),
the following symmetrically distributed point-set is chosen for the UKF:
where i = 1, 2, …, M and the ith column of covariance matrix S is denoted by Si.
M weighted points, centered around the zero weighted point
c0, w0}, is called the sigma-point set. The new parameter k is picked so as to scale the
Bayesian filtering for state estimation of the environment
M sigma points from the prior mean c0 = l; hence the reference to
k as the scaling parameter. Furthermore, to capture the kurtosis of the prior distribution
p (x) closely enough, it is recommended to set
The rationale behind this choice is to preserve exactly the moments in the approximation to the fifth order in the simple one-dimensional case of Gaussian distribution;
for details, see Appendices I in Julier and coworkers (Julier et al., 2000; Julier and
With the so-called unscented transformation, described by (4.53) to (4.58), the stage
is set to formulate a procedure for computing the posterior statistics of a new vector
y ∈ RMy, which is related to the vector x by the nonlinear transformation
Given the odd set of (2M + 1) weighted sigma-points, the unscented transformation is
now extended to approximate the mean and covariance matrix of the vector y by the
following set of projected sigma points in the RMy space:
( b( ) − E[ y ])) ( b( x ) − E[ y ])T ( x ) dx
Provided that the sigma-point set captures the first pth-order moments of the prior distribution p (x), then the sigma-point transformation to approximate the mean and covariance matrix of y, presented in (4.60) and (4.61), is correct for a pth-order nonlinearity.
This statement is proved by comparing the Taylor-series expansion of the nonlinear
function b(x) with the statistics computed by the unscented transformation.
With the unscented transformation in place, as just described, we have in effect linearizedd the nonlinear relationship of (4.59) in a way that preserves second-order information about x that is contained in y. So, finally, with additive system noise vn to the
vectorial nonlinear evolution of xn over time n in accordance with the system equation
(4.18) and additive measurement noise mn to the vectorial nonlinear dependence of
yn on xn in accordance with the measurement equation (4.19), where n denotes discrete time, we are now in a position to invoke linear Kalman filter theory. In so doing,
we will have developed a recursive formulation of the UKF, so named by virtue of
building on the combined use of unscented transformation and relevant parts of the
On the relationship between the cubature and unscented Kalman filters
From this exposition of UKF just presented and that of CKF presented in Section 4.7,
we readily see that the UKF and CKF share a common property:
To approximate the Bayesian filter, they both use a weighted set of symmetric points.
To illustrate this statement, Figure 4.7a and b, based on a two-dimensional space,
show distributions of the weighted sigma-point set for the UKF and the corresponding
weighted cubature-point set for the CKF respectively. The two sets of weighted points
are signified by the location and heights of the stems shown therein. This figure clearly
illustrates that these two sets of weighted points are quite different from each other.
Figure 4.7. Two kinds of distributions of point sets in two-dimensional space: (a) sigma point-set
for the UKF; (b) third-degree spherical–radial cubature point-set for the CKF.
Bayesian filtering for state estimation of the environment
To elaborate more deeply on the fundamental differences that distinguish these two
nonlinear approximations of the Bayesian filter from each other, the following three
important points are to be borne in mind.
Derivation of the UKF proceeds from an “unscented” transformation perspective, which
is heuristic in some of the algorithmic steps taken in formulating it; the evidence for this
point is to be found in (4.55)–(4.57), where the scaling parameter k is introduced. In a
related context, in the original proposition of UKF described by Julier et al. (2000) right
up to relatively recent work on the UKF reported by Sarkka (2007), the scaling parameter k is emphasized as an essential ingredient in computing the sigma-point set.
On the other hand, derivation of the CKF is mathematically rigorous from beginning
to end. Specifically, the third-degree cubature rule is used to numerically approximate
Gaussian-weighted integrals, as described in Section 4.7. Although, indeed, the idea of
using the cubature rule for constructing such weighted integrals has been known in the
mathematics literature for four decades, there has been no reference made to it in the
nonlinear filtering literature, except by Arasaratnam and Haykin (2009).
M-dimensional state space there are (2M + 1) sigma points that characterize the UKF. One sigma point is located at the origin of a 2M-dimensional
M sigma points are symmetrically distributed on the surface of the
ellipsoid. Occurrence of the sigma point at the origin of the ellipsoid is attributed to the
M-dimensional state space we have an even set of 2M
cubature points to characterize the CKF. These 2M
symmetrically on the surface of a 2M-dimensional
cubature points, the CKF distinguishes itself from the UKF in not having a cubature
Most importantly, locations of the points and associated weights that characterize
these two nonlinear filters are entirely different. Naturally, this fundamental geometric
difference has serious consequences. More precisely, in light of the two weighted-point
distributions portrayed in Figure 4.7 for the simple case of a two-dimensional state
space, we are emboldened to make the following statement:
M-dimensional state space, the presence of a weighted sigma-point at the origin of the
ellipsoid, characterizing the UKF, has the effect of weakening the approximating
power of the UKF, compared with the CKF that has no cubature point at the origin of its own, and
Weakness of the UKF is attributed to an oddd set of weighted sigma-points resulting from
k, which is directly responsible for its inability to match the
signal-processing power of the CKF characterized by an even set of weighted cubature
points. This weakness shows up when they are both confronted with a difficult nonlinear
filtering problem, where dimensionality of the state space, measurement space, or both
is high enough for the filtering computational complexity to become unmanageable due
to the curse-of-dimensionality problem, discussed in the next section. Suffice it to say,
the experimental results presented therein demonstrate superiority of the continuousdiscrete version of the CKF over its UKF counterpart.
The UKF, as originally described by Julier and coworkers (Julier et al., 2000; Julier
and Uhlmann, 2004), is fundamentally different from the CKF first described by
The fundamental difference between these two nonlinear filters can be traced to the
introduction of the scaling parameter k in the UKF and the emphasis placed on its use
not only by the originators of this filter, but also subsequently by many other investigators in the past 10 years.
What is truly surprising is that if the scaling parameter k is set equal to zero in the defining
equation (4.55) (i.e. the state-space dimensionality M = 3 in (4.58)), then the weight w0 is
reduced to zero, and with it the weighted sigma-point-set of the UKF collapses precisely
onto the weighted cubature-point set of the CKF. What is all the more surprising is another
fact: this observation based on setting k = 0 was rarely pointed out in the nonlinear filtering
literature; and, by the same token, the cubature rule, basic to the rigorous mathematical derivation of the CKF, was ignored by the nonlinear filtering community for four decades!
In solving sequential state-estimation problems, it is very important that we pay careful
Computational complexity of the state-estimation problem grows exponentially with increasing
This challenging computational issue is encountered whenever we are faced with solving
large-scale nonlinear filtering problems, where high dimensionality applies to the state
space, measurement space, or both. More than likely, problems of this kind will become
more common as we go on to tackle problems rooted in the study of a system of systems.
To explain the origin of the computational problem just stated, we begin by reminding
ourselves that, in mathematical terms, the hidden state of a physical environmentt may
naturally evolve across time in accordance with the stochastic differential equation
where t denotes continuous time. However, with the usual emphasis on digital processing
of the observables, given the abundant availability of computers nowadays, the measurement equation is formulated in discrete time. In reality, therefore, the state-space
Bayesian filtering for state estimation of the environment
d in nature, involving the use of continuous time in the system equation
and discrete time in the measurement equation. To proceed with the state-estimation
problem in practice, the usual first step, therefore, is to discretize the system equation,
so as to set the stage for solving the problem on a computer; the original hybrid model
is thereby converted into the fully discrete model, as formulated in (4.18) and (4.19). In
fact, it is because of this analog-to-digital conversion process that the state dimensionality becomes a problem in the first place. Simply put, if we wish to work in the discretetime domain, which in today’s world is the order of the day, then we may have a serious
To elaborate further on this issue, consider Figure 4.8a, b, and c, corresponding to
the dimensionality M = 1, 2, and 3 respectively. The figure clearly shows that growth
Figure 4.8. Illustrating the curse-of-dimensionality problem as the dimension M assumes the
in the number of “resolution cells” in a grid follows a linear law in Figure 4.8a, square
law in Figure 4.8b, and cubic law in Figure 4.8c; hence the statement on computational
complexity made at the beginning of this section.
The problem being described here is commonly referred to as the curse-ofdimensionality problem. The terminology was coined by Bellman in his classic book
Adaptive Control Processes. Quoting from the book (Bellman, 1961: 94):
In view of all that we have said in the foregoing sections, the many obstacles we appear to have
surmounted, what casts the pall over our victory celebration? It is the curse-of-dimensionality, a
malediction that has plagued the scientist from earliest days.
Case study on the curse-of-dimensionality problem
Arasaratnam et al. (2010) studied the case of a difficult radar tracking problem to compare the continuous-discrete (CD) versions of three nonlinear filters: EKF, UKF, and
CKF; correspondingly, these three filters are referred to as CD-EKF, CD-UKF, and
The study focused on a typical air-traffic control scenario, where the objective is to
track the trajectory of an aircraft that executes a maneuver at (nearly) constant speed
and turn rate in the horizontal plane. Specifically, the motion in the horizontal plane and
the motion in the vertical plane are considered to be decoupled from each other. In the
language used in aviation, this kind of motion is commonly referred to as a coordinated
turn (Bar-Shalom et al., 2001). The radar problem of tracking coordinated turns is considered to be challenging for the following reasons:
• The problem is nonlinear in both the system and measurement models (equations).
• The dimensionality of the state space is seven. This is accounted for by position and
velocity in each of the three Cartesian coordinates plus the coordinated turn; this
dimensionality is high for a radar tracking application.
• There is control over the degree of nonlinearity in the measurement space via the
coordinated-turn rate parameter; as this parameter is increased, the aircraft maneuvers
more quickly, which, in turn, makes the task of tracking the aircraft more difficult.
Simply put, this seven-dimensional state-estimation problem may well be the highest
dimensional problem in the radar tracking literature.
The conclusion drawn from results of the experiments performed under this case
Among the three approximate Bayesian filters, the CD-CKF is the most accurate and reliable, followed by the CD-UKF, and then the CD-EKF.
This ordering of the three approximate Bayesian filters for radar tracking applies equally
well to nonlinear state-space models of the discrete kind. Therefore, we are emboldened
Under the Gaussian assumption, a CKF is a method of choice for challenging radar tracking
Bayesian filtering for state estimation of the environment
As a corollary to this statement, we may reiterate Property 7 in Section 4.7, by saying
that although the CKF does not solve the curse-of-dimensionality problem, it does ease
it under the Gaussian assumption to a greater extent than is feasible with other approximate Bayesian filters.
Recurrent multilayer perceptrons: an application
For an interesting application of nonlinear sequential state estimation on a topic that
is highly relevant to cognitive dynamic systems, we look to the supervised training of
RMLP. This class of neural networks was discussed in Chapter 2. As discussed therein,
(1) The network consists of multiple layers of neurons that operate in parallel; hence the
(2) The network involves the use of feedback from one layer to another, thereby
enhancing the computational power of the network.
To describe how a nonlinear sequential state-estimator can be used to train an RMLP
in a supervised manner, consider such a network with W synaptic weights and p output
nodes. With n denoting a time step in the supervised training of the network, let the vector
wn denote the entire set of synaptic weights in the network, computed at time n. For
example, we may construct the vector wn by stacking the weights associated with neuron
1 in the first hidden layer on top of each other, followed by those of neuron 2, and then
carry on in this manner until we have accounted for all the neurons in the first hidden layer,
and then do the same for the second and any other hidden layer in the network, until all
the weights in the network have been accounted for in the vector wn in the orderly fashion
With sequential state estimation in mind, the state-space modell of the network under
training is defined by the following pair of equations:
(1) System equation, which is described by
The dynamic noise vn is a white Gaussian noise of zero mean and covariance
matrix Qv; the noise is purposely included in the system equation to anneal the
supervised training of the network over time. In the early stages of the training,
the covariance matrix Qv is large to encourage the supervised learning algorithm to escape local minima, and then it is gradually reduced to some finite but
(2) Measurement equation, which is described by
4.10 Recurrent multilayer perceptrons: an application for state estimation
Figure 4.9. Nonlinear state-space model depicting the underlying dynamics of a recurrent neural
network undergoing supervised training; Z−1 denotes a bank of unit-time delays.
• vn is the vector representing the recurrent node activities inside the network,
with its elements listed in an order consistent with those of the weight vector wn.
• un is the vector denoting the input signal applied to the network; that is, un serves
• mn is the vector denoting measurement noise corrupting the vector dn; it is
assumed to be a multivariate white noise process of zero mean and diagonal
covariance matrix Rn. The source of this noise is attributed to the way in which
The vector-valued measurement function b(.,.,.) in (4.65) accounts for the overall nonlinearityy of the RMLP from the input to the output layer; it is the only source of nonlinearity
in the state-space model of the recurrent network. This model is described in Figure 4.9.
Insofar as the notion of state is concerned, there are two different contexts in which
this notion naturally features in the supervised training of the recurrent network:
(1) Externally adjustable state, which manifests itself in adjustments applied to the
network’s weights through supervised training; hence the inclusion of the weight
vector wn in the state-space model described by both (4.64) and (4.65).
(2) Internally adjustable state, which is represented by the vector of recurrent node
activities vn; these activities are outside the scope of the presently configured supervised training process, and it is for this reason that the vector vn is included only
in the measurement model of (4.65). The externally applied driving force (input
vector) un, the dynamic noise vn, and the global feedback around the MLP account
Description of the supervised training framework using the EKF
Given the training sample { n , d n }nN 1 , the issue of interest is how to undertake the
supervised training of an RMLP by means of a sequential state-estimator. Since the
RMLP is nonlinear by virtue of the nonlinear measurement model of (4.65), the sequential state-estimator would have to be correspondingly nonlinear. With this requirement
in mind, we begin the discussion by considering how, for example, the EKF, summarized
in Table 4.1, can be used to fulfill this role.
Bayesian filtering for state estimation of the environment
To adapt Table 4.1 for our present needs, we first recognize the following pair of
equations, based on the state-space model of (4.64) and (4.65) of the RMLP:
where the desired (target) response dn plays the role of the “observable” for the EKF.
where ŵn|n–1 is the predicted (old) estimate of the RMLP’s weight vector w at time
n given the desired response up to and including time n − 1, and ŵn|n is the filtered
(updated) estimate (4.67) of w. The matrix Gn is the Kalman gain, which is an
Accordingly, we may formulate Table 4.2, describing the steps involved in supervised
training of the RMLP, using the EKF algorithm.
Table 4.2. Summary of the EKF algorithm for supervised training of the RMLP
where un is the input vector applied to the RMLP and dn is the corresponding desired response.
RMLP and Kalman filter: parameters and variables
b(.,.,.) vector-valued measurement function
linearized measurement matrix at time-step n
ˆ n|n −1 predicted estimate of the weight vector
vector of recurrent node activities in the RMLP
output vector of the RMLP produced in response to the input vector un
covariance matrix of the dynamic noise vn
covariance matrix of the measurement noise mn
For n = 1, 2, . . ., compute the following:
P1|0 = d I, where d is a small positive constant and I is the identity matrix
4.10 Recurrent multilayer perceptrons: an application for state estimation
Examining the underlying operation of the RMLP, we find that the term b(ŵn|n–1,
vn, un) is the actual output vector yn produced by the RMLP with its “old” weight
vector ŵn|n–1 and internal state vn in response to the input vector un. Therefore, we may
rewrite the combination of (4.66) and (4.67) as a single equation:
On the basis of this insightful equation, we may now depict the supervised training of
the RMLP as the combination of two mutually coupled components, forming a closedloop feedback system, as that shown in Figure 4.10. Specifically:
(1) Figure 4.10a depicts the supervised learning process, viewed partly from the network’s perspective. With the weight vector set at its old (predicted) value ŵn|n–1, the
RMLP computes the output vector yn in response to the input vector un. Thus, the
RMLP supplies the EKF with yn as the predicted estimate of the observable, namely
(2) Figure 4.10b depicts the EKF in its role as the facilitatorr of the training process.
Supplied with d̂ n|n–1 = yn, the EKF updates the old estimate of the weight vector
by operating on the current desired response dn. The filtered estimate of the weight
vector, namely ŵn|n, is thus computed in accordance with (4.67). The ŵn|n so computed is supplied by the EKF to the RMLP via a bank of unit-time delays.
With the transition matrix being equal to the identity matrix, as evidenced by (4.64),
we may set ŵn+1|n equal to ŵn|n for the next iteration. This equality permits the supervised training cycle to be repeated, until the training process is terminated.
Note that, in the supervised training framework of Figure 4.10, the training sample T
is split between the RMLP and EKF: the input vector un is applied to the RMLP as the
Predicted desired response, dˆ n|n−1 = yn
Figure 4.10. Closed-loop feedback system, embodying the RMLP and the EKF. (a) The RMLP,
ˆ n|n −1 , operates on the input vector un to produce the output vector yn.
(b) The EKF, with the prediction dn|n–1 = yn, operates on the desired response dn to produce the
ˆ n+1|n , preparing the feedback system for the next iteration.
Bayesian filtering for state estimation of the environment
excitation, and the desired response dn is applied to the EKF as the “observable of the
The predictor–corrector property is an intrinsic property of the Kalman filter, its
variants, and extensions. In light of this property, examination of the block diagram of
Figure 4.10 leads us to make the following insightful statement:
The recurrent neural network, undergoing training, performs the role of the predictor; and
the EKF, providing the supervision, performs the role of the corrector.
Thus, whereas in traditional applications of the Kalman filter for sequential state estimation the roles of predictor and corrector are embodied in the Kalman filter itself, in
supervised-training applications these two roles are split between the recurrent neural
network and the EKF. Such a split of responsibilities in supervised learning is in perfect
accord with the way in which the input and desired response elements of the training
For us to be able to apply the EKF algorithm as the facilitator of the supervised learning
task, we have to linearize the measurement equation (4.65) by retaining first-order terms
in the Taylor-series expansion of the nonlinear part of the equation. With b(wn, vn, un)
as the only source of nonlinearity, we may approximate (4.65) as
where Bn is the p-by-W measurement matrix of the linearized model, with p denoting
the dimensionality of the nonlinear vectorial function b(.,.,.); that is, the number of
output nodes in the network. The linearization process involves computing the partial derivatives of the p outputs of the RMLP with respect to its W weights, obtaining
where the vector vn in b(w, vn, un) is maintained constant at some value; reference to
the time index n has been omitted to simplify the presentation. The bi in (4.70) denotes
the ith element of the vectorial function b(wn, vn, un). The partial derivatives on the
right-hand side of the equation are evaluated at wn = ŵn|n–1, where ŵn|n–1 is the prediction of the weight vector wn at time n given the desired response up to and including
4.10 Recurrent multilayer perceptrons: an application for state estimation
The state-evolution equation (4.64) is linear to begin with; therefore, it is unaffected by the linearization of the measurement equation. Thus the linearized statespace model of the recurrent network permits the application of the EKF summarized
Computational requirements of the EKF are dominated by the need to store and update
the filter error covariance matrix Pn|n at each time step n. For a recurrent neural network
containing p output nodes and W weights, the computational complexity of the EKF is
highly demanding. In such situations, we may look to the decoupled extended Kalman
filter (DEKF) as a practical remedy for proper management of computational resources
The basic idea behind the DEKF is to ignore the interactions between the estimates
of certain weights in the recurrent neural network. In so doing, a controllable number
of zeros is introduced into the covariance matrix Pn|n. More specifically, if the weights
in the network are decoupled in such a way that we create mutually exclusive weight
groups, then the covariance matrix Pn|n is structured into a block-diagonal form, as illustrated in Figure 4.11.
Let g denote the designated number of disjoint weight groups created in the manner
just described. For i = 1, 2, …, g, correspondingly let w
for group i, Pn|n be the subset of the filtered error covariance matrix for group i, G (ni ) be
the Kalman gain matrix for group i, and so on for the other entries in the DEKF. Let Wi
Figure 4.11. Block-diagonal representation of the filtered error covariance matrix Pn(|in) pertaining
to the DEKF. The shaded parts of the square represent nonzero values of Pn(|in), where i = 1, 2, 3, 4
for the example illustrated in the figure. As we make the number of disjoint weight groups
g larger, more zeros are created in the covariance matrix Pn|n; in other words, the matrix
Pn|n becomes more sparse; the computational burden is therefore reduced, but the numerical
accuracy of the state estimation becomes degraded.
Bayesian filtering for state estimation of the environment
denote the number of weights in group i. The concatenation of the filtered weight vectors
ˆ (n|n) forms the overall filtered weight vector ŵn|n; similar remarks apply to Pn(|in) and G (ni )
and the other entries in DEKF. In light of these new notations, we may now rewrite the
DEKF algorithm for the ith weight group as follows:
⎢ ∑ B(n j ) Pn| n −1 ( B(n j ) )T + Q(ν,)n ⎥
Initialization of the DEKF algorithm proceeds in the same manner described previously
Computational requirements of the DEKF now assume the following orders:
Thus, depending on the size of g (i.e. the number of disjoint weight groups), the computational requirements of the DEKF can be significantly smaller than those of the EKF.
An attractive feature of using the EKF as the sequential state estimator for the supervised training of a recurrent neural network is that its basic algorithmic structure (and,
therefore, its implementation) is relatively simple, as evidenced by the summary presented in Table 4.1. However, it suffers from two practical limitations:
(1) The EKF requires linearization of the recurrent neural network’s vector-valued
(2) Depending on the size of the weight vector w (i.e. dimensionality of the state space),
we may have to resort to the use of a DEKF to reduce the computational complexity
and storage requirement, thereby sacrificing computational accuracy.
We may bypass both of these limitations by using a derivative-free nonlinear sequential
4.10 Recurrent multilayer perceptrons: an application for state estimation
Supervised training of neural networks using the CKF
In Section 4.7 we discussed the CKF, the formulation of which rests on applying the
cubature rule. Insofar as supervised training of a recurrent neural network is concerned,
the CKF has some unique properties, as summarized here:
(1) Compared with the EKF, the CKF is a more numerically accurate approximator of
the Bayesian filter, in that it completely preserves second-order information about
the state that is contained in the observations.
(2) The CKF is derivative free; hence, there is no need for linearizing the measurement
(3) Last, but by no means least, the cubature rule is used to approximate the time-update
integral that embodies the posterior distribution and all the other integral formulas
involved in the formulation of the Bayesian filter operating in a Gaussian environment; as a practical rule, integration is preferred over differentiation because of its
An interesting property of a recurrent neural network (e.g. RMLP), observed after
the network has been trained in a supervised manner, is the emergence of an adaptive
behavior. This phenomenon occurs despite the fact that the synaptic weights in the network have been fixed. The root of this adaptive behavior may be traced to a fundamental
theorem, which is stated as follows (Lo and Yu, 1995):
Consider a recurrent neural network embedded in a stochastic environment with relatively
small variability in its statistical behavior. Provided that the underlying probability distribution of the environment is fully represented in the supervised-training sample supplied to the
network, it is possible for the network to adapt to the relatively small statistical variations in
the environment without any further on-line adjustments being made to the synaptic weights
This fundamental theorem is valid only for recurrent networks. We say so because the
dynamic state of a recurrent network actually acts as a “short-term memory” that carries
an estimate or statistic of the uncertain environment for adaptation in which the network
This adaptive behavior has been referred to differently in the literature. Lo and
Bassu (2001) refer to it as accommodative learning. In another paper published in
the same year (Younger et al., 2001), it is referred to as meta-learning, meaning
“learning how to learn.” Hereafter, we will refer to this adaptive behavior as metalearning as well.
Regardless of how this adaptive behavior is termed, it is not expected that it will
work as effectively as a truly adaptive neural network, where provision is made for
automatic on-line weight adjustments if the environment exhibits large statistical variability. This observation has been confirmed experimentally by Lo and Bassu (2001),
where comparative performance evaluations were made between a recurrent neural
Bayesian filtering for state estimation of the environment
network with meta-learning and an adaptive neural network with long-term and shortterm memories; the comparative evaluations were performed in the context of system
Nevertheless, the meta-learning capability of recurrent neural networks should be
viewed as a desirable property in control and signal-processing applications, particularly where on-line adjustments of synaptic weights are not practically feasible or they
In this chapter we discussed another important basic issue in the study of cognitive
• state-space modeling of the environment in which the system is operating and
• Bayesian filtering for estimating the hidden state of the system given a set of statedependent measurements (observables) supplied by the environment.
The motivation for studying the Bayesian filter for state estimation is that it is
optimal, at least in a conceptual sense. Unfortunately, except for the special case of
a linear dynamic system with Gaussian noise processes and a couple of other special
cases, the Bayesian filter is not computationally feasible. Under the assumptions
of linearity and Gaussianity, the Bayesian filter reduces to the celebrated Kalman
When the dynamic system is nonlinear and/or non-Gaussian, as in often the case in
practice, we have to be content with an approximation of the Bayesian filter. In this
chapter we focused on nonlinear filtering for sequential state estimation under the
Gaussian assumption. The Gaussian assumption may be justified on the following
(1) From a mathematical perspective, Gaussian processes are simple and mathematically easy to handle.
(2) Noise processes encountered in many real-world problems may be modeled as
Gaussian due to the central limit theorem of probability theory.
Under the Gaussian assumption, we first described the EKF, which is simple to derive
and relatively straightforward to implement. Indeed, it is for these two reasons that the
EKF is widely used even to this day. However, a major limitation of the Kalman filter is
that its application is restricted to nonlinear dynamic systems, where the nonlinearity is
Practical limitations of the EKF can be overcome by using the CKF, the rigorous derivation of which is based on the third-degree cubature rule in mathematics. The distinguishing characteristic of this new nonlinear filter is its even set of weighted cubature
points. It is this unique characteristic that gives the filter the signal-processing power to
solve nonlinear sequential state-estimation problems, where dimensionality of the statespace model is well beyond the reach of the EKF and UKF. Most importantly, the CKF
has many other highly desirable properties, summarized in Section 4.7, which lead us to
conclude that the CKF is a method of choice for highly nonlinear state-estimation problems under the Gaussian assumption; this assertion will be well illustrated in Chapter 6
1. The system equation is also referred to as the process equation in the literature.
In the book entitled Perception as Bayesian Inference, edited by Knill and Richards (1996), strong arguments are presented for the Bayesian framework as a general
formalism for human perception. As stressed in that book, the Bayesian approach
has been successfully applied not only in computer vision (Grenander, 1976–1981;
Geman and Geman, 1984; Yuille and Clark, 1993), but also in the modeling of human
visual perception (Bennett et al., 1989; Kersten, 1990). The first part of the Knill–
Richards book addresses different issues under Bayesian frameworks, followed by
the second part devoted to implications and applications of the Bayesian framework.
Indeed, the Bayesian framework has developed strong roots in human perception
and neural coding, so much so that we are becoming emboldened to speak of “The
Bayesian brain”; see the paper by Knill and Pouget (2004) and the book co-edited
3. For an elegant treatment of probability theory on the basis of set theory, see the book
entitled Introduction to Probability, by Bertsekas and Tsitsiklis (2008).
Calculations based on Bayes’ rule, presented previously as (4.2), are referred to as
“Bayesian.” In actual fact, Bayes provided a continuous version of the rule; see (4.3).
In a historical context, it is also of interest to note that the full generality of (4.3) was
not actually perceived by Bayes; rather, the task of generalization was left to Laplace
It is because of the duality property that the Bayesian paradigm is sometimes referred
to as a principle of duality; see Robert (2007: 8). Robert’s book presents a detailed
and readable treatment of the Bayesian paradigm. For a more advanced treatment of
the subject, see Bernardo and Smith (1998).
Bayesian filtering for state estimation of the environment
In a paper published in 1912, Fisher moved away from the Bayesian approach
(Fisher, 1912). Then, in a classic paper published in 1922, he introduced the notion
In Appendix B of their book, Bernardo and Smith (1998) show that many nonBayesian inference procedures do not lead to identical inferences when applied to
the proportional likelihoods considered under the likelihood principle.
8. For detailed discussion of the sufficient statistic, see Bernardo and Smith (1998:
In another approach to parameter estimation, known as maximum likelihood estimation, the parameter vector p is estimated using the formula
That is, the maximum likelihood (ML) estimate θ
vector p that maximizes the conditional distribution fx|Θ(x|p), reformulated as
l(p|x). The ML estimator differs from the MAP estimator in that it ignores the
prior p (p); it may, therefore, be said that ML lies at the fringe of the Bayesian
Maximizations of the likelihood function may lead to more than one global maximum, with the result that the procedure used to perform the maximization may
diverge. To overcome this difficulty, maximum likelihood has to be stabilized. This
could be done by incorporating prior information on the parameter space, exemplified by the distribution p (p), into the solution, which brings us back to the Bayesian
In formulating the state-space model described in (4.12) and (4.13), we have started
to depart from the terminology in previous chapters by using a subscript for discrete
time. This has been done merely to simplify mathematical formalism of the state-space
To describe the central limit theorem in probability theory, let the set { i }iN=1
denote a sequence of independently and identically distributed
d (iid) random variables with common mean m and common variance s 2. Define the normalized
The stage is now set for stating the central limit theorem in terms of YN as
The probability density function of the normalized variable YN converges to that of the
standard Gaussian variable with zero mean and unit variance; that is,
in the sense that, for every y, we have the limiting probabilistic condition
In words, the central limit theorem asserts that as the number of iid variables Xi,
N, approaches infinity, the distribution of the normalized random variable YN assumes the standard Gaussian distribution.
The formulation of particle filters is rooted in Monte Carlo simulations. As such,
• simple to code and, therefore, enjoy a broad range of applications;
• computationally demanding and, therefore, unsuitable for large-scale, on-line
For a book on Monte Carlo statistical methods, see Robert and Casella (2004). For
books on particle filters, see Cappé et al. (2005), and Ristic et al. (2004).
As mentioned in the text, the term “monomials” is used to refer to a product of
powers of variables. Consider, for example, a single variable denoted by x; for
this simple example, the monomial is l or xd, where d is a positive integer. Suppose next that there are several variables, say x1, x2, and x3. In this more general
example, each variable is exponentiated individually, such that the monomial is of
the product form x1 1 x2 2 x3 3 , where the individual powers d1, d2, and d3 are all nonnegative integers.
The third-degree spheroidal–radial cubature rule was chosen in Arasaratnam and
Haykin (2009) for approximation of the Bayesian filter for a number of practical
(1) This rule entails the use of an even set of weights in the approximation procedure, all of which are positive. On the other hand, if we were to adopt fifth-degree
or any odd higher degree cubature rule, we would find that some of the weights
in the approximation assume negative values. Unfortunately, the appearance of
negative weights in the approximation may result in numerical instability, which
(2) It may well be that with negative weights resulting from using the fifth-order
or higher order cubature rule, it is difficult, if not impossible, to formulate a
square-root solution for the CKF that would solve the numerical instability
Bayesian filtering for state estimation of the environment
Figure 4.12. First two-order statistics of a nonlinearly transformed Gaussian random variable
computed by the third- and fifth-degree cubature rules.
(3) Last, but by no means least, experimental results comparing the third-degree
with the fifth-degree cubature rule, presented in Figure 4.12, appear to show
that estimation accuracy of the CKF based on the fifth-degree rule is marginally
improved over the third-degree rule; moreover, the marginal improvement in
accuracy was attained at the expense of significantly increased cost in computational complexity.
The previous two chapters of the book have focused on the basic issue: perception
of the environment from two different viewpoints. Specifically, Chapter 3 focused
on perception viewed as power-spectrum estimation, which is applicable to applications such as cognitive radio, where spectrum sensing of the radio environment is
of paramount importance. In contrast, Chapter 4 focused on Bayesian filtering for
state estimation, which is applicable to another class of applications exemplified
by cognitive radar, where estimating the parameters of a target in a radar environment is the issue of interest. Although those two chapters address entirely different
topics, they also have a common perspective: they both pertain to the perceptor of a
cognitive dynamic system, the basic function of which is to account for perception
in the perception–action cycle. In this chapter, we move on to the action part of this
cycle, which is naturally the responsibility of the actuator of the cognitive dynamic
Just as the Bayesian framework provides the general formalism for the perceptor to
perceive the environment by estimating the hidden state of the environment, Bellman’s
dynamic programming provides the general formalism for the actuator to act in the
environment through control or decision-making.
Dynamic programmingg is a technique that deals with situations where decisions
are made in stages (i.e. different time steps), with the outcome of each decision being
predictable to some extent before the next decision is made. A key aspect of such situations is that decisions cannot be made in isolation. Rather, the desire for a low cost at
the present is balanced against the undesirability of a high cost in the future. This is a
credit-assignment problem, because credit or blame must be assigned to each one in
a set of decisions. For optimal planning, therefore, it is necessary to have an efficient
tradeoff between immediate and future costs. Such a tradeoff is indeed captured by the
formalism of dynamic programming. In particular, dynamic programming addresses the
How can a decision-maker learn to improve its long-term performance when the attainment of this
improvement may require having to sacrifice short-term performance?
Bellman’s dynamic programming provides the mathematical basis for an optimal solution to this fundamental problem in an elegant and principled manner.
Dynamic programming for action in the environment
In the art of mathematical-model building, the challenge is to strike the right balance between two entities, one practical and the other theoretical. The two entities are
• the realistic description of a given problem and
• the power of analytic and computational methods to apply to the problem.
In this context, an issue of particular concern in dynamic programming is that of a
decision-makerr having to operate in a stochastic environment. To address this issue, we
build our modell around Markov decision processes. Given the initial state of a dynamic
system, a Markov decision process provides the mathematical basis for choosing a
sequence of decisions that will maximize the returns from a multistage decision-making
process. What we have just described here is the essence of Bellman’s dynamic programming. Therefore, it is befitting that we begin the study of dynamic programming
with a discussion of this new mathematical tool.
As already mentioned, it is the actuator of a cognitive dynamic system that is responsible for decision-making. Henceforth, we simplify matters by using the two terms
decision-maker and dynamic system interchangeably.
Consider, then, a decision-maker that interacts with its environment in the manner
illustrated in Figure 5.1. The decision-maker operates in accordance with a finitediscrete-time Markovian decision process, characterized as follows:
• The environment evolves probabilistically, occupying a finite set of discrete states.
However, the state does nott contain past statistics, even though those statistics could
• For each environmental state there is a finite set of possible actions that may be taken
• Every time the decision-maker acts in the environment, a certain costt is incurred.
• States are observed, actions are taken, and costs are all incurred at discrete times.
Figure 5.1. Block diagram of a decision-maker interacting with its environment.
So, with the notion of state being of profound importance in dynamic programming, we
The state of the environment is a summary of the entire past experience gained by a decisionmaker from its interactions with the environment, such that the information necessary for the
decision-maker to predict future behavior of the environment is contained in that summary.
The state at time step n is denoted by the random variable Xn and the actual state at time
step n is denoted by in; in other words, in is a sample value of Xn. The finite set of possible
states is denoted by H . A surprising aspect of dynamic programming is that its applicability
depends very little on the nature of the state. Unlike nonlinear filtering for state estimation
discussed in Chapter 4, we may, therefore, proceed without any assumption on the structure
of the state space. (This statement does not apply to Section 5.5.) Note that the complexity
of a dynamic-programming algorithm is quadratic in the dimension of the state space and
linearr in the dimension of the action space. Note also that we have introduced the notion
of action space. Therefore, it is the dimensionality of the state space, measurement space,
action space, or all three of them that is responsible for the curse-of-dimensionality problem
in dynamic programming, an issue that was discussed previously in Chapter 4.
For state i, for example, the available set of actions in the environment is denoted by
Ai = {aik}, where the second subscript k in the action aikk taken by the decision-maker
merely indicates the availability of more than one possible action when the environment
is in state i. Transition of the environment from state i to a new state j, due to action aik,
say, is probabilistic in nature. Most importantly, however, the transition probability from
state i to state j depends entirely on the current state i and the corresponding action
aik. This is a restatement of the Markov property that was discussed in Chapter 4. This
property is crucial, because it means that the current state of the environment provides
the necessary information for action taken by the decision-maker.
The random variable denoting the action at time step n is denoted by An. Note that
the symbol A representing a random variable for action at some unspecified time step
is different from the symbol A representing a set of actions for an unspecified state. Let
pij (a) denote the transition probability from state i to state j due to the action taken at
time step n, where An = a. By virtue of the Markovian assumption on state dynamics, we
The transition probability pij (a) satisfies two conditions that are imposed on it by probability theory:
where both i and j reside in the state space.
For a given number of states and given transition probabilities, the sequence of environmental states resulting from the actions taken by the decision-maker over time forms
a Markov chain, which was also discussed in Chapter 4.
Dynamic programming for action in the environment
At each transition from one state to another, the decision-maker naturally incurs a
cost. Thus, at the nth transition from state i to state j under action aik, the cost incurred
is denoted by g ng(i, aik, j ), where g(.,.,.) is an observed transition costt and g is a scalar
called the discount factorr that is confined to the range 0 ≤ g <1. The discount factor
reflects intertemporal preferences. By adjusting g , we are able to control the extent
to which the decision-maker is concerned with long-term versus short-term consequences of its own actions. In the limit, when g = 0, the decision-maker is said to be
myopic, which means that the decision-maker is concerned only with immediate consequences of its actions.
Our interest in dynamic programming is to formulate a policy, defined as the mapping
Policy is a rule used by the decision-maker to decide what to do, given knowledge of the current
state of the environment. The policy is denoted by
where mn is a function that maps the state Xn = i into an action An = a at stage n = 0, 1, 2, . . . . This
where Ai denotes the set of all possible actions available to the decision-maker in state i. Such
A policy can be nonstationary or stationary. A nonstationary policy is naturally time
varying, as indicated in (5.4). When, however, the policy is independent of time, as
the policy is said to be stationary. In other words, a stationary policy specifies exactly
the same action each time a particular state is visited. For a stationary policy, the underlying Markov chain may be stationary or nonstationary; it is possible to use a stationary
policy on a nonstationary Markov chain, but this is nott a wise thing to do. If a stationary
policy m is employed, then the sequence of states {Xn, n = 0, 1, 2, . . .} forms a Markov
chain with transition probabilities pij (m
( (i)), where m (i) signifies an action on state i
under policy m. It is indeed for this reason that the process is referred to as a Markov
A dynamic-programming problem can be of a finite-horizon or infinite-horizon kind.
In a finite-horizon problem, the cost accumulates over a finite number of stages. In an
infinite-horizon problem, on the other hand, the cost accumulates over an infinite number
of stages. Infinite-horizon problems are of particular interest, because discounting
ensures that the costs for all states are finite for any policy.
(Xn), Xn+1) denote the observed transition cost incurred as a result of transition from state Xn to state Xn+1 under the action of policy mn(X
cost in an infinite-horizon problem, starting from an initial state X0 = i and using a policy
j π (i ) = E ⎢ ∑ γ n g ( X n μn ( X n ) X n +1 ) | X 0
where the expected value (signified by the operator E) is taken over the Markov chain
X1, X2, . . .} and g is the discount factor. The function Jp (i) is called the cost-to-go
p, starting from state i. Its optimall value, denoted by J*(
The policy p is optimall if, and only if, it is greedy with respect to J*(
“greedy” is used here to describe the case when the decision-maker seeks to minimize
the immediate next cost without paying any attention to the possibility that such an
action may do away with access to better alternatives in the future. When the policy p is
stationary – that is, p = {m, m, . . .} – we use the notation J m(i) in place of J*(
We may now sum up the basic problem in dynamic programming for a stationary policy
Given a stationary Markov decision process describing the interaction between a decision-maker
and its environment, find a stationary policy p = {m, m, . . .} that minimizes the cost-to-go function
The dynamic-programming technique rests on a very simple idea known as the principle
of optimality, due to Bellman (1957). Simply stated, this principle says the following
An optimal policy has the property that whatever the initial state and initial decision are, the
remaining decisions must constitute an optimal policy starting from the state that results from the
For example, a “decision” used in this statement may be a choice of control applied
in the environment at a particular time, in which case a “policy” is the entire control
To formulate the principle of optimality in mathematical terms, consider a finitehorizon problem for which the cost-to-go function is defined by
J 0 ( X 0 ) = E ⎢ g L ( X L ) + ∑ gn ( X n , μn ( X n ), X n +1 ) ⎥ ,
Dynamic programming for action in the environment
where L is the planning horizon or horizon depth and gL(X
X0, the expectation in (5.8) is over the remaining states X1, . . ., XL. With this terminology,
we may now formally state the principle of optimality as follows (Bertsekas, 2005,
p* ={μ0* , μ1* , , μ *L−1} be an optimal policy for the basic finite-horizon problem. Assume that,
when using the optimal policy p *, a given state Xn occurs with positive probability. Consider the
subproblem where the environment is in state Xn at stage n and suppose we wish to minimize the
J n ( X n ) = E ⎢ g L ( X L ) + ∑ gk ( X k , μk ( X k ), X k +1 ) ⎥ ,
We may intuitively justify the principle of optimality by the following argument: if the
truncated policy {μn* , μn*+1 , , μ *L −1} was not optimal as stated, then, once the state Xn is
reached at stage n, we could reduce the cost-to-function Jn(X
policy that is optimal for the subproblem.
The principle of optimality builds on the engineering notion of “divide and conquer.”
Basically, an optimal policy for a complex multistage planning or control problem is
constructed by proceeding backward, stage by stage, as follows:
(1) Construct an optimal policy for the “tail subproblem” involving only the last stage of the
(2) Extend the optimal policy to the “tail subproblem” involving the last two stages of the system.
(3) Continue the procedure in this fashion, stage by stage, until the entire problem has been dealt
On the basis of the procedure just described, we may now formulate the dynamicprogramming algorithm, which runs backward in time, stage by stage. Let
(Xn+1) denote the cost-to-go function at stage n + 1 of the interaction between
the decision-maker and its environment. Then, running backward in time by one
stage, there will be the observed transition cost gn(X
result of the transition from state Xn to state Xn+1 under policy m n(X
(Xn+1) augmented by the observed transition cost,
Treating the state Xn+1 as a random variable, which it is, we therefore need to take the
expected value of the augmented cost in (5.10) at stage n with respect to Xn+1 and thus
E [ gn ( X n , μn ( X n ), X n +1 ) + J n +1 ( X n +1 )].
At this point, we go on to address the issue of finding the policy mn for which the expected
[ gn ( X n , μn ( X n ), X n ) J n +1 ( X n +1 )].
Generalizing this formula, we may now formally state the dynamic-programming
algorithm as follows (Bertsekas, 2005, 2007):
For every initial state X0, the optimal cost J*(X
(X0) of the finite-horizon problem is equal to J0(X
where the function J0 is obtained from the last time step of the dynamic-programming algorithm
i E [ gn ( X n , μn ( X n ), X n ) J n+1 ( X n+1 )],
which runs backwards in time for horizon depth L, with
Furthermore, if μn* minimizes the right-hand side of (5.13) for each state Xn and time step n, then
In its basic form, the dynamic-programming algorithm deals with a finite-horizon
problem. We are interested in extending the use of this algorithm to deal with the
infinite-horizon discounted problem described by the cost-to-go function of (5.5)
under a stationary policy p = {m, m, m, . . .}. With this objective in mind, we do two
• First, reverse the time index of the algorithm.
• Second, define the observed transition cost gn(X
where, in the right-hand side, we have introduced the discount factor g to account for
(Xn), Xn+1) and deleted the subscript n in gn.
We may then reformulate the dynamic-programming algorithm as
which starts from the initial condition J0(X
( ) = 0 for all X. The state X0 is the initial state;
X1 is the new state that results from the action of policy m on X0.
J i) denote the optimal infinite-horizon cost for the initial state X0 = i. We may
J i) as the limit for the corresponding L-stage optimal cost JL(i) as the
horizon depth L approaches infinity; that is,
Dynamic programming for action in the environment
This relation is the connecting link between the finite-horizon and infinite-horizon discounted problems. Substituting n + 1 = L and X0 = i into (5.16) and then applying (5.17),
we may go on to express the optimal infinite-horizon cost as
which is the expectation form of Bellman’s equation for a Markov decision process. We
may rewrite (5.18) by proceeding in two stages:
(1) Evaluate the expectation of the cost g(i, m (i), X1) with respect to X1 by writing
where N is the number of possible states occupied by the environment and pijj is the
transition probability from the initial state X0 = i to the new state X1 = j. The quantity
defined in (5.19) is the expected observed costt incurred at state i by following the
action recommended by the policy m. Denoting this cost by c(i, m (i)), we write
(X1) with respect to X1. Here, we note that if we know
(X1) for each state X1 of the finite-state system, then we may
readily determine the expectation of J*(X
(X1) in terms of the transition probabilities of
Thus, using (5.19)–(5.21) in (5.18), we obtain the desired formula:
Equation (5.22) is the standard form of Bellman’s optimality equation for a Markov
decision process. This equation should nott be viewed as an algorithm. Rather, it represents a system of N equations with one equation per state.1 The solution of this system
of equations defines the value of the optimal cost-to-go function for the N states of the
There are two methods for computing an optimal policy, namely policy iteration and
value iteration; these two methods are described in the next two sections.
To set the stage for a description of the policy iteration algorithm, we begin by introducing a new concept called the Q-factor. Consider an existing stationary policy m for
which the cost-to-go function J m(i) is known for all states i. For each state i ∈ H and
action a ∈ Ai, where Ai is the set of all possible actions in state i, the Q-factor is defined
as the expected observed cost plus the sum of the discounted costs of all successor states
where a = m(i) and the term c(i, a) is simply a rewrite of c(i, m(i)) in (5.20). In effect, the
Q-factor Q m(i, a) captures the cost of being in state i and taking action a under policy m.
Note also that the Q-factors denoted by Q m(i, a) for i = 1, 2, . . ., N contain more information than the cost-to-go function J m(i). For example, actions may be ranked on the
basis of Q-factors alone, whereas ranking on the basis of cost-to-go function requires
knowledge of the state-transition probabilities and costs. Note also that the optimal costto-go function J*(i) in (5.22) is obtained simply as min Q μ (i, a ) .
To develop insight into the meaning of the Q-factor, visualize a new system whose
states are made up of the original states i = l, 2, . . ., N and all the possible state–action
pairs (i, a), as portrayed in Figure 5.2. There are two distinct possibilities that can occur
(1) The system is in state (i, a), in which case no action is taken. Transition is made
automatically to state j, say, with probability pij(a) and cost g(i, a, j) is incurred.
(2) The system is in state i, say, in which case action a ∈ Ai is taken; deterministically,
In light of what was said previously in Section 5.1, the policy m is greedy with respect
to the cost-to-go function Jm(i) if, for all the states, m(i) is an action that satisfies the
Two noteworthy observations follow from (5.24):
• First, it is possible for more than one action to minimize the set of Q-factors for some
state, in which case there can be more than one greedy policy with respect to the
• Second, a policy can be greedy with respect to many different cost-to-go functions.
Figure 5.2. Illustration of two possible transitions: the transition from state (i, a) to state j is
probabilistic, but the transition from state i to (i, a) is deterministic.
Dynamic programming for action in the environment
Moreover, the following fact is basic to all dynamic-programming methods under a
Formulation of the policy iteration algorithm
With the notions of Q-factors and greedy policy at our disposal, we are now ready to
describe the policy iteration algorithm. Specifically, the algorithm operates by alternating between two steps2:
(1) Policy evaluation step. In this first step, the cost-to-go function Jm for some current
policy m and corresponding Q-factor Q m(i, a) are computed for all the states and
(2) Policy improvement step. In this second step, the current policy is updated in order
to be greedy with respect to the cost-to-go function computed in step 1.
These two steps are illustrated in the block diagram of Figure 5.3. To be specific, we start
with some initial policy m0 and then generate a sequence of new policies m1, m2, . . . . Given
the current policy mn, we perform the policy evaluation step by computing the cost-to-go
function J μ (i ) as the solution of the linear system of equations (5.22) reproduced here
where J μ ( ), J μn ( 2 ),…, J μ ( N ) are the unknowns and the transition probability pij is
mn(i)) for the purpose of generality. Using these results,
Figure 5.3. Block diagram of the policy iteration algorithm.
Table 5.1. Summary of the policy iteration algorithm
1. Start with an arbitrary initial policy m0.
2. For n = 0, 1, 2, . . ., compute the cost-to-go function J μ (i ) and Q-factor Q μ (i, a ) for all
4. Repeat steps 2 and 3 until mn+1 is not an improvement on mn, at which point the algorithm
terminates with mn as the desired policy.
we then follow (5.23) to compute the Q-factor for each state–action pair (i, a) as
Next, following (5.24), we perform the policy improvement step by computing a new
The two-step process just described is repeated with policy mn+1 used in place of mn in
(5.27), until we arrive at the final condition
at which point in time the algorithm is terminated with mn as the desired policy. With
J μn , we may then say that the policy iteration algorithm will terminate after a
finite number of iterations because the underlying Markov decision process has a finite
number of states. Table 5.1 presents a summary of the policy iteration algorithm, based
In the policy iteration algorithm, the cost-to-go function has to be recomputed entirely at
each iteration of the algorithm, which can be a computationally expensive proposition,
particularly when the number of states N is large. Even though the cost-to-go function
for the new policy may be similar to that for the old policy, there is, unfortunately, no
dramatic shortcut for this computation. There is, however, another method for finding
the optimal policy that avoids the burdensome task of repeatedly computing the cost-togo function. This alternative method, based on successive approximations, is known as
Dynamic programming for action in the environment
The value iteration algorithm involves solving Bellman’s optimality equation, given
in (5.22), for each of a sequence of finite-horizon problems. In the limit, the cost-togo function of the finite-horizon problem converges uniformly over all states to the
corresponding cost-to-go function of the infinite-horizon problem as the number of
iterations of the algorithm approaches infinity (Ross, 1983; Bertsekas, 2007). Simply
put, the value iteration algorithm is perhaps the most widely used algorithm in dynamic
• First, it is relatively simple to implement.
• Second, it provides a natural way to solve dynamic-programming problems.
Formulation of the value iteration algorithm
Let Jn(i) denote the cost-to-go function for state i at iteration (time step) n of the
value iteration algorithm. The algorithm begins with an arbitrary guess J0(i) for
N. If some estimate of the optimal cost-to-go function J*(
it should be used as the initial value J0(i). Once J0(i) has been chosen, we may use
(5.22) as a basis to compute the sequence of cost-to-go functions J1(i), J2(i), . . .,
where, here again, the transition probability is expressed as pij (a) for the purpose of
generality. Application of the update to the cost-to-go function described in (5.29) for
state i is referred to as the backing up of i’s cost. This backup is a direct implementation
of Bellman’s optimality equation. It is important to note that the values of the cost-to-go
functions in (5.29) for states i = 1, 2, . . ., N are backed up simultaneouslyy on each iteration
of the algorithm. This method of implementation represents the traditional synchronous
form of the value iteration algorithm. Thus, starting from arbitrary initial values J0(1),
(N), the algorithm described in (5.29) converges to the corresponding optimal
J (N) as the number of iterations n approaches infinity. So, ideally, the value iteration algorithm in (5.29) requires an infinite number of iterations.
Unlike the policy iteration algorithm, an optimal policy is not computed directly in
the value iteration algorithm. Rather, the optimal values J*(1),
computed using (5.29). Then, a greedy policy with respect to that optimal set is obtained
A summary of the value iteration algorithm, based on (5.29)–(5.31), is presented in
Table 5.2. This summary includes a stopping criterion for (5.29).
Approximate dynamic programming for problems with imperfect state information
Table 5.2. Summary of the value iteration algorithm
1. Start with an arbitrary initial value J0(i) for state i = 1, 2, . . ., N.
2. For n = 0, 1, 2, . . ., compute the cost-to-go function
Continue this computation until we arrive at the condition
where e is a prescribed small positive number.
Hence, determine the optimal policy as a greedy policy for J*(
Approximate dynamic programming for problems with imperfect
Thus far, up to this point in the chapter, the discussion presented on dynamic programming rests its applicability on the premise that a cognitive dynamic system satisfies two
(1) Assumption 1. The state of the environment has a discrete value.
(2) Assumption 2. The actuator of the system has access to the exact value of the state at
In certain examples of cognitive dynamic systems, exemplified by cognitive radar, both
assumptions are unfortunately violated. In a radar system, regardless of its kind, the
• the system equation, describing evolution of the state across time, is continuous in
• the measurement equation is discrete in time.
Clearly, then, Assumption 1 is violated, which means that the use of dynamic programming may suffer from the curse of dimensionality. Moreover, Assumption 2 is also violated because the actual state of the radar environment is hidden from the transmitter
(actuator). In fact, the only source of information about the state available to the transmitter is contained in a set of observables naturally corrupted by some kind of noise in
accordance with the measurement equation of the state-space model. In this section, we
address the important issue of how to get around Assumption 2. The curse-of-dimensionality problem attributed to Assumption 1 is deferred to Chapter 6 on cognitive radar.
Dynamic programming for action in the environment
The important point to note here is that we now have a new issue to deal
with, namely imperfect state information about the environment. Compared with
“idealized” problems with perfect state information, solutions to problems with
imperfect state information are computationally more demanding and suboptimal in
performance. It is in light of this latter point that dynamic-programming algorithms
intended for this new class of problems are naturally “approximate”; hence the title
Notwithstanding the comments just made, problems with imperfect state information
are no different in conceptual terms from their perfect state information counterparts
(Bertsekas, 2005). Specifically, through appropriate reformulations, problems with
imperfect state information can be reduced to corresponding problems with perfect state
Basics of problems with imperfect state information
Following the material presented in Chapter 4, the discrete state-space model for problems with imperfect state information is described, in its most generic form, as follows:
where un is the control applied to the environment at time n and vn is the system
noise. (In the case of a hybrid model, we are assuming that the continuous system
where yn is the observable at time n and mn is the measurement noise.
Let In denote the information vectorr available to the transmitter at time n, which is
In = [y0, y1, . . ., yn; u0, u1, . . ., un−1],
where n = 1, . . ., L − 1, with L denoting duration of the window of observables being
considered at time n. Note also that the initial value
Considering a controll application, let un be the function that maps In into the control un
that lies in the control space Un, as shown by
Such functions are said to constitute an admissible policy, for which, following (5.4),
Approximate dynamic programming for problems with imperfect state information
Now, casting our minds back to (5.8) and adapting that equation for dealing with stochastic imperfect state-information problems as described herein, we would like to find
an admissible policy p that minimizes the cost-to-go function
subject to the system equation written in an explicit form involving the information
vector as well as the policy, as follows:
the measurement equation is likewise written in the corresponding form:
At this point in the discussion, it is important to note the way in which this
mathematical statement differs from the case of perfect state information (Bertsekas,
In a perfect state-information problem, the aim is to find a policy that specifies the control un
to be applied to the environment for each state xn at time n. On the other hand, in an imperfect
state-information problem, the aim is to find a policy that specifies the control to be applied to
the environment for every possible information vector In, which corresponds to every sequence of
observables received and controls employed up to and including time n for a total off L iterations.
Indeed, it is this statement that distinguishes imperfect state-information problems from
their perfect state-information counterparts.
Reformulation of the imperfect state-information problem as a perfect
From the definition of the information vector In in (5.32), we readily infer the following
where, as before, I0 = y0. According to (5.39), the information vector In is viewed as
the state pertaining to a “reformulated system” with perfect information about the
environment; un is the control applied to the environment at time n and the observable yn+1 acts merely as a “disturbance” in evolution of the new state In across
The conditional probability P[yn+1 | In, un] may be expressed in the expanded form
P[yn+1 | In, un] = P[yn+1 | In, un, y0, y1, . . ., yn],
Dynamic programming for action in the environment
the validity of which is justified by the following fact: the sequence { k }kn =0 is, by definition, part and parcel of the information vector In. In other words, the probability distribution of the observable yn+1, viewed as a “disturbance,” is explicitly dependent on
the new state In and the control un of the reformulated system, but not dependent on the
prior sequence of disturbances { k }kn =0 .
E ⎡ E [ gn ( x n , u n , ω n ) | I n , u n ]⎤
we may proceed in a manner similar to that just described and formulate the cost-to-go
function for the new dynamic system described in (5.36)–(5.38). Specifically, the cost
per stage of the system expressed as a function of the new state In and control un is given
Accordingly, building on the new system equation (5.39) and the admissible policy
of (5.34), we may now go on to write the following pair of equations for a problem with
imperfect state information reformulated as a problem with perfect state information
i ⎨ E [[gg L (a L −1 ( x L 1 u LL−1 , ω L 1 ))] +
These two equations sum up the composition of a dynamic-programming algorithm for
tackling problems with imperfect state information. To develop the optimal admissible
policy, {μ0* , μ1* , , μ *L −1} for such a problem, we proceed as follows:
(1) Minimize the right-hand side of the terminal point in (5.43) for every possible value
of the new state-information IL−1, obtaining the optimal policy μ *L 1 ( L−
(2) Simultaneously, compute the cost-to-go function JL−1(IL−1) and use it to compute its
next value JL−2(IL−2), going backwards in time, via the minimizing equation (5.44);
this computation is carried out for every possible value of IL−2.
(3) Similarly, proceed to compute JL−3(IL−3) and μ *L −3 and so on until the final value
Finally, the optimal cost-to-go function is given by the expectation
In Chapter 6, on cognitive radar, we will illustrate how the dynamic-programming
algorithm described in this section can be used to solve an imperfect state-information
Reinforcement learning viewed as approximate dynamic programming
There is yet another practical issue with Bellman’s dynamic programming that needs
attention: its development assumes the availability of an explicit model that encompasses the transition probability from one state to another. Unfortunately, in many situations encountered in practice, such a model is not available. But, provided that a
dynamic system is well structured and its state space has a manageable size, we may
use Monte Carlo simulation to explicitly estimate the transition probabilities and corresponding observed transition costs; by its very nature, the estimation so performed
is approximate. Accordingly, we say this kind of an approach to approximate dynamic
g is direct, because the use of simulations as described herein facilitates the
“direct” application of dynamic programming methods.
Basically, the rationale behind the direct approximation of dynamic programming is
to use simulations to generate multiple system trajectories, which lead to the construction of a look-up table with a separate entry for the value of each state. Naturally, the
larger we make the number of system trajectories and, therefore, the size of the lookup table, the more reliable the simulation results will be at the expense of increased
computational complexity. In particular, a separate variable J(
time the state i is visited by a trajectory of the simulated system. In so doing, we will
have simulated a dynamic system with probabilistic transitions from state i to state j and
generated the corresponding observed transition cost g(i, j). Note, however, that it is
quite likely in the course of simulation that some of the states are never visited; hence the
reason for saying that the dynamic programming so performed is indeed approximate.
The stage is thus set for direct approximation of the two basic dynamic programming
• policy iteration, for which we obtain Q-learning, and
• value iteration, for which we obtain temporal-difference (TD) learning.
These two algorithms, discussed in the next two sections, are well known in the reinforcement learning literature for solving both finite-horizon and infinite-horizon problems. In other words, we may view reinforcement learning as direct approximation of
g through the use of Monte Carlo simulations.
One final comment is in order. Naturally, the construction of a look-up table is memory
limited. It follows, therefore, that the practical use of both algorithms, Q-learning and
TD learning, is limited to situations where the state space is of moderate dimensionality.
To motivate the discussion of Q-learning, consider again Figure 5.1, which may also
be viewed as the depiction of a reinforcement-learning system. The behavioral task of
this system is how to find an optimal (i.e. minimal-cost) policy after trying out various
possible sequences of actions, observing the state transitions that occur, and, finally, the
corresponding observed transition costs. The policy used to generate such a behavior is
Dynamic programming for action in the environment
called the behavior policy. This policy is to be distinguished from the target policy, the
purpose of which is to estimate the value of a policy. With these two policies being separated from each other, Q-learning is said to be an off-policy for control. A side benefit
gained from this separation may be stated as follows:
The target policy can be greedy, while the behavior policy is left to sample all the possible actions.
Thus, an off-policy method, exemplified by Q-learning, differs from an on-policy
method, in that the value of a policy is being estimated while it is used for control at the
To proceed with a derivation of the Q-learning algorithm, let
denote a four-tuple sample, consisting of a trial action an performed on state in that
results in a transition to the new state jn = in+1 at an observed transition cost defined by
where n denotes discrete time. Given such a scenario, we may now raise the following
Is there any on-line procedure for learning an optimal control policy through experiential interaction of a decision-maker with its environment, which is gained solely on the basis of observing
samples of the form sn defined in (5.46) and (5.47)?
The answer to this question is an emphatic yes, and the solution is to be found in
Q-learning (Watkins, 1989; Watkins and Dayan, 1992).
The Q-learning algorithm is an incremental dynamic-programming procedure that
determines the optimal policy in a step-by-step manner. It is highly suited for solving
Markov decision problems without explicit knowledge of the transition probabilities.
However, successful use of Q-learning hinges on the assumption that the state of the
environment is fully observable, which, in turn, means that the environment is a fully
As the name would imply, the Q-learning algorithm involves using the Q-factor
defined in (5.23). Moreover, its derivation follows from (5.23) in light of Bellman’s
optimality equation (5.22). By so doing and using the expected observed cost c(i, a)
defined in (5.20) with m (i) = ai, we may go on to write
which can be viewed as a new version of Bellman’s optimality equation. The solutions to
the linear system of equations in (5.48) define the optimal Q-factors, Q*(i, a), uniquely
To simplify computational matters, we may use the idea of value iteration described
in Section 5.4 in terms of the Q-factors to solve the linear system of equations in (5.48).
Thus, for one iteration of the algorithm, we have
where Q +(i, a) denotes the updated value of Q(i, a). The small step-size version of this
iteration is next formulated as the linear combination of two terms:
)Q(i, a ) + η ∑ pij ( a ) g (i, a, j ) + γ min Q( j, b )⎟
where h is a small learning-rate parameterr that lies in the range 0 < h < 1. In (5.49),
simulations are used to generate sample values of the pair ((j, g(i, a, j)) from the state–
action pair (i, a) in accordance with the transition probability pij(a). In other words, the
use of (5.49) requires the availability of prior knowledge in the form of a look-up table.
Given this provision, we may then refer to (5.49) as the expectation form of Q-learning.
We may eliminate the need for a look-up table by formulating a stochastic version of
(5.49). Specifically, the averaging performed in an iteration of (5.49) over all possible
states is replaced by a single sample equation, resulting in an update formula for the
Qn+1(i, a) = (1 − hn(i, a))Qn(i, a) + hn(i, a)[g(i, a, j) + g Jn(j
The index j in (5.50) and (5.51) is the successor state to the state i and, for the purpose of
generality, hn(i, a) is a learning-rate parameterr introduced at time step (i.e. iteration) n
for the state–action pair (i, a). The update formula of (5.50) applies to the current state–
action pair (in, an), for which j = jn in accordance with (5.46). For all other admissible
state–action pairs, the Q-factors remain unchanged, as shown by
Equations (5.50)–(5.52) constitute one iteration of the Q-learning algorithm.
The Q-learning algorithm may now be viewed in one of two equivalent ways:
• a Robbins–Monro stochastic approximation algorithm4 or
• a combination of value iteration and Monte Carlo simulation.
In any event, the algorithm backs up the Q-factor for a single state–action pair at each
(1) The Q-factor is defined on the basis of state–action pairs. It follows, therefore, that
Q-learning is an off-policy learning method
• First, the method learns about a greedy way of behaving in accordance with
(5.51); this part of the method is called the target policy. Typically, but not always,
the target policy is found to be an approximation to the optimal policy that is
Dynamic programming for action in the environment
• Second, for its learning, the target policy uses data generated through selective actions
in accordance with (5.50); this second part of the method is called the behavior
policy. Typically, the behavior policy is stochastic because, as part of identifying the
optimal policy, it involves having to explore all possible actions for each state.
(2) Despite the appearance of (5.50), the Q-learning algorithm is not a true gradientdescent method; consequently, compared with the least-mean-square (LMS) algorithm well known in adaptive filter theory (Haykin, 2002), it is not as robustt (Baird,
The idea of TD learning was first described by Sutton (1984, 1988), preceding original
formulation of the Q-learning algorithm by Watkins (1989). Just as the Q-factor plays a
dominant role in the Q-learning algorithm, so it is with the notion of TD – to be defined –
that plays a dominant role in formulation of the TD-learning algorithm.
J in+1) denote the cost-to-go function at time steps n and n + 1 respectively. Correspondingly, let the observed cost incurred through transition from time
step n to n + 1 be denoted by g(in, in+1). On this basis, the TD error, denoted by dn, is
For the one-step transition from state in to in+1, (5.53) represents the difference between
estimates of the cost-to-go function at two different times:
J in+1), represents the sample observation of the cost-to-go
function resulting from the move from state in to in+1.
J in), represents the current estimate of the cost-to-go function
In effect, the TD error dn provides the “signal” to determine whether the current estimate
d or decreased. Using the definition of (5.53), the one-step TD
J in) is the current estimate, J +(in) is the updated
hdn is the correction applied to the current estimate
in order to produce the updated one. The one-step update rule of (5.54) is commonly
referred to as the TD(0) algorithm, in which, as before, the abbreviation TD stands for
To develop a physical insight into what goes on in TD(0), let
in light of which we may depict a graphical representation of (5.53) and (5.54) as shown
in Figure 5.4. According to this figure, we are attempting to “back up” the cost-to-go
Figure 5.4. Signal-flow graph representation of the TD(0) algorithm; the figure illustrates the
notion of sample backup, shown by the feedback labeled h from dn to J(
function of being in the state in by an incremental amount equal to the TD error dn
multiplied by the learning-rate parameter h. To be more precise, the current cost-to-go
function for being in the previous state in is adjusted so as to assume a new value closer
to that of the subsequent state i ′n, namely J(
J i′n). This adjustment constitutes a sample
A more elaborate version of the iterative TD algorithm, also originated by Sutton (1988),
incorporates a new parameter, denoted by l, which lies in the range 0 < l ≤ 1. To be
specific, the algorithm is formulated as follows (Bertsekas and Tsitsiklis, 1996):
The expanded iteration algorithm of (5.55) is commonly referred to as TD(λ). With λ
lying in the range 0 < l ≤ 1, it plays the role of an exponential weighting factor that
decreases in time. The summation term on the right-hand side of (5.55) is called the
multistep TD error. In particular, we see that, at time step n, the correction in updating
J in) to the new value J +(in) involves an infinite summation of exponentially weighted values of the TD error dn; hence the reference to (5.55) as the infinitehorizon version of TD learning.
To obtain the corresponding finite-horizon version, we may simply truncate the summation on the right-hand side of (5.55), obtaining
where T is some prescribed time limit that defines the duration of an episode. By “episodes” we mean the subsequences into which the interaction between a decision-maker
and its surrounding environment split naturally.
In both (5.55) and (5.56), we have ignored the use of discount factor gg. The inclusion
of g in the learning process has the effect of replacing l in (5.55) and (5.56) with the
product term lgg Therefore, we may look upon l in both equations (5.55) and (5.56) as
an “algorithmic device” that could have exactly the same effect as gg. To this end, l is
commonly set to be less than the desired g to produce a form of heuristic discounting
Dynamic programming for action in the environment
Sutton and Barto (1998) introduced the idea of eligible traces in the context of a backward
view of TD learning, in which time is oriented backward. As illustrated in Figure 5.5a,
the computation starts with state in, for example, then it proceeds to in−1, in−2, and so on.
Accordingly, each state is associated with an additive memory variable, called the eligibility trace, that has a positive value. In the absence of a discount factor, for which a case
was made in the preceding subsection, the eligibility traces for all states decay by a factor
equal to l. Specifically, the eligibility trace for state i at time n is defined by
It is for this reason that l is appropriately called the trace-decay parameter. Moreover,
the accumulative nature of the eligible trace may be explained as follows: each time a
particular state is visited, the eligible trace accumulates in accordance with the number
of visits made and then decays gradually when the visits end, as illustrated in Figure 5.5b.
The idea of eligible traces has practical significance: they indicate the degree to which
each state is eligible to perform its learning process whenever a reinforcement event
occurs. As pointed out previously, the occurrence of a TD error provides the signal for the
estimate of the cost-to-go function to increase or decrease. To be specific, the incremental
J in), pertaining to state i visited at time n, is defined by
where h is the learning-rate parameter and dn is the TD error.
Figure 5.5. Backward and forward views ot TD learning. Note: an episodic sequence ends at
the “Terminal.” (a) Backward view of TD learning, with each update depending on the current
TD error combined with eligibility of past traces. (b) Illustrating the idea of accumulative
eligibility trace in backward view of TD learning. (c) Forward view of TD learning, with each
state updated by looking forward to future rewards and states. Note that the time scales of parts
(a), (b), and (c) are not intended to be coincident with each other.
Thus far, the discussion has focused on the backward view of TD learning. As an alternative, we may adopt a forward view
w of TD learning by orienting time in the forward
direction, as illustrated in Figure 5.5c. In this third part of the figure, the computation starts
with state in, then it proceeds to in+1, in+2, and so on. Unlike the backward view, there is
naturally no memory associated with states in the forward view. Nevertheless, Sutton and
Barto (1998) showed that the forward and backward views of TD learning are in actual
However, these two views of TD learning are radically different from each other:
• The forward view is noncausal, which means that TD learning cannot be implemented
• On the other hand, the backward view is causal and TD learning is, therefore, implementable in real time. Just importantly, this viewpoint is simple to understand conceptually and also relatively simple to implement computationally. But it requires the use
From a theoretical perspective, the eligibility trace for varying l in the range [0, 1]
provides a bridge from TD to Monte Carlo methods. To elaborate on this statement,
consider the following two limiting cases:
(1) If we let l = 0 and use the convention that 00 = 1, then (5.55) reduces to
which is a repeat of (5.54). Indeed, it is for this reason that the algorithm of (5.54) is
commonly referred to as TD(0), as pointed out previously. With l = 0, the eligibility
(2) For the other limiting case, if we let l = 1, then (5.55) reduces to
which, except for the scaling factor h, is recognized as a Monte Carlo simulation
procedure for updating the current estimate J(
J in) to its new value J +(in). One practical
disadvantage of the Monte Carlo method is the inability to learn from an episode
There is one other perspective for viewing the eligibility trace. Specifically, we may
think of it as a mechanism for dealing with the temporal credit assignment problem in
the following sense. When a TD error occurs, credit or blame is assigned to states or
(1) In contrast to Q-learning, which is an off-policy method well suited for control, TD
learning is an on-policy method well suited for prediction; by “on-policy” we mean
that it involves a single policy, namely the target policy.
Dynamic programming for action in the environment
(2) Through augmentation of TD learning with different values of the trace-decay
parameter l inside the range [0, 1], we will have generated a family of methods that
span a wide spectrum occupied by the one-step method (i.e. TD(0)) at one end of the
spectrum and the Monte Carlo method (i.e. TD(1)) at the other end.
(3) As with Q-learning, the TD learning algorithm of (5.56) is nott a true gradientdescent method, with the result that it is not as robust as the LMS algorithm whose
formulation involves the use of stochastic gradients.
(4) In a neurobiological context, there is evidence that reward signals resulting from
interactions of an animal with the environment are proceeded by midbrain neurons
known as dopamine neurons (Schultz, 1998). Viewing these neurons as “a retina of
the rewards system,” the responses produced by dopamine neurons may be considered as teaching signals for TD(l ) learning.
On the relationships between temporal-difference learning
Previously, in Section 5.6, we stressed the fact that reinforcement learning, exemplified
by TD learning, is an approximate form of dynamic programming. The essence of this
approximation may be summed up as follows:
Dynamic programming makes full use of backups, whereas TD learning uses a sample backup.
Actually, the relationship between TD learning and dynamic programming is
much deeper than just the issue of approximation, in that it manifests itself in other
important ways. For example, in Section 5.11 dealing with a new generation of
reinforcement learning algorithms and beyond, we will describe a ground-breaking
l which embodies ideas rooted in traditional reinforcement learning methods, but, most importantly, it looks to Bellman’s equation
of optimality (5.22) for the mathematical framework to formulate the objective function to be optimized.
However, there is an issue that needs attention, namely that of terminology. To elaborate, the terminology used in (5.54) for TD(0), the simplest form of TD learning, follows
that in the classical dynamic-programming literature. Yet, TD learning was originally
formulated as a reinforcement-learning algorithm. It would, therefore, be instructive
to use TD learning as the basis for establishing the relationships between the dynamicprogramming and reinforcement-learning terminologies.
To begin, the reinforcement-learning literature emphasizes the reward function,
The reward function, denoted by rn+1, is the reward (contribution) given to a decision-maker by
the environment at time n + 1 when the decision-maker is in state in for having taken an action on
In other words, the reward function indicates what is “good” in an immediate sense. The
dynamic-programming function that corresponds to the reward function is the observed
5.9 On the relationships between temporal-difference learning and dynamic programming
The next important function to be considered in reinforcement learning is the value
function. With the reward being typically random, the value function operating under
policy p is defined in terms of the reward function as follows:
where, as usual, g is the discount factor. In words, the value function Vp(i) specifies
what is “good” as a result of interactions with the environment for a long period of time.
Comparing (5.59) with (5.5), we see that the value function Vp(i) in reinforcement
learning corresponds to the cost-to-go function Jp(i) in dynamic programming. More to
the point, whereas Jp(i) is minimizedd in dynamic programming, Vp(i) is maximizedd in
Table 5.3 summarizes the correspondences between dynamic programming on the
left and TD learning on the right. For the sake of interest, this table also includes the
However, there is another concept, called the return, widely used in the reinforcement
learning literature, and for which there is no apparent counterpart in dynamic programming. The return, defined as some function of the reward sequence, signifies whether
a reinforcement-learning task is episodic or continued, discounted or nondiscounted.
l-return at time n is defined in terms of a
weighted sequence as follows (Sutton and Barto, 1998):
l) is introduced in (5.60) to ensure that the sum of the
weights, all the way from k = 1 to k = ∞, add up to unity. Moreover, from (5.60) we see
that the weight fades by the amount l (i.e. trace-decay parameter) after each additional
Table 5.3. Correspondences between dynamic-programming and reinforcement-learning terminologies
Dynamic programming for action in the environment
time step. Note also that after a terminal step is reached at time T in an episodic scenario, all the subsequent returns are equal to Rn, as shown by
Based on (5.61), we may distinguish two limiting cases:
(1) For l = 0, for which Rn( ) reduces to one. That is, the l
(2) For l = 1, for which Rn( ) reduces to the conventional return Rn. That is, the l-return
for l = 1 is the same as the Monte Carlo method.
These two limiting cases are in perfect accord with those discussed in Section 5.8.
Linear function approximations of dynamic programming
Typically, large-scale cognitive dynamic systems have a state space of high dimensionality. As a consequence, when we deal with such systems we may experience the
curse-of-dimensionality problem, which, as discussed previously in Chapter 4, refers
to the exponential growth of computational complexity with increasing dimensionality
of the state space. In situations of this kind, we are compelled to abandon the idealized
notion of optimality and be content with a suboptimal solution. In effect, performance
optimality is traded off for computational tractability. This kind of strategy is precisely
what the human brain does on a daily basis: faced with a difficult decision-making
problem, the brain provides a suboptimal solution that is the “best” in terms of reliability
and available resource allocation. Inspired by the human brain, we may go on to make the
following statement in solving difficult dynamic programming problems (Werbos, 2004):
With suboptimality in mind, several approximate dynamic-programming algorithms
have been described in the literature. In most cases of practical interest, they all follow
a linear approach to function approximation. The basic idea behind the linear approach
Algorithmic iterations are performed within a chosen subspace whose dimensionality is lower
than that of the original space of interest and, in mathematical terms, the subspace is spanned by
The basis functions are commonly referred to as feature vectors. The adoption of a
linear approach for dynamic-programming approximation is motivated by the fact
that it is insightful, easy to formulate and analyze in mathematical terms, and equally
easy to implement in computational terms. The iterations performed in formulating
the approximate dynamic-programming algorithm may be value iterations, in which
case the original space of interest is the state space. Alternatively, the iterations may be
policy iterations, in which case the original space to be approximated is a joint state–
During the past two decades, much has been written on approximate dynamic programming using the linear approach. In this context, we may identify two schools of
thought in addressing the issue of adaptation:
• one using the method of least squares and
• the other using the method of gradient descent.
For reasons that will become apparent in the next section, we will confine the discussion
to two recent contributions to the literature based on the method of steepest descent, as
(l) for prediction (Maei and Sutton, 2010). This first algorithm is a predictivelearning algorithm of the off-policy kind, embracing target policy as well as behavior
policy. It is applicable to problems required to connect low-level experience to highlevel representations:
• Low-level experience refers to rich signals received back and forth between a
cognitive dynamic system (e.g. cognitive radar) and the surrounding environment.
• High-level representations refer to experiential knowledge gained from successive interactions of the system with the environment.
(2) Greedy GQ for controll (Maei et al., 2010). This second algorithm is an extension of
l in which the target policy is greedy with respect to a linear function approximation of the Q-factor: Q(i, a) for state–action pair (i, a).
l and Greedy GQ algorithms constitute two ground-breaking methods in
a new family of modern reinforcement-learningg algorithms that are collectively called
l for predictive learning are presented in the
next section. Highlights of the greedy GQ for control are presented in Section 5.12
“Summary and discussion,” at the end of the chapter.
l algorithm, first described by Maei and Sutton (2010), distinguishes itself
from the traditional Q-learning and TD-learning algorithms in three important ways:
(1) Unlike Q-learning and TD learning, when it comes to adaptation, the GQ(l)
l algorithm exploits the method of gradient descentt in exactly the same way as it is done
in the LMS algorithm, known for its computational simplicity as well as robustness
(Haykin, 2002). So, the letter G in GQ(l)
l algorithm inherits the unique characteristic of
off-policy learning, whereby the target policy, denoted by p
estimation using data generated by the behavior policy, denoted by b, through selective actions on the environment.
(3) The use of trace-decay parameter l in the argument of GQ(l
us of the fact that, when function approximation is the issue of interest, one of the
key features of TD learning is the ability to generalize predictions to states that
Dynamic programming for action in the environment
may not have been visited during the learning process. It follows, therefore, that TD
learning is also a core ingredient in formulation of the GQ(l)
From what has just been described, the GQ(l)
l algorithm has the making of a “universal”
To be more to the point, we may go on to say that this new algorithm based on the
linear approach has the built-in methodologies to address the challenge that we have
In the context of predictive learning, how do we approximate Bellman’s dynamic programming
for large-scale applications in a computationally feasible and robust manner?
l algorithm appears to have the potential to rise to this challenge by offering
(1) The algorithm has the ability to learn about temporally abstract predictions, which,
in turn, makes it possible to learn experientially grounded knowledge.
(2) The learning process is performed in an on-line manner.
(3) Just as importantly, the computational complexity of the algorithm scales linearly
with the dimensionality of the approximating feature space, which is made possible
through the adoption of gradient descent for adaptation.
probability one is realized by the adoption of off-policy learning.
These four attributes, viewed together, are of profound practical importance. In particular, attributes (3) and (4) make all the difference when GQ(l)
linear approaches to approximate dynamic programming that resort to second-order
methods for guaranteed convergence to a fixed point with probability one. Specifically,
• The least-squares temporal difference (LSTD(l))
• The least-squares policy evaluation (LSPE(l))
l algorithm described by Bertsekas (2007).
Although these two algorithms are different in their approaches, they both rely on
the method of least squares for adaptation. With dimensionality of the features space
viewed as the frame of reference, the method of least squares scales according to a
square law, whereas the method of gradient descent scales according to a linear law
With computational complexity being of overriding importance in large-scale applications, the conclusion to be drawn from the four attributes of linear GQ(l)
follow-up remarks on attributes (3) and (4) may be summarized as follows:
With large-scale applications of approximate dynamic programming for predictive learning as the
challenge and guaranteed algorithmic convergence as the requirement, in light of what we know
l algorithm is the preferred method of choice.
Now that we have an appreciation for practical benefits of the linear GQ(l),
proceed with the task of deriving this novel algorithm.
Objective function setting the stage for approximation
First and foremost, the key element in deriving the linear GQ(l)
l-weighted objective function that accounts for two of its three algorithmic
To this end, we begin the formulation with Bellman’s optimal equation for a Markov
decision process, namely (5.22). Referring to the operator notation described in Note 1
at the end of the chapter, this equation is rewritten in the form
where we have simplified matters by putting aside the discount factor gg. The vectors c
and j respectively represent the expected observed cost { (i, μ (i ))}iN=1 and the cost-to-go
function { (i )}iN=1 , and the matrix P represents the transition probabilities { (i, )}iN, j 1 ;
as usual, N is the number of states. The “min” operator Mp(j) is linear, since the operations it performs are all additions and multiplications. For the task at hand, however, we
find it more convenient to work with the much simplified definition
where T p is called the Bellman operator for target policy p.
We are still not quite where we would like to be in formulating the objective function
l To get there, let q denote the vector of Q-factors under action a,
In a corresponding way to (5.63), we may define the off-policy, l
Bellman operator for target policy p as follows:
l for trace-decay parameterr and b for termination probability
of the target policy p ) are intended to remind us that (5.65) pertains to an off-policy
(1) By definition, the whole term Tλβπ q involves expectations in the form of transition
probabilities; therefore, we may perform direct samplingg on this term and obtain
(2) If we were to put the two parameters l and b aside, then (5.65) may be viewed as the
Thus, equipped with the definition in (5.65), the stage is now set for introducing feature vectors into the discussion, as shown by the N-byN
Dynamic programming for action in the environment
where s denotes the dimension of feature vector e(i) for state i that extends from 1
N. The approximate Q-factor for state–action pair (i, a) is defined by the inner
which includes the use of a weight vector w introduced herein for reasons that will
become apparent momentarily. Equivalently, we may express the vector of approximate
The weight vector w is to be adaptedd in such a way that the l returns provide the “best”
fit to the actual vector q in some statistical sense.
Following the format of (5.65), we may equally well define the off-policy, l-weighted
version of the Bellman operatorr for the approximate vector qw, as shown by
which provides the basis to formulate the objective function that we have been seeking
l To elaborate on (5.69), the operator Tλβ
vector qw as input and for each state–action pair it returns the expected corrected qw
as output under the following premise: the Markov decision process starts on the state–
action pair under study, actions are taken according to policy p
To provide a Euclidean basis for how close the corrected vector Tλβ
approximate vector qw, we introduce a matrix P that projects points in the Q-factor
space into the linear space occupied by qw. The stage is now set for formulating the
l-weighted version of the Bellman error function
where n denotes discrete time and the matrix D is an N-byN
only nonzero diagonal entries correspond to relative frequency (in probabilistic terms)
with which each state–action pair is visited under the behavior policy b. The squared
Euclidean norm in (5.70) is defined by the quadratic form
Figure 5.6 illustrates the comparison of two different criteria:
(1) One criterion is based on the objective function of (5.70), involving the composite
operator Tλβ multiplied by the projection matrix P.
(2) The other criterion is based on a simpler objective function that involves the operπ by itself.
This figure clearly illustrates superiority of the criterion based on ΠTλβ
over the simpler criterion; hence the justification for concentrating on the first criterion.
The symbol E used to denote the objective function in (5.70) is intended to distinguish it from the symbol J used previously to denote the cost-to-go function.
Resuming the discussion on the objective function in (5.70), we find it desirable to
reformulate this objective function in a way that would involve the use of statistical
expectations. This reformulation is doable by proceeding in two stages.
Stage 1. In this first stage, the objective function is expressed as a quadratic form
defined in terms of two expectations. The outer expectations in the quadratic form
are conditional on the target policy p and the expectation in the middle is conditional on the behavior policy b. A drawback of this quadratic form, however, is
that it is difficult to work with expectations conditional on the target policy.
Stage 2. This second stage overcomes the difficulty by conditioning the statistical
expectations solely on the behavior policy.
Thus, the objective function in (5.70) takes the desired quadratic form (Maei and Sutton,
where we have introduced the new term dofff, denoting the off-policy version of the
multistep TD error, and Eb denotes the expectation with respect to the behavior policy.
The error dofff is dependent on the trace-decay parameter l
and a new parameter r resulting from stage 2 of reformulating the objective function.
Figure 5.6. Illustrating the comparison between two criteria for selecting the objective function of
l RMSBE: root mean-square of Bellman error; RMSPBE: root mean-square projected
error. The term Vw denotes the value function for any w and the qw in (5.68) lies in the (F, D) space.
Dynamic programming for action in the environment
To proceed with derivation of the GQ(l ) algorithm, we now look to the third ingredient of the linear GQ(l ): the method of gradient descent, the application of which
requires evaluation of the gradient vector defined as the partial derivative of the
objective function E(wn) with respect to the weight vector wn. Specifically, using
b [δ off, n n ]E b [ n φn ] E b [δ offff, n φn ]),
where the scaling factor 1/2 has been introduced merely for mathematical convenience
and ∇ denotes the gradient operator. Bearing in mind that
• doff,n is the only term dependent on the weight vector wn,
• the scaling factor 1/2 is canceled as there is a pair of doff, to be differentiated, and
• the order of gradient operator ∇ and expectation operator Eb is interchangeable, as
By definition, the off-policy version of the multistep TD error is related to the corresponding off-policy l
is an estimate of the value function, which is meant to equal
the off-policy l-return in expected value. Hence, applying the gradient operator
∇ to the error doff,n in (5.73) with respect to w, yields the following relationship
Thus, substituting (5.74) into (5.72) and simplifying terms, we obtain
To simplify the formula of (5.75), we now introduce a secondary weight vector
The mathematical structure of wsec,n is exactly similar to that of the well-known
Wiener filterr in linear filter theory (Haykin, 2002). To elaborate, the expectation term
Eb [ o , n φnT ] represents the correlation matrix of the state–action feature vector en,
being viewed as an “input vector.” The second expectation term Eb [δ o , n φn ] represents
the cross-correlation vectorr of en with the TD error doff,n, being viewed as the “desired
response.” Thus, the use of the secondary weight vector w*sec, n in (5.75) results in the
Derivation of the gradient vector in (5.77) is based on the forward view of TD
learning. From a practical perspective, it is instructive to convert the forward view to
its backward-view equivalent that is more convenient for low-memory mechanistic
implementation. (The forward and backward views of TD learning were discussed in
Section 5.8.) With this conversion in mind, we make use of the following expectation
pair of TD forward-view and backward-view equivalences (Maei and Sutton, 2010):
where dn is the one-step TD error and the eligibility trace vector en is defined by
describes the composite feature vector e(in, a) weighted by the corresponding target
policy (in, a) over the entire action space A at time n. Thus, substituting the pair of equivalences in (5.78) and (5.80) into the formula for the gradient vector of (5.77), we get
However, a practical difficulty in using the gradient vector of (5.83) is the fact the two
expectations embodied in this formula are typically not calculable in practice, and so it is
with the secondary weight vector w*sec, n . In situations of this kind, we may follow the normal
procedure adopted in adaptive filter theory (Widrow and Stearns, 1985; Haykin, 2002):
Abandon the method of gradient descent in favor of its stochastic version.
That is, in the context of (5.83), we do the following: remove the two expectations Eb
with respect to the behavior policy. Thus, using the symbol g(wn) for the stochastic gradient vector, we may write
g( w n ) = −δ n e n + κ n + φn e nT w sec, n ,
Dynamic programming for action in the environment
To apply gradient descent, we obviously use the negative of the stochastic gradient
vector g(wn) to compose the correction term in the adaptation rule for the primary
weight vector. Accordingly, we may express this rule as follows:
= w n + ηw, δ w, n e off , n − κ n +1e Tn w sec, n φn +1 ),
where hw,n is the first of two learning-rate parameters in GQ(l).
To formulate the corresponding adaptation rule for the secondary weight vector, we
(1) As pointed out previously, the secondary weight vector in (5.76) may be viewed as
a Wiener filter with the state–action feature vector en as the input vector and doff,n as
the desired response. Such a filter is realized by minimizing the mean-square error
The stochastic gradient vector for the secondary weight vector is therefore defined by
Following LMS filter theory (Haykin, 2002), we may formulate the adaptation rule:
(2) Referring to the stochastic version of the equivalence in (5.78), we may make two
observations for the secondary weight vector:
• d off,nen is the forward view of TD update and
• d nen is the corresponding backward view of TD update.
Thus, substituting d nen for d off,nen in (5.86), we obtain the desired update rule for the
where h wsec,n is the second one of the two learning-rate parameters in GQ(l).
The pair of adaptation rules of (5.85) and (5.87) is unique to approximate dynamic
programming: the combination of these two rules working together is referred to as the
weight-doubling trick or Sutton trick, acknowledging its originator.
To elaborate, the main trick needed to make gradient-TD methods work in linear complexity is the introduction of a secondary weight vector that is updated in parallel with
the primary weight vector for the approximate Q-function. The secondary weight vector
is updated according to the classical LMS algorithm using the state–action feature vector
as input and the one-step TD error as the desired output. The actual output, linear in the
features, becomes an estimate of the expected TD error in the presence of the given feature vector. This estimate is then used in the update of the primary weight vector, such
that this update is in the direction of the gradient of mean-square projected Bellman error.
The idea of eligibility traces in TD learning was discussed in Section 5.8. Equation
(5.79) expands on that idea by defining the GQ(l)’s
vector en for state–action pair (in, an) at time n. The vector en represents a sort of memory
variable associated with each state–action pair. On each step at time n, this memory variable is incremented by the amount en for state–action pair (in, an) that is just visited and
is decayed by the amount of g nlnrn, where the discount factor g n = 1 − bn. The reason we
need these traces is because in the forward view we could not store the computed error
updates – simply put, it is not practical. By the forward view to backward view transition
trick, eligibility traces help us to estimate the error updates based on the whole history
of input data. The eligibility traces are particularly helpful to bridge temporal gaps when
the experience is processed at a temporally fine resolution.
The φn is an action–state feature vector for state in weighted by the corresponding
target policy over the entire action space; thereby, the resulting vector is independent
of any action. The reason we have it in the GQ(l)
any direct sampling from the gradient term (−1/2 gradient of the projected Bellman
error) and come up with stochastic updating rules – expectation of an update term is
over the target policy for actions taken from the next state.
Table 5.4 provides a summary of the GQ(l)
l algorithm. In a related context, it is instructive that we describe the meaning of each of the related parameters, namely b,
Parameter b is called the probability of termination from a given state; it lies in the
range [0, 1]. Moreover, the difference term, 1 − b
This relationship between b and g tells us that the time-varying discount factor g n may
be considered as the “continuation probability.”
Parameter k is defined in terms of the termination probability b and trace-decay
With 0 ≤ b ≤ 1 and 0 ≤ l ≤ 1, it follows that 0 ≤ k ≤ 1 for all n.
Off-policy reward r. Suppose the decision-maker takes action a at state i according to
behavior policy b; and it receives the reward r and moves to the next state i ′. In so doing,
nothing on target policy p has been used. It is from the state i ′ that the target policy p
is considered. In other words, in linear GQ(l),
l the reward r is independent of the target
Relation of z to r. In contrast to r, which plays the role of transient signals
while the decision-maker is behaving, the parameter z is the final outcome of target
Dynamic programming for action in the environment
• The primary weight vector w is initialized in an arbitrary manner.
• The secondary weight vector wsec is initialized to zero.
• The auxiliary memory vector e is also initialized to zero.
where (in, an) denotes the state–action pair at time n.
where 0 ≤ bn ≤ 1 and 0 ≤ ln ≤ 1 for all n.
hw,n = fixed or decreasing learning-rate parameter for updating the primary weight vector wn.
hwsec,n = fixed or decreasing learning-rate parameter for updating the secondary weight vector
Feature vectors summed over the action space
βn zn+1 + (1 − βn+1 ) w Tn φn+1 − w Tn φn
sec, n + η w sec , n (δ n e n (φn w sec, n ) n )
Parameter r represents an importance sampling ratio. More importantly, for any
given state and action pair (i, a), it is defined as the ratio of
• the probability of taking action a according to the target policy p to
• the behavior policy b, given the state i.
It follows, therefore, that r will vary from time to time.
Parameter l is the trace-decay parameter in TD learning; its value lies in the range [0, 1].
l algorithm summarized in Table 5.4 is limited to policy evaluations. As such,
in its current form, it is only applicable to predictive learning.
Another point of practical interest: Maei and Sutton (2010) show that the GQ(l)
algorithm converges with probability one to the TD(l)
In this chapter we studied Bellman’s dynamic programming for decision-making
and control of the environment. Simply put, the dynamic-programming technique
provides a decision-maker (e.g. transmitter in a cognitive dynamic system) with the
means to improve long-term performance at the expense of short-term performance.
With this goal in mind, interest in dynamic programming is to formulate a policy
that maps states into actions, for which Markov decision processes provide a wellsuited model.
The dynamic-programming technique rests on the principle of optimality, the
essence of which is that an optimal policy for decision-making can be constructed
in a piecemeal fashion. Given a complex multistage planning or control problem, we
first construct an optimal policy for the tail subproblem involving the last stage, then
extend the optimal policy to the tail subproblem involving the last two stages, and continue in this manner until the entire problem has been tackled. Given a dynamic system
with N states, the principle of optimality provides the framework for developing
Bellman’s optimality equation, which represents a system of N equations with one
To compute the optimal policy, we may use policy iteration or value iteration. In
the policy iteration algorithm, a sequence of stationary policies is generated, each of
which improves the cost-to-go function over the preceding one. On the other hand, value
iteration is a successive approximation algorithm, which, in theory, requires an infinite
number of iterations; a stopping criterion is usually built into the algorithm to terminate
Dynamic programming for action in the environment
it when the change in the cost-to-go function from one iteration to the next is small
enough to be ignored. Because of its simplicity, the value iteration algorithm is the most
widely used algorithm in dynamic programming.
Dynamic programming requires access to the exact value of the state of the environment. Unfortunately, this requirement is violated in cognitive radar, to name just one
important example. In Section 5.5 we described a procedure whereby, through certain
reformulations, problems with imperfect state information can be reduced to corresponding problems with perfect state information, thereby paving the way for the application of dynamic programming. We will have more to say on this issue in Chapter 6 on
The policy and value iteration algorithms are applicable when we have a mathematical
model of transition probabilities from one state to the next, including the cost incurred as
a result of each transition. In practice, however, it is difficult to construct such a model, in
which case we have to resort to approximation in one form or another. What is truly remarkable is that reinforcement learning, developed in neural computation as an intermediate
learning procedure that lies between supervised learning and unsupervised learning, is, in
reality, an approximate form of dynamic programming. In supervised learning, we have a
“teacher” that supplies a desired response for every input applied to a neural network, for
example, with the objective of adjusting the free parameters of the network such that the
resulting “output” is as close as possible to a desired response in some statistical sense.
In unsupervised learning, there is no teacher, but provision is made for a task-dependent
measure of the “quality” of representation that the network is required to learn, and free
parameters of the network are optimized with respect to that particular measure. In reinforcement learning, on the other hand, the network learns in on-line continuous interactions
with the environment in a manner similar to that in approximate dynamic programming.
Reinforcement learning embodies two well-known algorithms: Q-learningg and TD
learning, both of which bypass the need for transition probabilities. Q-learning is an
off-policy algorithm well suited for control. On the other hand, TD learning is an on-line
prediction method that learns how to compute an estimate, partly, on the basis of other
estimates through boot-strapping. Both Q-learning and TD learning are limited in their
application to situations where the state space is of moderate dimensionality.
To deal with more difficult situations involving large-scale applications, we may look
l algorithm. This new approximate dynamic-programming algorithm is
rooted in Q-learning, TD learning, and stochastic gradients for algorithmic adaptation; the approximation follows a linear approach. Insofar as the general setting of the
algorithm is concerned, there are two mechanisms to be considered, namely varying eligibility traces and off-policy learning, the combination of which is aimed at temporally
abstract predictions. Most importantly, its objective function is based on the off-policy,
l-weighted version of the Bellman error function.
The algorithm has the potential to make a significant difference in prediction modeling, for which it is well suited. Its algorithmic uniqueness lies in four distinctive
• first, ability to learn experientially grounded knowledge;
• third, linear computational scalability with respect to dimensionality of the feature
• fourth, convergence of the algorithm to the TD fixed point with probability one under
l is well suited for prediction, Greedy-GQ is well suited for control.
Specifically, the Greedy-GQ algorithm (Maei et al., 2010) is the first TD method for offpolicy control with unrestricted linear function approximation. Greedy-GQ uses linear
function approximation and possesses a number of important and desirable algorithmic
on-line, incremental adaptations of primary and secondary weight vectors;
linear complexity in terms of per-time-step computational costs and memory;
no restriction imposed on the features used in the linear approximation; and
In an algorithmic context, Greedy-GQ can be seen as a generalization of GQ(l)
applied to a controll setting by allowing certain changes in the target policy. In particular,
similar to Q-learning, the target policy is greedy with respect to a linear approximation to the optimal Q-function; hence its applicability to control. The update rules for
adaptations in Greedy-GQ are similar to GQ(l)
l for l = 0, and they are, therefore, analogous to those of Q-learning with linear function approximation, except that we have an
The main feature that distinguishes Greedy-GQ from popular and conventional offpolicy algorithms such as Q-learning is that it is based on minimizing a Bellman error
objective function and it has a stability guarantee.
Although the objective function in Greedy-GQ resembles that of GQ(l),
l the techniques used for derivations and stability analysis of the algorithm are substantially
different. This is due to the greedy policy, which highly affects the properties of
the objective function. It turns out that the objective function is neither convex nor
differentiable. However, it has some unique properties: it is in the form of a piecewise
quadratic function with respect to the primary weight vector and its global minimum
matches the Q-learning fixed-point (assuming that such a fixed-point exists).
Dynamic programming for action in the environment
An important future open problem is to study the properties of local minima of
the Greedy-GQ that lie in the border of quadratic functions. Because the Greedy-GQ
objective function is nondifferentiable, the algorithm is derived on the basis of a
New generation of approximate dynamic programming algorithms: l
l and Greedy-GQ represent a new generation of approximate dynamic
programming/modern reinforcement-learning algorithms. Though they are intended for
different applications, one for predictive modeling and the other for control, they do
share several distinctive characteristics:
incorporation of both Q-learning and TD learning;
common use of the Bellman error objective function;
linear scaling of computational dimensionality with respect to dimensionality of the
feature space achieved by exploiting the use of stochastic gradients; and
Between them, therefore, we may say that the GQ(l)
l and Greedy-GQ hold the potential for addressing the nagging issue of how to deal with the curse-of-dimensionality
problem when we are challenged with large-scale reinforcement-learning applications.
To substantiate this potential processing power, it is important that they are both tested
for their respective application areas under large-scale operating conditions.
1. Bellman’s optimality equation using operator notation
In (5.22) we have the description of Bellman’s optimality equation for a Markov
decision process as a system of N equations with one operation per state. This
optimality equation may be put into a compact mathematical form using operator
To this end, we first introduce the following vector and matrix notations:
where N is the number of states. We may then rewrite (5.22) in the simplified matrix
where g is the discount factor. To proceed further, we introduce two more notations:
(2) The J to denote the space of all possible cost-to-go functions.
m0, m1, m2, . . .} that includes the stationary policy
as a special case, we may finally go on to write
for some j ∈ J. In mathematical terms, Mp is referred to as a linear operator, since
the operations it performs are all additive and multiplicative. Moreover, the function
composed of the sum term, c + g Pj, is said to be an affine function. Thus, we may
view (5.88) as a descriptor of the affine N-byN
2. Actor–critic interpretation of policy iteration
In the reinforcement-learning literature, the policy iteration algorithm is referred
to as an actor–critic architecture (Barto et al., 1983). In this context, the policy
improvement in Figure 5.3 assumes the role of actor, because it is responsible for
the way in which a decision-maker (i.e. agent) acts. By the same token, the policy
evaluation assumes the role of critic, because it is responsible for criticizing the
The study of direct approximate dynamic-programming algorithms, namely reinforcement learning, would be incomplete without the inclusion of Sarsa (Sutton and
The name “Sarsa” is used for both a prediction algorithm and a control algorithm.
In both cases, the update of the weights of the Q-function is based on a short segment
of experience consisting of a State, the corresponding Action, the resultant Reward
and next State, and finally the next Action, hence the name S-A-R-S-A. Sarsa is the
only algorithm whose TD error and update is based on exactly these five real, experience events. The one-step tabular form of the Sarsa update is described by
Q+(in, an) = Q(in, an) + h rn+1 + h (Q(in+1, an+1) − Q(in, an)).
The difference term inside the parentheses on the extreme right-hand side is the TD
error. In the prediction form of the algorithm, the actions are selected according to a
fixed policy p i and the approximate Q-function converges to (Qˆ | in ) if the step-size
h is reduced over time in a consistent manner with the standard stochastic approximation conditions. Note that, in this case, the algorithm is formally equivalent to the
one-step tabular TD algorithm because the process transitioning from state–action
pair to state–action pair is simply an uncontrolled Markov process. Because of this,
all the results for TD, including the use of eligibility traces and function approximation, carry over to Sarsa.
The version of Sarsa with eligibility traces and linear function approximation,
l, is widely used in reinforcement learning. For the control case, the same update is used only with the policy not held fixed, but allowed
to change; for example, to be the ee-greedy policy for the current approximate
Dynamic programming for action in the environment
Q-function. Thereby, Sarsa becomes a simple and effective algorithm for on-policy
4. Robbins–Monro stochastic approximation
In the pioneering classic paper by Robbins and Monro (1951), the following stochastic approximation problem is addressed.
M(x) be a function of the real-valued parameter x. Suppose that, for each value
( ) with the conditional distribution function
is the expected value of Y given x. Neither the conditional distribution function
M(x) is known. But it is assumed that, for some prescribed
q. The problem of interest is to estimate q by making
successive observations on the random variable Y at levels denoted by x1, x2, . . .,
which are determined in accordance with some definite algorithm.
To this end, it is postulated that the following two standard conditions hold for
The Robbins–Monro algorithm is thus defined recursively for all n as follows:
where hn plays the role of a step-size parameter that decreases with n.
Convergence behavior of the Robbins–Monro algorithm has been well studied in
the literature. In particular, it has been shown (Robbins and Monro, 1951; Wolfowitz,
1952) that xn converges stochastically to the desired q provided that, in addition to
conditions (1) and (2), two other conditions also hold:
Y, denoted by s 2, is also finite; that is,
With the material presented in the previous four chapters dealing with fundamental
aspects of cognitive dynamic systems at our disposal, the stage is now set for our first
application: cognitive radar, which was described for the first time by Haykin (2006a).
Radarr is a remote-sensing system with numerous well-established applications in
surveillance, tracking, and imaging of targets, just to name a few. In this chapter, we
have chosen to focus on target trackingg as the area of application. The message to take
from the chapter is summed up as follows:
Cognition provides the basis for a “transformative software technology” that enables us to build a
new generation of radar systems with reliable and accurate tracking capability that is beyond the
This profound statement has many important practical implications, which may be
justified in light of what we do know about the echolocation system of a bat. The
echolocation system (i.e. sonar) of a bat provides not only information about how far
away a target (e.g. flying insect) is from the bat, but also information about the relative
velocity of the target, the size of the target, the size of various features of the target, and
azimuth and elevation coordinates of the target. The highly complex neural computations needed to extract all this information from target echoes are performed within the
“size of a plum.” Indeed, an echolocating bat can capture its target with a facility and
success rate that would be the envy of a radar or sonar engineer (Suga, 1990; Simmons
et al., 1992). How, then, does the brain of a bat do it? The bat is endowed with the
ability to build up its own rules of behaviorr over the course of time, through what we
usually call “experience.” To be more precise, experience, gained through bat’s interations with the environment, is built up over time with the development continuing well
beyond birth. Simply put, the “developing” nervous system of a bat, or for that matter
a human, is synonymous with a plastic brain: plasticity permits the developing nervous
system to adaptt to its surrounding environment (Haykin, 2009). We may, therefore, go
The echolocating bat is a living biological proof of existence for cognitive radar.
This statement is important because it recognizes that the echolocating bat is an active
sensorr just like the radar is; we say “active” in the sense that they both transmit a signal
to illuminate the environment and then “listen” to the echo from an unknown target to
extract from it valuable information about the target.
Before we proceed with the study of cognitive radar, it is instructive that we distinguish between three classes of radars, discussed next.
Given what we already know about radar systems already in existence or those described
in the literature, and how cognitive radar fits into the scheme of things, we may identify
three classes of radars, defined as follows:
This class of radars refers to traditional radar systems, in which the transmitter
illuminates the environment and the receiver processes the radar returns (echoes)
produced as a result of reflections from an unknown target embedded in the environment. In other words, a traditional active radar is a feedforward
d informationprocessing system. The classic handbook on radar by Skolnik (2008) covers the
many types of radar that belong to this class.
Radars belonging to this second class are intended to manage the allocation
of available resources for the purpose of control in an on-line adaptive manner
(Krishnamurthy and Djonin, 2009). The resources residing in the transmitter may
• library of transmit waveforms for target tracking or
• set of scan times for environmental surveillance.
To facilitate control of the receiver by the transmitter via the radar environment, there
has to be feedback information from the receiver to the transmitter. In other words,
a fore-active radar is essentially a closed-loop feedback control system.1 What we
have just described here is simply another way of referring to the perception–action
cycle. In other words, a fore-active radar may be viewed as the first stage towards
the cognition of radar. Indeed, it is for this reason that basically a fore-active radar
acts as a perception–action cycle, which is a viewpoint that we will follow in later
sections of the chapter dealing with simulations.
For a radar to be cognitive, it has to satisfy four processes (putting language aside):
• perception–action cycle for maximizing information gain about the radar
environment computed from the observable data:
• memory for predicting the consequences of actions involved in illuminating the
environment and parameter selection for environmental modeling;
• attention for prioritizing the allocation of available resources in accordance with
• intelligence for decision-making, whereby intelligent choices are made in the face
As pointed out in Chapter 2 commenting on the visual brain, perception is performed in
one part of the brain and action is performed in another part of the brain. And so it is with
cognitive radar: perception of the environment is performed in the receiver, and action
aimed at illuminating the environment is taken in the transmitter. Memory is physically
distributed throughout the radar system. Attention does not have a physical location of
its own; rather, its algorithmic mechanisms are based on perception and memory. As
for intelligence, its algorithmic decision-making and a control algorithms are based on
perception, memory, and attention; as such, among all four cognitive processes, intelligence, therefore, is the most computationally complex and the most profound in terms
of information-processing power. Simply put, for its existence, intelligence relies on
local and global feedback loops distributed throughout the radar system.
Now that we have identified the four processes that collectively define a cognitive radar
and recognize the fact that the perception–action cycle is the very baseline upon which
memory is built, it is logical that we begin the study of cognitive radar with the perception–action cycle in the next section. Indeed, this process will occupy our attention up to
and including Section 6.12, simply because the perception–action cycle of a cognitive radar
has many facets, all of which need detailed attention. In so doing, we will have then paved
the way for building memory on top of the perception–action cycle later on in the chapter.
The perception–action cycle of a cognitive dynamic system was first described in
Chapter 1. With emphasis on cognitive radar in this chapter, we may reproduce Figure 1.1
in that introductory chapter in the form shown in Figure 6.1 representing the first basic
Moreover, we have chosen target tracking as the application on which the study of
cognitive radar will be focused in this chapter. In this context, the primary function of
the environmental-scene analyzer, the only fundamental block in the receiver shown in
Figure 6.1, is, therefore, to provide an estimate of the state of the radar environment by
processing the observables. The term radar environment is used here to refer to the electromagnetic medium in which a target of interest is embedded. The observables refer to
the radar returns produced by reflections from the target due to illumination of the radar
Figure 6.1. The perception–action cycle of radar with global feedback as the first step towards
environment by a signal radiated from the transmitter. In effect, state estimation serves
as “perception” of the environment in the perception–action cycle of Figure 6.1.
Insofar as this cycle is concerned, another function of the receiver is to compute
feedback information that provides a compressed measure of information contained in
the radar returns about the unknown target. Typically, the transmitter and receiver of
the radar are collocated,2 in which case delivery of the feedback information to the
transmitter by the receiver is accomplished simply through a direct linkage, thereby
Turning next to the environmental scene actuator, which is the only functional block
in the transmitter shown in Figure 6.1, its primary function is to minimize a cost-to-go
function based on feedback information from the receiver, so as to act in the radar
environment optimally in some statistical sense. This optimization manifests itself
in the selection of a transmitted signal whose waveform controls the receiver via the
environment on a cycle-by-cycle basis. In this sense, therefore, we may look to the
environmental scene actuator as a controller.
With emphasis on the term “information” in what we have just discussed here, the
perception–action cycle in Figure 6.1 provides the basis for cyclic directed information –
flow across the entire radar system, inclusive of the environment.
Baseband model of radar signal transmission
With controll of the receiver by the transmitter via the environment as a basic design
objective of cognitive radar, we need an agile mechanism for waveform selection, which,
desirably, is digitally implementable. To this end, the transmitter may be equipped with
a library composed of a prescribed set of waveforms, hereafter referred to simply as the
The key question is: What kind of a waveform do we choose for the library? With
target tracking as the task of interest in this chapter, there is, unfortunately, no single
waveform that can satisfy the joint estimation of both range (i.e. distance of the
target from the radar) and range-rate (i.e. radial velocity of the target) (Wicks et al.,
2010). To elaborate, it is well known that a constant-frequency (CF) pulse exhibits
good range-rate resolution but relatively poor range resolution. On the other hand, a
linear frequency-modulated (LFM) pulse exhibits the converse resolution characteristics: good range resolution but poor range-rate resolution. As a compromise between
these two conflicting needs, we propose to opt for a transmit-waveform that combines
two forms of modulation: Gaussian envelope for amplitude modulation and LFM for
This composite modulation is imposed on a sinusoidal carrierr whose frequency is
compatible with the propagation properties of the electromagnetic medium, thereby
facilitating a wireless link from the transmitter to the receiver. Throughout the chapter,
it is assumed that the transmitted radar signal is narrowband, which means that the
bandwidth occupied by the composite modulated pulse is a small fraction of the carrier
Baseband model of radar signal transmission
Baseband models of the transmitted and received signals
The term “baseband” refers to the original frequency content of the composite modulated pulse produced by the waveform generator. In baseband modeling, we purposely
dispense with the carrier frequency analytically in such a way that there is no loss of
information contained in the composite modulated pulse. This objective is realized
through a band-pass-to-low-pass transformation, whereby a real-valued band-pass
signal is transformed into a complex-valued low-pass signal (Haykin, 2000).
To proceed then, let sT (t) denote the narrowband transmitted radar signal, the spectrum of which is centered on the carrier frequency fc; and let s (t ) denote the low-pass
complex envelope of sT (t). Then, by definition, we write
where ET is energy of the transmitted signal sT (t) and the operator Re[·] extracts the
real part of the complex quantity enclosed inside the square brackets. According to this
definition, the complex envelope s (t ) has unit energy, as shown by
Consider next the radar returns (i.e. received signal) at the receiver input. Let t
denote the round-trip delay time from the targett and fD denote the Doppler frequency
resulting from radial motion of the target. These two target parameters are respectively defined by
where r is the target range, ρ is the range-rate, and c is the speed of electromagneticwave propagation (i.e. the speed of light); the dot in ρ denotes differentiation with
respect to time. The Doppler frequency fD represents a “shift” in the carrier frequency
fc; it is positive when the target is moving toward the radar and negative otherwise. We
may then express the received signal denoted by r(t), in terms of the complex envelope
where ER is energy of the received signal, the phase f is a random variable incurred
 t ) is complex envelope of the front-end
from reflecting surface of the target, and n(
receiver noise centered on the carrier frequency fc.
Bank of matched filters and envelope detectors
Typically, at the front end of the radar receiver we have a bank of matched filters. The
impulse response of a matched filterr is defined as the complex conjugate of the complex
transmitted signal envelope s (t ), shifted in time and frequency by scaled versions of
Figure 6.2. Radar receiver front-end, made up of bank of matched filters followed by square-law
envelope detectors. In the absence of receiver noise, the composite output is the ambiguity
function of the complex envelope s (t ) , representing the noiseless version of the radar return
desired time- and frequency-resolutions in the time–frequency space of interest. Recognizing that a matched filter is basically equivalent to a correlator, it follows that the bank
of matched filters acts as a time–frequency correlatorr of s (t ) with time- and frequencyshifted versions of itself.
In the absence of receiver noise, the squared magnitude of the time–frequency correlator output constitutes the ambiguity function (Woodward, 1953). To this end, we follow
every matched filter by a square-law envelope detector, as illustrated in Figure 6.2.
t f ) to denote the ambiguity function and recognizing that the complex
envelope s (t ) has unit energy and, then, invoking the Schwarz inequality,4 we find that
Most importantly, the resulting real-valued two-dimensional output of each envelope detector, involving time delay t and Doppler shift fD, defines an inter-pulse vector
denoted by yn, where the subscript n denotes discrete time. The vector yn performs the
role of measurement in the state-space model of the radar target, as discussed next.
From Chapter 4, we infer that there are two equations that mathematically describe the
(1) System equation, which describes evolution of the target’s state across time in
Baseband model of radar signal transmission
where xn denotes the state of the radar target at discrete time n and vn denotes the
additive system noise accounting for environmental uncertainty about the target.
In (6.7), it is assumed that the underlying physics of interactions between the transmitted signal and the target is nonlinearr but not time varying.
(2) Measurement equation, which describes dependence of the measurement yn on the
where the vector ν(θ n 1) denotes the measurement noise that acts as the “driving
force.” It is by virtue of dependence of this noise on the waveform-parameter vector,
denoted by θ n−1, that the transmitter controls accuracy of state estimation in the
receiver. Here again, (6.8) assumes a radar environment that exhibits nonlinear but
not time-varying behavior. The rationale for using pn−1 as argument in ν(θ n 1 ) rather
than θ n is justified at the end of the section.
Application of the state-space model described in (6.7) and (6.8) hinges on four basic
(1) The nonlinear vectorial functions a(·) and b(·) in (6.7) and (6.8) are both smooth
and otherwise arbitrary; as already mentioned, the absence of subscript in these two
(2) The system noise vn and measurement noise mn are zero-mean Gaussian distributed and
statistically independent of each other; this assumption is justified for a target in space.
(3) The covariance matrix of system noise is known.
(4) The state is independent of both the system noise and measurement noise.
Examining (6.7) and (6.8), we immediately see that the state xn is hidden from the
observer (i.e. receiver). The challenge for the receiver, therefore, is to exploit dependence of the measurement vector on the state to compute a reliable estimate of the state
and do so in a sequential and on-line manner.
Dependence of measurement noise on the transmitted signal
With the objective of sequential and on-line estimation of the target’s state in mind, we need
to determine the statistical characteristics of the measurement noise. To this end, we first
recognize that the measurement noise covariance is dependent on the waveform-parameter
vector pn−1 of the waveform generator in the transmitter; hence the notation R(pn−1) for the
covariance matrix of m(pn−1). Moreover, the inverse of the Fisher information matrixx is the
CRLB5 on the state-estimation error covariance matrix for an unbiased estimator. Denoting
the Fisher information matrix by J, we may consider the inverse matrix J−1 as a suitable characterization for “optimal waveform selection” and thus write (Kershaw and Evans, 1994):
where, for a target in space, Γ is defined by the diagonal matrix
As before, c denotes the speed of light and fc denotes the carrier frequency. For convenience of presentation, it is desirable to separate contribution of the waveform parameter
vector in the Fisher information matrix from the received signal energy-to-noise spectral density ratio (i.e. the SNR) defined by
where N0 is the spectral density of the complex noise envelope n(
Accordingly, we rewrite (6.9) in the desired form (Kershaw and Evans, 1994)
where the matrix U( n 1 ) is merely a scaled version of the Fisher information matrix
J( n 1 ). This new matrix is a two-by-two symmetric matrix whose elements are
described as mean-square values of the following errors:
As pointed out previously, we have chosen to work with a transmit-waveform that
combines linear frequency modulation with Gaussian pulse-amplitude modulation. Let
l denote the pulse duration measured in seconds and b denote the chirp rate in hertz/second
across the pulse. The two-element waveform-parameter vector, therefore, is defined by
Correspondingly, the complex envelope of the transmit-waveform is given by
where t denotes continuous time. Moreover, Kershaw and Evans (1994) showed that the
covariance matrix of the measurement noise v(pn−1) is given by
where, for the sake of simplicity, we have omitted the dependence on time n in the righthand side of (6.16).
It is important to note that the formula for R(pn−1) in (6.16) is valid so long as the assumption that the energy per transmitted waveform remains constant from one cycle of the
perception–action cycle to the next. Otherwise, we would have to expand the waveform
parameter vector (pn−1) by adding a new time-dependent parameter h (pn−1).
We close this section by justifying the rationale for using (pn−1) as the waveformparameter vector. To account for the time delay incurred due to electromagnetic wave
propagation from the receiver to the transmitter, followed by the time delays for the
receiver and transmitter to complete their respective parts of perception and action in
the perception–action cycle, it is proposed that if (pn−1) denotes the waveform parameter
vector in the transmitter, then, without loss of generality, the measurement at the receiver
input is yn; hence the formulation of the measurement equation as shown in (6.8).
With this background on the radar environment at hand, it is apropos that we pause
to discuss the basic issues involved in designing the receiver and transmitter in the
perception–action cycle of Figure 6.1 for tracking a target in space. The underlying statistics of such an environment may be assumed to be Gaussian; this assumption is not
only justifiable, but also simplifies the system design, certainly so for a target in space.
Let us begin the discussion by considering the receiver first. With optimal performance
as the goal, the ideal framework for target-state estimation is the Bayesian filter. We say
so because the Bayesian filter propagates the posteriorr of the state-estimation error covariance matrix, which, in conceptual terms, is the best that we can find for state estimation.
Unfortunately, as discussed in Chapter 4, when the state-space model is nonlinear, as it
is in (6.7) and (6.8), the Bayesian filter is no longer computationally feasible; hence the
practical need for its approximation. For many decades past, the EKF has been used as the
method of choice to perform this approximation. From a practical perspective, however,
such an approach is limited to applications where nonlinearities in the system and measurement equations are of a “mild” sort. When this requirement is violated, as is it does
happen in practice, we have to look to other alternatives. Among the alternatives described
in the literature, the CKF stands out as a method of choice for approximating the Bayesian
filter in a nonlinear setting under the Gaussian assumption. With the state-space model of
(6.7) and (6.8) as the focus of interest, we look to the CKF discussed in detail in Chapter 4
and revisited in the next section as the “central” functional block for the environmentscene analyzer in the receiver for target-state estimation.
Turning next to the transmitter, the primary function here is to optimally control the
receiverr through selection of the transmitted waveform in response to feedback information from the receiver. Provided that the waveform parameters are selected optimally,
any action taken by the transmitter will be viewed as an optimal response to the environment as perceived by the receiver. With optimal control of the receiver in mind, we
may look to dynamic programmingg (Bellman, 1957) as the ideal framework for optimal
transmit-waveform selection, at least in a conceptual sense. In effect, we are looking
for a controllerr in a nonlinear closed-loop feedback system that “tunes” the transmitwaveform parameters so as to “improve” the behavior of the receiver in an effort to
minimize the target-tracking errors in a statistical sense.
Unfortunately, Bellman’s dynamic programming also has a practical limitation
of its own; it suffers from the curse-of-dimensionality problem when the state space,
measurement space, or action space is of high dimensionality; hence the practical
requirement to seek an approximation to this ideal framework in designing the transmitter. Section 6.9 addresses the issues involved in the algorithmic formulation of
an approximate dynamic-programming procedure for optimal waveform selection.
Cubature Kalman filter for target-state estimation
When the nonlinear system and measurement equations of the state-space model are
both Gaussian, formulation of the Bayesian filter reduces to the problem of how to
compute moment integrals whose integrands are of the following form:
(nonlinear function) × (Gaussian density)
To compute integrals numerically whose intergrands are of the form described in
this expression, we may use the cubature rule of third degree, discussed in detail in
Chapter 4. To recap briefly on the material presented therein, consider the example of
an integrand described in (6.17) that consists of a nonlinear function a(x) multiplied
by a multivariate Gaussian density of the vector x, denoted by N (x; l, S), where the
vector l is the mean and the matrix S is the covariance. According to the third-degree
cubature rule, the resulting integral may be approximated in terms of the mean l,
covariance S, and a set of weighted cubature points, denoted by { i }iN=1x as follows:
where the subscript Nx denotes dimensionality of the state space. The square-root factor
of the state-estimation error covariance S in (6.18) satisfies the factorization Σ
and the even set off 2Nx cubature points is given by
where e i ∈R N denotes the ith elementary column vector; the rule of (6.19) is exact for
integrands that are polynomials of degree up to three or any odd integer. In effect, the
Nx weighted cubature points are distributed uniformly over the surface of an ellipsoid
to provide an approximate representation of the nonlinear function a(x).
Probability-distribution flow-graph of the Bayesian filter
To develop insight into the underlying behavior of the CKF, Figure 6.3 depicts the flowgraph of probability distributions for one recursion of the Bayesian filter; such a recursin
occupies a fraction of the time occupied by one cycle in the perception–action cycle. This
figure has been structured in such a way that it corresponds to the steps involved in its
Cubature Kalman filter for target-state estimation
Figure 6.3. Probability-distribution flow-graph of the Bayesian filter under the Gaussian
assumption; the figure is structured in a way that corresponds to the steps involved in deriving
the CKF. The functional block, labeled conditioning, is intended to compute the updated
approximation using the cubature rule of degree three for deriving the CKF. The flowgraph of Figure 6.3 consists of two updates:
The time update starts with the “old” posterior, p(xn−1 | Yn−1), where xn−1 is the state
at time n − 1 and Yn−1 sums up the past history of measurements up to and including
time n − 1. Given the state transition matrix or prior, p(xn | xn−1), the old posterior is
used to compute the predictive distribution, p(xn | Yn−1), and with it the time update
Then, the measurement update begins with computation of the predictedmeasurement distribution p(yn | Yn−1), which, in combination with the predictive distribution p(xn | Yn−1), yields the joint state-measurement distribution p([ x Tn , Tn ]T | n 1) ;
as usual, the superscript T denotes matrix transposition. Finally, through the application
of Bayes’ rule to this joint distribution, the “updated” posterior p(xn | Yn) is computed.
With this computation, the measurement update is completed, ready for the next recursion of the Bayesian filter.
From the distribution flow-graph of Figure 6.3, we see that the Bayesian filter propagates the posterior from one recursion to the next. Similarly, the CKF follows the footsteps of the Bayesian filter, as described next.
The first in a two-step recursion in the CKF is the time-update step. In this update,
involving prediction at time n - 1, the CKF computes the mean xˆ n|n −1 and the associated covariance matrix Pn|n−1 of the Gaussian predictive density, numerically using the
cubature points. We write the predicted estimate
where E[·] is the statistical expectation operator and Yn−1 is the history of past measurements up to and including time n − 1. Substituting the system equation (6.7) into (6.20)
With the system noise vn assumed to be zero-mean and uncorrelated with the measurement sequence, (6.21) takes the form
where, as before, N (.;.,..) is the conventional symbol for Gaussian distribution. Similarly, given the measurement sequence y1:n−1, which is another way of describing Yn−1,
we obtain the associated error covariance matrix
a ( x n )a T ((xx n 1 ) N ( x n ; xˆ n −1|
where Qn is covariance matrix of the system noise vn.
The second update in the two-step recursion of the CKF is the measurement-update
step. To deal with this second update, by definition, we first recognize that in a Gaussian environment the innovations process, defined by the prediction error ( y n yˆ n|n−1 ) ,
is not only white but also zero-mean Gaussian. Under these conditions, the predicted
measurement may be estimated in the least-squares error sense; that is, yˆ n|n −1 is
the least-squares prediction of the measurement yn given its past history up to and
including time n − 1. In particular, we may express the predicted measurement distribution as follows:
Cubature Kalman filter for target-state estimation
Using the measurement equation, we find that the predicted measurement itself and the
associated covariance matrix are respectively given by
b( x n ) b T ( x n ) N ( x n ; xˆ n| n , Pn| n ) dx n − yˆ n| n − yˆ Tn| n −1 + R (
An important point to note in (6.26) is the fact that dependence on the waveformparameter vector pn−1 is confined to the covariance matrix R(pn−1); in effect, the control
action of the transmitter applied to the receiver via the environment manifests itself only
by affecting the measurement noise in the state-space model.
We may now compactly express the Gaussian conditional distribution of the joint
On receipt of the new measurement vector yn, the Bayesian filter computes the updated
posterior p(xn | Yn) from (6.27), yielding
Turning next to the Kalman gain, this is defined by
We may then express the filtered estimate of the state xn as
where, as mentioned previously, the prediction error ( y n yˆ n|n−1 ) defines the innovations
process. Correspondingly, the filtered state-estimation error covariance is given by
Equations (6.30)–(6.32) follow from linear Kalman filter theory.
The material presented in this section on the CKF for perception of the environment is
(1) The third-degree cubature rule, which provides the means for numerical approximation of the Gaussian-weighted integrals that characterize the Bayesian filter.
(2) Linear Kalman filter theory, known for its mathematical elegance, the use of which
completes the mathematical formulation of the CKF.
The net result is a nonlinear filter that is a method of choice for approximating the
optimal Bayesian filter under the Gaussian assumption in a nonlinear setting. Moreover,
derivation of this new filter is mathematically rigorous from the beginning to the end ,
At this point in the discussion, we need to think about closing the feedback loop in the
perception–action cycle of Figure 6.1 with the aim of preparing the transmitter for its
role as “actuator” in the radar environment, under “perceptual guidance” of the receiver.
To this end, there are two basic issues to be considered: feedback information at the
receiver output, followed by cost-to-go function formulated in the transmitter; they are
both discussed in what follows in this order.
Referring back to Figure 6.1, the baseband form of “action” involves the transmitter
illuminating the radar environment by transmitting the complex low-pass waveform
sn−1. Allowing for the time taken for “propagation delay” from the transmitter to the
receiver,6 the corresponding observable at the receiver input is yn at time n. Recognizing
that the waveform-parameter vector responsible for the generation of sn−1 is pn−1 at time
n − 1, it follows that yn is dependent on pn−1. This dependence shows up in (6.26) solely
in the measurement noise covariance matrix R(pn−1).
Now, looking ahead to the “action” to be taken by the transmitter, which has to be at
time n for the perception–action cycle to continue, we clearly have to think in terms of
“one-step prediction into the future.” To be more specific, we have to build on the filtered
estimate of the state xˆ n|n , defined in (6.31), and formulate an expression for the one-step
prediction xˆ n +1|n, computed at time n, given all the measurements up to and including
time n. Indeed, examining (6.22), we see that this computation is perfectly feasible on
replacing n with n + 1. We say so because, with this replacement, the terms under the integral in (6.22) are available, as they all pertain to the system equation at time n.
With the corresponding future value of the state at time n + 1 being xn+1, we may now
define the desired “predicted state-estimation error” at time n + 1 as
Moreover, the covariance matrix of ε n +1|n, namely Pn+1|n, is also computable at time n.
Here again, saying so is justified by the fact on replacing n with n + 1 in (6.23), the
resulting terms on the right-hand side of the equation are available, as they also all
pertain to the system equation at time n.
We may now make our first statement on the transition from the receiver output to the
Given the measurements up to and including time n, the one-step prediction error dn+1|n and its
covariance matrix Pn+1|n are computable and, therefore, available at the receiver output at time n.
So, indeed, in the error-covariance matrix Pn+1|n we have the feedback information
we need for the receiver to pass it onto the transmitter to initiate the “action” part of the
perception–action cycle at time n. For this action to be performed accurately, accurate
performance of the CKF in computing Pn+1|n is of crucial practical importance.
Posterior expected error covariance matrix
Reiterating what we have already said: in transmitting the complex waveform ŝn−1 to
enable the receiver to perform its own “perception” part of the perception–action cycle
at time n, we already have knowledge of the transmit-waveform parameter vector pn−1.
What we need, therefore, is for the transmitter to compute the “updated” parameter
vector pn for completion of the cycle at time n and thereafter continue the perception–
action cycle for the next cycle, and so on.
With this objective in mind, we need to have a “cost-to-go function” for the transmitter
to operate on, such that two requirements are satisfied:
(1) Formulation of the cost-to-go function utilizes the feedback information Pn + 1|n
computed at the receiver output, so as to link the transmitter to the receiver.
(2) The formulation is expanded to incorporate the updated parameter vector pn as the
To satisfy these two points, we propose to use the posterior expected
d errorcovariance matrix, denoted by Pn+1|n+1(pn) as the appropriate cost-to-go function.
The term “posterior” is used here to signify the fact that the cost-to-go function is to
be formulated “after” delivery of the feedback information from the receiver to the
transmitter. The second term “expected” is used to anticipate the filtering benefit to be
gained as a result of transition from the “old” pn−1 to the “new” pn for the perception–
First, examining (6.32), rewritten by substituting n + 1 for n, we see that the expected
Pn+1|n+1 does include Pn+1|n as its first term. Hence, point (1) is satisfied.
Second, the remaining term Gn+1Pyy,n+1|nGTn+1 in the formula for Pn+1|n+1 involves the
measurement yn+1; hence, the dependence of Pn+1|n+1 on pn is satisfied. But, we have to
establish that pn is the only unknown. To this end, we use (6.30) to write
where we have made use of the symmetric property that applies to the covariance
matrices on the right-hand side of (6.33). Now, we make two observations:
• From (6.25) and (6.28), we see that Pxy,n+1|n is independent of pn.
• From (6.26), we see that Pyy,n+1|n is indeed dependent on pn and, most importantly, this
dependence is limited solely to the measurement covariance matrix R(pn).
In conceptualized terms, we may now sum up the discussion up to this point as
Correspondingly, in words, we may make our second and final statement on the transition from perception in the receiver to action in the transmitter as follows:7
The posterior expected error-covariance matrix Pn+1|n+1 is dependent on the new waveformparameter vector pn for completing the perception–action cycle at time n and this dependence is
confined entirely to an additive term defined by the measurement-noise covariance matrix R(pn).
The sole dependence of Pn+1|n+1 on R(pn) makes the computation of pn at time n practically feasible.
Associating the posterior error vector dn+1|n+1(pn) with Pn+1|n+1(pn), we will, henceforth,
use g(ddn + 1|n + 1(pn)) as the cost-to-go function for the transmitter to optimize it with respect
to the unknown pn for preparing the perception–action cycle to go to the next cycle at
time n. In particular, we look upon the error dn+1|n+1 as the “most logical” random vector
to deliver information about the radar environment to the transmitter from the receiver. We
say so in light of what we know from, first, the state-space model and, second, optimal estimation of the state given the measurements at the receiver input, past as well as the present.
Our next task is to develop a formula for the cost-to-go function g(dn+1|n+1(pn)) that
is computationally optimizable with respect to the unknown waveform-parameter
vector pn. With this objective in mind, we will consider two criteria: one based on the
mean-square error and the other based on Shannon’s information theory.
Cost-to-go function using mean-square error
The posterior expected estimation error dn+1|n+ 1(pn), corresponding to the errorcovariance matrix Pn+1|n+1(pn), is defined by
Using the mean-square error criterion, our first cost-to-go function is thus given by
where the symbol ||·|| signifies the Euclidean norm. Using straightforward algebraic
manipulations, we may equivalently express the cost-to-go function in (6.35) in terms of
the Fisher information metric as follows:
where the operator Tr[.] denotes the trace of the enclosed matrix. By definition, the trace
of a matrix is equal to the sum of its diagonal terms, hence the simplicity of the formula
However, a serious limitation of the cost-to-go function defined in (6.36) is the fact
that it only accounts for diagonal terms of the posterior expected error-covariance
metric Pn+1|n+1(pn), which means that valuable information about the target contained
in the off-diagonal terms is ignored. In other words, the mean-square-error criterion
for the cost-to-go function violates the principle of information preservation that was
Cost-to-go function using Shannon’s entropy
To overcome the information-preserving limitation of the mean-square error criterion,
we next look to Shannon’s entropy as the measure of information content of a random
vector. To this end, we use the formula for the posterior estimation error vector dn+1|n+1(pn)
to define the second formula for cost-to-go function as follows:
= ∫ ε n++ |n +1 (θ n ) ( ε n +1|n +1 ( n ) d
where the integration is performed with respect to the error vector itself. The integral formula of (6.37) may be expressed in the following compact form (Cover and Thomas, 2006):
where the operator det[.] denotes determinantt of the enclosed matrix.
Now, we find that elements of the expected error-covariance matrix are all accounted for
in formulating the entropy-based cost-to-go function. This second formula of (6.38) for
the cost-to-go function, therefore, is superior to the first one of (6.36) in preserving all the
information content of the expected-error covariance matrix Pn+1|n+1(pn), but the improved
information preservation is attained at the cost of increased computational complexity.
Another information-theoretic viewpoint of the entropy-based cost-to-go function
Equation (6.37) for the cost-to-go function is defined in terms of Shannon’s entropy. It
is illuminating to reformulate the definition in terms of another concept in Shannon’s
information theory, known as the mutual information.8 To be specific, we would like
to know the mutual information between the posterior estimation error vector dn+1|n+1
and the related waveform parameter vector pn. This mutual information is equivalently
expressed as the difference between two conditional entropies, as follows:
The term on the left-hand side of (6.39) denotes the mutual information between dn+1|n+1
and pn, conditioned on the past history p0:n−1 that extends up to and including time n - 1.
The first term on the right-hand side of the equation defines the conditional entropy of
dn+1|n+1, given the past history p0:n−1. As for the second term, it denotes the conditional
entropy of dn+1|n+1 given the complete history p0:n up to and including time n in light
The first conditional entropic term in (6.39) may be ignored on account of the fact
that the past history p0:n−1 is already known at time n − 1 and, therefore, there is no new
information to be had. Accordingly, (6.39) reduces to the simplified form:
where the entropic term H ( n |n | θ :n ) is another way of writing H (
light of (6.41), we may now make the following statement:9
(θ :n )) is equivalent to maximizing the mutual
Minimizing the conditional entropy H ( n | n (θ
information between the posterior estimation error vector ε n+1|n+1 and the waveform-parameter
In expressing the cost-to-go function in terms of mutual information as in (6.41), we are,
in effect, stating that there is information gain about the radar environment to be had in
going from one cycle of the perception–action cycle to the next, and so on.
At this point in the discussion, in light of what we have already learned about perception
of the radar environment in the receiver and action in the transmitter, it is instructive that
we restructure the perception–action cycle of Figure 6.1 of the cognitive tracking radar
in the form of a cyclic directed information-flow graph, as depicted in Figure 6.4; this
graph follows from the state-space model of (6.7) and (6.8). Examination of this figure
with global feedback reveals two fundamental transmission paths, one being bottom up
(i.e. feedforward) and the other top down (i.e. feedback).
The bottom-up (i.e. forward) transmission path in Figure 6.4 embodies the “perception”
part of the perception–action cycle in the receiver. It begins with the measurement yn at
the receiver input, initiating the following sequence of computations all at time n:
• filtered estimation of the state xn, denoted by xˆ n|n , and associated error covariance
• predicted estimation of the future state xn + 1, denoted by xˆ n +1|n , and, finally,
• covariance matrix of the prediction error vector ε n+1|n , denoted by Pn+1|n.
The bottom-up transmission path, centered on the cubature Kalman filtering in the
receiver, is depicted on the right-hand side of Figure 6.4. Basically, this path of directed
information flow describes the perceptual dynamics of the receiver. Herein, it is assumed
that the radar environment is nonlinear, hence justifying the use of cubature Kalman
Figure 6.4. The basic cycle of directed information flow-graph, based on the state-space model
of (6.7) and (6.8). (a) Right-hand side: perceptual dynamics of the receiver, using bottom-up
processing of the measurement yn. Here, it is assumed that the radar environment is nonlinear;
hence the use of the CKF. (b) Left-hand side: executive dynamics of the transmitter, using topdown processing of the cost-to-go function, g(εn+1, n+1(pn)). The symbol Z-1 represents a bank of
The top-down (i.e. feedback) transmission path in Figure 6.4 embodies the “action”
part of the perception–action cycle in the transmitter. It begins with the feedback
information Pn+1|n at the transmitter input delivered by the receiver, which initiates
the following sequence of computations that look into the future by one time step:
• formulation of the cost-to-go function g ( n |n +1 (θ n )), defined in terms of the posterior state-estimation error vector ε n +1|n +1 with dependence on the “to be updated”
• optimization of the cost-to-go function, yielding the unknown pn, and, finally,
• setting the stage for a repeat of the perception–action cycle at time n + 1.
Most importantly, the cost-to-go function provides the transmitter with a measure of
how well the receiver is doing in extracting information about the environment from the
The top-down transmission path, centered on approximate dynamic programming, to
be discussed in the next section, is depicted on the left-hand side of Figure 6.4. Basically, this path describes the executive dynamics of the transmitter, a primary function of
which is to compute the “new” waveform-parameter vector pn at time n + 1.
Approximate dynamic programming for optimal control
To begin the discussion on indirect control of the receiver by the transmitter through
optimal selection of the waveform-parameter vector, we assume that the condition
for starting the perception–action cycle is defined by setting the measurement at the
receiver input at some initial value, denoted by y0. With time n denoting the nth cycle,
the perception–action cycle progresses forward iteratively in accordance with the cyclic
directed information flow-graph of Figure 6.4.
Before proceeding further with formulation of the procedure for transmit-waveform
selection, however, we have to resolve a practical problem in the following sense: the formulation of Bellman’s dynamic-programming algorithm presented in Chapter 5 not only
demands that the environment be Markovian, but also that the algorithm has perfect knowledge of the state. In reality, however, the transmitter of a radar tracker has an imperfectt estimate of the state reported to it by the receiver. Accordingly, we are faced with an imperfect
state-information problem that is subject to the state-space model of the radar environment.
To resolve this problem, we introduce an information vectorr that is available to the transmitter at time n, as discussed in Chapter 5. Specifically, the information vector is defined by
where, moving on beyond this initial condition, we have
From these two equations, we readily obtain the recursion
Equation (6.44) may be viewed as the descriptor of state evolution of a “new” dynamic
system with perfect-state information and, therefore, amenable to dynamic programming. According to (6.44), we have:
• In−1 is the old (past) value of the state;
• pn−1 is the transmit-waveform parameter vector selected at time n - 1; it is responsible
for generating the transmit waveform sˆn−1 ,
• the measurement yn is viewed as a random disturbance resulting from the control
policy in the transmitter, attributed to the waveform-parameter pn−1; and
Approximate dynamic programming for optimal control
Note that the terminology adopted in (6.44) is, in its own way, consistent with the system
At cycle time n, the control policy for waveform selection in the transmitter
seeks to find the set of best waveform parameters, for which the cost-to-go function
g(dn+1|n+1(pn)) is to be minimized in the time steps denoted by n : n + L − 1 for a rolling
horizon of L steps. Correspondingly, the control policy for this set of steps is denoted
+ −1} with the policy function μ ( n ) = θ n−
maps the feedback information into an action in the waveform library denoted by P. The
objective, therefore, is to find an optimal policy π n∗ at time n that updates the waveform
selection by solving the following minimization problem:
For example, using the cost-to-go function of (6.35), the expected mean-square error of
where it is understood that the posterior expected state estimate xˆ i 1||ii +1 is dependent on
Ii, xi+1, yi+1, and pi; the parameter vector pi is the only unknown to be determined. However, for an L-step dynamic-programming algorithm, we need to predict L steps ahead,
which means that accurate performance of the CKF in the receiver is of crucial importance to overall performance of the cognitive tracking radar.
Referring back to the top-down transmission path on the left-hand side in the
cyclic directed information-flow graph of Figure 6.4, the function of the approximate
dynamic-programming algorithm in the transmitter is to look into the future by one time
step and compute the “to-be-selected” parameter vector pn for the next cycle by minimizing the cost-to-go function g ( n |n +1 (θ n )) with respect to pn.
When, however, there is provision for a horizon looking a prescribed number of
steps into the future, then the dynamic-programming algorithm solves the optimization
problem through a recursive procedure starting at time n. Specifically, the dynamicprogramming algorithm is made up of two parts (Haykin et al., 2011):
is a fixed cost and i = n, . . ., n + L − 2. The Ti+1 in (6.47) is itself
As mentioned previously, P in (6.47) and (6.48) is the waveform library.
With (6.47) pertaining to the terminal point, (6.48) pertains to the intermediate points
that go backward in time from the terminal point in (L
policy, {μn∗ , . . .,, μn∗+ L 1} for optimal control is thus obtained in the following two-step
(1) The terminal point, defined in (6.47), is minimized for every possible value of the
+ −1 is substituted into calculation of the intermediate points defined
repeated until the optimal policy μn has been computed, at which point the controller
acts by selecting the waveform-parameter vector pn for signal transmission at time n,
and with it the next cycle in the perception–action cycle begins.
Unfortunately, this two-step procedure is computationally too demanding to be
feasible in practice. Therefore, we have to simplify the computational complexity of
the dynamic-programming algorithm by introducing two approximations, described
Step 1: cost-to-go function for compressing information about the
Inclusion of the state xi+1 under the expectation on the right-hand side of (6.48) speaks
for itself. As for the observable yi+1, its inclusion under the expectation is justified
on the following ground: in accordance with (6.31), the expected estimate xˆ i 1||ii +1
depends on yi+1, which itself depends nonlinearly on xi+1. Consequently, we have to get
around the computational difficulty of evaluating this expectation. To do this, information about the radar environment (contained in the measurements) is compressed by
looking to the cost-to-go function for minimization in the transmitter. For example,
opting for the mean-square error criterion, we use (6.36) to define the cost-to-go function, as shown by
As discussed previously, Pn +1, n +1 ( n ) is the posterior expected estimation errorcovariance matrix, which is dependent on the unknown parameter vector pn that is to be
Step 2: approximation in the measurement space
In a physical context, the state-space model of the radar environment across time is an
infinite-dimensional continuous-valued space with respect to both the state and measurement equations. Moreover, the dimension of this model grows exponentially with depth
of the optimization horizon L. Specifically, at each step of the dynamic-programming
algorithm, we need to examine an infinite number of possibilities such that the perfect
state information vector In can evolve to a new value on the next time step. To simplify
Approximate dynamic programming for optimal control
this very cumbersome computation, we apply the same approximation technique used
in developing the CKF in Section 6.5; that is, the expectation operation on the righthand side of (6.48) is approximated by using the third-degree cubature rule. According
to the CKF formulation, the predicted measurement at time i is Gaussian-distributed
with mean yˆ i 1|i as in (6.25) and covariance matrix Pyy,i+1|i as in (6.26). Therefore, the
expectation term on the right-hand side of (6.48) may now be redefined in terms of the
Note that, in the first line on the right-hand side of the equation, we have interchanged
the orders of expectation and trace operators, which is justified because they are both
linear. The integrand on the right-hand side of the second-line in (6.50) is now in a
form similar to that described in (6.17). Accordingly, using the cubature rule of (6.18)
to approximate this integral, we obtain the desired approximation in the measurement
Pi 1|i +1 ( yˆ i +1| + P1/ 2, i +1|1|i ( i )α i1/ 2 )⎟ , (6.51)
where Ny is the dimensionality of the measurement space. It is important to note that, in
(6.51), the posterior expected-error covariance matrix Pi 1|i +1 is expressed as a function of
the sum term yˆ i | P1/ 2, i 1|i ( i )α i1/ 2 in accordance with the cubature rule, and Pyyy
the square root of the measurement covariance matrix Pyyyy , |i| . As for the cubature points a i,
they are defined in accordance with (6.19), with 2N
points, since the approximation is being performed in the measurement space.
Finally, using the approximation of (6.51) based on the cubature rule, the dynamicprogramming algorithm in (6.47)–(6.49) is simplified into the following pair of equations:
for i = n, . . ., n + L − 2. In both (6.52) and (6.53), we have made use of the formula in
(6.36), assuming the use of the mean-square error criterion for the cost-to-go function. It
should also be reemphasized that the mean yˆ i 1|i is independent of the waveform parameter vector pi but that the covariance matrix Pyy, i+1|i is dependent on it, in accordance
To summarize, the terminal point (6.52) computes the cost-to-go function looking
L cycles into the future, where L is the prescribed depth of horizon. Then, starting
+ −1), the approximate dynamic-programming algorithm
described in (6.53) takes over by computing the sequence of cost-to-go functions by
working backward in time from the terminal point step by step until we arrive at the
cycle time n. At this point, the dynamic-programming computation is completed.
The approximate dynamic-programming algorithm of (6.52) and (6.53) includes
dynamic optimization of the cognitive radar tracker as a special case, for which there is
no provision of looking into the future; that is, L = 1. In this special case, the terminal
point in (6.52) defines the dynamic optimization algorithm, with only a single cost-to-go
function to be optimized. For example, using the mean-square error formula of (6.36) as
In words, the dynamic optimization algorithm encompasses the top-down (feedback)
transmission path, which starts with the cost-to-go function formulated in the transmitter at the current cycle, time n, and moves on to the waveform-selection process
from pn−1 to pn to prepare for the next cycle. Most importantly, the whole computation
is practically feasible in an on-line mannerr by virtue of setting the horizon depth L = 1.
The curse-of-dimensionality problem was first discussed in Chapter 4 in the context of nonlinear filtering, where there are only two spaces to consider: the state space and measurement
space. In cognitive radar, however, the problem is compounded in difficulty by two issues:
• the presence of three different spaces and
• predictive modeling, attributed to a horizon of depth into the future in the dynamicprogramming algorithm.
The three spaces that characterize cognitive tracking radar are as follows:
(1) State space. The system equation for this space describes evolution of the state xn
over time n. Naturally, the state space lies in the radar environment where the target
(2) Measurement space. The equation for this second space defines dependence of the
measurement yn on the state xn at time n. The measurement space lies in the receiver
where “perception” of the environment is carried out.
(3) Action space. It is in this space where “action” in the radar environment is performed
in response to feedback information delivered to the transmitter by the receiver.
Accordingly, the action space lies in the transmitter.
6.11 Two-dimensional grid for waveform library
In order to put these spaces into an analytic perspective, we introduce the following
Ng, the waveform-parameter grid size used for the control policy; and
L, the dynamic-programming horizon depth.
The last two parameters naturally relate to the action space. In general, complexity of
the dynamic-programming algorithm for finding the control policy in the transmitter is
(Ny, Nx) and the term N s2 is the number of matrix inversions needed for
computation of the posterior expected error-covariance matrix; the term 2N
number of cubature points in the measurement space used for computing the approximate summation term in (6.53). Herein, it is assumed that all individual optimizations
in each stage of the dynamic-programming algorithm are performed over the complete
We see from (6.55) that the main source of complexity lies in the dynamicprogramming algorithm due to the exponential growth of computations arising from the
horizon depth of L steps into the future; the dimensions Ny and Ng are involved in this
exponential growth. More specifically, at each stage of the dynamic-programming algorithm and for each cubature point in (6.53), a new search in the waveform library needs
to be performed. We refer to such a complete search of the waveform library as the global
search. As L increases, the level of computation becomes exponentially unsustainable,
and with it the curse-of-dimensionality problem comes into play in a serious way.
To mitigate this problem, we may have to reduce the horizon depth L to try to perform the optimization by searching a locall neighborhood of the cubature point in the
immediate past cycle. Moreover, a case could be made for an explore–exploit strategy
for waveform selection. In so doing, the dynamic-programming algorithm is constrained
to search for a limited-size neighborhood in the waveform-parameter grid that is centered on the old cubature point; this search would have to be repeated at each stage of
the dynamic-programming algorithm. The rationale for the explore–exploit strategy can
be justified on the grounds that a change in the way in which the receiver perceives
the environment is typically small from one cycle to the next. It follows, therefore,
that exploring a local neighborhood of the cubature point in the immediate past makes
Two-dimensional grid for waveform library
Discussion of the cognitive radar for target tracking would be incomplete without a
description of how a two-dimensional grid is used for the library of modulated waveforms in the transmitter. The grid is two-dimensional because the modulated waveform
has two parameters, namely the chirp rate l and envelope’s pulse duration b, as indicated
For practical reasons, the waveform parameter vector p is divided into a set of grid
points, with each grid point represented by a subset in the two-dimensional space of the
waveform library P. To elaborate, let l min and l max denote the minimum and maximum
values for the chirp rate l respectively; and let b min and b max denote the minimum
and maximum values of the envelope’s pulse duration respectively. These minimum and
maximum values are predetermined for the transmitter. Hence, the waveform library P
P = {λ ∈[ λ min : Δλ : λ max ], b ∈ [bmin : Δb : bmax
l and Δb in (6.56) denote the step sizes of the chirp rate and envelope’s pulse
The two-stage analysis–synthesis procedure for construction and deployment of the
A parameter vector, denoted by pn in the library P, is assigned to that particular
value of the posterior expected estimation-error covariance matrix Pn+1|n+1(pn) for
which the cost-to-go function is minimized. This computation involves the use of
(6.32), assisted by (6.23), (6.26), and (6.30). Here, we must reemphasize that evaluating the integrals in these four equations will require application of the cubature
This second stage of the procedure involves grid searching, where the task of the
transmitter is to search over the waveform library P with the objective of locating
the particular grid point θ*n , for which the trace or determinant of Pn+1|n+1(pn) attains
its minimum value, depending on the criterion adopted for the cost-to-go function.
With the optimum parameter vector θ*n so located, the task of grid searching in the
transmitter is terminated for the cycle in question.
Case study: tracking a falling object in space
Insofar as cognitive information processing is concerned, the primary purpose of
this case study is to demonstrate that information gain is obtained through use of
the perception–action cyclic mechanism of Figure 6.1 with a selectable transmit waveform. In practical radar terms, the information gain manifests itself in terms of improved
tracking accuracy, compared with the corresponding traditional active radar.
To be specific, we consider an extensively studied problem in the tracking community: the problem of target reentry in space that was first described by Athans et al.
(1968). More specifically, a ballistic target reenters the Earth’s atmosphere after having
traveled a long distance at high speed, which makes the remaining time to ground impact
relatively short. The goal of a tracking radar in this scenario is to intercept and track
Case study: tracking a falling object in space
Figure 6.5. Geometry of the falling target’s scenario in space.
the ballistic target. Figure 6.5 describes the underlying geometry of the falling target’s
scenario. In the reentry phase, two types of force are in effect:
• The most dominant force is drag, which is a function of speed and has a substantial
• The second force is due to gravity, which accelerates the target toward the center of
the Earth. This tracking problem is particularly difficult because the target’s dynamics
Under the combined influence of drag and gravity acting on the target, the following
differential equation governs its motion (Athans et al., 1968):
where x1, x2, and x3 are respectively the altitude, velocity, and ballistic coefficientt of the
three-dimensional state x. The ballistic coefficient depends on the target’s mass, shape,
cross-sectional area, and air density. The dots in the left-hand side of (6.57) denote
differentiation with respect to continuous time t. The term ξ ( 1 ) in the second line of
(6.57) is air density; it is modeled as an exponentially decaying function of the altitude
with the proportionality constant ξ0 = 1.754 and the exponential factor γ = 1.49 × 10−4.
In the second line of (6.57), the acceleration due to gravity g = 9.8 ms −2.
To convert (6.57) into a form that corresponds to the state-space model, we define the
The system equation for continuous time t can now be expressed by the first-order nonlinear differential equation
where the dot in x t denotes differentiation with respect to time t and f(
f .) is a known nonlinear function. Using the Euler approximation with a small integration step dd, we may
reformulate this differential equation in the desired discrete form:
where, as before, n denotes discrete time. In order to account for imperfections in the
“noiseless” system equation (6.59) that are caused by the combinations of lift force,
small variations in the ballistic coefficient, and spinning motion, we add the zero-mean
Gaussian system noise vn, obtaining the expanded noisy system equation:
The two new matrices F and G in (6.61) are defined as follows:
where d is the small integration step. The drag D(xn), expressed in terms of the three
components of the state vector xn, namely xl,n, x2,n, and x3,n at time n, is defined by
Case study: tracking a falling object in space
where ξ ( 1, n ) is the air density of (6.58) evaluated at time n. We assume that the system
noise vn is zero-mean Gaussian with the covariance matrix
The parameters q1 and q2 control the amount of system noise in target dynamics and
ballistic coefficient respectively. For the simulations to be presented, we used the values
q1 = 0.01, q2 = 0.0l, and d = 1 s. The system equation (6.60) for the object falling in
We use LFM with both up-sweep and down-sweep chirps and Gaussian pulse-amplitude
modulation, which composes the waveform library with the parameter vector
and grid step-sizes Δl = 10−5 and Δb = 1010. The bandwidth was set to be 5 MHz. The
0 dB SNR was set at 80 km. Referring to Figure 6.5, an X-band radar is fixed at the point
labeled (0, 0) and operated at a fixed carrier frequency of 10.4 GHz, with the speed of
electromagnetic-wave propagation c = 3 × 108 m/s. The depth of horizon for the cognitive tracking radar is fixed at two different values, L = 1 and L = 2.
• The traditional active radar was equipped with fixed waveform parameters: downsweep chirp rate and pulse duration l = 20 μs.
• The sampling rate was set to Ts = 100 ms and the simulations were conducted for
• The radar was located at height H = 30 m measured with respect to ground; and its
horizontal distance to the track (trajectory) M = 30 km.
• The measurement space is two-dimensional, consisting of range and range rate.
Denoting these two radar parameters by y1, n and y2, n at time n respectively, we write
where n 1,n and n 2,n are the two elements of measurement noise vector mn. For the
Gaussian-distributed measurement noise, we have
where the noise covariance matrix R(pn−1) is dependent on the waveform parameter
vector pn−1. Note also that with dimensionality of the measurement space being
two, Ny = 2 in the approximate formula for the dynamic programming algorithm
With the transmitter and the receiver being collocated, the received signal energy
depends inversely on the fourth power of the target range r. For this reason, the SNR of
the radar returns for the target observed at range r was modeled according to
where r0 denotes the range at which we have the 0 dB SNR. To reiterate, for the simulations, r0 was set to be 80 km.
The true initial state of the target was defined as
and the initial state estimate and its covariance matrix were respectively assumed to be
With simplified computational cost in mind, we used the ensemble-averaged root meansquare error (EA-RMSE) as the metric to evaluate performance of the fore-active
tracking radar, compared with the traditional active radar of fixed transmit-waveform.
where ( 1(, n) , x2( , n) ) and ( xˆ1,(, n) , xˆ2( , n) ) are respectively the true and estimated positions of
the target at time n in the kkth Monte Carlo run with n = 1, 2, . . ., and N is the total number
of simulation runs. In a similar manner, we may also define the EA-RMSE for range rate.
Here we show the simulation results for two different scenarios (Xue, 2010):
• The perception–action cycle of Figure 6.1 with selectable transmit-waveform. As
mentioned previously, this mechanism is the first step towards radar cognition. The
purpose of the case study is to demonstrate the information gain resulting from the
perception–action cycle as the first stage towards cognition.
• The corresponding traditional active radar with fixed transmit-waveform, which is
included so as to provide a frame of reference for the information gain.
Case study: tracking a falling object in space
With the curse-of-dimensionality problem as an issue of computational concern, we
study two cases: L = 1 for the special case of dynamic optimization and L = 2 for the case
of dynamic programming. Figures 6.6, 6.7, and 6.8 respectively plot the RMSEs for the
following three elements of target’s state: altitude, velocity, and ballistic coefficient, all
three of which occupy the same time scale.
Examination of the RMSE results plotted in Figures 6.6–6.8 leads us to make three
(1) In all three figures, the perception–action cyclic of Figure 6.1 outperforms the traditional active radar with fixed transmit-waveform by a large margin.
(2) For estimates of all three target parameters, altitude, velocity, and ballistic coefficient,
the trajectories computed for horizon depths L = 1, 2 appear to be essentially identical until we reach time n = 3 s. Thereafter, the use of dynamic programming for
L = 2 begins to increasingly outperform the use of dynamic optimization for L = 1.
Of course, this improvement in estimation accuracy is achieved at the expense of
(3) Tracking accuracies of the target altitude and target velocity follow roughly similar
trajectories, which appears to indicate that the choice of linear frequency modulation
combined with amplitude modulation is a good compromise for transmit-waveform
Figure 6.6. RMSE of altitude for: (a) traditional active radar with fixed waveform (TAR);
(b) perception–action cycle with dynamic optimization (PAC); (c) perception–action cycle
Figure 6.7. RMSE of velocity (range rate) for: (a) traditional active radar with fixed waveform
(TAR); (b) perception–action cycle with dynamic optimization (PAC); (c) perception–action
Figure 6.8. RMSE of the target ballistic coefficient for: (a) traditional active radar with fixed
waveform (TAR); (b) perception–action cycle with dynamic optimization (PAC); (c) perception–
action cycle with dynamic programming (PAC-DP).
The message to take from this case study is confirmation that there is significant information gain and accelerated rate of convergerce for range and range-rate estimetion to
be had by virtue of the perception–action cycle as the first stage towards radar cognition,
compared with the corresponding traditional active radar.
Cognitive radar with single layer of memory
Cognitive radar with single layer of memory
Now that we have thoroughly studied the practical benefit gained from the perception–
action cycle providing the first functional pillar for a radar to be cognitive, we next turn
our attention on the benefit to be gained from the second functional pillar for cognition:
To be specific, the memory to be considered consists of a single layer: perceptual
memory in the receiver, executive memory in the transmitter, and working memory.
Specifically, as shown in Figure 6.9, we have the following linkages:
(1) The perceptual memory is reciprocally coupled to the environmental scene analyzer
(2) The executive memory is reciprocally coupled to the environmental scene actuator in
(3) The working memory reciprocally couples the perceptual memory and executive
With the addition of memory, the cognitive radar of Figure 6.9 now satisfies all four
basic properties of cognition, as outlined in Section 6.1 and reiterated here for convenience of presentation:
• perception in the receiver followed by action in the transmitter through global feedback from the receiver to the transmitter;
Figure 6.9. Cognitive radar with single level (layer) of memory.
• memory configured to predict the consequences of action (i.e. transmit-waveform selection taken by the transmitter and state-equation’s parameter selection made by the receiver);
• memory-based attentional mechanism for the prioritized allocation of available
• decision-making mechanism for selecting intelligent choices in the face of environmental uncertainties.
As emphasized previously, intelligence builds on the perception–action cycle, memory
and attention; moreover, it relies on the many local and global feedback loops in the
Most importantly, we expect that the addition of memory to the perception–action
cycle of Figure 6.1 will contribute its own enhancement to the information-processing
power of the cognitive radar, which will be demonstrated experimentally later on in the
Cyclic directed information flow in cognitive radar with single layer of memory
Although the structure of the cognitive radar with one layer of memory in Figure 6.9
is much more elaborate than that of the perception–action cycle pictured in Figure 6.1,
it is logically intuitive to find that, insofar as evaluation of the feedback information is
concerned, these two structures are both governed by the same cyclic directed information flow of Figure 6.5 with a fundamental difference:
The successive information-processing cycles carried out by the triple-memory system, within their
own local and global feedback loops, have the resultant effect of always reducing the amount of feedback information passed to the transmitter by the receiver, continually from one cycle to the next.
To justify this profound statement, we make two related observations:
(1) The perception–action cycle acting alone
For prescribed nonlinear function a(.) and system noise covariance matrix in the
receiver and prescribed waveform library in the transmitter, the informationprocessing capability of the perception–action cycle of Figure 6.1 is powerful enough
to select the optimal transmit-waveform to illuminate the environment. However,
this optimum selection “may not be the best” using prescribed parameters, as the
system equation could be misaligned with respect to the actual state of the target.
(2) The perception–action cycle with added memory
With the addition of a library of nonlinear state-transition functions and system
noise variances assigned to the perceptual memory in the receiver in Figure 6.9,
d the resources (libraries) available to the transmitter and receiver
of the cognitive radar. Accordingly, through bottom-up and top-down adaptations
performed between the environmental scene analyzer and perceptual memory in
the receiver, and the corresponding bottom-up and top-down adaptations performed
between the environmental scene actuator and executive memory in the transmitter,
both being augmented by the reciprocal coupling between the perceptual memory
and executive memory through the working memory, the triple-memory system
Cognitive radar with single layer of memory
just described impacts the information-processing cycle of Figure 6.5 through two
• First, the CKF is enabled to optimize the filtered estimate of the state, given the
nonlinear function a(.) and system noise variance retrieved by the perceptual
memory, so as to match the incoming measurements.
• Second, the approximate dynamic programming optimizes retrieval of the transmitwaveform for the next cycle, given the subset of waveform library selected by the
Consequently, at each cycle in time in Figure 6.9, we find that the amount of feedback information sent from the receiver to the transmitter is successively reduced
as we proceed from one cycle to the next. In other words, the use of memory, an
inherent structural ingredient of cognitive radar, will always result in improved
tracking accuracy, albeit at the expense of increased system complexity.
Communication among subsystems in cognitive radar
To understand how the subsystems constituting the cognitive radar communicate with
each other, we may refer to Figure 6.10. The communication process in this figure starts
with the measurements received at the radar input from the environment, thereafter
proceeding along the following six steps:
Step 1. Through the bottom-up link, the receiver sends the current measurements to the
perceptual memory; the goal here is to retrieve the particular nonlinear function a(.
associated system noise variance that are the best match to the current measurements.
Figure 6.10. Communication flow-diagram of cognitive radar with single level of memory.
Step 2. Through the top-down link, the new parameters of the system model are fed
back to the environmental scene analyzer, thereby updating the system equation
(6.7) and, with it, the state-space model of the radar environment. Meanwhile, the
new features about the environment learned in the hidden layer of the perceptual
memory are sent as Information Ain to the working memory to be temporarily
Step 3. With the updated state-space model at hand in the environmental scene analyzer, the CKF is now ready to process the received measurements for computing
the predicted state-estimation error vector one step into the future. Step 3 is completed by computing the feedback information, based on this error vector, which
is sent to the environmental scene actuator in the transmitter by the environmental
Step 4. Through the bottom-up link, the feedback information is sent to the executive
memory; the goal here is to retrieve that particular subset of the transmit-waveform
library that is the best fit to the radar environment represented by the feedback
information. Meanwhile, the features about the feedback information and, therefore, the environment learned in the hidden layer of the executive memory are sent
as Information Binn to be temporarily stored in the working memory for Step 6.
Step 5. Through the top-down link, the new subset of waveform library is fed back
to the environmental scene actuator. Therein, the cost-to-go function is formulated, at which point in time the optimum transmit waveform is selected from
the retrieved set of waveform library by the dynamic-programming algorithm for
Step 6. Finally, with the features obtained from the hidden layer of the perceptual
memory as Information Ain and the corresponding features obtained from the
hidden layer of the executive memory as Information Bin, these two sets of features
are processed in the working memory, producing two outputs: the output Aout, corresponding to Information Ain, is sent to the perceptual memory; and the other
output Bout, corresponding to Information Bin, is sent to the executive memory as
the consequences of action taken in the receiver or transmitter.
Then, Steps 1–6 are repeated for the next cycle, and so on.
Communications between scene analyzer and perceptual memory
Irrespective of the application of interest, a cognitive radar’s perception of the environment is continually affected by the current measurements as well as cognitive information
about the environment stored in the perceptual memory. Stated in another way, we may say
that every percept (i.e. snapshot of the perception process) is made up of two components:
• The first component of the percept refers to recognition and, therefore, retrieval of a
set of nonlinear functions and associated system noise variances, which are stored in
the perceptual memory for the purpose of representing past measurements.
• The second component of the percept refers to categorization (classification) of
features in the new measurements that are matched
Cognitive radar with single layer of memory
The end result of these two components is the updated selection of nonlinear state-transition function and system noise variance that match the current measurements. In both
components, the processing is performed in a self-organized and synchronous manner.
Communications between environmental scene actuator and executive memory
Just as the receiver is continually influenced by incoming measurements, so it is with the
transmitter being continually influenced by the incoming feedback information about
the environment from the receiver. Stated in another way, every execution (i.e. snapshot
of the decision-making process) is made up of two components:
• The first component refers to recognition and, therefore, retrieval of a particular set
in the waveform library, which is stored in the executive memory for the purpose of
representing past values of feedback information from the receiver.
• The second component refers to categorization of the current feedback information
The end result is selection of that updated transmit-waveform in the waveform library
that matches the current value of feedback information. Here again, the processing in
both components is carried out in a self-organized and synchronous manner.
Communications within the triple-memory system
Finally, we come to the communication process carried out between the perceptual
memory in the receiver and the executive memory in the transmitter. This communication is carried out through the working memory, which is reciprocally coupled to the
perceptual memory on the one side and reciprocally coupled to the executive memory
on the other side. In effect, the working memory acts as on the “mediator” between the
Referring to Figure 6.10, we have Information Ain that is input into the working
memory from the perceptual memory. In light of what was described previously under
Step 2, this information represents the features learned in the hidden layer of the perceptual memory. In other words, Ain provides the working memory with information
about the environment, viewed directly through perception by the environmental scene
Moreover, we also have Information Binn that is input into the working memory from the
executive memory. In light of what was described under Step 4, this second information
represents the features learned in the hidden layer of the executive memory. In other
words, Binn provides the working memory with information about the radar environment,
viewed indirectly through the feedback linkk from the environmental scene analyzer.
It follows, therefore, that there will be matching
Bin. Accordingly, it is the function of the working memory to exploit this matching,
thereby producing the respective outputs, Aout and Bout. The outputs Aout and Bout are
fed back to the perceptual memory and the executive memory respectively, supplying
them both the relationships discovered: first, between the particular nonlinear function
a(.) and associated system noise variance selected in the receiver for perception of the
environment and, second, between the particular waveform selected in the transmitter
for action in the environment. In so doing, the working memory reports a pair of consequences for actions taken:
• One consequence for the perceptual memory is to have selected the function, a(.),
and system-noise variance as the possible match for the state equation (6.7), given the
• The other consequence for the executive memory is to have selected a particular
transmit waveform as the possible match for the incoming feedback information.
This twofold statement is in accord with the previous statement first made in Chapter 2:
the function of memory is to predict the “consequences of selection/action taken” by a
cognitive dynamic system and do so in a self-organized and synchronous manner.
Although the radar environment is common to both the receiver and transmitter,
they view it differently: the receiver perceives the environment directly through the
incoming measurements, whereas the transmitter views it indirectly through feedback information from the receiver. Obviously, then, there must be matching between
the environmental modeling process carried out in the perceptual memory and the
waveform-categorization process carried out in the executive memory. Through its
predictive capability, the function of the working memory is to maximize this matching
process on each cycle, so that action taken in the environment by the transmitter
matches perception of the environment by the receiver in a way better than it did on
Experimental validation of improved tracking performance attributed to memory
To validate the improvement in target-tracking accuracy resulting from addition of the
triple-memory system in Figure 6.9, we present the results herein of a difficult computer
experiment, in which a target performs a “coordinated turn” in a three-dimensional
Cartesian space. The dimensionality of the state-space model is seven, made up of range
and velocity in each of the three Cartesian coordinates plus the curvature of the coordinated turn itself; this radar scenario was discussed previously in Chapter 4. Three
scenarios are considered in the computer experiment (Xue, 2010):
• a traditional active radar with fixed transmit waveform;
• the perception–action cycle pictured in Figure 6.1 with selectable transmit-waveform,
representing the first step towards radar cognition; and
• the cognitive radar pictured in Figure 6.9.
• the initial range across the three Cartesian coordinates is [10 km, 10 km, 5 km]
• the corresponding initial range-rate is; and [100 km/s, 150 km/s, 0 m/s]
The results of the experiment are presented in Figure 6.11.
Cognitive radar with single layer of memory
Figure 6.11. RMSE for: (a) range; (b) range rate. TAR: traditional active radar with fixed
transmit waveform; PAC: perception–action cycle as the first stage towards radar cognition;
Figure 6.11a plots the RMSE for range estimation for the three scenarios: Figure 6.11b
plots the corresponding RMSE for range-rate (velocity) estimation. Examination of the
figure leads to the performance results summarized in Table 6.1.
From the graphical plots of Figure 6.11 and the summarizing results presented in
Table 6.1, we now make three observations:
(1) We see successive improvements in estimating the target range and velocity as we
proceed through the perception–action cycle of Figure 6.1 with selectable transmitwaveform, and then the cognitive radar of Figure 6.9, both compared with the corresponding traditional active radar with fixed transmit-waveform.
(2) The perception–action cycle improves the accuracy of range estimation more so
(3) On the other hand, the cognitive radar improves the accuracy of velocity estimation more profoundly than range estimation; that is, the opposite to that under
Moreover, convergence rates of estimating both range and range rate are accelerated
considerably, compared with the traditional active radar.
Table 6.1 Comparison of the three scenarios for tracking accuracy
(ii) Perception–action cycle with selectable
Intelligence for dealing with environmental uncertainties
In Section 6.1, following Chapter 2, we made the statement that to be cognitive a
radar has to embody four processes: the perception–action cycle, memory, attention, and intelligence. Among these four processes, intelligence is the most complex
process involved in cognition and the hardest one to define. The difficulty in pinning
down the role of intelligence in cognition is attributed to the fact that the other three
processes, perception, memory, and attention, make their individual contributions to
intelligence in varying degrees. Nevertheless, in Chapter 2, insofar as intelligence is
concerned, we did make the following statement, reiterated here for convenience of
Intelligence is the ability of a cognitive dynamic system to continually adjust itself through an
adaptive process by making the receiver respond to new changes in the environment so as to create
new forms of action and behavior in the transmitter.
This is a very profound statement; unfortunately, it cannot be proven mathematically,
not yet. An informative way to develop insight into the role of intelligence in cognitive
dynamic systems is through experimental means. In this context, the cognitive radar
with one level of memory is well suited for such a study.
Experimental demonstration of the information-processing power of
cognition in the presence of environmental disturbance
To explore the importance of cognition experimentally, we revisit the simple computer
experiment presented in Section 1.4 of the introductory chapter. As in that section, the
state-space model of the radar is described by the following pair of linear equations:
Compared to the experiment in Section 1.4, we have made only one “significant”
change: whereas the system noise vn was white and Gaussian in Section 1.4, its power
spectrum is now no longer constant; rather, its intensity changes with time. Despite
this difference, vn is still Gaussian distributed, which, in turn, means that we are still
dealing with a state-space model that remains to be linear and Gaussian. Accordingly,
use of the Kalman filter satisfies the perceptual needs of the receiver, as was the case in
With the state consisting of target range and range-rate, the transition matrix A in
6.14 Intelligence for dealing with environmental uncertainties
where T is the sampling period. Correspondingly, the covariance matrix of the system
where s 2 characterizes the “intensity” of system noise vn, which may vary with time; its
dimension is measured in (length)2/(time)3; see Bar-Shalom et al. (2001). Figure 6.12a
and b plots a sample waveform of the system noise and its bell-shaped amplitude
spectrum, which are respectively characterized as follows:
(1) The amplitude waveform includes a disturbance, whose duration extends from 2 to
4 s with intensity s 2 = 16; outside this duration, the system noise is essentially white
(2) The disturbance was generated by passing white Gaussian noise through a linear
filter with a Gaussian-shaped transfer function, thereby yielding the bell-shaped
amplitude spectrum plotted in Figure 6.11b.
Thus, as already mentioned, insofar as the distribution of the overall system noise is
Figure 6.12. (a) Waveform of system noise vn that includes a disturbance extending from 2 to
4 s. (b) Amplitude spectrum of the system noise vn.
Figure 6.13. Illustrating the impact of unexpected disturbance arising in three different radar
scenarios: traditional active radar (TAR), perception–action cycle (PAC), and cognitive radar.
The results presented in the figure pertain to: (a) range estimation; (b) range-rate estimation.
It is assumed that the target is traveling towards the radar along a straight line from a
distance of 3 km and with a speed of 200 m/s. Figure 6.13a and b plots the RMSE results
for range and range-rate respectively for all three scenarios as usual.
Examining the experimental results presented in Figure 6.13b, where the case of
range-rate estimation appears to be most sensitive to environmental uncertainties, three
(1) Tracking performance of the traditional active radar, with a prescribed system model
and a fixed transmit waveform, is severly sensitive to the environmental uncertainties occurring in the system noise vn; this deficiency in tracking performance is
attributed to a lack of intelligence in the radar system.
(2) The perception–action cycle, the first step towards radar cognition, appears to exhibit
• First, tracking performance remains sensitive to environmental uncertainties,
albeit to a smaller extent than in the traditional active radar; this slightly reduced
sensitivity is attributed to the “limited form of intelligence” facilitated by the
embodiment of global feedback around the environment.
• Second, once the disturbance comes to an end, the radar tracker is unable to
recover its trajectory prior to occurrence of the disturbance.
(3) Overall, the cognitive radar handles the presence of environmental uncertainties in the
system noise vn reasonably well; most importantly, the radar’s tracking trajectry provides a clear timing indication of where the disturbance actually occurred.
The conclusion to be drawn from these experimental observations can be summarized
The intelligent decision-making capability of cognitive radar in dealing with environmental
uncertainties is enhanced significantly by two processes built into the system design:
(1) The use of an integrated memory system consisting of perceptual memory, executive memory,
New phenomenon in cognitive radar: chattering
(2) The increased number of local and global feedback loops distributed throughout the cognitive radar, with the end result that the cognitive radar is capable of adjusting itself through an
adaptive process to withstand the presence of environmental uncertainties.
To elaborate on this statement, from feedback theory it is well known that feedback
d in the following sense: feedback can work for you or it can
work against you. In the context of this statement just made on intelligence, the distributed feedback loops under point (2) provide the basis for improved tracking performance, but, at the same time, they may raise the potential for instability (i.e. oscillatory
behavior). This is where the integrated use of memory appears to come to the rescue
by acting as the stabilizer. Although, as of yet, we have no mathematical proof for this
statement, the simulations presented in Figure 6.13b do justify the practical importance
of memory and intelligence in a cognitive radar, which is intuitively satisfying.
New phenomenon in cognitive radar: chattering
In the course of the computer experiment just described, a highly interesting phenomenon, called the chattering phenomenon, was discovered (Xue, 2010). This new phenomenon refers to “intensity enhancement” in random fluctuations in the measurement
noise waveforms observed in the estimation of range as well as range-rate, and the
observation applies to two scenarios: the cognitive radar and the first step to radar cognition (i.e. the perception–action cycle), both of which involve the use of feedback. The
interesting point is that, despite this enhancement in the measurement noise in both of
these scenarios, the tracking performance is improved over that of the traditional active
radar as it is chatter free due to the lack of global feedback. The observations just made
are substantiated by examining the measurement noise processes for the usual three scenarios (Xue, 2010): traditional active radar, the perception–action cycle of Figure 6.1,
and cognitive radar of Figure 6.9. Sample waveforms of the experimental results are
presented in Figure 6.14 for range estimation.
To explain the reasons behind the chattering phenomenon, we offer the following
(1) The measurement noise in the receiver is indirectly controlled
through the radar returns from the target, which is a fact regardless of whether the
radar is cognitive or not. Indeed, it is because of this fact that the measurement noise in
the state-space model of the radar is denoted by m(pn−1), where pn−1 is the waveformparameter vector responsible for generating the transmitted signal at time n − 1.
(2) The single feature that is common to the cognitive radar and its perception–action
cycle mechanism acting alone, and which distinguishes both of them from the traditional active radar, is global feedback from the receiver to the transmitter.
(3) The reason for a significantly more intensive chattering in cognitive radar (shown in
Figure 6.14c) than that in the corresponding perception–action cycle is attributed to
the fact that cognitive radar has more global feedback loops (embodying the environment) than the perception–action cycle.
Figure 6.14. The chattering phenomenon experienced in the estimation of range for (a) traditional
active radar (TAR), (b) perception–action cycle (PAC) for first stage towards radar cognition,
So, the conclusion to be drawn from these two points prompts us to make the following
The chattering phenomenon is an inherent property of “all” radars with global feedback, be they
Interestingly enough, the chattering phenomenon is also known to occur in ordinary
nonlinear feedback control systems (Utkin, 1992; Khalil, 2002; Habibi and Burton, 2003;
Utkin et al., 2009). However, the chattering phenomenon appears to operate differently in
cognitive radars than in ordinary closed-loop feedback control systems, as summarized
(1) In cognitive radar, the chattering phenomenon is attributed to measurement noise
observed in the receiver, which is indirectly affected by the controlling action of the
transmitter; most importantly, the chattering appears to be “harmless.”
(2) In ordinary nonlinear feedback control systems, on the other hand, the chattering
phenomenon is attributed to system noise; the phenomenon, therefore, is considered
to be “harmful,” as it leads to a low control accuracy.
One other point that should be emphasized, which is common to cognitive radar and
ordinary nonlinear feedback control systems: the actions taken in both cases must be
So, we conclude this discussion on the chattering phenomenon with the follow-up
The chattering effect is a physical phenomenon characterized by high-frequency and finiteamplitude oscillations that appear in a closed-loop feedback control system, where actions are
In the case of a cognitive radar, the action taken by the transmitter is certainly discontinuous by virtue of the fact that the transmit waveform is selected in a discontinuous
manner as the controller (i.e. approximate dynamic-programming algorithm) switches
from one grid point to another in the waveform library.
In light of what we now know about the information-processing power of cognition
applied to radar, albeit confined to a single layer of memory as in Figure 6.9, we may
further expand the structural complexity of cognitive radar by introducing multiscale
memory into the perception–action cycle building on the single layer of memory along
The multiscale distributed memory of this new cognitive radar is structured in
(1) Multiscale perceptual memory, which is reciprocally coupled with the environmental scene analyzer in the receiver.
(2) Multiscale executive memory, which is reciprocally coupled with the environmental
(3) Working memory, which couples the supervised layer of the perceptual memory
on the right-hand side of Figure 6.15 with the corresponding supervised layer
of the executive memory on the left-hand side of the figure. In so doing, the
perceptual memory and executive memory are enabled to operate in synchrony
with each other from one cycle to the next. In implementational terms, we may
think of the working memory as a heterogeneous associative memory that is activated for a relatively short period of time; this kind of memory was discussed in
Also, as explained in Chapter 2 on the perception–action cycle, a “deep” neural architecture for the distributed memory system permits the representation of a wide family
of signal-processing functions in a more compact fashion than is possible with a singlelayered “shallow” architecture. Simply stated:
The use of a multiscale memory makes it possible to trade off space for time, wherein features
extracted by the individual layers of memory become increasingly more abstract and, therefore,
easier to recognize as we go up in the hierarchy.
Just as importantly, as a result of using the complex multiscale memory system, the
numbers of global and local feedback loops distributed throughout the entire cognitive
radar tend to increase exponentially. In a corresponding way, the radar acquires multiple levels of cyclic directed information flows, which, in turn, strengthen the radar’s
capabilities of perception, memory, attention, and intelligence. In other words, the
information-processing power of a cognitive radar is enhanced with the increased depth
of the multiscale memory in Figure 6.15, up to a certain point.
Figure 6.15. Cognitive radar with multiscale memory. Acronyms: (1) Perceptual memory: PM;
unsupervised learning: PM-U1, and PM-U2; supervised learning: PM-S. (2) Executive memory:
EM; unsupervised learning: EM-U1, and EM-U2; supervised learning: EM-S.
Cyclic directed information flow in cognitive radar with multiscale memory
Following on from what we said previously in Section 6.8, the cyclic directed
information flow-graph of Figure 6.4 applies equally well to cognitive radar with
multiscale memory. Of course, we should bear in mind the fundamental difference covered under points (1) and (2) on page 200 between the cognitive radar with single-layer
memory in Figure 6.9 on the one hand and its own perception–action cyclic mechanism
in Figure 6.1 acting alone on the other hand. Those differences become even more pronounced in the case of cognitive radar with multiscale memory. Accordingly, we may go
on to state that, in terms of radar-tracking performance:
The estimation accuracy of range and range-rate will progressively improve with increasing depth
However, we may eventually reach a point in the memory expansion beyond which we
Other potential benefits of multiscale memory
Two other practical benefits, which we may expect to be gained from the increased
information-processing power of a cognitive radar with multiscale memory, are summarized here.
(1) Environmental disturbances. The occurrence of environmental uncertainties has
a direct effect on the system equation (6.7), thereby affecting the measurement
equation (6.8) on account of the dependence of the measurements on the state. When
the cognitive radar is embodied with multiple layers of memory as structured in
Figure 6.15, the cognitive radar is expected to deal progressively better with these
(2) Interrupted transmission. If the incoming measurements happen to suffer a temporal discontinuity for some unknown reason, then the perceptive capability of the
radar receiver is correspondingly interrupted. In a difficult situation of this kind, it
is possible for the working memory to step in and close the perception–action cycle,
thereby bridging the temporal gap in the measurements at the receiver input.
Features of features: distinctive characteristic of multiscale memory
Up to this point in the discussion on multiscale memory, an issue that has not received
the detailed attention it deserves is the following:
Why do we need multiscale perceptual and executive memories?
The simple answer to this fundamental question is the fact that the complexity of reallife radar returns (signals) is typically too difficult for a perceptual memory to capture
it in a single-layered neural network. To elaborate, radar returns are characterized by an
underlying sparse composition, which means that any given set of radar returns can be
represented by a relatively small number of descriptors, called features.
In the extraction of inherent features of radar returns, it turns out that the most effective
and computationally efficient perceptual memory structure is one based on a deep-belief
neural network. To be specific, network depth is considered to be highly desirable because
it provides the basis for trading off space for time (Bengio and LeCun, 2007). Accordingly, the bottom-up processing of radar returns in the perceptual memory proceeds as
• The first layer of the memory captures a “coarse” set of features characterizing the
• The second layer is stimulated by the features captured in the first layer, thereby
producing a “finer” representation of the radar returns.
• This procedure of extracting features of features (Selfridge, 1958) is continued in the
memory’s successive layers for as long as required.
In an information-theoretic sense, the objective behind the features-of-features strategy
is to find a low-entropy representation of the radar returns, and the lower the better. To elaborate on this statement, we know from the definition of entropy as a measure of information that for a random event to have low entropy, its probability of occurrence must
be close to unity (Shannon, 1948). It follows therefore that an output with low entropy
is easier to recognize then the original radar returns. Just as importantly, the featuresof-features strategy described herein can be realized by using an unsupervised learning
process, which is accounted for by the two so-called unsupervised layers in Figure 6.15.
From a practical perspective, however, it is important that the matching process just
described be optimal. To achieve this optimality, this requires the use of supervised learning,
which is the last stage in the features-of-features strategy as depicted in Figure 6.15. Most
importantly, the particular set of system-model parameters residing in the perceptual memory’s library and picked as a result of the optimal matching process is passed on to the
environmental scene analyzer in a top-down manner for the purpose of state estimation; the
job of the perceptual memory is then finished for the particular cycle under test.
Much of what has been said thus far on the multiscale perceptual memory in the
receiver applies equally well to the multiscale executive memory in the transmitter,
except for two minor modifications in conceptual terms:
(1) Feedback information from the receiver to the transmitter is substituted for the role
(2) Matching the transmit-waveform in the executive memory’s library to information
about the environment contained in the feedback process is substituted for matching
the system-model in the perceptual memory’s library to information about the targetstate contained in the radar returns.
Earlier in Section 6.10 dealing with the curse-of-dimensionality problem, we mentioned
the explore–exploit strategy as a way of easing the computational burden that arises usually when dynamic programming is employed in the transmitter for waveform selection.
But this strategy was mentioned therein in the case of a fore-active radar employing
the perception–action cycle all on its own (i.e. in the absence of memory). Now, in a
cognitive radar, memory is an integral part of the system. Building on the discussion
presented in the preceding section, the exploration part of the strategy involves selection
of a cognit (i.e., piece of knowledge), representing a small set of waveform vectors in
the transmitter’s library, so as to “closely” match the feedback information about the
environment on a Eudidean basis; this selection is made in immediate neighborhood
of the previously transmitted waveform. Then, this particular set of waveform vectors
is downloaded to the controller, where “optimality” is performed by picking the one
waveform vector that minimizes the cost-to-go function; this second part of the strategy
defines exploitation. In this way, a “global search” of the entire waveform library is
replaced with a “local search” confined to a subset of the whole library, thereby easing
the computational burden. With changes in execution being relatively slow, we expect
that evolution of the local search to be correspondingly smooth.
Clearly, the explore–exploit strategy can be equally applied to the matching process
aimed at the optimal selection of system-model parameters in the receiver.
The explore–exploit strategy, intended for easing the computational burden incurred in addressing
the matching needs of both the transmitter and receiver in cognitive radar, provides the algorithmic
mechanisms for top-down attention based on memory in both the transmitter and receiver.
Another important issue emerging from the material presented in Section 6.16 on cognitive radar is how to design a multiscale memory. To this end, we may follow the traditional approach of extracting feature representations by reducing the dimensionality of
the incoming radar returns as we successively move from one layer to the next. Such
an approach is exemplified by identity mapping (also referred to as autoencoding),
which was discussed in Chapter 2. The motivation behind this approach is redundancy
However, several findings on sensory information processing, reported recently
in the literature, advocate an entirely different approach, namely sparse coding. In
this new way of thinking, the preference is to go for overcomplete representations of
natural images. In reality, the radar returns from an object illuminated by the transmitter are just as natural as ordinary natural images. It follows, therefore, that there
is much to be gained from sparse coding in the design of multiscale memory. In so
doing, the analogy between cognitive radar and the visual brain becomes that much
To be specific on what is meant by sparse coding in sensory information processing,
we may make the following statement (Olshausen and Field, 1996):
The principle of sparse coding refers to a neural code, in which each sensory input of a nervous
system is represented by the strong activation of a relatively small number of neurons out of a
large population of neurons in the system.
From a practical perspective, the advantages of sparse coding include the following
• increased storage capacity in memories;
• representation of complex signals in a manner easier to recognize at higher levels of
memory by virtue of the fact that the features are more likely to be linearly separable
Before delving into a detailed account of sparse coding, it is instructive that we begin
with a brief exposé of the mathematical basics of sparseness (Donoho and Elad, 2003).
Suppose we are given an input signal x ´ RN and the issue of interest is to find a code
M matrix, denoting a dictionary of generating
elements defined by the set { k }k =1 , where w k ∈ RN . These generating elements (i.e.
columns of the matrix W) are normalized; that is, w Tk w k = 1 for all k; this normalization is done in order to prevent trivial solutions. However, there is no fixed relationship
the number of generating elements is larger than the dimensionality of the input signal
x. It is for this reason that the dictionary is said to be overcomplete, in the sense that
some of the generating elements are linearly dependent. As such, it would be improper
to refer to the generating elements as basis vectors; rather, they are referred to simply
With sparseness as the issue of interest, we would like to find the most sparse code z
for solving the following constrained optimization problem (Donoho and Elad, 2003):
Minimize the norm ||z||0 subject to x = Wz,
norm, highlighting nonzero entries in the code z. Unfortunately,
the solution to this problem requires a combinatorial search, which is computationally
intractable in high-dimensional spaces. To get around this difficulty, the problem statement in (6.72) is reformulated as an unconstrained optimization problem (Kavukcuoglu
where ||z||1 denotes the l 1 norm of the code z. In a mathematical sense, (6.73) may be
viewed as the closest convexication of the statement in (6.72). Convex optimization is well
studied in the literature (Boyd and Vandenberghe, 2004), hence the practical importance of
solving (6.73) over (6.72). In any event, given the tractability of (6.73) and intractability
of (6.72), what is rather surprising is the following statement (Donoho and Elad, 2003):
For certain dictionaries that are sufficiently sparse, solutions to (6.73) are the same as the solutions to (6.72).
This statement makes (6.73) as a basis for sparse coding all that more important.
To elaborate on (6.73): the function F (x, z; W) on the left-hand side of this equation
is the cost function to be minimized. This cost function is made up of two terms:
(1) With coding in mind, we may view the matrix product Wz as an approximate reconstruction, that is, a decoded version of the input signal x. Hence, the first term
|| x Wz ||2 , represents the energy of the decoding error, measuring how well the
(2) Explanation of the second term, l z||z||1, is more subtle. It is introduced to ensure
sparsity of the code z by penalizing nonzero values of the code units. To be more
specific, this second term may be viewed as a regularization term, with l z denoting
the regularization parameter, and the l1 norm ||z||1 performing the role of regularizer. As such, assigning a small value to the regularization parameter l z implies that
we have high confidence in the dictionary W. On the other hand, assigning a high
value to l z implies that we have high confidence in the code z that makes up for
low confidence in the dictionary W. Typically, the regularization parameter l z is
assigned a value somewhere between these two limiting conditions.
One last comment is in order. The learning algorithm obtained by minimizing the
cost function of (6.73) is essentially similar to the sparse coding algorithm described
With this brief mathematical introduction to sparse coding, our next task is to formulate a neural network structure for its implementation. The first approach we have
chosen to follow is based on the principle of the encoder–decoder, which we are
encouraged to do so by virtue of what we know about autoencoding (identity mapping), discussed in Section 2.8.2. Thus, recognizing that the cost function F(
in (6.73) only accounts for decoding error, we need to expand it so as to also account
for encoding error. To shed light on how this expansion could be accommodated, we
refer to Figure 6.16, which is motivated by bottom-up and top-down processing that
are a distinctive characteristic of perceptual and executive memories.
In Figure 6.16, referred to as SESM, we clearly see the following two operations:
(1) Bottom-up processing of the input signal x, which leads to the following formula for
Figure 6.16. The sparse encoding symmetric machine (SESM), embodying bottom-up processing
of the input signal x and top-down processing of the code z.
(2) Top-down processing of the code z, which yields the decoding-error energy
where z is a nonlinearly transformed version of the code z. For example, the transformation may be achieved by using the point-wise logistic function:
for some fixed gain a. This nonlinear transformation has the effect of limiting the
amplitude of each unit in the code z to lie inside the range [0, 1].
Putting the two pieces of bottom-up and top-down processing described in (6.74) and
(6.75) together, we may now formulate the overall cost function for sparse coding as
To describe the overall cost function L(W, z), we may identify the following:
• The sum of the first two terms in (6.76) represents a linear combination of the two
energy terms of encoding and decoding errors, with h as a learning-rate parameter.
• The third term in (6.76) is a rewrite of the regularization term introduced previously
_ difference: the code z in (6.73) is replaced by its nonlinearly transformed version z , defined in accordance with the activation function of (6.76). Specifically, the regularizer in (6.76) is defined by11
which assesses sparseness of the code z for a given input signal x by assigning a penalty that depends on how activation is distributed across units of the code z. These
representations in which activation is spread over many of the code units should be
penalized to a greater extent than those in which only a few code units carry the load
• The fourth term in (6.76) is an l1 regularization on the set of linear filters in the
encoder, so as to suppress noise in the signal vector x and push for individual localization of the filters.
The encoding- and decoding-error energy terms in (6.76) are respectively defined as
both of which couple the unknowns, W and z (through z), in their own individual ways.
It follows, therefore, that it is not possible to minimize the overall cost function L(W, z)
with respect to both W and z simultaneously. If the code z is known, then minimization of
this cost function is a convex optimization problem. On the other hand, if W is fixed, then
the optimum code z* may be computed by using gradient descent to minimize the cost
function with respect to z. Building on the twofold rationale just described, the learning
algorithm for computing the matrix W and code z is as follows (Ranzato et al., 2007):
(1) For a given input signal x and encoder-parameter setting W, the method of gradient
descent is used to obtain the optimal code z*.
(2) With both the input signal x and optimal code z* clamped using the values computed
in step 1, one step of gradient descent is enough to update the matrix W, as shown by
where ∂ ∂W is the gradient and a is the step-size parameter.
After this two-step training is completed, the sparse coding algorithm converges to a
state where we find the following twofold condition:
• the decoder produces a good reconstruction of the input signal x from a sparse
• the encoder predicts the optimal code through a simple feed-forward propagation process.
The last issue to be considered is that of building a multiscale memory, using the
SESM of Figure 6.16 as the basic functional module. To do so, we may use unsupervised learning and proceed along the features-of-features strategy that is descriptive
of autoencoding. Specifically, the features captured by the encoder of the first module
are applied as input to the second layer of the memory. This feedforward procedure is
continued for as many layers as required. Construction of the multiscale memory is
completed by adding a supervised classifier as the top layer of the hierarchy, using a
multilayer perceptron trained with the back-propagation algorithm.
From the discussion presented herein on the SESM, it is apparent that although there
are three unknowns to be considered (namely, the sparse code and the encoder and decoder
matrices), the SESM reduces them to two by virtue of the fact that the encoder and decoder
matrices are the transpose of each other in accordance with the encoding–decoding principle. Unfortunately, this computational advantage of the SESM is the source of its computational weakness, in that it is sensitive to the way in which the input data stream is
scaled. To illustrate, suppose that the input is scaled up by the factor K
maintain the sparse code unchanged, the encoder matrix would like to be scaled down by
K. Correspondingly, the decoder matrix would like to be scaled up by the same
factor K in order to perform the data reconstruction correctly. The net result is that the
SESM is non-robust with respect to scaling the input data stream and learns very slowly.
To overcome this limitation of the SESM, we look to another closely related sparse
The predictive sparse decomposition (PSD) model, first described by Kavukcuoglu et al.
(2008), overcomes the scaling problem by untying the algebraic relationship between
the encoder and decoder matrices. To be specific, whereas these two matrices in the
SESM are respectively denoted by W and WT, in the PSD model they are respectively
denoted by W and U. Thus, the encoder matrix U of dimensions M × N is now independent of the decoder matrix W of dimensions N × M.
In algorithmic terms, the PSD model follows the same mathematical steps as the
SESM up to and including (6.73). In particular, to make the learning process computationally efficient, the PSD model does the following:
A nonlinear regressor is trained to map the input vector x to sparse representations defined by the
To this end, consider the following nonlinear mapping vector:
where b is a bias vector. The hyperbolic tangent function on the right-hand side of
the equation is responsible for the nonlinearity, operating on its argument, element by
element. As for the new matrix G, it is a diagonal matrix whose gain coefficients are
adjustable so as to permit the mapping outputs in (6.80) to compensate for input-scaling
variations, given that data reconstruction is performed in the decoder with a filter matrix W
whose generating elements are all normalized.
denote the parameters that are to be learned by the encoder in a predictive manner. Let
where the function F (x, z; W) is defined in (6.73). We may then formulate the algorithmic
Make the prediction of the nonlinear regressor m (x; Pf) as close as possible to the optimal code z*.
A straightforward approach to this optimization is to carry it out after the optimal
code z* has been computed by solving (6.82). However, for a computationally
efficient learning process, it is preferable to optimize the encoder’s Pf and the
decoder matrix W jointly. To set the stage for this joint optimization, the cost function in (6.73) is expanded by including an additional term to represent the encoder,
where h is the learning-rate parameter. In effect, the additional term representing the
regression-error energy enforces the code z to be as close as possible to the nonlinear
With the cost function of (6.83) at hand, the learning process proceeds on the basis of
an on-line coordinate gradient-descent algorithm that alternates between two steps for
each training sample x as follows (Kavukcuoglu et al., 2008):
Step 1. Keeping the parameters Pf and W fixed in the encoder and decoder respectively,
F x, z; W, Pf) of (6.83) with respect to the code z,
starting from the initial condition supplied by the nonlinear regressor m (x, Pf).
Step 2. Using the optimal value of the code z computed in Step 1, update the parameters
Pf and W using the one-step gradient-descent method.
It appears that, after training the PSD model using this two-step learning algorithm,
the model provides a computationally fast and smooth approximation to the optimal
sparse representation, and yet it achieves better accuracy than exact sparse coding algorithms on visual object recognition tasks (Kavukcuoglu et al., 2008). Therefore, it may
be said that the PSD model is a method of choice for implementing multiscale memory.
One other issue of interest is how to infer the code z, once the learning process has
been completed. For an approximate inference, a single matrix-vector multiplication
suffices. To be specific: in light of (6.80) and (6.81), computation of the approximate
code is obtained by passing the input vector x through the nonlinear regressor denoted
by m (x, Pf). For a more accurate inference, we may use (6.83), which, however, is computationally more demanding.
For a radar to be cognitive in the full sense of the word as we know it in human cognitive
terms, the radar has to embody four fundamental processes (putting language aside):
(1) Perception of the environment by the environmental scene analyzer in the receiver,
followed by action taken in the environment by the environmental scene actuator
in the transmitter through global feedback from the receiver to the transmitter. The
resulting perception–action cycle manifests itself in information gain about the
target state, which will increase from one cycle to the next.
(2) Memory with one layer or more, distributed across the entire radar system. The function of memory is to predict the consequences of actions taken by the radar and do
so in a self-organized and adaptive manner; action examples include the selection
of waveform-parameter vector in the transmitter and the selection of parameters
characterizing the system equation in the receiver.
(3) Attention driven by memory. Its function is to prioritize the allocation of available
resources in accordance with the incoming streams of radar returns from the target
through memory-based algorithmic attentional mechanisms.
(4) Intelligence driven by perception, memory, and attention. Its function is to enable
the algorithmic control and decision-making mechanism in the transmitter to
identify intelligent choices in the face of environmental uncertainties. As such,
intelligence is by far the most profound of all the four fundamental processes
Cognitive functional integration across time
One of the objectives of the perception–action cycle in cognitive radar is to bridge time.
In this context, time plays three key roles in the operation of a cognitive radar, regardless
of how many layers of memory are built into its design:
(1) Time separates the incoming stimuli (i.e., radar returns) so as to guide the overall
(2) Time separates the stimuli responsible for perception of the environment by the
receiver from corresponding actions taken in the environment by the radar transmitter in response to feedback information from the receiver.
(3) Time separates the occurrence of sensory feedback in the receiver from further
To elaborate further, temporal organization of the radar system behavior hinges on
integration of the percepts and actions across time, as follows:
This temporal way in which the information-processing steps is performed in cognitive
radar is termed the functional integration across time, which is a cardinal property of
Given the nonlinear state-space model of (6.7) and (6.8), to satisfy optimality of perception in the receiver as well as optimality of control in the transmitter, we may consider
(1) CKF for perception (i.e. environmental scene analysis). In a nonlinear environment and under the Gaussian assumption, the CKF provides the “best” known
approximation to the optimal Bayesian filter in the radar receiver. Stated another
way, when we are confronted with nonlinear state estimation in a Gaussian
environment, the CKF provides a method of choice. Here, we are assuming that
the nonlinearity is beyond the reach of the EKF that is known for its computational
simplicity. Simply put, the CKF improves the state-estimation process over the
EKF at the cost of increased computational complexity, particularly when the state
(2) Approximate dynamic programming for control. Here again, when it comes to action
in the environment, approximate dynamic programming
for optimal control of the receiver by the transmitter indirectly via the environment.
To elaborate more, for optimal waveform selection in the transmitter in response
to feedback information from the receiver, we may not be able to do better than
Thus, when it comes to designing a cognitive radar in a nonlinear and Gaussian
environment, there is much to be said for cubature Kalman filtering for state estimation in the receiver and approximate dynamic programming for control in the transmitter. For a non-Gaussian environment, we may consider the use of Gaussian-sum
approximation for nonlinear filtering and look to parallel processing for implementing
Continuing on the concluding statements just made on state estimation and control that
are the dual of each other, we first recall that the cyclic directed information flow-graph
of Figure 6.4 was originally formulated for the perception–action cycle of Figure 6.1,
representing the first step towards radar cognition. As stressed along the way, this
same baseline flow graph applies to a cognitive radar, bearing in mind the following
The amount of feedback information (i.e., entropy), measured in accordance with Shannon’s
information theory, is progressively reduced as the number of layers in the memory of the cognitive radar is correspondingly increased.
In other words, in a tracking application, the tracking accuracy of the radar is progressively improved with increasing layers of memory in the cognitive radar. Equally well,
other information-processing capabilities of the radar are also progressively improved.12
Analogy between exploring radar environment via feedback information and
saccadic eye movements in the visual brain
Feedback information computed in the receiver of a cognitive radar plays a key role
in how the transmitter is enabled to explore a dynamic environment for tracking target
movements. To be more precise, the environmental scene actuator in the transmitter sees
the radar environment indirectly through the feedback information passed onto it by the
environmental scene analyzer in the receiver that has direct access to the observable
data. Most importantly, despite the fact that the feedback information link resides inside
a closed feedback loop, the cognitive radar remains stable.
The ability of the environmental scene analyzer to explore the environment via the
feedback information may be likened to the phenomenon of saccadic eye movements in
To elaborate, as explained by Morrone and Burr (2009), the discrete ballistic movements of the eyes, from one point of interest to another in a quick manner, direct our
eyes toward a visual target; yet, we are typically unaware of the actual eye movements.
Just as importantly, despite the continually saccadic eye movements, the vision is always
From these realities, therefore, it follows that here is another physical phenomenon to
justify the analogy between cognitive radar and the visual brain.
For the effective and computationally efficient implementation of perceptual memory for
cognitive radar, we need only to look at human vision and find that sparse coding plays
a key role in perceiving the outside world. In recent years, sparse coding has attracted
the attention of researchers on many fronts: vision, neural computation, mathematics,
and signal processing. The net result is that we now have a plethora of sparse-coding
procedures. In this chapter, we focused on two models, SESM and PSD, both of which
are rooted in signal processing and, therefore, easy to understand.
The SESM is simple to implement by virtue of the fact that the encoder’s filter
matrix is the transpose of the decoder’s filter matrix. However, this encoding–
decoding property is also responsible for the lack of robustness to scaling variations
The PSD model overcomes this limitation of the SESM by untying the algebraic
relationship between filter matrices in the encoder and decoder. Its algorithmic implementation is based on the minimization of a cost function that consists of two parts:
• the first part is essentially the cost function employed in Olshausen–Field’s sparse
• the second part is defined in terms of a nonlinear regressor that maps a patch of the
input data stream to sparse representation of that patch.
The net result is an algorithm that is computationally fast and accurate in performance.
We close the discussion on cognitive radar by viewing it in two different ways:
(1) Cognitive radar is a complex correlative-learning system, in which control-based
perception is performed in the receiver, perception-based control is performed in
the transmitter, feedback information links them together, and memory acts as the
stabilizer guarding against the cumulative destabilizing tendency of the local and
global feedback loops distributed across the cognitive radar.
(2) Cognitive radar is an intelligent decision-making system, capable of risk management by selecting intelligent choices in both the transmitter and receiver in the face
Putting these two viewpoints together, we conclude with a final statement that sums up
the information-processing power of cognition (Wicks, 2010):
If a radar system can be made to work by traditional methods, then cognition can make it work
Most importantly, cognitive information processing is software centric. As such, it can
be applied not only to a new radar system, but equally well to an old one.
In Kershaw and Evans’s (1994) classic paper, a theory was described for optimal
transmit-waveform selection for illuminating the environment by linking the
receiver to the transmitter. In so doing, a closed loop around the environment was
formed, whereby it became possible for the transmitter to exercise indirect control over the receiver via the environment. Although the computation described in
that paper was performed in an off-line manner, it may be justifiably argued that,
in mathematical terms, the Kershaw–Evans paper was the first formulation of a
fore-active radar; that is, the first step towards radar cognition. However, no such
statement was originally made in the paper, as cognition was not known in the radar
Another related publication that should be noted is the book by Guerci (2010).
This book, consisting of five chapters, covers material on two classes of radar:
multiple-input multiple-output (MIMO) radar and fully adaptive knowledge-based
radar. With regard to the latter, recognizing that knowledge is no substitute for
memory as emphasized in Chapter 2, the fully adaptive knowledge-based radar in
figures 5.1 of the book is basically a sophisticated fore-active radar; that is, it is the
first step towards cognition. In the final analysis, the fact of the matter is that, no
matter how fully adaptive a radar is, adaptivity, be it full or otherwise, will not result
For a radar to be cognitive, there has to be learningg through continued interactions with the environment; the learning process is embodied in memory, which
is an essential ingredient for the radar to be cognitive. Simply put, cognition implies
adaptivity, but not the other way around.
In a monostatic radar, the transmitter and receiver are collocated. On the other hand,
in a bistatic radar, they are located in different places; accordingly, the use of global
feedback in a bistatic radar requires the use of a physical link.
3. Controlled oscillator for waveform generator
Adaptive waveform generation in the transmitter may be accomplished in an on-line
manner through the use of a controlled oscillator. However, implementation of such
an approach may well be more complicated than the use of a waveform library that
Consider two functions of the variable x, denoted by f 1(x
where the equality holds if, and only if, we have
5. The Fisher information matrix and Cramér–Rao lower bound
In the estimation of a parameter vector p, we are usually interested in two properties:
• the covariance matrix of the estimation error.
θ̂ , the mean is simply the expectation of θ
ariance matrix is defined by the expectation of the outer product (θ θˆ )(θ θˆ )T ,
where the superscript denotes matrix transposition. Provided that the estimator is
unbiased, the Cramér–Rao bound establishes a lower bound on the error covariance
To elaborate, let p(x | p) denote the conditional probability density function of a
random vector X represented by the observed sample vector x given the parameter
vector p. The issue of interest is to estimate p given x. From Chapter 4, we recall that
this estimation can be carried out using the likelihood function
Specifically, the maximum likelihood estimate of p, denoted by θ
where log denotes the natural logarithm. Taking the partial derivative of the loglikelihood function log l (p | x) with respect to p, we get the score function:
A score function is said to be “good” if it is near zero; otherwise, it is said to be
“bad.” The covariance matrix of the score function is called the Fisher information
which, after some manipulation, leads to the following formula (Van Trees, 1968):
where E is the expectation operator. The information matrix J is named in honor of
Fisher, who originated the idea of maximum likelihood estimation.
Returning to the basic issue at hand, it turns out the error covariance matrix C of
where J−1 is the inverse of the Fisher information matrix. Correspondingly, the
mean-square error of θˆi , the ith element of the estimator θ
where (J−1)ii is the iith element of the inverse of the Fisher information matrix. This
bound defines the Cramér–Rao lower bound.
The time taken for the completion of a single perception–action cycle is made up
essentially of three terms: t target, the round trip to and from the unknown radar target;
t rec, the time taken for the state-estimation algorithm in the environmental scene analyzer (e.g. CKF) to reach a “steady-state value” for the state estimate; t trans, the time
taken for the environmental scene actuator (e.g. approximate dynamic-programming
algorithm) to complete the updating of the transmit-waveform parameter vector.
For convenience of mathematical presentation, two assumptions are made:
(1) Time is normalized by considering the cognition-cycle duration as one unit of
(2) If the transmitter illuminates the radar environment at time n − 1, then the corresponding measurement is made at the receiver input at time n, assuming a delay
of one time unit. In effect, the waveform-parameter vector pn−1, generated in the
transmitter at time n - 1, reaches the receiver input at time n.
Note that the sum term, t target + t rec + t trans, is upper bounded by the pulse-repetition
7. Statement on the transition from perception to action
The statement made on p. 182, on the transition from perception of the environment
carried out in the receiver to action performed in the transmitter in response to feedback information from the receiver, follows an insightful observation first made by
Kershaw and Evans (1994). In mathematical terms, the observation is traced to (6.26),
referring to the covariance matrix, Pyy,n|n−1, the dependence of which on the waveform-parameter vector pn−1 is confined entirely to the measurement-noise covariance matrix R(pn−1).
Consider a pair of random variables X and Y
information between these two random variables is defined by
∫−−∞ ∫−−∞ pX ,Y ( x, y ) ⎜⎝ log pX ( x ) pY ( y )⎟⎠ d x dy,
( , y) is the joint probability density function of X and Y
( ) are its two marginals. If the random variables X and Y are statistically independent, then
in which case the mutual information I (X; Y ) reduces to zero. In general, we have
The mutual information is related to entropy as follows:
H(X | Y ) is the conditional entropy of X given Y.
The entropy of X is defined in terms of its probability density function pX (x
where log denotes the logarithm to base 2, with information being measured in bits.
Moreover, entropy provides a measure of information.
Williams et al. (2007) describe another information-theoretic metric. This new
metric, termed the state conditional entropy, provides a measure of “uncertainty”
about the current state xn, given all the measurements up to and including time n.
Following through the steps described therein, it is shown that minimizing the conditional state entropy given past history of the measurements, y0:n, is equivalent to
maximizing the conditional mutual information between the state xn and measurement yn at time n, given past history of the measurements up to and including n - 1,
The concept of source coding based on redundancy originated in the 1948 classic
paper by Claude Shannon, which has formed the basis for information theory as we
know it today. That paper addressed many of the fundamental concepts that are of
profound importance to communication engineering.
In the years following Shannon’s paper, information theory aroused the research
interests of a few neuroscientists, including Horace Barlow. In a classic publication (Barlow, 1961), it was observed that, at later stages of sensory information
processing, neurons were less active than those at early stages. Influenced by
Shannon’s information theory on source coding, this phenomenon was referred to in
the 1961 paper as “redundancy reduction.”
However, recognizing that the brain has to decide upon actions in the face of
uncertainties, there is now a clear indication in neuroscience that probability theory
and statistical inference, basic to the Bayesian framework, are of profound importance, more so than Shannon’s information theory. It is in light of this new way of
thinking that in the follow-up paper (Barlow, 2001), entitled “Redundancy reduction
revisited,” the original hypothesis of redundancy reduction is said to be wrong, in
that compressed representations of stimuli are unsuitable for the brain, but right in
drawing attention to the fact that redundancy is still important in sensory information processing.
Referring to the decoding cost function described in (6.73), we see that the regularization term pertaining to the code z is defined in terms of its l1 norm, as shown by
From an applied mathematics perspective, use of the l1 norm is favored, as it lends
itself to convex optimization. However, from a sensory information-processing perspective, the preferred choice for the sparse-code regularizer is one that mimics
the prior distribution of linear-filter coefficients in the encoder applied to natural
images; such distributions are heavy-tailed.
Olshausen and Field (1996) chose the logarithmic expression log (1 + z2) because
it induces sparsity and lends itself to optimization. Later, it was discovered that this
distribution is the well-known Student-t distribution (Olshausen, 2011).
12. Cognitive radar with multilayer memory
For experimental results demonstrating the information-processing power of cognitive radar with multilayer memory using simulations, the reader is referred to
Interest in a new generation of engineering systems enabled with cognition, started with
cognitive radio, a term that was coined by Mitola and McGuire (1999). In that article, the
idea of cognitive radio was introduced within the software-defined radio (SDR) community.
Subsequently, Mitola (2000) elaborated on a so-called “radio knowledge representation language” in his own doctoral dissertation. Furthermore, in a short section entitled “Research
issues” at the end of his doctoral dissertation, Mitola went on to say the following:
‘How do cognitive radios learn best? merits attention’. The exploration of learning in cognitive
radio includes the internal tuning of parameters and the external structuring of the environment
to enhance machine learning. Since many aspects of wireless networks are artificial, they may be
adjusted to enhance machine learning. This thesis did not attempt to answer these questions, but
Then, in Haykin (2005a), the first journal paper on cognitive radio, detailed expositions
of signal processing, control, learning and adaptive processes, and game-theoretic
ideas that lie at the heart of cognitive radio were presented for the first time. Three fundamental cognitive tasks, embodying the perception–action cycle of cognitive radio,
• radio-scene analysis of the radio environment performed in the receiver;
• transmit-power control and dynamic spectrum management, both performed in the
• global feedback, enabling the transmitter to act and, therefore, control data transmission across the forward wireless (data) channel in light of information about the radio
environment fed back to it by the receiver.
In effect, emphasis in that 2005 paper was placed on cognitive radio as a “closed-loop
feedback control system” with practical benefits and the need for precautionary measures,
recognizing that feedback is a “double-edged sword.”
Since its inception about a decade ago, interest in cognitive radio, its theory and
applications, has grown exponentially. The driving force behind this exponential growth
Cognitive radio has the potential to mitigate the radio-spectrum underutilization problem in
With this engineering challenge in mind, we begin the study of cognitive radio with this
7.1 The spectrum-underutilization problem
The electromagnetic radio spectrum is a natural resource, the use of which by transmitters
and receivers is licensed by governments. In November 2002, the Federal Communications
Commission (FCC) published a report prepared by the Spectrum-Policy Task Force aimed
at improving the way in which this precious resource is managed in the USA (FCC, 2002).
The task force was made up of a team of high-level, multidisciplinary professional FCC
staff – economists, engineers, and attorneys – from across the commission’s bureaus and
offices. Among the task force’s major findings and recommendations, the second finding on
page 3 of the report is rather revealing in the context of spectrum utilization:
In many bands, spectrum access is a more significant problem than physical scarcity of spectrum,
in large part due to legacy command-and-control regulation that limits the ability of potential
Indeed, if we were to scan portions of the radio spectrum including the revenue-rich
(1) some frequency bands in the spectrum are largely unoccupied most of the time;
(2) some other frequency bands are only partially occupied; and
(3) the remaining frequency bands are heavily used.
The underutilization of the electromagnetic spectrum leads us to think of a new term
commonly called spectrum holes, for which we offer the following definition:
A spectrum hole is a band of frequencies assigned to a primary (licensed) user, but, at a particular
time and specific geographic location, the band is not being utilized by that user.
Spectrum utilization can be improved significantly by making it possible for a secondary
(cognitive radio) user (who is not being serviced) to access a spectrum hole unoccupied
by the primary (legacy) user at the right location and time in question. Cognitive radio
offers a new way of thinking on how to promote efficient use of the radio spectrum by
exploiting the existence of spectrum holes.
The spectrum-utilization efficiency of cognitive radio is assessed in the context of
(1) Accuracy and reliability, with which the spectrum holes are identified (detected).
(2) Computational speed, with which the spectrum-hole identification is accomplished.
(3) Management of resources, which involves the allocation of spectrum holes among
competing secondary users in the cognitive radio network.
(4) Coexistence of the cognitive radio network alongside the legacy radio network,
which will have to be accomplished in a harmonious manner for the good of all
Requirements (1) and (2) are responsibilities of receivers in the cognitive radio network,
while the transmitters in the network are responsible for requirement (3). As for requirement (4), with the legacy radio network having paid for the use of the spectrum and
legally approved by regulatory agencies, the responsibility for this last requirement rests
with the cognitive radio network, viewed as a system of systems.
Directed information flow in cognitive radio
With every cognitive dynamic system having its own characteristic perception–action
cycle, so it is with cognitive radio. However, before proceeding to address this issue,
it is instructive to come up with a definition for what we mean by a “user” in a radio
network. To this end, we first recognize that at each end of a wireless communication
channel we have a transceiver, which embodies a transmitter and a receiver combined
together as one whole unit. So, when we speak of a radio user, we offer the following
A user is a communication link that connects the transmitter of a transceiver at one end of
the link that is in communication with the receiver of another transceiver at the other end
of the link. Moreover, the term “secondary user” is adopted for a cognitive radio, so as to
distinguish it from the term “primary user”, which is reserved for a legacy (i.e. licensed)
Note the terms “primary user” and “secondary user” were used in the previous section,
Referring to Figure 7.1a, we see that on the right-hand side of the figure we have
an RX-CR unit whose cognitive function is radio scene analysis (RSA), where CR
stands for cognitive radio. On the left-hand side of the figure, at some remote location we have a TX-CR unit whose cognitive function is dynamic spectrum managementt (DSM) and transmit power controll (TPC). The RSA of the RX-CR unit senses the
radio environment with the objective of identifying spectrum holes (i.e. underutilized
subbands of the radio spectrum). This information is passed onto the TX-CR unit via
the feedback channel. At the same time, through its own RSA the TX-CR unit will have
identified the spectrum holes in its own specific neighborhood. The combined function
of the DSM and TPC in the TX-CR unit is to identify a spectrum hole that is common
to it as well as the RX-CR unit, through which transmission over a data channell in the
radio environment can be carried out. In this way, directed information flow across the
cognitive radio is established on a cycle-by-cycle basis.
The perception–action cycle in cognitive radio
What we have just described is the very essence of the perception–action cycle for
a communication link in cognitive radio; this cycle is depicted in Figure 7.1b, representing a subset of Figure 7.1a with a minor difference: the functional block labeled
“nonparametric spectrum estimation” in the receiver has been used in Figure 7.1b so as
to add more specificity to the notion of radio scene analysis.
From Chapter 2, we recall that, for a dynamic system to be cognitive, it has to embody
four distinct processes: perception, memory, attention, and intelligence. On the basis
of Figure 7.1b, we now address how these four processes are indeed satisfied, one
by one. In so doing, we will have not only justified the rationale for radio cognition,
but also paved the way for the material to be covered in subsequent sections of
7.2 Directed information flow in cognitive radio
Figure 7.1. (a) Directed-information flow in cognitive radio. DSM: dynamic spectrum manager;
TPC: transmit-power controller; RSA: radio-scene analyzer; RX: receiver; TX: transmitter; TX
CR: transmitter unit in the transceiver of cognitive radio; RX CR: receiver unit in the transceiver
of cognitive radio. (b) Perception–action cycle of cognitive radio unit.
By using nonparametric spectrum estimation for perception in the receiver, the task
of finding spectrum holes is achieved without having to formulate a “model” of
the radio environment; hence, we may bypass the possible need for perceptual
memory. In Chapter 3 we pointed out that spectrum estimation is an ill-posed
inverse problem, which, therefore, requires the use of regularization. In Chapter 3,
we also established that the MTM satisfies this requirement. Moreover, through
the use of time–space processing, MTM provides the means for identifying the
spectrum holes at a particular point in time as well as location in space. It is for
these two reasons and a few others to be elaborated on in Section 7.5 that we view
the MTM as a method of choice for perception (i.e. spectrum sensing) of the radio
The task of dynamic spectrum management, to be discussed in Section 7.16, relies
on the use of a learning process called Hebbian learning, inspired by the human
brain; Hebbian learning was discussed in some detail in Chapter 2. An important
characteristic of Hebbian learning is the inherent capability of self-organization.
Thus, the dynamic spectrum manager has the practical means to dynamically
choose and assign a set of appropriate links for communication to each cognitive-radio unit by learning the underlying environmental communication patterns.
Knowledge thus learned about the communication patterns of the primary users
in a radio network and, to some extent, those of other secondary users in the local
neighborhood is stored in memory. Moreover, the “synaptic” weights of a selforganized feature map are adaptively updated in response to new inputs, on a cycleby-cycle basis.
Looking at the block diagram of Figure 7.1b, we see that the dynamic spectrum manager is reciprocally coupledd to the transmit-power controller. Specifically, through the use of game-theoretic ideas, to be discussed in Section 7.9,
and by virtue of information received from the nonparametric spectrum estimator through the feedback channel about interference levels in chosen feedforward communication channels, the transmit-power controller is enabled to
adaptively adjust the transmitted radio signal, subject to prescribed constraints.
In this resource-allocation game, the cognitive radio acquires the ability to reach
To illustrate how the process of attention manifests itself in cognitive radio, consider the following example. A serious accident has occurred at some particular
point in time and specific location in space, thereby resulting in a “surge” in
wireless-communication traffic. By virtue of built-in space–time processing, the
nonparametric spectrum estimator sends information to the dynamic spectrum manager in Figure 7.1b, identifying which particular subbands of the radio spectrum
have become congested due to the accident. Furthermore, the dynamic spectrum
manager itself focuses its attention on those remaining subbands with lower interference levels. In so doing, communication over the newly found cognitive radio
link with users at both ends is maintained, bypassing the congested subbands.
Moreover, through its own self-organized learning process, the dynamic spectrum
manager builds a predictive modell of the radio environment. Using this model, the
cognitive radio is enabled to do the following:
• pay attention to communication patterns and
• predict the availability-duration of spectrum holes, which, in turn, determines the
predicted horizon of the transmit-power control mechanism.
As it is with human cognition, intelligence in cognitive radio builds itself on the processes of perception, memory, and attention, just described under points (1), (2), and
(3) respectively. To appreciate the importance of intelligence, consider a cognitive
radio network with multiple secondary users whose communication needs would
have to be accommodated in a satisfactory manner. Accordingly, the perception–
action cycle of Figure 7.1b would have to be expanded in a corresponding way, such
• the radio environment for their individual forward communication needs and
• separate wireless channel for their individual feedback requirements.
In such a scenario, intelligence manifests itself as follows:
Through a decision-making mechanism involving intelligent choices in the transmitter, the
available resources (i.e. spectrum holes and battery power) are equitably assigned to the secondary users in accordance with a prescribed protocol in the face of environmental uncertainties, and in such a way that the transmit-power of each user does not exceed a prescribed limit.
The environmental uncertainties include the reality that spectrum holes come and
go in some stochastic manner, which may, therefore, mandate robustification of the
transmit-power controller, an issue that is discussed in Section 7.13.
From the rationale just presented under points (1)–(4), it is now apparent that the four
cognitive processes involved in radio for communication are satisfied quite differently
from those in radar for remote sensing. This difference should not be surprising, bearing
• cognitive radio and cognitive radar are intended for entirely different applications and
• the transmitter and receiver are located in different places in cognitive radio, whereas
they are commonly collocated in cognitive radar.
As noted above, a cognitive radio networkk has to accommodate the individual communication needs of multiple secondary users. In so doing, significant complexity is
appended to the functional operation of the network over and above that of cognitive
radio. To be more precise, the cognitive radio network is no longer a system; rather, it
assumes the form of a system of systems, demanding brand new practical considerations
that will be addressed in forthcoming sections of the chapter. The purpose of this section
is also to pave the way forward for those considerations.
In a cognitive radio network, there will naturally be a multiplicity of perception–
action cycles that go on simultaneously across a common radio environment, with
one cycle for each secondary user (i.e. communication link connecting a pair of
cognitive radio transceivers). We may, therefore, think of a cognitive radio network as
an ensemble of perception–action cycles, with the term “ensemble” being used here
in a stochastic sense. What makes the study of such an ensemble not only fascinating
in theoretical terms, but also challenging in practical terms, is the following list of
(1) The spectrum holes come and go. This issue arises, because, in reality, a spectrum
hole is a subband that is actually owned by a licensed primary user who happens not
to need that particular subband for its own employment at a certain point in time
and location in space. Legally speaking, therefore, the subband in question must
be given up by the secondary user in the cognitive radio network if, and when, the
primary user demands it back for its own specific need.
(2) Time delay that is incurred in the feedback channel. This second issue arises because
information extracted about the environment in the RX-CR unitt naturally requires
some time for transmission across the feedback channel to the TX-CR unit. Moreover,
there is variability in this time delay from one perception–action cycle to another.
(3) Security of the network. This third issue is needed to protect the communication
privacy of each secondary user in the face of attacks applied to the network by
malicious users at random points in place and time.
Over and above these practical issues, there are, of course, propagation effects that also
arise in the forward transmission across the data channel. From an analytic point of
view, we may argue (with some justification) that this latter issue can be taken care of
through the use of well-known diversity techniques (Molisch, 2005).
With this kind of provision in place, in this chapter we focus on issues (1) and (2).
Issue (3) on security is beyond the scope of this chapter; nevertheless, Chapter 8 includes
a brief discussion of security under “Unexplored issues.”
Previously, we referred to a cognitive radio network as a system of systems. To be
more precise, we may think of it as a complex, stochastic, multiloop feedback control
• complexity is inherited from the network itself,
• stochasticity is due to issues (1), (2), and (3) addressed above, and
• multiloop feedback controll is attributed to the feedback channel from the RX-CR user
to the TX-CR user for each communication link in the network.
From this brief characterization of a cognitive radio network, it is apparent that control
plays a dominant role in the study of such networks. In this context, therefore, we may
look to control as a fundamental process that is common not only to cognitive radio, but
also cognitive radar that was considered in Chapter 6. Indeed, we may generalize this
profound statement by going one step further:
Control is at the heart of all cognitive dynamic systems.
But, then, recognizing that control (applied in the transmitter) implies perception
(performed in the receiver), we are simply reemphasizing the critical role of the
Summing up, the overall objective of a cognitive radio network is:
Reliable communication among secondary users in the cognitive radio network, whenever and
Moreover, recognizing the temporary availability of spectrum holes, the network must
have the agility to realize this important objective as fast as practically feasible.
With spectrum holes playing a critical role in the underlying theory and design of
cognitive radio networks, the key question is: where are they to be found in the radio
spectrum? To set the stage for addressing this basic question, it is instructive to categorize the subbands of the radio spectrum in terms of occupancy:
(1) White spaces, which are free of RF interferers, except for noise due to natural and/
(2) Gray spaces, which are partially occupied by interferers and noise.
(3) Black spaces, the contents of which are completely full due to the combined presence
of communication among primary users of the legacy radio network and (possibly)
The first place where we may find spectrum holes is the television (TV) band. The
transition of all terrestrial television broadcasting from analog to digital, using the
ATSC (Advanced Television Systems Committee) Standard, was accomplished in 2009
in North America. Moreover, in November 2008, the FCC in the USA ruled that access
to the ATSC-digital television (DTV) band be permitted for wireless devices. Thus, for
the first time ever, the way was opened in 2009 for the creation of “white spaces” for use
by low-power cognitive radios. The availability of these white spaces will naturally vary
across time and from one geographic location to another. In reality, however, noise is not
likely to be the sole occupant of the ATSC-DTV band when a TV broadcasting station is
switched off. Rather, interfering signals of widely varying power levels do exist below
the DTV pilot. In other words, some of the subbands constituting the ATSC-DTV band
may indeed be actually “gray,” not “white”.1
Consider next the commercial cellular networks deployed all over the world. In the
current licensing regime, only primary users have exclusive rights to transmit. However, it is highly likely to find small spatial footprints in large cells where there are no
primary users. Currently, opportunistic low-power usage of the cellular spectrum is not
allowed in these areas, even though such usage by cognitive radios in a femtocelll with
a small base station is not detrimental to the primary userr (Buddhiko, 2007). Thus,
spectrum holes may also be found in commercial cellular bands; naturally, spread of
the spectrum holes varies over time and space. In any event, account has to be taken of
interference arising from conflict relationships between transmitters (base stations) of
various radio infrastructure providers that coexist in a region (Subramanian and Gupta,
2007). Consequently, the spectrum holes found in cellular bands may also nott all be
The important point to take from this discussion is that, regardless of where the spectrum holes exist, be they in the ATSC-DTV band or cellular band, we are confronted with
the practical reality that the spectrum holes may be made up of white and gray spaces.
This possibility may, therefore, complicate applicability of a simple hypothesis-testing
procedure that designates each subband as black (blocked space) or white (exploitable)
space, using energy detection or cyclostationarity characterization.
The detection of spectrum holes is a difficult signal-processing problem. Unfortunately,
this problem is made that much more difficult due to signal fadingg that manifests itself
in two ways: multipath and shadowing (Molisch, et al. 2009).
Over a short distance scale (separating the receiver from the transmitter) comparable
to one wavelength, we find significant fluctuations in the received signal power, which
are attributed to the multipath phenomenon. This phenomenon arises due to the fact that
the receiver’s antenna picks up multiple reflections of the transmitted signal, with each
reflected component having its own amplitude, phase, and delay. These components
tend to interfere with each other in a constructive or destructive manner, depending on
how the antenna is positioned. This first source of fading is referred to as small-scale
fading. In statistical terms, small-scale fading is described by a Rayleigh distribution if
there is no line of sight from the receiver to the transmitter and by a Rician distribution
if a line of sight does exist (Molisch, 2005). The special case of Rayleigh distribution is
The shadowing phenomenon arises due to the presence of major obstacles (e.g. hills,
buildings, and foliage) along the radio propagation between the transmitter and receiver.
On account of the physical size of these obstacles, there can be significant variations
in the received signal power, with the distance scale being in the order of hundreds of
wavelengths. For this reason, shadowing is referred to as large-scale fading. In statistical terms, large-scale fading is described by a log-normal distribution (Molisch, 2005).
To illustrate how challenging the detection of spectrum holes can be in practice, we consider the spectrum-sensing requirements involved in using cognitive radio for wireless
The wireless microphones that operate in the TV white space typically operate in
the ultrahigh frequency (UHF) band and can be tuned over a range of frequencies.
They must operate on a multiple of 25 kHz from the edge of a TV channel; but with a
local oscillator (LO) frequency offset, they can be anywhere within the TV channel in
practice. To ensure that a TV white space does not cause harmful interference to the
wireless microphone receiver, the TV white-space-sensingg device must cope with very
weak signals. In the FCC (2008) report and order, the spectrum-sensing threshold
was set at –114 dBm, where dBm stands for average power expressed in decibels with
1 mW as the frame of reference. However, recently the FCC issued a second memorandum opinion and order (FCC, 2010) in which the sensing threshold was changed to
–107 dBm. This power level is referenced to a 0 dBi sensing antenna (i.e. an isotropic
antenna). If the gain of the antenna is less than 0 dBi, then the sensing threshold must
The noise power in a 6 MHz TV channel is –106 dBm plus the noise figure of the
receiver, which, for low-cost devices, could be as high as 10 dB. For such a low-cost
consumer device with a 10 dB noise figure, the noise floor is approximately –96 dBm. In
that case the SNR of the wireless microphone signal is –11 dBm, and can be lower if the
antenna gain is less than 0 dBi. In addition to thermal noise at the receiver input, there is
also man-made noise in the UHF band due to out-of-band spurious emissions; this manmade noise adds to thermal noise of the receiver. The termal noise is well modeled as
additive white Gaussian noise (AWGN); this mathematically tractable model is not true
for the case of man-made noise, which can make sensing for weak wireless microphones
Wireless microphone signals are narrowband signals with a maximum bandwidth
of 200 kHz as specified by the FCC and with a typical bandwidth of 100 kHz or less.
These narrowband signals are subject to multipath fading. Over a small bandwidth the
fading is typically flat; so, the fading phenomenon experienced by the wireless microphone can be considered to be Rayleigh fading. In any event, the FCC has not specified
if TV white-space-sensing devices are required to operate at an average power level of
–107 dBm or at an instantaneous power level of –107 dBm. If sensing is only required
at the instantaneous power level of –107 dBm, and when the wireless microphone signal
fades below −107 dBm, then the TV white-space-sensing device is not required to detect
the wireless microphone signal. However, if the FCC is going to require sensing at an
average power level of −107 dBm, then when the wireless microphone fades due to multipath, the need for sensing would still be required; future FCC testing plans will clarify
this issue. In any case, sensing techniques that are robust in the presence of multipath
fading are highly desirable and may ultimately be required by the FCC.
In light of these practical realities we discussed above, we may now identify the desirable attributes of a spectrum sensor for cognitive-radio applications:
(1) Detection of spectrum holes and their reliable classification into white and gray
spaces; this classification may require an accurate estimation of the power spectrum,
particularly when the spectrum hole is of a gray-space kind.
(2) Accurate spectral resolution of spectrum holes, which is needed for efficient utilization of the radio spectrum; after all, this efficient utilization is the driving force
(3) Estimation of direction-of-arrival (DoA) of interferers, which provides the cognitive
(4) Time–frequency analysis for highlighting cyclostationarity, which could be used
as an additional method for the reinforcement of spectrum-hole detection and
also modulated-signal classification when the subband of interest is occupied by a
What choice do we have for spectrum sensing in cognitive radio, which collectively satisfies this list of desirable attributes? One choice could be to opt for model-dependent
energy detection, which is known for its simplicity. Unfortunately, energy detection is
not only limited in scope but, most seriously, it lacks robustness. A recommended choice
is the multitaper method considered next.
The subject of spectrum sensing of the radio environment was discussed at some
length in Chapter 3. For reasons explained in that chapter, we stressed the adoption
of a nonparametric approach as it avoids the need for modeling the RF data. Moreover, we argued in favor of the MTM as a method of choice for designing the coherent
multifunction spectrum sensor. To recap on the rationale for this choice, it is apropos
that we remind ourselves of the desirable features of the MTM:
(1) In multitaper spectral estimation, the bias is decomposed into two quantifiable
• local bias, due to frequency components residing inside the user-selectable band
• broadband bias, due to frequency components found outside this band.
(2) The resolution of a multitaper spectral estimator is naturally defined by the bandwidth of the passband, namely 2W.
(3) Multitaper spectral estimators offer an easy-to-quantify tradeoff between bias and
variance; accordingly, the bias–variance dilemma is replaced by the variance–
(4) Direct spectrum estimation can be performed with more than just two DoFs; typically, the DoFs vary from 6 to 10, depending on the time–bandwidth product.
(5) The MTM has built-in regularization, in that it provides an analytic basis for computing the best approximation to a desired power spectrum, which is not possible
from observable data alone. Moreover, the MTM is robust not only in estimating the
power spectrum but also in hypothesis testing for the detection of spectrum holes
(6) Through the inclusion of a multiple array of antennas on receive, the MTM acquires
a space–time processing capability, whereby information on the state of the radio
environment (i.e. spectrum holes) in time as well as space can be computed; as discussed previously in Section 7.2, this kind of information can be of practical importance to the cognitive process on attention in cognitive radio.
(7) The MTM provides a rigorous mathematical basis for computing the cyclostationary
characteristic of incoming radio signals, which can be exploited for identifying the
legacy user responsible for occupying a subband of the radio spectrum of specific
interest; as such, cyclostationarity can provide another way of further enhancing the
(8) Last, but by no means least, multitaper spectral estimates can be used to distinguish
spectral line components within the band (f
F-test, which was discussed in Chapter 3.
Reliability, efficiency and computational issues
Cognitive radio applications require reliable identification of spectrum holes that
are observable directly from the incoming RF data. The term “reliable’’ can be interpreted in many ways, but we use it to mean that the estimate should have a low
and quantifiable bias plus low variance. The term ‘‘reliable’’ can also be interpreted
as ‘‘robust,’’ in the sense that the estimate should not depend on whether the data
have a Gaussian, Laplacian, or any other possible distribution, it should not matter
too much if the data are ‘‘random’’ or contain deterministic components, and the
estimate is relatively tolerant of outliers. Simultaneously, identification of spectrum
holes should also make efficientt use of the incoming RF data: there is little point in
attempting to estimate a spectrum to check what bands are available if it requires
such a large sample size that the spectrum changes significantly while the data are
being acquired. Finally, the computations involved in identifying the spectrum holes
should be performed rapidly, recognizing that the time-scale of operations in the
cognitive radio network is relatively short compared with that of the legacy radio
What we have just described here is that reliable, efficient, and rapid identification
of spectrum holes is a tough and challenging task. The MTM satisfies the requirements
of reliability and efficiency by virtue of properties (1)–(6). Moreover, from a computational perspective, multitaper power spectrum estimators can be fast:2
By precomputing the Slepian tapers and using the state-of-the-art FFT algorithm, computation of
the MTM for spectrum sensing can be accomplished in a matter of 5–20 μs, which is relatively
As discussed in Chapter 3, the Slepian tapers are at the heart of the MTM. We may,
therefore, go on to say that the MTM is well suited for cognitive radio applications on
(1) In practical terms, the MTM is a reliable, efficient, and computationally fast method
(2) In theoretical terms, the MTM provides a mathematical basis not only for spectrum estimation, but also space–time processing, time–frequency analysis, and
The issues, just mentioned under point (2), were all discussed in Chapter 3.
In cognitive radio applications, the requirement is for the receiver of a cognitive radio
transceiver to make estimates of the average power of incoming RF data rapidly; hence,
estimating the spectrum on short data blocks (each consisting of N samples, say) is a
priority. Moreover, reliable spectrum sensing for cognitive radio applications is complicatedd not only by the many characteristics of wireless communication signals, but also
by interfering signals, channel fading, and receiver noise, all of which make the task of
spectrum sensing a highly demanding task.
To illustrate the applicability of the MTM to cognitive radio, we consider the DTV
band in this first case study of the chapter.
Spectral characteristics of ATSC-DTV signal
The ATSC-DTV is an eight-level vestigial sideband (VSB) modulated signall used for terrestrial broadcasting that can deliver a Moving Picture Expert Group-2 Transport Stream (MPEG2-TS) of up to 19.39 megabits per second (Mbps) in a 6 MHz channel with 10.72 mega-symbols
per second (over-the-air medium). A pilot is inserted into the signal at 2.69 MHz below the
center of the signal (i.e., one-quarter of the symbol rate from the signal center). Typically,
there are also several other narrowband signals found in the analysis bandwidth, from different
sources of interference, which complicate the spectrum-sensing problem even more.
Using the MTM, we may generate a reasonable estimate of the available spectrum
To this end, Figure 7.2 shows the minimum, average, and maximum of 20 multitaper
estimates of the ATSC-DTV power spectrum. Here, the block length used was N = 2200
Figure 7.2. Multitaper estimates of the spectrum of the ATSC-DTV signal. Each estimate was
made using Co = 6.5 with K = 11 tapers. In this example, N = 2200, or 110 µs. The lower and
upper blue curves represent the minimum and maximum estimates over 20 sections. When the
figure is expanded sufficiently it can be seen to be two closely overlapping curves: the arithmetic
(black) and geometric (lower blue) means. Reproduced with permission from “Spectra sensing
for cognitive radio” S. Haykin et al. (2009). Proc. IEEE, 97, 849–877.
samples, or 110 µs in duration. The blocks were offset 100% from each other. The pilot
carrier is clearly visible in the figure at the lower band-edge of the MTM spectrum.
In particular, there are considerable white spaces in the spectrum to fit in additional
signals; hence the interest in the use of white spaces in the TV band for cognitive radio
The ATS-DTV signal is clearly seen to be 40 dB or more above the noise and the
sharp narrowband interfering components. Both below and above the main ATSC-DTV
signal, the slowly varying shapes of the baseline spectrum estimates are reminiscent
of filter skirts, which make it improbable that the true thermal noise level is reached
Note also that, as anticipated, there are several narrowband interfering signals visible
• four strong signals (one nearly hidden by the vertical axis) and three others on the
• two weak signals, one at 1.6 MHz (maximum level of about 3 × 10–7, about halfway
between the peaks at 1.245 and 1.982 MHz) and one halfway down the upper skirt at
If we were to use a conventional periodogram or its tapered (windowed) version to
estimate the ATS-DTV spectrum, then the presence of interfering narrowband signals
such as those in Figure 7.2 would be obscured. Accordingly, in using the white spaces
of the TV band, careful attention has to be given to the identification of interfering
The overall message to take from the experimental results plotted in Figure 7.2 is
(1) Interfering signals do existt in the ATSC-DTV, regardless of whether the TV station
is switched on or off, as clearly illustrated in Figure 7.2.
(2) Although, indeed, it is well recognized that the ATSC-DTV band is open for
cognitive radio applications when the pertinent TV station is switched off, the
results presented in Figure 7.2 are intended to demonstrate the high spectral
resolution that is afforded by using the MTM for spectrum sensing. Obviously, an immediate benefit of such high spectral resolution is improved radiospectrum utilization, which, after all, is the driving force behind the use of
cognitive radio, a point we have made over and over again because of its practical importance.
(3) The current FCC ruling on the use of white spaces in the ATSC-DTV band for
cognitive radio applications permits such use by low-power cognitive radios only
when the TV station is actually switched off. Nevertheless, the experimental results
displayed in Figure 7.2 clearly show the feasibility of using low-power cognitive
radios in white spaces that exist below the pilot; moreover, this is done without
interference to the TV station when it is actually on, thereby offering the potential
for additional improvement in radio-spectrum utilization.
Spectrum sensing in the IEEE 802.22 standard
In the article by Stevenson et al. (2009), a high-level overview of the IEEE 802.22
standard was described for cognitive wireless regional area networks (WRANs). For
convenience of presentation, this standard is referred to simply as the IEEE 802.22
hereafter. In this standard, provision is made to define a single air-interface based on
2048-carrier orthogonal frequency-division multiplexing
g (OFDM) for a reliable end-toend link for near line-of-sight (NLOS). Application of the IEEE 802.22 was aimed at a
rural area of typically 17–30 km or more in radius (up to a maximum of 100 km) from a
wireless network base station. The OFDM is described later, in Section 7.11.
Several issues relating to cognitive radio are covered in the IEEE 802.22. Two topics
(1) Spectrum sensing, which is included as a mandatory feature. The 802.22 network
consists of a base station (BS) and a number of client stations, referred to as customer premise equipment (CPE). The BS controls when spectrum sensing and all
such results are reported to it, which is where the final decision is made on the
availability of a television channel for use by cognitive radio. As such, the spectrum
sensing may be viewed as a signal processing and reporting function (Shellhammer,
2008). In the IEEE 802.22, three different licensed transmissions were specifically
(2) Dynamic spectrum access, the function of which is to dynamically identify and use
the so-called “white spaces” (i.e. those subbands of the radio spectrum which are not
Expanding on wireless microphones as a potentially useful medium for the application of cognitive radio, their employment by secondary users is considered to
be more challenging and difficult than television bands, be they analog or digital.
To this end, in 2010 the Qualcomm company, San Diego, CA, conducted an open
contest applicable to wireless microphones. Out of the many submissions received,
three of them were selected by Qualcomm as the top ones; details of these three
submissions are described at the end of the chapter.5 Suffice it to say, although the
procedures for spectrum sensing adopted by the top three winning contestants were
quite different, they did have one feature in common: all three submissions used
power-spectrum estimation, in one form or another, as the basis for their procedures.
Noncooperative and cooperative classes of cognitive radio networks
In a conventional radio network, the network is built around BS that are designed to
accommodate the allocation of primary resources, namely powerr and bandwidth, among
multiple users in some optimal manner. On the other hand, for a cognitive radio network to be broadly applicable, it is highly desirable for the network to operate in a
7.8 Noncooperative and cooperative classes of cognitive radio networks
decentralizedd manner. Insofar as the issue of multiple access is concerned, there are two
basic mechanisms for accommodating communication needs of the secondary users,
depending on how the users’ utility functions are defined:
(1) Noncooperative mechanism. In this first approach, each user operates in a “greedy”
manner so as to optimize its own utility function without paying attention to other
(2) Cooperative mechanism. As the name implies, users of the network optimize their
individual utility functions by agreeing to cooperate with each other.
Although, indeed, these two mechanisms are radically different and aimed at different
applications, they do share a common prescription:
All the secondary users of a cognitive radio network follow the same protocol, the formulation
of which involves the imposition of prescribed procedures to ensure that they work alongside the
primary users of the legacy radio network in a harmonious manner.
Moreover, for ideas needed to formulate the protocol, we may look to the following
(1) Game theory. Here, the secondary users of a cognitive radio network are viewed as
players or agents, who are collectively engaged in a game. Selection of the appropriate game is determined by whether the resource-allocation problem follows a
cooperative or noncooperative approach. Typically, the selected game-theoretic
strategy is characterized by an equilibrium stationary point, to which the evolution
of each player’s trajectory in the game converges. However, game theory does not
tell us how to get there; rather, we look to information theory for guidance.
(2) Information theory. Earlier in this section, we mentioned there are two primary
resources on which we have to focus attention: power and bandwidth. The tradeoff
between these two resources is well described by Shannon’s information capacity
formula, which, in a remarkable way, embodies the following three parameters in a
• average value of the transmitted power,
• power spectral density of additive noise at the channel output (i.e. receiver input).
The formula assumes an ideal channel model, which is lossless and the channel
(link) noise is Gaussian. The lossless assumption is taken care of by accounting for
the path loss from the transmitter to the receiver across a wireless communication
link. As for the Gaussian assumption, it is justified by invoking the central limit
theorem of probability theory, applied to the multitude of interfering signals attributed to users in the network other than the particular user under scrutiny; the central
limit theorem was discussed in Note 11 of Chapter 4.
(3) Optimization theory. Here, we look to a constrained optimization procedure, the
utility function for which embodies the information-capacity formula and the
constraints imposed on it in accordance with the noncooperative or cooperative
mechanism. Application of the optimization procedure leads to an algorithmic
solution to the resource-allocation problem, desirably in iterative form.
The algorithm so obtained serves the purpose of a cognitive controller, for the experimental evaluation of which we look to principles of control theory.
With this overview, we proceed, first, by studying noncooperative cognitive radio
networks, the current status of which is well developed. Under the noncooperative
approach, the Nash equilibrium in game theory stands out as a logical choice. This study
occupies the material presented in Sections 7.9–7.15.
Then, in Section 7.16, we consider cooperative cognitive radio networks. In particular, we focus the discussion on a new class of cooperative networks known as coalitional networks.
In a game-theoretic framework,6 involving a competitive multi-agent environment with
limited resources, the actions of all players in the environment are coupled with each
other via the available resources. Accordingly, the task of finding a global optimum for
the resource-allocation problem can be not only time consuming, but also computationally intractable. Moreover, the optimization would require large amounts of information
being exchanged between different agents, and thereby consume precious resources. To
get around these difficulties, we have to be content with a suboptimal solution. Prominent among the game-theoretic ideas, embodying such solutions for finite games with
each agent having only a finite number of alternative courses of action, is that of a Nash
equilibrium,7 for which we offer the following definition:
A Nash equilibrium is an action profile (i.e. vector of players’ actions) in a noncooperative game,
in which each action is a best response to the actions of all the other players in the game.
According to this definition, the Nash equilibrium is a stable operating (i.e. equilibrium)
pointt in the sense that there is no incentive for any rationall player involved in a finite
noncooperative game to change strategy given that all the other players continue to
follow the equilibrium policy. The important point to note here is that the Nash-equilibrium approach provides a powerful tool for modeling nonstationary processes. Simply
put, it has had an enormous influence on the evolution of game theory by shifting its
emphasis toward the study of equilibria as a predictive concept.
With the learning process modeled as a repeated stochastic game (i.e. repeated
version of a one-shot game), each player gets to know the past behavior of the other
players, which may influence the current decision to be made. In such a game, the task
of a player is to select the best mixed strategy, given information on the mixed strategies of all other players in the game; hereafter, other players are referred to simply as
“opponents.” A mixed strategy is defined as a continuous randomization by a player of
its own actions, in which the actions (i.e. pure strategies) are selected in a deterministic
manner. Stated in another way, the mixed strategy of a player is a random variable
whose values are the pure strategies of that player.
To explain what we mean by a mixed strategy, let aj, k denote the kkth action of player
j with j = 1, 2, . . ., m, and k = 1, 2, . . ., K
K, where m is the total number of players and
K is the total number of possible actions available to each user in the game. The mixed
strategy of player j, denoted by the set of probabilities { j , k }kK=1 , is an integral part of
Equivalently, we may express qj as the inner product
is the deterministic action vector. The superscript T denotes matrix transposition. For
all j, the elements of the mixed strategy vector pj satisfy the two basic conditions of
Note also that the mixed strategies for the different players are statistically independent.
The motivation for permitting the use of mixed strategies is the well-known fact that
every stochastic game has at least one Nash equilibrium in the space of mixed strategies, but not necessarily in the space of pure strategies; hence the preferred use of mixed
strategies over pure strategies. The purpose of a learning algorithm is that of computing
a mixed strategy, namely a sequence {q(1), q(2), . . ., q(t)} for each player over the course
It is also noteworthy that the implication of (7.1)–(7.4) is that the entire set of mixed
strategies lies inside a convex simplex or convex hull, whose dimension is K − 1 and
whose K vertices are the aj,k. Such a geometric configuration makes the selection of the
best mixed strategy in a multiple-player game a more difficult proposition to tackle than
the selection of the best base action in a single-player environment.
To sum up, the Nash equilibrium is considered to be a concept of fundamental importance in game theory. This equilibrium point is the solution to a game-theoretic problem
in which none of the players in the game has the incentive to deviate from it unilaterally.
In other words, at a Nash-equilibrium point, each player’s chosen strategy is the “best
response” to strategies of the other players. From a practical perspective, even though
a Nash-equilibrium solution is not always the best solution to the resource-allocation
problem, it is a reasonable candidate for solving this problem in a game-theoretic framework; hence its practical relevance to addressing the resource-allocation problem in
Water-filling in information theory for cognitive control
With this brief exposition of the Nash equilibrium, we may now resume our discussion
of cognitive radio networks, in which a user, referring to a communication link, plays
the role of a player in a finite-game theoretic problem. The issue of interest is that of
developing a cognitive controller for resource allocation with particular emphasis on
transmit-power control. To this end, adoption of the Nash equilibrium is well suited for
the issue at hand, bearing in mind the following attributes:
• decentralized implementation of the resource-allocation algorithm for achieving a
• fast convergence to a reasonably good solution represented by a Nash-equilibrium point.
To achieve these attributes, unfortunately, game theory cannot do it all by itself. Rather,
as pointed out previously in Section 7.8, we may look to information theory for direction.
More specifically, the integration of the Nash equilibrium with the information-theoretic
water-filling model works nicely for solving the resource-allocation problem. For a statement of the water-filling model, we say (Cover and Thomas, 2006):
Given a set of parallel Gaussian communication links (channels), application of Shannon’s
channel capacity formula provides the theoretical basis for a procedure by means of which the
available power is distributed among the various links in a manner identical to the way in which
It is because of this analogy that we speak of a water-filling model in information theory.
Correspondingly, formulation of the controller is referred to as a water-filling controller.
Statement of the transmit-power control problem
Consider a cognitive radio network involving m secondary users. Communication
among the users is to be asynchronous, with the communication process being viewed
as a noncooperative game. The objective of each user in the network is to maximize
its own data transmission rate in a greedy manner. We may then state the essence of
transmit-power control for this noncooperative scenario as follows (Haykin, 2005a):
Given a limited number of spectrum holes, select the transmit-power levels of m unserviced secondary
users in a cognitive radio network so as to jointly maximize their data-transmission rates, subject to
the constraint that the interference-power limit imposed on each link in the network is not violated.
It may be tempting to suggest that the solution of this problem lies in simply increasing
the transmit-power level of each unserviced transmitter. However, increasing the
transmit-power level of any one transmitter has the undesirable effect of also increasing
7.10 Water-filling in information theory for cognitive control
the level of interference to which the receivers of all the other transmitters are subjected. The conclusion to be drawn from this reality is that it is not possible to represent the overall system performance with a single index of performance. Rather,
we have to adopt a tradeofff among the data rates of all unserviced users in some
Ideally, we would like to find a global solution to the constrained optimization of
the joint set of data-transmission rates in the network. Unfortunately, finding this global
solution requires an exhaustive search through the space of all possible power allocations scenarios, in which case we find that the computational complexity needed for
attaining the global solution assumes a prohibitively high level.
To overcome this computational difficulty, we use the competitive optimality criterion8 (Yu, 2002) for solving the transmit-power control problem, which may now be
Considering a cognitive radio network viewed as a noncooperative game, maximize the performance of each secondary user, regardless of what all the other users in the network do, but subject
to the constraint that a prescribed interference-power limit for each user is not violated.
This formulation of the distributed transmit-power control problem leads to a solution
that is of a locall nature; although the solution is suboptimal, it is not only insightful, but
Example: two-user noncooperative scenario
Consider the simple scenario of Figure 7.3 involving two users communicating across a flatfading channel. The complex-valued baseband channel matrix of the network is denoted by
Viewing this scenario as a noncooperative game, we may describe the two players
• The two players are represented by communication links 1 and 2.
• The pure strategies (i.e. deterministic actions) of the two players are defined by the
(f ) that respectively pertain to the transmitted
signals radiated by the transmitters of communication links 1 and 2.
• The payoffs (i.e. rewards) to the two players are defined by the data-transmission
rates R1 and R2, which correspond to communication links 1 and 2 respectively.
To proceed with a solution to this two-user transmit-control problem, we characterize
(1) The noise floor of the RF radio environment is characterized by a frequencydependent parameter: the power spectral density SN (f
“noise floor” above which the transmit-power controller must fit the transmissiondata requirements of both communication links 1 and 2.
Figure 7.3. Signal-flow graph of a two-user communication scenario.
(2) In order to assure reliable communication under varying operating conditions, an
SNR gap is included in calculating the transmission data of each user in the network.
On this basis, we may now define the cross-coupling
links in Figure 7.3 in terms of two normalized interference gains a 1 and a 2 by writing
where Γ is the SNR gap. Assuming that receivers of the two communication links do
nott perform any form of interference cancellation regardless of the received signal
strengths, we may respectively formulate the achievable data-transmission rates R1 and
R2 as two definite integrals involving spectrum holes 1 and 2, as shown by
The integrands in (7.8) and (7.9) follow from the application of Shannon’s informationcapacity formula9 to links 1 and 2 respectively. The term a 2S2(f
(f ) in the denominator of (7.9) are due to the cross-coupling
between the transmitters and receivers of communication links 1 and 2; in short, they
are both the effects of interfering signals. The remaining two terms N1(f
noise terms, which are respectively defined by
7.11 Orthogonal frequency-division multiplexing
( ) are respectively parts of the noise-floor’s power spectral density SN ( f ) that define the spectral contents of spectrum holes 1 and 2.
We are now ready to formally state the competitive optimization problem for the twouser scenario of Figure 7.3 as follows:
Given that the power spectral density S2(f
(f ) of the transmitter of communication link 2 is fixed,
maximize the transmission-data rate R1 of link 1, subject to the constraint
where PI,max is the maximum allowable interference power.
A similar statement applies to the competitive optimization of the transmitter of communication link 2. Of course, it is understood that both S1(f
(f ) remain nonnegative for all ff, as they must be in accordance with the definition of power spectral density.
Solution to the optimization problem just described follows the allocation of transmit
power to users 1 and 2 in accordance with the iterative water-filling procedure. However,
before discussing this new procedure in Section 7.12, we digress briefly in the next section to present a brief review of OFDM, on which the air-interface in the IEEE 802.22
is based; hence its practical importance for cognitive radio networks–see Section 7.7.
Orthogonal frequency-division multiplexing
A multicarrier modulation scheme that commends itself for deployment in cognitive
M In this scheme, closely spaced orthogonal subcarriers (e.g.
2048 in number in the IEEE 802.22) are used to transmit narrowband data segments
simultaneously across the forward wireless link connecting the transmitter at one end of
the link in the network to the receiver at the other end of the link. In effect, the forward
wireless link, which commonly suffers from the frequency-selective fading phenomenon, is divided into a number of narrowband “flat-fading” subchannels, with each
OFDM has many practical advantages over single-carrier transmission, as summarized here (Li and Stüber, 2006; Hanzo et al., 2010):
(1) OFDM improves the efficiency of spectrum utilization by simultaneous use of multiple orthogonal subcarriers, which are densely packed.
(2) First of all, the OFDM waveform is generated in the frequency domain and then it is
transformed into the time domain, thereby providing flexible bandwidth allocation.
g (i.e., randomly shuffling) the information over different OFDM symbols
(before transmission) provides against the loss of information caused by flat-fading and
noise effects, and then de-interleavingg at the receiver to recover the original information.
(4) Although the spectrum tails of subcarriers overlap with each other, at the center frequency of each subcarrier all other subcarriers are zero in accordance with the property
of orthogonality in the frequency domain. Theoretically, this property prevents intercarrier interference (ICI). However, time- and frequency-synchronizations are critical
for ICI prevention, as well as for correct demodulation; therefore, it is a major challenge
in the physical layer design of cognitive radio.
(5) Since a narrowband signal has a longer symbol duration than a wideband signal,
OFDM takes care of intersymbol interference (ISI) caused by multipath delay of
wireless channels. However, guard-time intervals, which are longer than the channel
impulse response, are ordinarily introduced in practice between OFDM symbols
to eliminate the ISI by giving enough time for each transmitted OFDM symbol to
(6) Owing to relatively low ISI, the complexity of equalization at the receiver of each
user in the cognitive radio network is reduced considerably, which, in turn, leads to
In summary, OFDM enables the provision of frequency diversity for combating
the multipath phenomenon in wireless communications, provides for the following
practical advantages compared with single-carrier data transmission schemes:
• the accomplishment of higher data rates;
• more flexibility in the selection of transmit-waveform characteristics; and
• greater robustness against channel noise and fading.
Just as importantly, application of the FFT algorithm provides a powerful and wellstudied tool for a computationally efficient implementation of the OFDM and, therefore,
the cognitive radio network. In this context, it is noteworthy that the choice of 2048 (an
integer multiple of 2) subcarriers in the IEEE 802.22 is recommended by virtue of the
Iterative water-filling controller for cognitive radio networks
At long last, we are now ready to formulate the cognitive controller for resource
allocation in cognitive radio networks. To be precise, there are two primary resources to
(1) the transmit-power level permissible to each secondary user in the network and
(2) the number of subcarriers available in the OFDM.
The subcarriers provide frequency-domain representations of spectrum holes (i.e.
unused subbands of the radio spectrum) identified by the receiver of each secondary
user in the network, with a set of 2l subcarriers for each spectrum hole, where l is an
For dynamic spectrum management of the network, the number of possible moves
is finite. This is because the manager can only choose a subset of subcarriers from the
Iterative water-filling controller for cognitive radio networks
total set of available subcarriers per spectrum hole, which is finite since the OFDM is
a finite scheme. On the other hand, power is a continuous variable by its very nature;
the transmit-power controller, therefore, has infinitely many options available to it in
adjusting the power levels assigned to the subcarriers by the spectrum manager. Stated
in another way: although the action space of the controller is of finite dimensionality,
the number of possible actions is infinitely large.
To proceed, then, with mathematical formulation of the cognitive controller, let m
denote the number of secondary users in the cognitive radio network and let n denote
the number of subcarriers that can be potentially available for secondary communication in a noncontiguous OFDM scheme. Since spectral efficiency is a driving force
behind cognitive radio, the utility function of each secondary user in the network of
cognitive radios to be maximized is the data transmission rate. Thus, adoption of
the water-filling model requires that each user in the network solve the following
constrained optimization problem (Setoodeh and Haykin, 2009):
where pki denotes user i’s transmit power over subcarrier k, CAP denotes the maximum
allowable level of interference power, and PS stands for a set of subcarriers employed by
primary users. The noise plus interference experienced by user i at subcarrier k, because
of transmissions from other users, is given by
Recognizing that cognitive radio is receiver centric, in that it is at the receiver of
each communication link in the network where the radio environment is sensed, I ki
is measured at receiver i. Note also that, with the OFDM integrated into the cognitive
radio design, the logarithmic term log2 ⎢1+ i ⎥ in (7.13) is a discrete representation
of the information capacity formula that was invoked previously in (7.8) and (7.9). To
be more precise, the discrete representation of SNR in (7.13) is formulated in terms of
OFDM subcarriers. As such, the following point should be carefully noted. Whereas
in the two-user example described in Section 7.10 the formulation of data rates was
based on spectrum holes, in (7.13) the formulation is based on subcarriers. As such, it is
permissible for the subcarriers assigned to a spectrum hole to be employed by more than
one secondary user in the cognitive radio network, thereby providing for a more efficient
utilization of the OFDM scheme and broader applicability of the water-filling model.
Yet another advantage of basing the water-filling model on OFDM is discretization,
whereby integration in the information capacity formula is replaced by a summation
of subcarrier powers. There is, therefore, much to be said on the use of (7.13) as the
algorithmic basis of the iterative water-filling controllerr (IWFC). However, it should
be noted that, for subcarriers to be employable by more than one secondary user,
provisions must be made for the avoidance of cross-modulation between the subcarriers
of two or more secondary users sharing the same subcarrier. This provision can be satisfied by assigning a specific code to each secondary user, as is done in code-division
The positive parameter σ ki in (7.15) is the normalizedd background noise power at the
receiver input of user i on the kkth subcarrier. Here again it should be noted the normalized noise parameter s should nott be confused with the variance of a noise source.
The nonnegative parameter α kij is the normalized interference gain from transmitter
j to receiver i at subcarrier k; of course, we have α kij = 1. The term α kij is the combined
• propagation path-loss from transmitter j to receiver i at subcarrier k;
• subcarrier amplitude reduction due to the frequency offset Δf
where Γ is the SNR gap and hkij is the channel gain from transmitter j to receiver i over
the flat-fading subchannel associated with subcarrier k. Regarding the empirical formula for the path loss (Haykin, 2005a), we have
where dijj is the distance from transmitter j to receiver i. The path-loss exponent r varies
from two to five, depending on the environment, and the attenuation parameter b k is
frequency dependent. Therefore, we may write
In any event, if userr i’s receiver is closer to its transmitter compared with other active
transmitters in the network, we will have a kij ≤ 1.
Iterative water-filling controller for cognitive radio networks
Referring to lines 1 and 2 of constraint inequalities in (7.14), pmax
power and CAPk is the maximum allowable interference power at subcarrier k. CAPk is
determined in a way to make sure that the permissible interference power level limit will
not be violated at the primary users’ receivers.
In the IWFC, user i assumes that pkj is fixed for j ≠ i. Therefore, the optimization
problem in (7.13) is a concave maximization problem in the power vector
pi = [ p1i , , pni ]T , which can be converted to a convex minimization problem
by introducing a minus sign and considering ––ff i as the objective function to be
minimized. The first constraint in (7.14) states that the total transmit power of user i
at all subcarriers should not exceed its maximum power (power budget). The second
constraint in (7.14) guarantees that the interference caused by all secondary users in
the cognitive radio network at each subcarrier will be less than the maximum allowed
interference at that subcarrier. If primary users in the legacy radio network do not
let the secondary users employ the nonidle subcarriers in their frequency bands,
then secondary users should not use those subcarriers for transmission. The third
constraint in (7.14) guarantees this requirement by forcing the related components of
the secondary user i’s power vector to be zero. If primary users allow the coexistence
of cognitive radio users at nonidle subcarriers on condition that they do not violate
the permissible interference power level, then the third constraint in (7.14) can
be relaxed, in which case the second constraint in the set described therein suffices.
As mentioned previously, the IWFC is implemented in a decentralized manner. In
order to solve the optimization problem (7.13), it is not necessary for user i to know
the value of pkj for all j ≠ i. The I ki defined in (7.15) is measured by user i’s receiver
rather than calculated; accordingly, secondary users do not need to exchange information. Furthermore, it is not necessary for user i to know the number of other users in
the network. Therefore, changing the number of users in the network does not affect the
complexity of the optimization problem that should be solved by each user. Hence, there
is no scaling problem, which is another attribute of the IWFC.
While the action of user i is denoted by its power vector pi, following the notation
in the game-theory literature, the joint actions of the other m − 1 secondary users in
the cognitive radio network are denoted by p–i, where a minus sign is introduced in the
superscript. Three major types of adjustment scheme S can be used by the cognitive
radio users to update their actions (Setoodeh and Haykin, 2009):
(1) Iterative-water-filling users update their actions in a predetermined order, as
p−i (SSt) = [p1 (t + 1), . . ., pi −1(t + 1), pi +1 (t), . . ., pm (t)].
According to (7:20), user i updates its action by having access to the updated information from users 1, . . ., i − 1 but not from users i + 1, . . ., m.
(2) Simultaneous iterative-water-filling. Users update their actions simultaneously by
respecting the most recent actions of the others, as shown by
(3) Asynchronous iterative-water-filling, which is an instance of an adjustment scheme
that user i receives update information from user j at random times with some delay,
where τ ti j is an integer-valued random variable satisfying the condition
which means that the delay does not exceed d time units.
Recognizing the desire for avoiding both the use of central scheduling and the need
for synchronization between different users in a cognitive radio network, the asynchronous adjustment scheme of (7.22) and (7.23) is the most realistic one when network
The cognitive radio network has a dynamic nature, in which users are on the move all
the time: they can leave the network and new users can join the network in a stochastic
manner. Because of these stochastic phenomena, the interference-plus-noise term
(7.15) in the objective function and the second constraint in (7.14) are both time varying. The IWFC, therefore, assumes the form of an optimization problem under uncertainty. Also, the appearance and disappearance of the spectrum holes, which depend on
the activities of primary users, will change the third constraint in (7.14). The behavior
of primary users and, therefore, the availability and duration of the availability of spectrum holes, can be predicted to some extent using a predictive model, on which more
will be said in Section 7.16. During the time intervals that the activity of primary users
does not change and the available spectrum holes are fixed, two approaches can be
taken to deal with the uncertainty caused by the joining and leaving of other cognitive radio users and their mobility: stochastic optimization and robust optimization
(Fukushima, 2007). The pros and cons of these two approaches are discussed in what
Let the noise-plus-interference term be the summation of two components, a nominal
In the following, the objective functions for both stochastic and robust versions of the
optimization problem (7.13) are presented.
If there is good knowledge about the probability distribution of the uncertainty term
ΔI, then the uncertainty can be dealt with by means of probability theory and related
concepts. In this case, calculation of the expected value will not be an obstacle;
therefore, the iterative water-filling algorithm problem (7.13) can be formulated as a
stochastic optimization problem with the following objective function:
where E denotes the statistical expectation operator with respect to
In practice, however, little may be known about the probability distribution ΔI
means that the stochastic optimization approach utilizing the expected value is not a suitable practical approach. In stochastic situations of the kind described here, robust optimization techniques, based on the worst-case analysis and thereby bypassing the need for
probability theory, are more appropriate. The price paid for this alternative approach is an
overly conservative one, in the sense that suboptimality in performance is traded in favor
of robustness. In spite of this shortcoming, robust optimization is in the spirit of what
successful engineering network designs require in practice; it is a hallmark of cognition.
The formulation of the IWFC as a robust game in the sense described by Aghassi
and Bertsimas (2006) is basically a max–min problem, in which each user tries to
maximize its own utility while the environment and the other users are trying to minimize that user’s utility (Danskin, 1967; Basar and Bernhard, 1995); worst-case interference scenarios have been studied for digital subscriber lines (DSL) by Brady and Cioffi
(2006). Considering an ellipsoidal uncertainty set, the IWFC problem (7.13) can be
reformulated as the following robust optimization problem:
A large e in objective function of (7.27) and the second constraint of (7.28) account
for large perturbations. Basically, the new set of constraints in (7.28) guarantees that
the permissible interference power level will not be violated for any perturbation
Stochastic optimization guarantees some level of performance on average, but sometimes the desired quality of service may not be achieved, which, in effect, means a
lack of reliable communication. On the other hand, robust optimization guarantees an
acceptable level of performance under the worst-case conditions, but it is a conservative
approach. We say so because real-life systems are not always in their worst behavior, but,
from a reliability perspective, it can provide seamless communication even in the worst
situations. Recognizing the dynamic nature of a cognitive radio network and the delay
introduced by the feedback channel, the statistics of interference that are used by the
transmitter to adjust its power may not represent the current situation of the network. In
such situations, robust optimization is equipped with the means to prevent permissible
interference power-level violation by taking into account the worst-case uncertainty in
the presence of interference and noise. Therefore, sacrificing optimality for robustness
seems to be a reasonable proposition in practice, and very much in the spirit of cognition. As a possible refinement, the adoption of a predictive model may make it possible
for the user to choose the uncertainty set adaptively according to changes in environmental conditions and, therefore, may lead to a less conservative design; however, this
design gain will have been achieved at the expense of increased system complexity.
It is worth noting that a cognitive radio can benefit from two predictive models. One is
part of the DSM that makes predictions about primary users’ behavior, which determines
the available subcarriers and durations of their availability. These, in turn, provide estimates of the dimension of the optimization in (7.27) and (7.28) and of the control horizon
respectively. The other predictive model is part of the TPC and makes predictions about
secondary users’ behavior and provides an estimate of the size of the uncertainty set. As
will be explained later, these two predictive models work on different time scales.
In addition to conservatism, there is yet another price to be paid for achieving robustness. Although the constrained IWFC problem formulated in (7.13) and 7.14 is a convex
optimization problem, appearance of the perturbation term ΔI
signal-to-(interference plus noise) ratio (SINR) in the objective function of the robust
iterative water-filling algorithm problem (7.27) makes it a nonconvex optimization
problem. A robust optimization technique is proposed by Teo (2007) for solving nonconvex and simulation-based problems. The proposed method is based on the assumption that the cost and constraints, as well as their gradient values, are available. The
required values can even be provided by numerical simulation subroutines. It operates
directly on the surface of the objective function and, therefore, does not assume any specific structure for the problem. In this method, the robust optimization problem is solved
in two steps, which are applied repeatedly in order to achieve better robust designs.
• Neighborhood search. The algorithm evaluates the worst outcomes of a decision by
obtaining knowledge of the cost surface around a specific design.
• Robust local move. The algorithm excludes neighbors with high costs and picks an
updated design with lower estimated worst-case cost. Thereby, the decision is adjusted
in order to counterbalance the undesirable outcomes.
The linearity of constraints of the robust optimization problem described in (7.27) and
(7.28), especially the second set of constraints in (7.28) that involve the perturbation terms,
improves the computational efficiency of the algorithm.
Transient behavior of cognitive radio networks, and stability of equilibrium solutions
In describing the design of a noncooperative cognitive radio network based on the material covered in Sections 7.9–7.13, we have introduced two levels of robustification into
Examining (7.6), we see the inclusion of an SNR gap denoted by Γ in this equation.
As pointed out previously in Section 7.10, the gap Γ is included to assure reliable
communication under varying operating conditions all the time. To be more precise,
this gap is included in the formula for calculating the data transmission rate of a
secondary user so as to account for the “gap” between the performance of a practical
coding-modulation scheme and the theoretical value of channel capacity.
Next, examining the second constraint introduced in (7.28), we see by using a large
enough e that we account for the possibility of large perturbations that may arise in
the cognitive radio network due to uncertainties in the network.
In building this two-level robustification strategy into the network design, one at the
local level and the other at the global level, we have provided the means needed to
assure reliable communication across the cognitive radio network despite the possible
presence of uncertainties that defy modeling. Reliability of the network, however, has
been accomplished at the expense of suboptimality that manifests itself in a reduced
data transmission performance of the network. Simply put, there is no free lunch.
Transient behavior of cognitive radio networks, and stability
Although, over a short time scale, components of the cognitive radio network may
remain essentially unchanged in complex and large-scale networks, the general behavior
of the network can change drastically over time. If the SINR of a communication link
drops below a specified threshold for a relatively long time, the connection between
the transmitter and receiver for that link will be lost. For this reason, in addition to the
equilibrium resource allocation, which was discussed in previous sections, the transient
behavior of the network also deserves attention. In other words, studying the equilibrium states in a dynamic framework by methods that provide information about the
disequilibrium behavior of the system is critical, which is the focus of this section.
In previous sections, the IWFC was proposed as an approach to find an equilibrium
solution for the resource-allocation problem in cognitive radio networks. Following
the approach of Luo and Pang (2006), the IWFC is reformulated as a variational inequality (VI) problem.10 To explain this problem, let X be a nonempty subset of the
n-dimensional real space Rn, but let F; denote a mapping from Rn into itself, where
n denotes the number of subcarriers in the OFDM. According to Harker and Pang
(1990), the variational inequality problem, denoted by VI(X
holds. Having formulated the IWFC as a VI problem, projected dynamic systems
(PDS) theory, described by Nagurney and Zhang (1996), can be utilized to associate an
ordinary differential equation (ODE) to the VI problem. A projection operator, which
is discontinuous, appears on the right-hand side of the ODE to incorporate the
feasibility constraints of the VI problem into the dynamics. This ODE provides a dynamic
model for the competitive system whose equilibrium behavior is described by the VI.
Also, the stationary (equilibrium) points of the ODE coincide with the set of solutions of
the VI, which are the equilibrium points. Using the procedure just described, the equilibrium problem can be studied in the context of a dynamic framework. This dynamic model
enables us not only to study the transient behavior of the network, but also to predict it.11
The stability of a cognitive radio network is an important issue and deserves
special attention. It can be interpreted as the ability of the network to maintain or restore
its equilibrium state against external perturbations. In other words, network stability is
linked to network sensitivity to perturbations.
Case study II: robust IWFC versus classic IWFC
Simulation results are now presented to support the theoretical discussions of the previous
sections. In the simulations reported by Setoodeh and Haykin (2009), the background noise
levels s ki , the normalized interference gains a kij , and the power budgets pmax
randomly from the intervals {0, 0.1/(m − 1)}, {0, 1/(m − 1)}, and {n/2, n} respectively, with uniform distributions; n is the number of subcarriers in the OFDM and m is the number of users
(communication links) in the cognitive radio network. The randomly chosen values for normalized interference gains a kij , which are less than 1/(m − 1), guarantee that the tone matrices
in the OFDM will be strictly diagonally dominant. For scenarios with time-varying delay in
the control loops, the delays are chosen randomly.
In a cognitive radio network, when a spectrum hole disappears, users may have to
increase their transmit powers at other spectrum holes, and this increases the interference. Also, when new users join the network, current users in the network experience
more interference. Therefore, the joining of new users or the disappearance of spectrum holes makes the interference condition worse. Also, the cross-interference between
users is time-varying because of the mobility of users. Results related to two typical
but extreme scenarios are presented here to show the superiority of the robust IWFC
described in (7.27) over the classic IWFC described in (7.13).
The first scenario, considered in the experiments, addresses a network with m = 5
communication links (users) and n = 2 available subcarriers, and all of the users simultaneously update their transmit powers using the interference measurements from the
previous time step. At the fourth time step, two new users join the network, which
increases the interference. The interference gains are also changed randomly at different time instants to consider mobility of the users. Figures 7.4 and 7.5 show the
transmit powers of three users (users one, four, and seven) at two different subcarriers
for the classic IWFC and robust IWFC respectively. At the second subcarrier, the classic
IWFC is not able to reach an equilibrium. Data rates achieved by the chosen users are
Case study II: robust IWFC versus classic IWFC
Figure 7.4. Resource-allocation results of simultaneous classic IWFC when two new users join
a network of five users and interference gains are changed randomly to address the mobility of
the users. (a) Transmit powers of three users at two subcarriers. (b) Data rates of three users and
the total data rate in the network. Reproduced, with permission, from “Robust transmit power
control for cognitive radio,” P. Setoodeh and S. Haykin, 2009, Proc. IEEE, 97, 915–939.
also shown. Also, the total data rate in the network is plotted against time, which is a
measure of spectral efficiency. Moreover, although the average sum rate achieved by the
classic IWFC is close to the average sum rate of the robust IWFC, it fluctuates, and in
some time instants the data rate is very low, which indicates lack of spectrally efficient
communication. Moreover, although the oscillation occurs mainly because of using a
Figure 7.5. Resource-allocation results of simultaneous robust IWFC when two new users join
a network of five users and interference gains are changed randomly to address mobility of the
users. (a) Transmit powers of three users at two subcarriers. (b) Data rates of three users and the
total data rate in the network. Reproduced, with permission, from “Robust transit power control
for cognitive radio networks,” P. Setoodeh and S. Haykin, 2009, Proc. IEEE, 97, 915–939.
simultaneous update scheme, it also highlights the practical effectiveness of the robust
In the second scenario, a network with m = 5 users and n = 4 available subcarriers
is considered. Again, at the fourth time step, two new users join the network, but at
the eigth time step the third subcarrier is no longer available (i.e. a spectrum hole
disappears). The results are presented in Figures 7.6 and 7.7, which again show the
Case study II: robust IWFC versus classic IWFC
Figure 7.6. Resource allocation results of simultaneous classic IWFC when two new users join
a network of five users, a subcarrier disappears, and interference gains are changed randomly
to address mobility of the users. (a) Transmit powers of three users at four subcarriers. (b) Data
rates of three users and the total data rate in the network. Figure reproduced, with permission,
from the paper entitled “Robust transmit power control for cognitive radio,” P. Setoodeh and S.
Figure 7.7. Resource allocation results of simultaneous robust IWFC when two new users join a
network of five users, a subcarrier disappears, and interference gains are changed randomly to
address the mobility of the users. (a) Transmit powers of three users at four subcarriers. (b) Data
rates of three users and the total data rate in the network. Figure reproduced, with permission,
from the paper entitled “Robust transmit power control for cognitive radio,” P. Setoodeh and
S. Haykin, 2009, Proc. IEEE, 97, 915–939.
Self-organized dynamic spectrum management
superiority of the robust IWFC. For classic IWFC, immediately after the disappearance
of the third subcarrier, power in the fourth subcarrier starts to oscillate. After changing
the interference gains randomly, we observe the same behavior in other subcarriers. In
contrast to the robust IWFC, the classic IWFC fails again to achieve an equilibrium.
As mentioned previously, sporadic feedback introduces a time-varying delay in
the transmit-power control loop, which causes different users to update their transmit
powers based on outdated statistics. For instance, when the network configuration and,
therefore, interference pattern changes, some users receive the related information after
a delay. If the interference at a subcarrier increases and the transmitter is not informed
immediately, it will not reduce its transmit power and may violate the permissible interference power level for a while until it receives updated statistics of the interference in
the forward channel. Similarly, this may happen to some users that update their transmit
powers at lower rates compared with others. In the third scenario, a new user joins a
network of three users who are competing for utilizing two subcarriers. Each user’s
transmitter receives statistics of the interference plus noise with a time-varying delay.
Figure 7.8a shows the randomly chosen time-varying delays introduced by each user’s
feedback channel. The sum of transmit power and interference plus noise at the second
subcarrier in the receiver of each user is plotted in Figure 7.8b and c for classic IWFC
and robust IWFC respectively. Dashed lines show the limit imposed by the permissible
interference power level. Although the classic IWFC is less conservative, it is not as
successful as the robust iterative water-filling algorithm at preventing violations of the
permissible interference power level. Similar results are obtained when users update
their transmit powers with different frequencies.
Evan though the results presented in Figures 7.4–7.8 are for simple cognitive radio
scenarios, they do demonstrate the practical importance of a robust iterative waterfilling controller over the classic one.
Self-organized dynamic spectrum management
Having covered TPC, we now turn to DSM which is about distributing the available spectrum holes among cognitive radio users and it is one of the most challenging problems in
cognitive radio for several reasons (Haykin, 2005a; Khozeimeh and Haykin, 2009):
(1) DSM is equivalent to the graph-coloring problem,12 which is an NP-complete
(3) The dimensionality of the problem can assume a relatively high value, depending on
the density of cognitive radio users (units).
For these practical reasons, the traditional centralized approaches that try to solve the
DSM problem for the whole cognitive radio network are not practical, compelling us to
find a decentralizedd approach that can achieve a satisfactory suboptimal assignment. In
other words, suboptimality of the DSM is traded off for scalability.
Figure 7.8. Resource allocation results of IWFC when interference gains change randomly with
time and users use outdated information to update their transmit powers. (a) Time-varying
delays introduced by each user’s feedback channel. Sum of transmit power and interference plus
noise for the users achieved by (b) classic IWFC and (c) robust IWFC. Dashed lines show the
limit imposed by the permissible interference power level. Figure reproduced, with permission,
from the paper entitled “Robust transmit power control for cognitive radio,” P. Setoodeh and
S. Haykin, 2009, Proc. IEEE, 97, 915–939.
7.16 Self-organized dynamic spectrum management
Inspired by the human brain, the self-organized DSM approach described herein tries
to find the best channels to use for each cognitive radio unit by applying the idea of selforganizing maps. Self-organizing maps are a specific class of neural networks whose
main goal is to adaptively transform an incoming signal-pattern of arbitrary dimension
into a one- or two-dimensional discrete map in a topologically ordered manner. This
kind of approach to self-organization relies on a form of Hebbian learning rule to extract
patterns or features of the data and match the map to the patterns; this learning rule
was discussed in Chapter 2. Recapping on the material presented therein, the Hebbian
postulate of learning (Hebb, 1949) is one of the oldest learning rules. It states that
changes in biological synaptic weights are proportional to the correlation between
presynaptic (input) and postsynaptic (output) signals. The learning process extracts
information about the radio environment and stores it in the synaptic weights of each
neuron in the map. It follows, therefore, that at each step of the learning process an
appropriate adjustment is applied to each synaptic weight of the neuron under consideration. The general form of weight adjustment in Hebbian learning is defined by
F(y(n), xj(n)) is a function of correlation between the neuronal output y(n) and
jth input signal xj; the symbol n used to denote discrete time in (7.30) should not
be confused with the number of subcarriers in OFDM. The key point for (7.30) to
work is that there must be correlation or redundancy in the input signals. Given this
requirement, which is typically a practical reality, the neural network tries to find the
correlation of the input and use the correlation for future use.
In self-organized DSM, each cognitive radio unit continuously monitors its own
environment to extract the radio activity pattern in its surrounding neighborhood, using
the Hebbian learning rule. Specifically, it tries to estimate the presence of a legacy user
in each subband and when a new communication link is created, it is established on a
common spectrum hole between the transmit- and receive-cognitive radio link that has
the lowest probability of there being a legacy user. Simply stated:
In order to increase spectrum utilization using the self-organized DSM technique, the cognitive
radio network tries to complement the legacy network’s spectrum occupancy pattern by matching
its own spectrum usage pattern to the pattern of channels which have had the least or no legacy
Although it is impossible to predict exactly how the spectrum holes come and go and
for how long they remain available, they do nott appear to have a completely random
behavior. Rather, if we look at the radio spectrum in a specific location in a time window
of interest, we do see that there is correlation between the presence of legacy users
and the radio activity in their channels. For example, we rarely see activity in those
channels that have no legacy users around them, whereas, on the other hand, we do see
activity from time to time on those channels that do have legacy users around them.
In addition to the pattern attributed to the physical presence of legacy users forming a
spatial pattern, evaluation over time can give rise to patterns appearing in the spectrum
holes. For example, we may see no activity in a cellphone channel during the night
even though some users are present. The important point to note here is that the
self-organized DSM scheme has the built-in capability to capture both patterns by
employing the Hebbian learning rule, which extracts useful patterns from the incoming
Thus, in a cognitive radio network employing self-organized DSM, the cognitive
radio units form a common control channell with their neighboring cognitive radio units
to exchange sensing information and control data with each other; in other words, cooperation is established among cognitive radio users. Sharing the spectrum-sensing information, in the manner just described, significantly improves the ability of these units to
discover what the legacy users are doing.
To summarize, the self-organized DSM algorithm works as follows:
• Each cognitive radio unit creates a list of synaptic weights, with each weight being
associated with one of the spectrum subbands.
• The weights are continuously updated using Hebbian learning, based on spectrumsensing information obtained by the cognitive radio unit and information received
• When a new communication link needs to be established, a common spectrum hole
with a high weight in both the transmitter and receiver cognitive radio units is used to
• If a legacy user is detected on a link under use, the cognitive radio units immediately stop
using the link and try to find another common spectrum hole with high weights to use.
To conclude, the self-organized DSM algorithm is suitable for solving the dynamic
spectrum management problem for three compelling reasons:
(1) The Hebbian learning process is a time-varying and highly local learning rule; this,
therefore, provides a good match for DSM, which is also a time-varying and local
(2) The algorithm is computationally very simple to implement.
(3) Most importantly, the network is decentralized and its complexity depends on the
density of cognitive radio units, not the total number of them; therefore, it is scalable.
The study of a decentralized and self-organized cognitive radio networkk may also be
approached using cooperative game theory, albeit for a different application. To be more
specific, we look to a special form of cooperative game known as a coalitional game
(Saad et al., 2009). To elaborate, a coalitional game involves a group of rational players
who have agreed among themselves to work together as a single entity with the objective of each player being that of strengthening its own position in the game. Thus, the set
of players, denoted by M = {1, 2, . . ., m}, constitutes the first fundamental concept in a
coalitional game. The second fundamental concept in a coalitional game is the coalition
value, denoted by v, which quantifies the worth of the coalition carried out in the game.
Thus, in mathematical terms, a coalitional game is described by the doublet {M,
On the basis of how the coalition value v is defined, we may identify two types of
(1) Coalitional games of the characteristic form, having transferable utilityy (TU). The
utility is said to be transferable in the sense that the coalition value depends solely
on the players, regardless of how they are individually structured (von Neumann and
Morgenstern, 1944). The total utility, represented by a real number, can be divided
between the coalition players in a flexible manner. More specifically, the coalition
values in TU games represent payoffs, which can be distributed among players in the
game by using an appropriate fairness rule. For example, the fairness rule could be
simply that of having the total utility distributed equally among all the coalition players.
(2) Coalitional goals having nontransferable utilityy (NTU). In this second type of coalitional game, there may exist fixed
d restrictions imposed on how the total utility is to
be distributed among the coalition players (Autmann and Peleg, 1960). In particular,
there may be a subgroup of players in the coalitional game who would prefer to
operate in a noncooperative manner, yet remain members of the coalitional game. In
this second type of coalitional game, the coalitional value v is no longer a function
of the players over the real line. Rather, we now have a set of playoff vectors to deal
with. To elaborate, let S denote the set of noncooperative players, with S ⊆ M
S represents a payoff that player i can obtain within
coalition S. It is presumed that, for this payoff, player i has selected a certain noncooperative strategy for itself while remaining a member of the coalition subset S.
From the classification of coalitional games into the two types just described, it is
apparent that the TU type may be viewed as a special case of the NTU type. Of particular interest is the fact that these two types constitute the most important class of
coalitional games encountered in practice.
Application: cooperative spectrum sensing for cognitive radio networks
Saad et al. (2009) proposed the idea of cooperative spectrum sensingg (CSS) for
cognitive radio networks, with the aim of providing a tradeoff between two probabilities:
• The probability of miss; that is, the probability of missing the detection of a white space.
• The probability of false alarm; that is, the probability of detecting a subband
occupied by a primary user and declaring it to be a white space.
With this tradeoff in mind, the CSS problem is modeled as a dynamic coalitionformation game between secondary users in a cognitive radio network. Bearing in mind
the following factors for a given coalition:
• imposition of an upper bound on false alarm,
• increased false alarm with increasing size of the coalition, and
• distances between users in the coalition,
Saad et al. (2009) show that a grand coalition is seldom formed. The term “grand coalition”
refers to whether the cooperation of all users in the coalition can be taken for granted.
So, to get around this difficulty, Saad et al. go on to propose a coalition-formation
Phase II. In this phase, the secondary users in a coalition perform local sensingg of the
I In this second phase, the secondary users engage in adaptive coalition
formation, based on a set of merge and split rules (Apt and Witzel, 2006). Let
M denote a set of secondary users and { i }il =1 denote any collection of disjoint
coalitions, where Si ⊂ M for all i. The Si agree to merge into a single coalition
G ∪il =1Si if this new coalition is preferred by all the secondary users over the
previous state of affairs. Similarly, a coalition S ⊂ M agrees to splitt into a number
of smaller coalitions if the resulting set { i }il =1 is preferred by the secondary users
I Once the coalitions are formed, each secondary user reports its sensing
information about the radio environment to its own coalition “head” that makes
the final decision on whether the subband in the radio spectrum is occupied by a
This three-phase coalition-formation algorithm has been tested for a toy experiment
on a collaborative game involving a relatively small number of secondary users with
interesting results; the interested reader is referred to Saad et al. (2009) for more
Emergent behavior of cognitive radio networks
The cognitive radio environment is naturally time varying. Most importantly, it exhibits
a unique combination of characteristics (among others): adaptivity, awareness, cooperation, competition, and exploitation. Given these characteristics, we may wonder about
the emergent behavior of a cognitive radio network in light of what we know on two
relevant fields: self-organizing systems and evolutionary games.
First, we note that the emergent behavior of a cognitive radio network viewed as a
game is influenced by the degree of couplingg that may exist between the actions of different players operating in the game. The coupling may have the effect of amplifying
local perturbations, and if they are left unchecked, the amplifications of local perturbations may ultimately lead to instability. From the study of self-organizing systems, we
know that competition among the constituents of such a system can act as a stabilizing
force (Haykin, 2009). By the same token, we expect that competition among the users
of cognitive radio for limited resources (e.g. spectrum holes) may have the influence of
For additional insight, we next look to evolutionary games. The idea of evolutionary
games, developed for the study of ecological biology, was first introduced by Maynard
Smith in 1974. In his landmark work, Maynard Smith (1974, 1982) wondered whether
the theory of games could serve as a tool for modeling conflicts in a population of animals. In specific terms, two critical insights into the emergence of so-called evolutionary
7.18 Emergent behavior of cognitive radio networks
stable strategies were presented by Maynard Smith, as succinctly summarized by
• The animals’ behavior is stochastic and unpredictable, when it is viewed at the
• The theory of games provides a plausible basis for explaining the complex and
unpredictable patterns of the animals’ behavior.
(1) Complexity.13 The emergent behavior of an evolutionary game may be complex, in
the sense that a change in one or more of the parameters in the underlying dynamics
of the game can produce a dramatic change in behavior. Note that the dynamics must
be nonlinear for complex behavior to be possible.
(2) Unpredictability. Game theory does not require that animals be fundamentally unpredictable. Rather, it merely requires that the individual behavior of each animal be
unpredictable with respect to its opponents (Maynard Smith, 1982; Glimcher, 2003).
From this brief discussion on evolutionary games, we may conjecture that the emergent
behavior of a cognitive radio network is explained by the possible unpredictable action
of each user, as seen individually by the other secondary users.
Moreover, given the conflicting influences of cooperation, competition, and exploitation on the emergent behavior of a cognitive radio environment, we may identify two
(1) Positive emergent behavior, which is characterized by orderr and, therefore, a harmonious and efficient utilization of the radio spectrum by all users of the cognitive radio. (The positive emergent behavior may be likened to Maynard Smith’s
(ii) Negative emergent behavior, which is characterized by disorderr and, therefore, a
culmination of traffic jams, chaos,14 and unused radio spectrum.
From a practical perspective, what we first need is a reliable criterion for the early detection of negative emergent behavior (i.e. disorder) and, second, corrective measures for
dealing with this undesirable behavior. With regard to the first issue, we recognize that
cognition, in a sense, is an exercise in assigning probabilities to possible behavioral responses, in light of which we may say the following.
In the case of positive emergent behavior, predictions are possible with nearly complete confidence. On the other hand, in the case of negative emergent behavior, predictions are made with far
less confidence. We may thus think of a likelihood function based on predictability as a criterion
for the onset of negative emergent behavior.
In particular, we may envision a maximum-likelihood detector, the design of which is
based on the predictability of negative emergent behavior.
Network measures should be sought that provide estimates about the global behavior of
the network based on local observations made by users. This highlights the importance of
analytical models that enable us to predict the future. Based on the knowledge so obtained,
we can engineer the future to improve network robustness against potential disruptions.
A discussion of cognitive radio networks would be incomplete without describing how
we can provide for the feedback channel, which plays a critical role in the cognitive
information-processing cycle of each user in the network. The question is how this provision can be satisfied in an effective and practical manner.
To be specific, the feedback channel can be established in three ways:
(1) A dedicated universal channel for cognitive radio. In this approach, a specific spectrum band in the radio spectrum is licensed and reserved for the feedback channel.
This solution has the advantage of a simple system design and reliability. However,
it is expensive (due to spectrum licensing) and also it is hard to find a worldwide
common-free channel for this purpose due to different spectrum utilization policies
in different countries. Furthermore, the cognitive radio can be easily interrupted
by “jamming” the feedback channel. Finally, dedicating such a spectrum band to
cognitive radio contradicts one of the main goals of cognitive radio, which is that
of increasing the radio spectrum utilization; the dedicated spectrum band in this
approach would be wasted whenever cognitive radio units are not in use.
(2) Using available spectrum holes. In this second approach, cognitive radio can use
spectrum holes both for data transmission and as a feedback channel. Using spectrum holes is more flexible and efficient in terms of spectrum utilization than using
a dedicated channel. However, the cognitive radio network cannot always be established because sometimes it is possible that there is no spectrum hole available in the
radio environment to tap on. When there are no spectrum holes available, there is no
feedback channel and cognitive radio users lose communication and synchronization. Thus, the moment some spectrum holes become available, they cannot immediately start data transmission and must wait until the necessary synchronization and
negotiations are established. Furthermore, the radio spectrum is a highly dynamic
environment and the spectrum holes may change in time. Therefore, every time the
feedback channel becomes unavailable, cognitive radio users lose synchronization
and need to stop data transmission until the feedback channel is established again.
(3) Using unlicensed bands. In this third and last approach, cognitive radio users can
readily establish their own feedback channel using “unlicensed” bands. In this case,
the feedback channel is always available and cognitive radio users never lose synchronization. The cognitive radio network can always be established even when there
is no spectrum hole available in the environment. However, in adopting the use of
unlicensed bands for the provision of feedback channel, the cognitive radio users may
need to combat a high level of noise and interference due to other radios working in
the unlicensed bands. In this scenario, the underlying theory and design of the cognitive radio network would have to be expanded to account for noise in the feedback
channel. This is an issue that has not received the attention it deserves in the literature.
Using the unlicensed bands for establishing the feedback channel appears to be a good
(1) The cognitive radio network is always established and users in the network never
lose synchronization. Furthermore, even when there is no spectrum hole available in
the environment, the user may employ the unlicensed bands to transmit data. Thus,
users in the network can always provide communication and achieve one of their
primary goals, which is providing reliable communication whenever and wherever
(2) Even when there is no spectrum hole, cognitive radio users can always cooperate
and share their radio-scene analysis information, which results in better and faster
(3) When a spectrum hole used by a communication link (user) becomes unavailable,
the users can negotiate through the feedback channel to find another common spectrum hole and change their data channel momentarily.
There is, therefore, much to be said for adopting an unlicensed band in the radio spectrum as the medium for establishing the feedback channel for cognitive radio networks
In this chapter, we focused on the second application of cognition, namely cognitive
radio. For a clear understanding of how cognitive radio works, Figure 7.1 pictures two
cognitive radio units in communication. Each such unit is equipped with its own transceiver, made up of two parts: transmitter and receiver. Moreover, each cognitive radio
unit is also equipped with its own radio scene analyzer (RSA) in the receiver part of
the transceiver, and dynamic spectrum manager (DSM) and transmit-power controller
(TPC) in the transmitter part. Examining the directed information flow in Figure 7.1, we
clearly see that the perception–action cycle embodies the following four components:
RSA of the cognitive radio unit on the right;
DSM and TPC of the cognitive radio unit on the left; and
In terms of terminology, the figure teaches us that, when we discuss a cognitive radio network, the term “user” refers to the “communication link” that connects the transmitter of
the cognitive radio unit on the left to the receiver of the cognitive radio unit on the right.
The next issue discussed in the chapter was spectrum sensing, which is another way of
referring to radio scene analysis. Needless to say, spectrum sensing is one of the primary
tasks involved in the design of cognitive radio. Among the many techniques on spectrum
sensing described in the literature, we picked power spectrum estimation as the method
of choice. In particular, we revisited the MTM, described in Chapter 3, as a mathematically rigorous procedure for accomplishing this task.
We also presented highlights of a contest on spectrum sensing for wireless
microphones, which is more difficult than spectrum sensing of ATSC-DTV signals for
cognitive radio. The contest was carried out by Qualcomm, San Diego, CA, in 2010.
What is truly satisfying is the fact that all three top winners of the contest used powerspectrum estimation, in one form or another, for solving this difficult spectrum-sensing
problem; the MTM was among those three winners. This contest confirms a point that
was made in the influential paper by Haykin (2005a). Nonparametric power-spectrum
estimation is the best procedure for spectrum sensing in cognitive radio applications.
Another important task in cognitive radio is that of transmit-power control, which is
performed in the transmitter. To satisfy this requirement, we looked to ideas in game
theory, information theory, optimization theory, and control theory. Under game theory,
we focused on the Nash equilibrium that is a predictive concept well suited for modeling
nonstationary processes, involving rational players (users). Under information theory, we
focused on the iterative water-filling algorithm, the attractive features of which include:
relatively fast rate of convergence, implementation in a decentralized manner, and the
efficient use of OFDM. Under optimization theory, we focused on robustification of the
iterative-water filling algorithm to guard against uncertainties of a physical nature. Under
control theory, we focused on the variational inequality problem and projected dynamic
systems to transform the IWFC into a new mathematical representation: an ODE framework for cognitive radio networks that is convenient for analysis of the network behavior.
We then put all these ideas together and performed experiments to demonstrate the utility
of the robust IWFC as a practical tool for solving the transmit-power control problem in a
noncooperative cognitive radio network built on OFDM.
Self-organizing networks are emerging as one of the most promising solutions for
wireless networks in general by offering reliable service, improved energy efficiency,
and ease of implementation. For their design, we may look to one of two approaches:
The human brain is a highly complex dynamic system, yet it is capable of selforganization in ways that can only be termed as truly remarkable. In this context,
Hebb’s 1949 book entitled The Organization of Behaviorr has played a significant
role in the study of self-organizing systems over the past six decades. This classic
book introduced what is now commonly referred to as Hebb’s postulate of learning
that inspires us to this day. Most importantly, the book laid out a general framework for relating behavior to organization at the localized synaptic level through the
dynamics of neural networks (Seung, 2000).
The approach taken by Khozeimeh and Haykin (2010) in the study of selforganized dynamic spectrum management for cognitive radio networks is inspired
by Hebb’s postulate of learning. Rephrasing this postulate in the context of wireless
communications, we may make the following statement:
The weighting assigned to a link, connecting the transmitter at one end to a receiver at the
other end in a communication network, varies in accordance with the correlation between the
For obvious reasons, some form of constraint has to be added to this postulate to
assure stability of the link (Khozeimeh and Haykin, 2010).
Coalitional game theory–a specialized class of cooperative game theory – provides
another approach for the design of self-organized cognitive radio networks. With the
secondary users of a cognitive radio network assuming the role of rational players,
coalitional cognitive radio networks are categorized into two types:
• TU coalitional networks, in which the total utility is divided arbitrarily among the
secondary users, subject to a fairness rule.
• NTU coalitional networks, in which arbitrary division of the total utility among
the secondary users is prevented by a subset of the users who adopt a noncooperative strategy of their own.
The TU type may be viewed as a special case of the NTU type.
Saad et al. (2009) applied the principles of coalitional networks to spectrum
sensing in a cognitive radio network. To this end, the network is modeled as a dynamic
coalition-formation game between secondary users. A distinctive step in the algorithm
so developed is an adaptive merge-and-split phase that converges to a partitioning of
the secondary users with an optimal payoff allocation, whenever this partition exists.
With the ever-increasing growth of data-hungry wireless devices, exemplified by smart
phones and net books, serious concern is being raised about the issues of data overload
and outage in cellular networks. To tackle these practical issues, network providers are
looking into the use of femtocells to increase network capacity and improve spectrum
utilization. (Femtocells were mentioned previously in Section 7.4.) From a wirelesscommunication point of view, femtocells may be viewed as low-power, indoor base
stations that are connected to cellular networks through high-speed Internet links typically available in most homes and small offices. Thus, following Ortiz (2008), we say:
Femtocells are mini-cell towers located inside homes or small offices for the purpose of providing
reliable wireless communication with the outside world.
Insofar as indoor users are concerned, the advantages offered by the deployment of
• reduced power consumption and lengthened battery life brought about by the use of
By the same token, the network providers gain advantages of their own through the
• improved spectrum utilization, thereby serving more customers without having to
invest in building more towers and buying expensive radio bands;
• savings in network infrastructure realized through use of the Internet linking homes
and small offices to the outside world; and
• addressing the issue of user dissatisfaction resulting from electromagnetic propagation loss, which is achieved by exploiting the fact that femtocells are able to provide
indoor wireless communication at maximum speed.
However, for indoor users and network providers to gain the advantages just described,
two practical issues would have to be resolved:
(1) Interference. With femtocells communicating among themselves as well as macrocells in the outside world, novel transmit-power control and dynamic spectrum
management algorithms will have to be found to mitigate the interference problem.
(2) Heterogeneous network support. In the context of femtocell networks, the term
“heterogeneous” applies to the varying size of femtocells from one location to
another and the different capabilities of the base stations.
To tackle both of these problems, femtocell networks will have to be self-organizing
To become so, they have to be aware of the underlying wireless communication
environment, learn from it, and adaptt to it on a continuous-time basis, so as to provide
reliable communication whenever and wherever needed. What we have just summarized
here is nothing but the definition of cognitive radio. In other words, for femtocells to
achieve their goals, they would have to be cognitive with one difference; whereas in
traditional cognitive radio networks we speak of primary (legacy) users and secondary
users, in the new generation of cognitive femtocell networks there are no secondary users.
Rather, there is uniformity in the network, with only legacy users of varying needs.
Despite this difference, the fundamental principles of traditional cognitive radio
covered in this chapter, namely spectrum sensing in the receiver, feedback linkage from the
receiver to the transmitter, and transmit-power control and dynamic spectrum management
in the transmitter, are all applicable to cognitive femtocell networks subject to the inherent
1. Impulsive noise in wireless communications
The most common natural source of noise encountered at the front end of communication receivers is thermal noise, which is justifiably modeled as AWGN.
By far the most important artificial source of noise in mobile communications is
man-made noise, which is radiated by different kinds of electrical equipment across
a frequency band extending from about 2 MHz to about 500 MHz (Parsons, 2000).
Unlike thermal noise, man-made noise is impulsive in nature; hence the reference to
it as impulsive noise. In urban areas, the impulsive noise generated by motor vehicles is a major source of interference to mobile communications.
With the statistics of impulsive noise being radically different from the Gaussian
characterization of thermal noise, the modeling of noise in a white space due to the
combined presence of Gaussian noise and impulsive noise in urban areas may complicate procedures for identifying spectrum holes.
Assuming that the Slepian tapers (windows) are precomputed for a prescribed size
K, each eigencoefficient in the MTM can be computed using the FFT algorithm.
With routines such as FFTW, described by Frigo and Johnson (2005), the computation is fast to begin with and ‘‘tricks’’ could be used to speed up the process even
further. To be specific, the FFTW is not tuned to a fixed machine; rather, it uses a
plannerr that adapts its algorithms to maximize performance.
When it comes to implementation, it is ironic, but not surprising, that the FFT
algorithm plays such a fundamental role not only in implementing the MTM for
spectrum sensing, but also the IWFC for transmit-power control based on OFDM.
The experimental results on ATSC-DTV signals presented in Section 7.6 are a
shortened version of material on this same topic in Haykin et al. (2009). For more
detailed results and related discussions, the reader is referred to that paper.
In its 2008 report and order on TV white space (FCC, 2008), the FCC established
rules to allow new wireless devices to operate in the white space (unoccupied spectrum) of the broadcast TV spectrum on a secondary basis. It is expected that this will
lead to new innovative products, especially broadband applications. To avoid causing
interference, the new devices will incorporate geolocation capability and the ability to
access over the Internet a database of primary spectrum users, such as locations for TV
stations and cable systems’ headends. The database itself is insufficient to offer interference guarantees, given the ability to predict propagation characteristics accurately
while efficiently using the spectrum. The portability of low-power primary users, such
as wireless microphones, makes the database approach infeasible, motivating the need
for sensitive and accurate spectrum-sensing technology to include these devices.
5. The 2010 Qualcomm competition on wireless microphones
With the need to shed more light on spectrum sensing for wireless microphones, the
Qualcomm Company, San Diego, conducted an open contest in 2010 for the best technique for this challenging application of cognitive radio. The power level of wirelessmicrophone signals is in the range of −100 to −110 dBm, where dBm refers to power
expressed in decibels with 1 mW of power as the reference. No further information
about the test data was provided to the contestants. For each signal in the test data,
a file was supplied listing whether a wireless microphone was present or not and at
which specific frequency within the channel. The top three submissions selected by
Qualcomm (in alphabetical order) were: Queen’s University, Canada; University of
California at Los Angeles (UCLA); and University of Illinois at Urbana-Champaign
(UIUC). The overall winner was the submission from UIUC. The criteria for evaluating
the submissions were twofold: results of the computations and paper organization.
The top three submissions, which are illuminating in their individual ways, and
(1) MTM (Burr, 2010, Queen’s University, Canada)
In the submission made by Queen’s University, the MTM was used, with access
• harmonic F-testt for identifying the significance of a potential nearby periodic
signal against a background of noise; and
• magnitude-squared coherence (MSC) between two antenna sources.
Both tests were used in the overall MTM-based technique, in addition to the
measurement of SNR of possible signals being present in the test data. Both
statistical tests were discussed in Chapter 3.
Work was done only on three sets of a possible 10 sets of test data due to time
limitation. Each of these sets was considered as separate antennas. The antenna
sets were subdivided yet again into 50 independent sub-blocks, each of length
120 μs. On each of these sub-blocks, an MTM spectrum estimation and accompanying harmonic F
F-test were computed. A histogram-based binning scheme
was used to determine the best five candidates for a wireless-microphone signal
across the antenna set, and the results returned to the base station. For each
candidate frequency, 50 total MSCs were computed between the two antennas,
one for each sub-block, and the minimum of the set was taken.
This procedure was repeated for the three data sets and the results passed
through a multistage logical testing procedure to determine which candidates were
truly wireless-microphone signals. The testing procedure also allowed for strong
and weak interference signals to be identified to aid in the possible (temporary)
allocation of subbands to wireless microphones. The results were returned with
indications of the minimum MSC estimate between the two antennas across all
three sets for the candidate, as well as individual antenna results. Careful examination showed that certain interference signals were detected at great strength
on one antenna and weakly or not at all on the other. This result confirmed the
intuition that using multiple antennas may well have provided better detection
possibilities than a single antenna, especially through use of the MSC and normal
coincidence results. (This observation on the use of multiple antennas made in
the submission by Burr is interesting. In Chapter 3 on spectrum sensing, expansion of the MTM for space–time processing was discussed, wherein the use of
multiple antennas was identified as a method for estimating the directions of
(2) Two-stage spectrum estimation procedure (Dulmange et al., UCLA)
In the submission made by UCLA, a two-stage spectrum estimation procedure
was used. The first stage of the procedure was coarse sensing, which divided
the channel into 400 kHz subbands with 100 kHz overlap. The division of the
channel into sub-bands was accomplished using a 64-point FFT, which was
used to estimate the power spectrum. Coarse sensing identified candidate subbands that were likely to contain a wireless microphone. This was accomplished
by comparing the power in a given subband with the power in the adjacent and
second adjacent subbands. Subbands with sufficiently high power compared
with neighboring subbands were processed further by a fine-sensing stage.
Each candidate subband selected by the coarse sensing stage was filtered and
decimated by a factor of 16. The power spectrum of the decimated signal was
then calculated using a 512-point FFT to provide a high-resolution spectrum
estimate of the 400 kHz subband. Multiple power spectral estimates were averaged to increase the SNR. A frequency-domain correlator was used to match the
spectrum from the training data. There was one frequency-domain template from
the training set, where the audio signal input to the wireless microphone was music.
The second frequency-domain template was from the training data, when there was
no audio signal into the wireless microphone, so the audio signal was silent.
The fine-sensing stage found the largest correction to the two frequencydomain templates and then used that as its test statistic. If the test statistic was
larger than a specified threshold, then the subband was declared to contain a
wireless microphone; if, on the other hand, the test statistic was less than the
threshold, then the subband was declared not to contain a wireless microphone.
(3) High-resolution power-spectrum estimation using Hamming window (Sreckanth
The UIUC contest submission began by calculating a high-resolution power
spectral density using the FFT and Hamming window. Multiple power-spectral
estimates were then averaged to increase the SNR. Next, it was observed that
wireless microphones must be on multiples of 25 kHz as measured from the
lower band edge of the TV channel. They included a frequency tolerance of
±5 kHz to allow for transmitter and sensing-receiver LO frequency errors. Since
wireless microphones must be entered on a multiple of 25 kHz, that meant
within a 6 MHz TV channel there are 240 small subbands centered on multiples
of 25 kHz on which to search for the wireless microphones. Thus, only those
subbands were considered; over each of them, the power spectral estimate over
a 20 kHz subband was multiplied by a window that weighted the middle of
the subband more than the edges. Next, the resulting windowed power spectral
estimator over the 20 kHz subband was accumulated to represent the power in
that subband. Then, again, in order to eliminate interference, the median of the
power in the 240 subbands was calculated and used as an estimate of the noise
floor power. Subsequently, for each subband the bandwidth was calculated as
the number of power spectral estimator bins, within 6 kHz, that exceeded the
noise floor; this procedure was used to eliminate narrowband interference.
Next, for each subband, a nonlinear combination of the signal power and the
bandwidth was calculated. The nonlinear function combined the signal power
and bandwidth into test statistics for each subband. The maximum of each of
these test statistics was calculated, providing the final test statistic, which was
compared with a prescribed threshold. If the test statistic exceeded the threshold,
then a wireless microphone was declared to be present in the channel; on the
other hand, if it did not exceed the threshold, then the channel was declared
The message to take from the Qualcomm test
Although, indeed, the three approaches described above for tackling the spectrumsensing problem for wireless microphones are quite different, they do, however,
share a common point: all three approaches appeal to power-spectrum estimation
In a historical context, the formulation of game theory may be traced back to the
pioneering work of John von Neumann in the 1930s, which culminated in the publication of the co-authored book entitled Theory of Games and Economic Behavior
(von Neumann and Morgenstern, 1944). For modern treatments of game theory, see
the books by Fundenbergh and Levine (1999) and Basar and Olsder (1999).
7. Classic papers on the Nash equilibrium
The Nash equilibrium is named for Nobel Laureate John Nash; the Nash (1950, 1951)
8. Origin of the competitive optimality criterion
The competitive optimality criterion is discussed in Chapter 4 of Yu’s doctoral
dissertation (Yu, 2002). In particular, Yu developed an iterative water-filling algorithm for a suboptimal solution to the multi-user DSL environment, viewed as a
9. Shannon’s information capacity formula
Consider a band-limited, power-limited Gaussian channel. The channel output is
perturbed by AWGN of zero mean and power spectral density N0/2. A sample of
this noise process, therefore, is Gaussian distributed with zero mean and noise variance N0W
W, where W is the channel bandwidth. According to Shannon’s information
theory, the information capacity of the channel is defined by
where the logarithm is to base 2. According to this formula, it is easier to increase
the information capacity of a communication channel by expanding the channel
bandwidth than by increasing the average transmitted power P for a prescribed noise
Let χ be a nonempty subset of Rn and let F be a mapping from Rn into itself. The VI
problem, denoted by VI(χ, F) is to find a vector x* ∈ χ such that
Loosely speaking, this VI states that the vector F(x*) must be at an acute angle with
all feasible vectors emanating from x*. Formally, x* ∈ χ is a solution to VI(χ, F ) if,
and only if, F(x*) is inward normal to χ at x*.
11. Sensitivity considerations in cognitive radio networks
For more detailed theoretical considerations of the dynamic model of a cognitive
radio network, the reader is referred to Setoodeh and Haykin (2009). Therein, the
network stability is linked to network sensitivity perturbations in theoretical terms.
Graph-coloring refers to the problem of coloring the vertices of a given graph with a
minimum number of colors. To explain, consider a unidirectional graph denoted by
V E) when V is the set of |v| = n vertices and E is the set of edges. A k-coloring
of the graph G is said to be a mapping φ: V → Γ, where Γ = {1, 2, . . . , k} is the set
of |Γ| = k integers, each of which represents a specific color. The graph-coloring is
Otherwise, the graph-coloring is said to be invalid. In other words, for a graphcoloring to be valid, any two vertices connected by an edge have different colors.
For further details on graph-coloring for DSM and pertinent references, the reader
is referred to Khozeimeh and Haykin (2009).
The new sciences of complexity (whose birth was assisted by the Santa Fe Institute,
New Mexico) may well occupy much of the intellectual activities in the 21st century
(Stein, 1989). In the context of complexity, it is perhaps less ambiguous to speak
of complex behavior rather than complex systems (Nicolis and Prigogine, 1989).
A nonlinear dynamic system may be complex in computational terms but be incapable of exhibiting complex behavior. By the same token, a nonlinear system can
be simple in computational terms but its underlying dynamics can be rich enough to
The possibility of characterizing negative emergent behavior as a chaotic phenomenon needs some explanation. Idealized chaos theory is based on the premise that
dynamic noise in the state-space model (describing the phenomenon of interest) is
zero (Haykin et al., 2002). However, it is unlikely that this highly restrictive condition is satisfied by real-life physical phenomena. So, the proper thing to say is that it
is feasible for a negative emergent behavior to be stochastic chaotic.
We begin this final chapter of the book by reemphasizing the basic ideas that bind the
study of cognitive dynamic systems to the human brain, where the networks dealing
with perception are collocated with those dealing with action.
For a very succinct statement on the overall function of a cognitive dynamic system
made up of actuator and perceptor, we simply say:
Perception for control via feedback information
Elaborating on perception performed in the perceptor and action performed in the
(1) Cognitive perception addresses optimal estimation of the state in the perceptor by
processing incoming stimuli under indirect control of the actuator.
(2) Cognitive control addresses optimal decision-making in the actuator under feedback
These two closely related functions reinforce each other continually from one cycle
of directed information processing to the next. The net result is the perception–action
cycle, discussed in detail in Chapter 2. This cyclic directed information flow is a cardinal
characteristic of every cognitive dynamic system, be it of a neurobiological or artificial kind. Naturally, the exact details of the perception–action cycle will depend on the
application of interest. In any event, a practical benefit of the perception–action cycle is
information gain about how the state of the environment is inferred by the system, with
the gain increasing from one cycle to the next.
When we speak of the perception–action cycle in a cognitive dynamic system, we
immediately think of four other basic cognitive processes: memory, attention, intelligance, and language; all five processes, collectively, define a cognitive dynamic system.
(1) Perceptual memory, which is reciprocally coupled to the environmental scene
analyzer in the perceptor. Its function is to store knowledge gained about the
environment by having processed past data, and have that knowledge updated in
light of information contained in new environmental data; this updating is continued from one cycle to the next. Simply stated, the perceptual memory uses the
Summarizing remarks on cognitive radar and cognitive radio
incoming stimuli in two ways. First, past stimuli are used to learn a model of the
environment. Second, the model is adaptively updated in an on-line manner by
exploiting new information contained in the stimuli. With such a model of the
environment available on each cycle, it is left to the environmental scene analyzer
to compute an optimal estimate of the environmental state and update it from cycle
(2) Executive memory, which is reciprocally coupled to the environmental scene actuator. In a manner similar to the perceptual memory, the executive memory uses
feedback information from the perceptor in two ways. First, given past feedback
information on the state of the environment sent by the perceptor, the executive
memory learns to categorize the decision-making process performed by the environmental scene actuator. That categorization is updated from one cycle to the next
by exploiting newly received feedback information. In so doing, the environmental
scene actuator is enabled to continually improve its decision-making process and
(3) Working memory reciprocally couples the perceptual and executive memories by
mediating between them on every cycle. The net result of this mediation is twofold.
First, the cognitive dynamic system, as a whole, assumes the form of a synchronous,
self-organized correlative-learning machine. Second, the working memory provides
predictive information on the consequences of model selection made in the perceptor and action taken by the actuator on or in the environment.
Turning next to attention as the third process essential for cognition: its function is the prioritized allocation of computational resources. In a manner similar
to memory, we have perceptive attention in the perceptor and executive attention
in the actuator. Both of these attentional mechanisms look after their respective
resource-allocation domains. However, unlike perception and memory, the attentional mechanism does not have a physical location of its own. Rather, it builds
on the perception–action cycle and memory to exercise its function by using algorithmic mechanisms.
Finally, we come to intelligence, which is the most complex and most powerful of all
the four cognitive processes. It exercises information-processing power by building on
perception, memory, and attention through the combination of local and global feedback loops distributed throughout the system. As we increase the hierarchical depth
of the memory, the number of such loops is enlarged exponentially, thereby making
intelligence that much more powerful in decision-making in the face of environmental
uncertainties; hence the profound importance of intelligence.
Language was not discussed in the book as it is outside its scope.
Summarizing remarks on cognitive radar and cognitive radio
What we have just summarized on the perception–action cycle, viewed particularly in
the context of the visual brain, applies appropriately to a cognitive radar whose actuator
However, in the case of cognitive radio, there are some practical differences that
need careful attention. First, cognitive radio is intended for wireless communications,
whereas cognitive radar is intended for remote-sensing applications. Moreover, unlike
cognitive radar, the actuator and perceptor of cognitive radio are separately located.
Thus, although the perception–action cycle applies equally well to cognitive radio, we
have to use engineering ingenuity as to how the four cognitive processes, namely perception, memory, attention, and intelligence, are actually implemented.
To illustrate how, indeed, this implementation was done in Chapter 7 on cognitive
(1) With spectrum sensing as the basic function of the perceptor, aimed at the identification of spectrum holes (i.e. underutilized subbands of the radio spectrum), the
need for modeling the environment may no longer be a requirement, provided a
nonparametric (i.e. model-free approach) is adopted. For example, the MTM is a
method of choice that is used to estimate the power spectrum by directly processing
the incoming RF stimuli; we may therefore eliminate the need for modeling the
(2) Turning next to the actuator, we may look to the human brain for ideas on selforganization to implement the function of DSM so as to distribute the available
spectrum holes among competing secondary users in a cognitive radio network. This
kind of thinking leads us to build a self-organizing map based on Hebbian learning,
whereby information on the spectrum holes is stored in the synaptic weights of the
map. Provision for memory is thereby made. Moreover, the self-organizing map
develops a predictive-modeling capability to do two things:
• pay attention to the continually changing user-communication patterns in the network and
• predict the duration-availability of spectrum holes.
There is one other function that needs to be performed: transmit-power control. To
implement this second function, we may look to ideas in game theory, then information theory, and optimal control, as described in detail in Chapter 7. In particular,
as explained by Glimcher (2003), the use of game theory does provide a basis for
learning. Moreover, the control mechanism has a built-in capability for adaptation.
So, the integration of DSM and transmit-power control provides the actuator of cognitive radio the desired capability for intelligent decision-making in the face of environmental uncertainties (e.g. when the spectrum holes become available and when
they disappear from availability on demands made by legacy users, with both events
Lessons learned from cognitive radar and cognitive radio
We may now summarize the first lesson learned from the two applications, cognitive
• When an application of interest closely follows human cognition in conceptual terms,
we may look to the perception–action cycle explained in detail in Chapter 2 for the
structural design of a cognitive dynamic system to satisfy the practical requirements
• When, on the other hand, the application of interest lends itself to the use of cognition
but does not closely follow human cognition in conceptual terms, we may look to the
human brain for inspiration with the objective of identifying those learning tools that
are relevant to that application. Moreover, we have to look to engineering ingenuity
for other ideas on how to complete the structuring of a cognitive dynamic system for
There is one other important lesson learned from the study of cognitive radar and
cognitive radio. As different as they are, they do share three practical issues:
(1) Perception is an ill-posed inverse problem, in that one or more of Hadamand’s three
conditions for well-posedness is violated; hence the need for regularization.
(2) The environment is nonstationary; hence the need for adaptation to deal with
statistical variations of environmental stimuli.
(3) The presence of environmental uncertainties is equally unavoidable; hence the need
for intelligent choices in the decision-making mechanism.
The use of cognition provides practical solutions to all three issues.
In this final section of the final chapter of the book, we briefly discuss a list of topics that
are important to the study of cognitive dynamic systems, but they would have taken us
much too far afield or else they are in their early stages of development.
Network of cooperative cognitive radars (Haykin, 2005b)
In radar applications for remote sensing, the traditional approach has been to go for
a single radar, powerful enough for the application at hand. However, there are certain remote-sensing applications where there is merit for a new approach, based on a
network of low-cost radars working in a cooperative manner and thereby reinforcing
each other over time. One such application is dense networks of cooperative radars for
weather hazard forecasting and warning (McLaughlin et al., 2009).
Dense networks of cooperative radars for weather forecasting
In today’s operational networks (e.g. NEXRAD in the USA) for hazard weather forecasting and warning, the emphasis has been on a small number of highly powerful
coherent radars with large antennas (e.g. 9 m diameters) spaced hundreds of kilometers
apart from each other. In contrast, in the dense networks of cooperative radars described
by McLaughlin et al. (2009), the antennas are expected to be 1 m diameter in size, and
the radars are spaced tens of kilometers apart. Figure 8.1 illustrates the dense-network
Thousands of radars provide nationwide coverage from
the tops of storms down to planetary boundary layer
Figure 8.1. Dense network of inexpensive radars for predicting weather hazards and warning
across the entire USA. (© Copyright 2011 American Meteorological Society (AMS)).
concept, blanketing the contiguous USA. It is envisioned that the inexpensive radars
used in the network would require less than 100 W of average actuator power, yet they
would be capable of mapping storms with about 0.5 km spatial resolution throughout
An important advantage of a dense network of low-cost radars over large radars in
operational sensors is the following (McLaughlin et al., 2009).
The short range of radars in the dense network defeats the Earth curvature-blocking problem,
thereby enabling the network to comprehensively map damaging winds and heavy rainfall from
tops of the storms down to the boundary layer beneath the view of today’s operational radar
Moreover, the short-range operation of radars in the dense network offers the potential
for significant improvements in spatial resolution and forecasting-update times. These
improvements, in turn, enable a better characterization of storm morphology and
analysis. The net result of all these improvements is more accurate weather-hazard
forecasting and warning, compared with operational networks of large radars in
Advantage of cognitive radars over traditional ones
In Chapter 6, the practical advantages of cognitive radars over traditional radars were
described in detail, supported by computer simulations. With a dense network of
cooperative radars for weather forecasting as the application of interest, it is apropos
(1) Information gain about the state of the environment, which is achieved through the
use of global feedback from the perceptor to the actuator; by virtue of the perception–
action cycle; this information gain is increased from one cycle to the next.
(2) Prediction of consequence of actions taken by the radar, which is realized by distributing multiscale memory across each low-cost radar system.
(3) With attention naturally based on perception and memory, the cognitive radar
acquires the ability to allocate computational resources in order of practical
(4) Finally, with intelligence based on perception, memory, and attention, the coherent
radar’s decision mechanism is provided with the means to select intelligent choices
in the face of environmental uncertainties that are highly likely to arise in practice.
These important practical benefits realized through the use of cognition are compelling
reasons for the adoption of cognitive coherent low-cost radars in a dense network for
weather forecasting in place of their traditional counterparts. So much so, when it comes
to hazard-weather forecasting and warning, the network of cognitive radars acquires the
information-processing power that would enhance the likelihood of outperforming the
traditional networks of large radars that much more.
The current thinking behind the dense-network concept is to link radar nodes in the network to the Internet for the purposes of communication, closed-loop data dissemenation,
and control. Here again, there is the potential for further improvements to deployment
of the dense network by exploiting the self-organizing capability of the human brain.
The human brain is a highly powerful, parallel distributed information-processing,
correlative learning machine (Haykin, 2009). Most importantly, the brain is capable
of self-organization through a mechanism known as Hebbian learning, which is
one of the oldest methods of learning (Hebb, 1949). The idea behind this learning
process is based on correlation between the presynaptic (input) signal applied to a
synaptic weight in a neural network and the corresponding postsynaptic (output)
signal produced as response to the input. The synaptic weight refers to a linkage (i.e.
synapse) with an adjustable weight connecting one computational unit (i.e. neuron)
In Chapter 7 we described the application of Hebbian learning to solve the DSM
problem in a cognitive radio network, catering to the communication needs of a large
number of users. In effect, through Hebbian learning, the network assumes the form of
a self-organizing map with built-in memory and short-time predictive-modeling capability. Emboldened by the successful application of Hebbian learning to cognitive radio
networks, it is our belief that much may also be gained by applying it to a dense network
To sum up, there is much to be gained by expanding the scope of the dense-network
concept for weather forecasting through the combined use of cognition at the elemental
radar level and self-organization at the network level. Putting it altogether, we envision
a self-organized network of cooperative cognitive radars based on the use of low-cost
coherent radars, each of which is capable of range and Doppler estimation. With accurate weather forecasting and warning as the objective, such a network has the potential
for outperforming the traditional network of large radars with operational improvements
Double-layer network dynamics (Setoodeh, 2010)
There are two worlds of wireless communications: the legacy (old) wireless world and
the cognitive (new) wireless world. Spectrum holes are the medium through which the
two worlds interact. Releasing subbands by primary users allows the cognitive radio
users to perform their normal tasks and, therefore, to survive. In other words, the old
world affects the new world through appearance and disappearance of the spectrum
holes and there is a master–slave relationship between them. Hence, the two worlds
of wireless communications are going on side by side. This makes a cognitive radio
network a multiple-time-scale dynamic system: a large-scale time in which the activities of primary users change and a small-scale time in which the activities of secondary
users change accordingly. Such systems are called double-layer dynamic systems. A
theoretical framework must, therefore, be developed to capture the multiple-time-scale
nature of cognitive radio networks and lay the groundwork for further research. This
topic is similar to an uncharted territory and has a great deal of potential, both in theoretical and practical terms.
A cognitive radio network, which is a system of systems, is a goal-seeking system. The
following classes of problems are involved in developing a cognitive radio network:
(1) Specifying the goal that the system is pursuing (i.e. efficient spectrum utilization
(2) Discriminating between the available alternatives based on the meaning of a desirable decision.
(3) Choosing a desirable action based on a decision-making process.
By the same token, every subsystem in the network (i.e. every cognitive radio) is a
Owing to the master–slave relationship between the legacy and the cognitive wireless worlds, the spectrum supply chain network has a hierarchical structure. Figure 8.2
depicts this hierarchical structure for S secondary users and L spectrum legacy owners,
each of which owns spectrum subbands including Ml subcarriers (l = 1, . . ., L) and provides a service to a number of primary users. In an open spectrum regime, the activities
of the cognitive radio users should not affect the performance of primary users. In other
words, the existence of the cognitive radio users in a spectrum legacy owner’s band
should not be noticed by the primary users that receive service from that legacy owner.
While the primary customers of the legacy owners do not need to know anything about
the secondary users, secondary users should be quite cautious about the activities of the
A cognitive radio network is a dynamic system formed by a group of interacting
subsystems (i.e. cognitive radios). Since the activities of the primary users determine
the available subbands for secondary usage, the role of the primary users in a cognitive radio network can be interpreted as the role of a high-level-network controller
that decides which resources can be used by cognitive radio users. Then, the resourceallocation algorithm used by each cognitive radio user determines the share of that user
The resource-allocation algorithms play the role of local controllers that control the
corresponding subsystems in a decentralized manner. Regarding the master–slave relationship between the two wireless worlds, a decentralized hierarchical control structure
can be considered for a cognitive radio network as depicted in Figure 8.2. Actions of
the high-level controller, which are discrete events, and actions of the local controllers,
which are continuous, are associated with slow and fast dynamics of the network and
The resource-allocation problem is solved in two stages, regarding discrete events
and continuous states. Therefore, the local controllers in Figure 8.3 are two-level controllers. The corresponding two-level control scheme is shown in Figure 8.4. The supervisory-level (i.e. the higher level) controller is, in effect, an event-driven controller that
deals with appearance and disappearance of spectrum holes. The radio scene analyzer
will inform the supervisory-level controller if it detects a change in the status of the
available spectrum holes. In that case, the supervisory-level controller calls for reconfiguration of the actuator in order to adapt the transmitting parameters to the new set of
available channels. The field-level (i.e. the lower level) controller is a state-based controller that adjusts the transmit power over the set of available channels chosen by the
supervisory-level controller according to the interference level in the radio environment.
A cognitive radio may build an internal model for the external world. This model is
used to predict the availability of certain subbands, the duration of their availability, and
the approximate interference level in those subbands. This information will be critical
for providing seamless communication in the dynamic wireless environment. Both the
Figure 8.2. The spectrum supply chain network.
(spectrum legacy owners & their primary customers)
Figure 8.3. Decentralized hierarchical control structure in a cognitive radio network.
Figure 8.4. Two-level control scheme for cognitive radio.
supervisory-level and the field-level controllers will benefit from a predictive model,
which determines the control horizon, to plan ahead.
Security in cognitive radio networks (Clancy and Goergen, 2008)
A discussion of cognitive radio networks would be incomplete without devoting this last
part of the Epilogue to security in these networks.
Recognizing that, by definition, a cognitive radio has a built-in ability to adapt to
the environment, it is essential that the cognitive radio be equipped to select optimal
and secure means of communications. It follows, therefore, that attention ought to be
focused on attacks that are fundamental to the cognitive radio, leaving issues such as
data integrity and confidentiality to higher layer cryptographic techniques (Clancy and
Clancy and Goergen (2008) define three classes of attacks, all of which pertain to the
In this class of attacks, all that an attacker needs to do, for example, is to create a
modulated waveform sufficiently similar to that of the primary user, thereby triggering a “false” positive in the spectrum-sensing algorithm.
To optimize multigoal objective functions, a cognitive radio would have to be
equipped with adaptive-filtering algorithms that are designed to realize such objective functions. In this second class of attacks, the attacker essentially tries to interfere with the adaptive-filtering algorithm so as to “poison” whatever beliefs that the
The multigoal objective function manipulation is just one example of belief
manipulation attacks. In a generic sense, this second class of attacks includes any
attack that is aimed at manipulating the state of an adaptive-filtering algorithm that
jeopardizes the long-term behavior of a cognitive radio.
In the first two classes of attacks, the attacker’s aim is to make the cognitive radio
act in a suboptimal manner. In the third class of attacks, the attacker’s aim is to
“teach” the radio to become unknowingly malicious in a self-propagating manner.
For example, suppose a primary user employs a narrowband modulation scheme and
the secondary user employs OFDM equipped with a few pilot tones superimposed
on a few subcarriers for the purposes of perceptor channel estimation and synchronization. In such a scenario, all that an attacker has to do is to transmit a carrier
on these pilot tones, thereby blocking any useful communication among secondary
To mitigate the effectiveness of attacks, various approaches are described by Clancy
and Goergen (2008); a summary of the approaches follows:
To reduce the gullibility of a cognitive radio to attacks, the radio would need to
be equipped with the means to discriminate between ambient perceptor noise and
RF interference, thereby being able to distinguish between natural and man-made
RF events. This is where the MTM with the harmonic F-test for spectral line components can come to the rescue (Haykin et al., 2009).
In this second approach, attacks against individual radios are mitigated by embedding “common sense” in the design of each radio. To illustrate what we mean by
common sense, consider the following example (Newman, personal communication, 2010). Suppose that a digital radio has been trained to believe that the DTV
signals of interest are 6 MHz wide. Given this knowledge, the attacker slowly causes
the cognitive radio to start believing that the DTV signals are 6.5 MHz wide instead
of 6 MHz wide by emulating a bunch of signals that look like DTV signals except
they are 6.5 MHz wide. In reality, this kind of attack depends on the attacker having
the ability to adapt to noisy conditions through the use of adaptive-filtering algorithms of its own. This example attack may be viewed as the “teaching” attack. As
such, belief-manipulation attacks are the real threat to cognitive radio users, which
conventional wireless systems do not have to worry about. In any event, once the
cognitive radio is taught to misclassify the real DTV signals, the radio may go on
to transmit in those bands and thereby jam all nearly DTV perceptors. This scenario
may, in turn, teach other cognitive radios in the same network that it is apropos to
transmit in the misclassified band, thereby spreading havoc across the entire cognitive radio network.
Even though the ideal cognitive radio has the notion of autonomy and is able
to adapt to environmental changes, there should be some form of anchor so as to
avoid the cognitive radio from being led far off the correct path by a malicious
user. To this end, there could be “common sense” rules built into design of the
cognitive radio, such as primary users will never have a bandwidth larger than 6.4
MHz or the ambient noise will never be larger than −50 dBm (Newman, personal
In this third approach, it is presumed that there is control-channel connectivity
between cognitive radios, which, in turn, suggests the use of swarm intelligence;
swarm intelligence is a set of algorithms that are designed to mimic animal behaviors. One such technique of particular applicability to security is the so-called particle swarm optimization. To elaborate briefly, each cognitive radio in a network
represents a “particle,” with each particle having its own hypothesis about what the
best behavior is in a particular situation, and with that particular hypothesis being a
weighted average of all the hypotheses in the network.
Advanced Television Systems Committee–Digital Television
continuous-discrete cubature Kalman filter
Continuous-discrete extended Kalman filter
continuous-discrete unscented Kalman filter
Federal Communications Commission (in the USA)
gradient Q, where the Q refers to Q-learning
independently and identically distributed
Institute of Electronics and Electrical Engineers
modified forward–backward linear prediction
Moving Picture Expert Group-2 Transport System
multitaper method–singular value decomposition
orthogonal frequency-division multiplexing
perception–action cycle-dynamic programming
two-frequency magnitude-squared coherence
University of Illinois at Urbana-Champaign
Aghassi, M. and Bertsimas, D. (2006). Robust game theory. Mathematical Programming, Series B,
Aleksander, I. and Morton, H. (1990). An Introduction to Neural Computing. London: Chapman
Anderson, J. (1995). An Introduction to Neural Networks. Cambridge, MA: MIT Press.
Anderson, B. D. O. and Moore, J. B. (1979). Linear Optimal Control. Englewood Cliffs, NJ:
Annastasio, T. J. (2003). Vestibulo-occular reflex. In M. A. Arbib, ed., The Handbook of Brain
Theory and Neural Networks, second edition. Cambridge, MA: MIT Press, pp. 1192–96.
Apt, K. R. and Witzel, A. (2006). A generic approach to coalition formation. International Game
Arasaratnam, I. and Haykin, S. (2009). Cubature Kalman filters. IEEE Transactions on Automatic
Arasaratnam, I., Haykin, S., and Hurd, T. R. (2010). Cubature Kalman filtering for continuousdiscrete systems: theory and simulations. IEEE Transactions on Signal Processing, 58, 4977–4993.
Athans, M., Wishner, R. P., and Bertolini, A. (1968). Suboptimal state estimation for continuoustime nonlinear systems from discrete noise measurements. IEEE Transactions on Automatic
Autmann, R. J. and Peleg, B. (1960). Von Neumann–Morgenstern solutions to cooperative games
without side payments. Bulletin of the American Mathematical Society, 6, 173–9.
Baddeley, A. (2003). Working memory: looking back and looking forward. Nature Reviews
Baird, L. C. (1999). Reinforcement learning through gradient descent. Ph.D. thesis, CarnegieMellon University, May.
Barlow, H. (1961). The coding of sensory images. In W. H. Thorpe and O. L. Zangwill, eds,
Current Problems in Animal Behaviour. Cambridge: Cambridge University Press, pp. 331–60.
Barlow, H. (2001). Redundancy reduction revisited. Network: Computational Neural Systems,
Bar-Shalom, Y., Li, X., and Kirburajan, T. (2001). Estimation with Applications to Tracking and
Barto, A. G., Sutton, R. S., and Anderson, C. W. (1983). Neuronlike elements that can solve difficult
learning control problems. IEEE Transactions on Systems, Man, and Cybernetics, 13, 835–46.
Basar, T. and Bernhard, P. (eds) (1995). H ∞-Optimal Control and Related Minimax Design Problems: A Dynamic Game Approach, second edition, Boston. MA: Birkhäuser.
Basar, T. and Olsder, G. J. (1999). Dynamic Noncooperative Game Theory. SIAM.
Bellman, R. E. (1957). Dynamic Programming. Princeton, NJ: Princeton University Press.
Bellman, R. E. (1961). Adaptive Control Processes: A Guided Tour. Princeton, NJ: Princeton
Bellman, R. E. and Dreyfus, S. E. (1962). Applied Dynamic Programming. Princeton, NJ: Princeton
Bengio, Y. and LeCun, Y. (2007). Scaling learning algorithms toward AI. In L. Bottou, O. Chapelle,
D. DeCoste, and J. Weston, eds, Large Scale Kernel Machines. Cambridge, MA: MIT Press.
Bennett, B. Hoffman, D. Nichola, J., and Prokash, C. (1989). Structure from two orthographic
views of rigid motion. journal of the Optical Society of America, A.6, 1052–69.
Bernardo, J. M. and Smith, A. F. M. (1998). Bayesian Theory. Wiley.
Bertsekas, D. P. (2005). Dynamic Programming and Optimal Control, vol. 1, third edition. Athena
Bertsekas, D. P. (2007). Dynamic Programming and Optimal Control, vol. 2, third edition. Athena
Bertsekas, D. P. and Tsitsiklis, J. N. (1996). Neuro-Dynamic Programming. Belmont, MA: Athena
Bertsekas, D. P. and Tsitsiklis, J. N. (2008). Introduction to Probability, second edition. Belmont,
Bishop, C. M. (2006). Pattern Recognition and Machine Learning. Springer.
Boyd, S. and Vandenberghe, L. (2004). Convex Optimization. Cambridge University Press.
Bradtke, S. J. and Barto, A. G. (1996). Linear least-squares algorithms for temporal difference
Brady, M. H. and Cioffi, J. M. (2006). The worst-case interference in DSL systems employing
dynamic spectrum management. EURASIP Journal on Advanced Signal Processing, 1–11.
Bronez, T. P. (1992). On the performance advantage of multitaper spectral analysis. IEEE
Transactions on Signal Processing, 40, 2941–46.
Buddhiko, M. M. (2007). Understanding dynamic spectrum access: models taxonomy, and
challenges. In Proceedings of IEEE DYSPAN, April.
Cappé, O., Moulines, E., and Ryden, T. (2005). Inference in Hidden Markov Models. Springer.
Churchland, P. S. and Sejnowski, T. J. (1992). The Computational Brain. Cambridge, MA: MIT Press.
Clancy, T. C. and Goergen, N. (2008). Security in cognitive radio networks: threats and mitigation.
In International Conference on Cognitive Radio Oriented Wireless Networks and Communucations (Crowncom), May, pp. 1–8.
Cohen, L. (1995). Time–Frequency Analysis. Englewood Cliffs, NJ: Prentice-Hall.
Cools, R. (1997). Constructing cubature formulae: the science behind the art. Acta Numerica, 6, 1–54.
Cover, T. M. and Thomas, J. A. (2006). Elements of Information Theory, second edition. New York: Wiley.
Danskin, J. (1967). The Theory of Max–Min. Springer-Verlag.
Dayan, P. and Abott, L. (2001). Theoretical Neuroscience: Computational and Mathematical
Modeling of Neural Systems. Cambridge, MA: MIT Press.
Donoho, D. L. and Elad, M. (2003). Optimally sparse representation in general (nonorthogonal)
dictionaries via l1 minimization. Proceedings of the National Academy of Sciences of the United
Doya, K., Ishi, S., Pouget A., and Rao, R. (2007). Bayesian Brain: Probabilistic Approaches to
Drosopoulos, A. and Haykin, S. (1992). Adaptive radar parameter estimation with Thomson’s
multiple-window method. In S. Haykin and A. Steinhardt, eds, Radar Detection and Estimation. New York: Wiley.
Dupuy, J.-P. (2009). On the Origins of Cognitive Science: The Mechanization of the Mind.
Elliott, R. J. and Haykin, S. (2010). A Zakai equation derivation of the extended Kalman filter,
FCC. (2002). Spectrum Policy Task Force, Report ET Docket No. 02-135, Federal Communications Commission, November.
FCC. (2008). Second Report and Order and Memorandum Opinion and Order In the Matter of
Unlicensed Operation in the TV Broadcast Bands, Additional Spectrum for Unlicensed Devices
Below 900 MHz and in the 3 GHz Band. Docket number 08-260, Federal Communication Commission, November.
FCC. (2010). Second Memorandum Opinion and Order In the Matter of Unlicensed Operation
in the TV Broadcast Bands, Additional Spectrum for Unlicensed Devices Below 900 MHz and
in the 3 GHz Band. Docket number 10-174, Federal Communication Commission, September.
Fisher, R.A. (1912). On an absolute criteria for fitting frequency curves. Messenger of Mathematics, 41, 155–60.
Fisher, R. A. (1922). On the mathematical foundation of theoretical statistics. Philosophical
Transactions of the Royal Society of London, Series A, 222, 309–68.
Frigo, M. and Johnson, S. G. (2005). The design and implementation of FFTW3. Proceedings of
Fukushima, M. (2007). Stochastic and robust approaches to optimization problems under uncertainty. Proceedings of International Conference on Informatics Research for Development of
Knowledge Society Infrastructure (ICKS), pp. 87–94.
Fundenberg, D. and Levine, D. K. (1999). The Theory of Learning in Games. Cambridge, MA:
Fuster, J. M. (2003). Cortex and Mind: Unifying Cognition. Oxford, UK: Oxford University Press.
Gardner, W. A. (1988). Signal interception: a unifying theoretical framework for feature detection.
IEEE Transactions on Communications, 36, 897–906.
Gardner, W. A. ed. (1994). Cyclostationarity in Communications and Signal Processing. New
Geman, S. and Geman, D. (1984). Stochastic relaxation, Gibbs distributions, and the Bayesian restoration of images. IEEE Transactions on Pattern Analysis and Machine Intelligence, PAMI-6, 721–41.
Giannakis, G. B. and Serpedin, E. (1998). Blind identification of ARMA channels with periodically modulated inputs. IEEE Transactions on Signal Processing, 46, 3099–104.
Gjessing, D. T. (1986). Target Adaptive Matched Illumination Radar: Principles and Applications.
Peter Peregrinus Ltd on Behalf of the Institution of Electrical Engineers, London, UK.
Glimcher, P. W. (2003). Decisions, Uncertainty, and the Brain: The Science of Neuroeconomics.
Golub, G. H. and Van Loan, C. F. (1996). Matrix Computations, third edition. Johns Hopkins
Grenander, U. (1976–1981). Lectures in Pattern Theory I, II and III: Pattern Analysis, Pattern
Synthesis and Regular Structures. Springer-Verlag.
Gross, B. (1964). The Managing of Organizations: The Administrative Struggle. New York: Free
Grossberg, S. (1988). Neural Networks and Natural Intelligence. Cambridge, MA: MIT Press.
Guerci, J. R. (2010). Cognitive Radar: The Knowledge-Aided Fully Adaptive Approach. Artech
Habibi, S. R. and Burton, R. (2003). The variable structure filter. Journal of Dynamic Systems
Hanzo, L., Akhtman, Y., Wang, L., and Jiang, M. (2010). MIMO-OFDM for LTE, WiFi and
WiMAX: Coherent versus Non-Coherent and Cooperative Turbo-Transceivers. Wiley–IEEE.
Harker, P. T. and Pang, J.-S. (1990). Finite-dimensional variational inequality and nonlinear
complementarity problems: a survey of theory, algorithms and applications. Mathematical
Hastie, T., Tishbirani, R., and Friedman, J. (2001). The Elements of Statistical Learning: Data
Mining, Inference, and Prediction. Canada: Springer.
Haykin, S. (2000). Communication Systems, third edition. New York: Wiley.
Haykin, S. (2002). Adaptive Filter Theory, fourth edition. Prentice-Hall.
Haykin, S. (2005a). Cognitive radio: brain-empowered wireless communications. IEEE Journal
on Selected Areas in Communications, 23, 201–20.
Haykin, S. (2005b). Cognitive radar networks. In 1st IEEE Workshop on Computational Advances
in Multi-sensor Adaptive Processing, Jalisco State, Mexico.
Haykin, S. (2006a). Cognitive dynamic systems, point-of-view article. Proceedings of the IEEE,
Haykin, S. (2006b). Cognitive radar: a way of the future. IEEE Signal Processing Magazine, 23, 30–41.
Haykin, S. ed. (2007). Adaptive Radar Signal Processing. Wiley.
Haykin, S. (2009). Neural Networks and Learning Machines. Upper Saddle River, NJ:
Haykin, S. and Thomson, D. J. (1998). Signal detection in a nonstationary environment reformulated as an adaptive pattern classification problem. Proceedings of the IEEE, 86, 2325–44.
Haykin, S., Bakker, R., and Currie, B. (2002). Uncovering nonlinear dynamics: the case study of
sea clutter. Proceedings of the IEEE, 90, 860–81.
Haykin, S., Thomson, D. J., and Reed, J. H. (2009). Spectrum sensing for cognitive radio. Proceedings of the IEEE, 97, 849–77.
Haykin, S., Zia, A., Xue, Y., and Arasaratnam, I. (2011). Control-theoretic approach to tracking
radar: first step towards cognition. Digital Signal Processing 21, 576–85.
Haykin, S. and Xue, Y. (2012). Cognitive Radar. To be published.
Hebb, D. O. (1949). The Organization of Behavior: A Neurosychological Theory. New York: Wiley.
Hecht-Nielsen, R. (1995). Replicator neural networks for universal optimal source and coding.
Ho, Y. C. and Lee, R. C. K. (1964). A Bayesian approach to problems in stochastic estimation and
control. IEEE Transactions on Automatic Control, AC-9, 333–9.
Hurd, H. L. and Miamee, A. (2007). Periodically Correlated Random Sequences. New York: Wiley.
Julier, S. J. and Uhlmann, J. K. (2004). Unscented filtering and nonlinear estimation. Proceedings
Julier, S. J., Uhlmann, J. K., and Durrant-Whyte, H. F. (2000). A new method for nonlinear transformation of means and covariances in filters and estimators. IEEE Transactions on Automatic
Kalman, R. (1960). A new approach to linear filtering and prediction problems. Transactions of
the ASME, Journal of Basic Engineering, Series D, 82, 35–45.
Kavukcuoglu, K., Ranzato, M., and LeCun, Y. (2008). Fast inference in sparse coding algorithms
with applications to object recognition. Technical Report, CBLL, Courant Institute, NYU,
Kershaw, D. J. and Evans, R. J. (1994). Optimal waveform selection for tracking systems. IEEE
Transactions on Information Theory, 40, 1536–50.
Kersten, D. (1990). Statistical limits to image understanding. In C. Blakemore, ed., Vision: Coding
and Efficiency. Cambridge, UK: Cambridge University Press.
Khalil, H. K. (2002). Nonlinear Systems, third edition. Upper Saddle River, NJ: Prentice-Hall.
Khozeimeh, F. and Haykin, S. (2009). Dynamic spectrum management for cognitive radio: an
overview. Wireless Communications and Mobile Computing, 9, 1447–59.
Khozeimeh, F. and Haykin, S. (2010). Self-organizing dynamic spectrum management for
cognitive radio networks. In The 8th Conference on Communication Networks and Services
Knill, D. C. and Pouget, A. (2004). The Bayesian brain: the role of uncertainty in neural coding
and computation for action. Trends in Neuroscience, 12, 712–19.
Knill, D. C. and Richards, W. eds. (1996). Perception as Bayesian Inference. Cambridge, UK:
Krishnamurthy, V. and Djonin, D. V. (2009). Optimal threshold policies for POMDPs in radar
resource management. IEEE Transactions on Signal Processing, 57, 3954–69.
Kumarasan, R. and Tufts, D. W. (1983). Estimating the angles of arrival of multiple plane waves.
IEEE Transactions on Aerospace and Electronic Systems, AES-19, 134–9.
Li, Y. and Stüber, G. (2006). Orthogonal Frequency Division Multiplexing for Wireless Communications. Springer.
Lo, J. T. and Bassu, D. (2001). Adaptive vs. accommodative neural networks for adaptive system
identification. In Proceedings of the International Joint Conference on Neural Networks, Washington, DC, July, pp.1279–1284.
Lo, J. T. and Yu, L. (1995). Adaptive neural filtering by using the innovations approach. In Proceedings of the 1995 World Congress on Neural Networks, vol. 2, July, pp. 29–35.
Loève, M. (1946). Fonctions aléatoires de second ordrer. Revue Scientifique Paris, 84, 195–206.
Loève, M. (1963). Probability Theory. Van Nostrand.
Luo, Z. and Pang, J. (2006). Analysis of iterative waterfilling algorithm for multiuser power control
in digital subscriber lines. EURASIP Journal of Applied Signal Processing, 2006, ID 24012,
Maei, H. R. and Sutton, R. S. (2010). GQ(l ): a general gradient algorithm for temporal difference
prediction learning with eligibility traces. In E. Baum, M. Hutter, and E. Kitzelmann, eds, AGI
Maei, H. R., Szepesva'ri, Cs., Bhatnagar, S., and Sutton, R. S. (2010). Toward off-policy learning
control with function approximation. In Proceedings of the 27th International Conference on
Marple, L. S. L. (1987). Digital Spectral Analysis with Applications. Englewood Cliffs, NJ:
Maybeck, P. S. (1982). Stochastic Models, Estimation, and Control, vol. 2. New York: Academic
Maynard Smith, J. (1974). The theory of games and the evolution of animal conflicts. Journal of
Maynard Smith, J. (1982). Evolution and the Theory of Games. Cambridge, UK: Cambridge
McLaughlin, D., Payne, D., Chandrasekar, V., Philips, B., Kurose, J., Zink, M., et al. (2009).
Short-wavelength technology and the potential for distributed networks of small radar systems.
Bulletin of the American Meteorological Society, 90, 1797–1817.
Mitola, J. (2000). Cognitive radio: an integrated agent architecture for software defined radio.
Doctor of Technology Dissertation, Royal Institute of Technology, Sweden.
Mitola, J. and McGuire, G. Q. (1999). Cognitive radio: making software radios more personal.
IEEE Personal Communications, 6(4), 13–18.
Molisch, A. F. (2005). Wireless Communications. Chichester, UK: IEEE Press/Wiley.
Molisch, A. F., Greenstein, L. J., and Shafi, M. (2009). Propogations issues for cognitive radio,
Mooers, C. N. K. (1973). A technique for the cross-spectrum analysis of pairs of complex-valued
time series, with emphasis on properties of polarized components and rotational invariants.
Morrone, C. and Burr, D. (2009). Visual stability during saccadic eye movements. In M. S.
Gazzaniga, editor-in-chief, The Cognitive Neurosciences, fourth edition. MIT Press, pp. 511–24.
Morse, P. M. and Feshbach, H. (1953). Methods of Theoretical Physics, Part I. New York:
Nagurney, A. and Zhang, D. (1996). Projected Dynamical Systems and Variational Inequalities
Narendra, K. S. and Parthasarathy, K. (1990). Identification and control of dynamical systems
using neural networks. IEEE Transactions on Neural Networks, 1, 4–27.
Nash, J. F. (1950). Equilibrium points in n-person games. Proceedings of the National Academy of
Sciences of the United States of America, 86, 48–9.
Nash, J. F. (1951). Non-cooperative games. Annals of Mathematics, 54, 286–95.
Nicolis, G. and Prigogine, I. (1989). Exploring Complexity: An Introduction. W. H. Freeman.
Olshausen, B. A. (2011). Personal communication.
Olshausen, B. A. and Field, D. J. (1996). Emergence of simple-cell receptive field properties by
learning a sparse code for natural images. Nature, 381, 607–9.
Ortiz, S. (2008). The wireless industry begins to embrace femtocells. Computer, 41, 14–17.
Parsons, J. (2000). The Mobile Radio Propagation Channel. New York: Wiley.
Percival, D. B. and Walden, A. T. (1993). Spectral Analysis for Physical Applications. Cambridge,
Picinbone, B. (1996). Second-order complex random vectors and normal distributions. IEEE
Transactions on Signal Processing, 44, 2637–40.
Posner, M., ed. (1989). Foundations of Cognitive Science. Cambridge, MA: MIT Press.
Powell, W. B. (2007). Approximate Dynamic Programming: Solving the Curses of Dimensionality.
Press, W. and Teukolsky, S. (1990). Orthogonal polynomials and Gaussian quadrature with nonclassical weighting functions. Computers in Physics, 4, 423–26.
Puskorius, G. V. and Feldkamp, L. A. (2001). Parameter-based Kalman filter training: theory and
implementation. In S. Haykin, ed., Kalman Filtering and Neural Networks. Wiley.
Pylyshyn, Z. (1984). Computation and Cognition: Toward a Foundation for Cognitive Science.
Ranzato, M., Boureau, Y., and LeCun, Y. (2007). Sparse feature learning for deep belief networks.
In Neural Information Processing Systems (NIPS), Vancouver, B.C., Canada.
Reber, A. (1995). Dictionary of Psychology. London: Penguin Books.
Ristic, B., Arulampalam, S., and Gordon, N. (2004). Beyond the Kalman Filter: Particle Filters
for Tracking Applications. Boston, MA: Artech House.
Robbins, H. and Monro, S. (1951). A stochastic approximation method. Annals of Mathematical
Robert, C. P. (2007). The Bayesian Choice: From Decision-Theoretic Foundations to Computational Implementations, second edition. Springer.
Robert, C. P. and Casella, G. (2004). Monte Carlo Statistical Methods, second edition. Springer.
Rogers, T. and McLelland, J. L. (2004). Semantic Cognition: A Parallel Distributed Processing
Ross, S. M. (1983). Introduction to Stochastic Dynamic Programming. New York: Academic
Rumelhart, D. E. and McLelland, J. L. eds. (1986). Parallel Distributed Processing: Explorations
in the Microstructure of Cognition, vol. 1. Cambridge, MA: MIT Press.
Rumelhart, D. E., Hinton, G. E., and Williams, R. J. (1986). Learning internal representations
by error propagation. In D. E. Rumelhart and J. L. McLelland, eds, Parallel Distributed Processing: Explorations in the Microstructure of Cognition, vol. 1. Cambridge, MA: MIT Press,
Saad, W., Han, Z., Debbah, M., Horungnes, A., and Basar, T. (2009). Coalitional game theory for
communication networks. IEEE Signal Processing Magazine, 26, 77–97.
Sanger, T. D. (1989). Optimal unsupervised learning in a single-layer linear feedforward neural
Sarkka, S. (2007). On unscented Kalman filtering for state estimation of continuous-time nonlinear systems. IEEE Transactions on Automatic Control, 52, 1631–41.
Schmidt, R. (1981). A signal subspace approach to multiple emitter of location and spectral estimation. Ph.D. dissertation, Stanford University, Stanford, CA.
Schultz, W. (1998). Predictive reward signal of dopamine neurons. Journal of Neurophysiology,
Schuster, H. G. (2001). Complex Adaptive Systems: An Introduction. Springer-Verlag.
Sejnowski, T. J. (2010). Personal communication.
Selfridge, O. G. (1958). Pandamonium: a paradigm for learning. In Proceedings of a Symposium
held at the National Physical Laboratory, November. London: HMSO.
Serpedin, E., Panduru, F., Sari, I., and Giannakis, G. B. (2005). Bibliography on cyclostationarity.
Setoodeh, P. (2010). Dynamic models of cognitive radio networks Ph.D. thesis, McMaster University, Ontario.
Setoodeh, P. and Haykin, S. (2009). Robust transmit power control for cognitive radio. Proceedings of the IEEE, 97, 915–39.
Seung, S. (2000). Half a century of Hebb. Nature Neuroscience, 3, 1166–1167.
Shannon, C. E. (1948). A mathematical theory of communication. Bell System Technical Journal,
Shellhammer, S. J. (2008). Spectrum sensing in IEEE 802.22. In Cognitive Information Processing Workshop, Greece, June 2008.
Shellhammer, S. J. (2010). Personal communication.
Simmons, J. A., Saillant, P. A., and Dear, S. P. (1992). Through a bat’s ear. IEEE Spectrum, 29(3),
Skolnik, M. I. (2008). Radar Handbook. McGraw-Hill.
Slepian, D. (1965). Some asymptotic expansions for prolate spheroidal wave functions. Journal of
Slepian, D. (1978). Prolate spheroidal wave functions, Fourier analysis and uncertainty. Bell
Stein, D. L., ed. (1989). Lectures in the Sciences of Complexity. Addison-Wesley.
Stevenson, C. R., Chouinand, G., Lei, Z., Hu, W., Shellhammer, S. J., and Caldwell, W. (2009).
IEEE 802.22: the first cognitive radio wireless regional area network standard. IEEE Communications Magazine, 47(1), 130–38.
Stroud, A. H. (1966). Gaussian Quadrature Formulas. Englewood Cliffs, NJ: Prentice-Hall.
Stroud, A. H. (1971). Approximate Calculation of Multiple Integrals. Englewood Cliffs, NJ:
Subramanian, A. P. and Gupta, S. H. (2007). Fast spectrum allocation in coordinated spectrum
access based cellular networks. In 2nd IEEE International Symposium on New Frontiers in
Dynamic Spectrum Access Networks, DySPAN 2007, Dubin, 17–20 April, pp. 320–330.
Suga, N. (1990). Cortical computational maps for auditory imaging. Neural Networks, 3, 3–21.
Sutton, R. S. (1984). Temporal credit assignment in reinforcement learning. Ph.D. dissertation,
University of Massachusetts, Amherst, MA.
Sutton, R. S. and Barto, A. G. (1998). Reinforcement Learning: An Introduction. Cambridge, MA:
Sutton, R. S. (1988). Learning to predict by the methods of temporal differences. Machine
Teo, K. (2007). Nonconvex robust optimization. Ph.D. thesis, Massachusetts Institute of Technology, Cambridge, MA.
Thomson, D. J. (1982). Spectrum estimation and harmonic analysis. Proceedings of the IEEE,
Thomson, D. J., (2000). Multitaper analysis of nonstationary and nonlinear time series data. In W.
Fitzgerald, R. Smith, A. Walden, and P. Young, eds, Nonlinear and Nonstationary Signal Processing, Cambridge, UK: Cambridge University Press.
Trappenberg, T. P. (2010). Fundamentals of Computational Neuroscience. Oxford University
Utkin, V. I. (1992). Sliding Modes in Control and Optimization. Springer-Verlag.
Utkin, V. I., Guldner, J., and Shi, J. (2009). Sliding Mode Control in Electro-Mechanical Systems,
Van Trees, H. L. (1968). Detection, Estimation, and Modulation Theory, Part 1. Wiley, New York.
Van Trees, H. L. (1971). Detection, Estimation, and Modulation Theory, Part III. New York: Wiley.
Von Neumann, J. and Morgenstern, O. (1944). Theory of Games and Economic Behavior. Princeton, NJ: Princeton University Press.
Watkins, C. I. C. H. (1989). Learning from delayed rewards. Ph.D. thesis, Cambridge University,
Watkins, C. I. C. H. and Dayan, P. (1992). Q-learning. Machine Learning, 8, 279–92.
Weisbunch, G. (1991). Complex System Dynamics. Addison-Wesley.
Welch, P. D. (1967). The use of fast Fourier transform for the estimation of power spectra: a
method based on time-averaging over short modified periodograms. IEEE Transactions on
Werbos, P. (2004). ADP: goals, opportunities and principles. In J. Si, A. G. Barto, W. B. Powell,
and D. Wusch II, eds Handbook of Learning and Approximate Dynamic Programming. Wiley.
Wicks, M. C. (2010). Waveform diversity: the way forward. Keynote Lecture. In 5th International
Waveform Diversity and Design Conference, Niagara Falls, Ontario, Canada, August.
Wicks, M. C., Mokole, E., Blunt, S., Schneible, R., and Amuso, V. (2010). Principles of Waveform
Widrow, B. and Stearns, S. D. (1985). Adaptive Signal Processing. Prentice-Hall.
Wiener, N. (1948). Cybernetics: Or Control and Communication in the Animal and the Machine.
Williams, J. L., Fisher III, J. W., and Wilsky, A. S. (2007). Approximate dynamic programming for
communication-constrained sensor network management. IEEE Transactions on Signal Processing, 55, 4300–11.
Wolfowitz, J. (1952). On the stochastic approximation method of Robbins and Monro. Annals of
Woodward, P. (1953). Probability and Information Theory, with Applications to Radar, London:
Xue, Y. (2010). Cognitive radar: theory and simulations, Ph.D. thesis, McMaster University, Ontario.
Younger, S., Hockreiter, S., and Conwall, P. (2001). Meta-learning with backpropagation. In Proceedings of the Joint International Conference on Neural Networks, Washington, DC, pp. 2001–6.
Yu, W. (2002). Competition and cooperation in multi-user communication environments. Doctoral dissertation, Stanford University, Stanford, CA.
Yuille, A. L. and Clark, J. (1993). Bayesian models, deformable templates and competitive priors.
In L. Harris and M. Jenkins, eds, Spatial Vision in Humans and Robots. Cambridge, UK: Cambridge University Press.
Zeng, Y., Liang, Y.-C., Hoang, A. T., and Zhang, R. (2010). A review on spectrum sensing for
cognitive radio: challenges and solutions. EURASIP Journal on Advances in Signal Processing,
Zhang, Q. T. (2011). Theoretical performance and thresholds of the multitaper method for spectrum sensing, IEEE transaction on Vehicular Technology, 60, 2128–38.
Bellman, R. E. (1971). Introduction to the Mathematical Theory of Control Processes, vol. II.
Debreu, G. (1952). A social equilibrium existence theorem. Proceedings of National Academy of
Sciences of the United States of America, 38, 886–93.
Dreyfus, S. E. and Law, A. (1977). The Art and Theory of Dynamic Programming. New York:
Facchinei, F. and Pang, J. S. (2003). Finite-Dimensional Variational Inequalities and Complementarity Problems. Springer.
Jazwinski, H. (2007). Stochastic Processes and Filtering Theory. New York: Dover Publications.
Krazios. R. S. and. Stone, L. S (2003). Pursuit eye movements. In M. A. Arbib, ed., The Handbook
of Brain Theory and Neural Networks, second edition, MIT Press, pp. 929–34.
Langoudakis, M. and Parr, R. (2003). Least-squares policy iteration. Journal of Machine Learning
Laplace, P. (1812). Théorie Analytique de Probabilités, Paris: Courcier.
Mumford, D. (1996). Pattern theory: a unifying perspective. In D. C. Knill and W. Richards, eds,
Perception as Bayesian Inference. Cambridge, UK: Cambridge University Press.
Olshausen, B. A. and Field, D. J. (1997). Sparse coding with an overcomplete basis set: a strategy
employed by VI? Vision Research, 37, 3311–25.
Osborne, M. and Rubenstein, A.T. (1994). A Course in Game Theory. Cambridge, MA: MIT
Pang, J. S. and Facchinei, F. (2003). Finite-Dimensional Variational Inequalities and Complementarity Problems. Springer-Verlag.
Putterman, M. L. (1994). Markov Decision Processes: Discrete Stochastic Dynamic Programming. New York: Wiley.
Cognitive radar: perception–action cycle, 169
transition from perception to action, 180
Cognitive radar: single-level memory, 199
environmental scene actuator and executive
environmental scene analyzer and perceptual
orthogonal frequency division multiplexing, 244,
Cramèr representation of stochastic process, 58
cubature rule, third degree, 97, 123, 189
relationship with unscented Kalman filter, 97
Bellman’s dynamic programming algorithm, 130
Emergent behavior of cognitive radio networks, 270
Fundamental equation of spectral analysis, 74
Fundamental integration across time in cognition, 15
Generalized Fourier transform of nonstationary
Hadamard’s condition for well-posedness, 39
Loève’s spectral coherences, first and second, 61
Moore’s approach to complex stochastic processes,
time–frequency analysis, Loève’s nonstationary
Qualcomm competition on wireless microphones,
approximate form of dynamic programming, 141
relationship with dynamic programming, 148
Robbins–Monro stochastic approximation, 166
Ensemble-averages root mean-square error, 196
Self-organized dynamic spectrum management, 265
Spectral coherences of nonstationary processes, 60
