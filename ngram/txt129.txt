2. 12 Bibliographicaland Historical Remarks
Unified Notationfor Episodicand ContinuingTasks
3.11 BibliographicalandHistorical Remarks
EvaluatingOnePolicy While Following Anodier
R-Learningfor UndiscountedContinuingTasks
Games, Afterstates, andOther SpecialCases
6.10 BibliographicalandHistorical Remarks 158
EligibilityTracesfor Actor- CriticMethods 185
Generalizationand FunctionApproximation 193
8.1 ValuePredictionwith FunctionApproximation 194
8.4 Controlwith FunctionApproximation 210
I ampleasedto havethis book by RichardSuttonandAndrewBarto asoneof the first
booksin the new AdaptiveComputationandMachineLearningseries. This textbook
inttoductionto the excitingfield of reinforcementlearning.
pioneersin this field, it providesstudents, practitioners, and
researcherswith an intuitive understandingof the centtal conceptsof reinforcement
learning as well as a precisepresentationof the underlyingmathematics
also communicatesthe excitementof recentpracticalapplicationsof reinforcement
learningandthe relationshipof reinforcementlearningto the corequestionsin artificial
intelligence. Reinforcementlearningpromisesto be an extremelyimportantnew
technologywith immensepracticalimpact and importantscientific insightsinto the
The goal of building systemsthat can adaptto their environmentsand learnfrom
from manyfields, including computerscience
of this researchhas come a wide variety of learning techniquesthat have the potential
to transformmany industrial and scientific fields. Recently, severalresearch
communitieshave begunto convergeon a common set of issuessurroundingsupervised
, andreinforcementlearningproblems. The MIT Pressseries
on Adaptive Computationand Machine Learning seeksto unify the many diverse
strandsof machinelearningresearchand to fosterhigh quality researchand innovative
We first cameto focuson what is now known asreinforcementlearningin late 1979.
We were both at the University of Massachusetts
projectsto revivethe ideathat networksof neuronlikeadaptiveelementsmight prove
to be a promising approachto artificial adaptiveintelligence. The project explored
the " heterostatictheory of adaptivesystems
work was a rich sourceof ideas, and we were permitted to explore them critically
and comparethem with the long history of prior work in adaptivesystems. Our
task becameone of teasingthe ideas apart and understandingtheir relationships
and relative importance. This continuestoday, but in 1979we cameto realize that
perhapsthe simplestof theideas, which hadlong beentakenfor granted, hadreceived
surprisingly little attentionfrom a computationalperspective
idea of a learningsystemthat wants something, that adaptsits behaviorin order to
maximizea specialsignalfrom its environment.This wasthe ideaof a " hedonistic"
learningsystem, or, as we would say now, the ideaof reinforcementlearning.
Like others, we had a sensethat reinforcementlearninghad beenthoroughlyexplored
in the early days of cyberneticsand artificial intelligence. On closer inspection
, though, we found that it had beenexploredonly slightly. While reinforcement
learninghadclearly motivatedsomeof the earliestcomputationalstudiesof learning,
most of theseresearchershad gone on to other things, such as pattern classification, supervisedlearning, and adaptivecontrol, or they had abandonedthe study of
learning altogether. As a result, the specialissuesinvolved in learning how to get
somethingfrom the environmentreceivedrelatively little attention. In retrospect,focusing
on this idea was the critical step that set this branchof researchin motion.
Little progresscould be madein the computationalstudy of reinforcementlearning
until it was recognizedthat such a fundamentalidea had not yet beenthoroughly
The field hascomea long way sincethen, evolving andmaturingin severaldirections
. Reinforcementlearninghasgraduallybecomeone of the most activeresearch
areasin machinelearning, artificial intelligence, and neural network research
field has developedstrong mathematicalfoundationsand impressiveapplications.
The computationalstudy of reinforcementlearning is now a large field, with hundreds
of activeresearchersaroundthe world in diversedisciplinessuchas psychology
theory, artificial intelligence, and neuroscience
havebeenthe contributionsestablishinganddevelopingthe relationshipsto the theory
of optimal control and dynamicprogramming. The overall problemof learning
from interactionto achievegoals is still far from being solved, but our understanding
of it has improved significantly. We can now place componentideas, such as
temporal-differencelearning, dynamic programming, and function approximation,
within a coherentperspectivewith respectto the overall problem.
Our goal in writing this book was to provide a clear and simple accountof the
key ideas and algorithms of reinforcementlearning. We wanted our treatmentto
be accessibleto readersin all of the relateddisciplines, but we could not cover all
of theseperspectivesin detail. Our treatmenttakesalmost exclusivelythe point of
view of artificial intelligenceand engineering, leaving coverageof connectionsto
, and other fields to others or to anothertime. We also
chosenot to producea rigorousformal treatmentof reinforcementlearning. We did
not reachfor the highestpossiblelevel of mathematicalabstractionand did not rely
on a theorem- proof format. We tried to choosea level of mathematicaldetail that
points the mathematicallyinclined in the right directionswithout distractingfrom
the simplicity andpotentialgeneralityof the underlyingideas.
The book consistsof threeparts. Part I is introductoryand problemoriented. We
focus on the simplestaspectsof reinforcementlearningandon its main distinguishing
features. One full chapteris devotedto introducingthe reinforcementlearning
problemwhosesolution we explorein the rest of the book. Part II presentswhat we
seeasthe threemostimportantelementarysolutionmethods:dynamicprogramming,
simple Monte Carlo methods, and temporal-differencelearning. The first of these
is a planning methodand assumesexplicit knowledgeof all aspectsof a problem,
whereasthe other two are learningmethods. Part III is concernedwith generalizing
thesemethodsandblendingthem. Eligibility tracesallow unificationof MonteCarlo
and temporal-differencemethods, and function approximationmethodssuchas artificial
neural networksextendall the methodsso that they can be appliedto much
larger problems. We bring planningand learningmethodstogetheragainand relate
them to heuristic search. Finally, we summarizeour view of the stateof reinforcement
learningresearchand briefly presentcasestudies, including someof the most
impressiveapplicationsof reinforcementlearningto date.
This book was designedto be usedas a text in a one-semestercourse, perhaps
supplementedby readingsfrom the literatureor by a more mathematicaltext such
as the excellentone by Bertsekasand Tsitsiklis ( 1996). This book can also be used
aspart of a broadercourseon machinelearning, artificial intelligence, or neuralnetworks
. In this case, it may be desirableto cover only a subsetof the material. We
recommendcoveringChapter1 for a brief overview, Chapter2 throughSection2.2,
Chapter3 exceptSections3.4, 3.5 and 3.9, and then selectingsectionsfrom the remaining
chaptersaccordingto time and interests. Chapters4, 5, and6 build on each
; of these, Chapter6 is the most important
for the subjectand for the rest of the book. A coursefocusing on machinelearning
or neural networksshouldcover Chapter8, and a coursefocusingon artificial
intelligenceor planning shouldcover Chapter9. Chapter 10 shouldalmost always
be coveredbecauseit is short and summarizesthe overall unified view of reinforcement
learning methodsdevelopedin the book. Throughoutthe book, sectionsthat
aremoredifficult andnot essentialto the restof the book aremarkedwith a * . These
can be omitted on first readingwithout creatingproblemslater on. Someexercises
are marked with a * to indicate that they are more advancedand not essentialto
understandingthe basicmaterialof the chapter.
The book is largely self-contained.The only mathematicalbackgroundassumedis
familiarity with elementaryconceptsof probability, suchas expectationsof random
variables.Chapter8 is substantiallyeasierto digestif the readerhassomeknowledge
of artificial neuralnetworksor someotherkind of supervisedlearningmethod, but it
canbereadwithout prior background.Westronglyrecommendworking theexercises
providedthroughoutthe book. Solutionmanualsareavailableto instructors. This and
other relatedandtimely materialis availablevia the Internet.
At the end of most chaptersis a sectionentitled " Bibliographical and Historical
Remarks," whereinwe credit the sourcesof the ideaspresentedin that chapter, provide
pointersto furtherreadingandongoingresearch
background.Despiteour attemptsto makethesesectionsauthoritativeandcomplete,
we haveundoubtedlyleft out someimportantprior work. For that we apologize, and
welcomecorrectionsandextensionsfor incorporationinto a subsequent
In some sensewe have been working toward this book for twenty years, and
we havelots of peopleto thank. First, we thank thosewho havepersonallyhelped
us developthe overall view presentedin this book: Harry Klopf , for helping us
recognizethat reinforcementlearningneededto be revived; Chris Watkins, Dimitri
, John Tsitsiklis, and Paul Werbos, for helping us see the value of the
relationshipsto dynamic programming; John Moore and Jim Kehoe, for insights
and inspirations from animal learning theory; Oliver Selfridge, for emphasizing
the breadthand importanceof adaptation; and, more generally, our colleaguesand
studentswho havecontributedin countlessways: Ron Williams, CharlesAnderson,
, SteveBradtke, Bob Crites, PeterDayan, and
LeemonBaird. Our view of reinforcementlearninghasbeensignificantly enriched
by discussionswith Paul Cohen, Paul Utgoff, Martha Steenstrup
Mike Jordan, Leslie Kaelbling, Andrew Moore, Chris Atkeson, Tom Mitchell, Nils
Nilsson, Stuart Russell, Tom Dietterich, Tom Dean, and Bob Narendra. We thank
Michael Littman, Gerry Tesauro, Bob Crites, Satinder Singh, and Wei Zhang for
providing specificsof Sections4.7, 11.1, 11.4, 11.5, and 11.6 respectively
the Air Force Office of Scientific Research
GTE Laboratoriesfor their long andfarsightedsupport.
We also wish to thank the many people who have read drafts of this book and
provided valuablecomments, including Tom Kalt, John Tsitsiklis, PawelCichosz,
Olle Gallrno, Chuck Anderson, StuartRussell, Ben Van Roy, Paul Steenstrup
Richard Coggins, Cristina Versino, John H. Hiett, AndreasBadelt, Jay Ponte, Joe
, Satinder Singh, Tommi Jaakkola, Dimitri
Engelbrecht,Torbjom Ekman, ChristinaBjorkman,
JakobCarlsttOm,andOlle Palmgren.Finally, we thankGwyn Mitchell for helpingin
manyways, andHarry StantonandBob Prior for beingour championsat MIT Press.
The idea that we learn by interacting with our environmentis probably the first
to occur to us when we think about the natureof learning. When an infant plays,
wavesits arms, or looks about, it has no explicit teacher, but it doeshave a direct
sensorimotorconnectionto its environment. Exercisingthis connectionproducesa
wealthof informationaboutcauseandeffect, aboutthe consequences
areundoubtedlya major sourceof knowledgeaboutour environmentandourselves.
Whether we are learning to drive a car or to hold a conversation
awareof how our environmentrespondsto what we do, and we seekto influence
what happensthroughour behavior. Learningfrom interactionis a foundationalidea
underlyingnearly all theoriesof learningandintelligence.
In this book we explore a computationalapproachto learning from interaction.
Ratherthan directly theorizing abouthow peopleor animalslearn, we exploreidealizedlearningsituationsandevaluatethe effectivenessof variouslearningmethods.
That is, we adoptthe perspectiveof an artificial intelligenceresearcheror engineer.
We exploredesignsfor machinesthat are effectivein solving learningproblemsof
scientificor economicinterest, evaluatingthe designsthroughmathematicalanalysis
. The approachwe explore, calledreinforcementlearning
, is much more focusedon goal-directedlearningfrom interactionthan are other
Reinforcement learning is learning what to do - how to map situations to actionsso as to maximize a numerical reward signal . The learner is not told which actions
to take, as in most forms of machine learning , but instead must discover which
actions yield the most reward by trying them. In the most interesting and challenging
cases, actions may affect not only the immediate reward but also the next situation
and, through that , all subsequentrewards. These two characteristics- trial -and-error
search and delayed reward- are the two most important distinguishing features of
Reinforcement learning is defined not by characterizing learning methods, but by
characterizing a learning problem . Any method that is well suited to solving that
problem , we consider to be a reinforcement learning method. A full specification of
the reinforcement learning problem in terms of optimal control of Markov decision
processes must wait until Chapter 3 , but the basic idea is simply to capture the
most important aspects of the real problem facing a learning agent interacting with
its environment to achieve a goal . Clearly , such an agent must be able to sense
the state of the environment to some extent and must be able to take actions that
affect the state. The agent also must have a goal or goals relating to the state of
the environment . The formulation is intended to include just these three aspectssensation, action , and goal- in their simplest possible forms without trivializing any
Reinforcement learning is different from supervised learning , the kind of learning
studied in most current research in machine learning , statistical pattern recognition ,
and artificial neural networks. Supervised learning is learning from examples provided
by a knowledgable external supervisor. This is an important kind of learning ,
but alone it is not adequate for learning from interaction . In interactive problems
it is often impractical to obtain examples of desired behavior that are both correct
and representative of all the situations in which the agent has to act. In uncharted
territory - where one would expect learning to be most beneficial - an agent must be
One of the challenges that arise in reinforcement learning and not in other kinds
of learning is the trade- off between exploration and exploitation . To obtain a lot of
reward, a reinforcement learning agent must prefer actions that it has tried in the
past and found to be effective in producing reward. But to discover such actions, it
has to try actions that it has not selected before. The agent has to exploit what it
already knows in order to obtain reward, but it also has to explore in order to make
better action selections in the future . The dilemma is that neither exploration nor
exploitation can be pursued exclusively without failing at the task. The agent must
try a variety of actions and progressively favor those that appear to be best. On a
stochastic task, each action must be tried many times to gain a reliable estimate
of its expected reward. The exploration - exploitation dilemma has been intensively
studied by mathematicians for many decades (see Chapter 2) . For now, we simply
note that the entire issueof balancingexplorationand exploitation does not even
arisein supervisedlearningas it is usuallydefined.
Another key featureof reinforcementlearning is that it explicitly considersthe
whole problem of a goal-directedagentinteractingwith an uncertainenvironment.
esthat considersubproblemswithout addressing
how they might fit into a larger picture. For example, we havementionedthat
much of machinelearning researchis concernedwith supervisedlearning without
explicitly specifyinghow suchan ability would finally be useful. Other researchers
have developedtheoriesof planning with generalgoals, but without considering
planning s role in real-time decision-making, or the questionof where the predictive
modelsnecessaryfor planning would come from. Although theseapproach
haveyieldedmanyusefulresults, their focuson isolatedsubproblemsis a significant
Reinforcementlearning takesthe oppositetack, starting with a complete, interactive
, goal-seekingagent. All reinforcementlearning agentshave explicit goals,
. Moreover, it is usually assumedfrom the beginning that the agent
hasto operatedespitesignificantuncertaintyaboutthe environmentit faces. When
reinforcementlearning involves planning, it has to addressthe interplay between
planningandreal-time actionselection, aswell asthe questionof how environmental
modelsareacquiredandimproved. Whenreinforcementlearninginvolvessupervised
learning, it doesso for specificreasonsthat detenninewhich capabilitiesarecritical
and which are not. For learningresearchto makeprogress, important subproblems
haveto be isolatedand studied, but they shouldbe subproblemsthat play clearroles
in complete, interactive, goal-seekingagents, evenif all the detailsof the complete
One of the larger trendsof which reinforcementlearningis a part is that toward
greatercontactbetweenartificial intelligenceand other engineeringdisciplines. Not
all that long ago, artificial intelligencewas viewedas almostentirely separatefrom
control theoryandstatistics. It hadto do with logic andsymbols, not numbers.Artificial
intelligencewas largeLISP programs, not linear algebra, differential equations,
or statistics. Over the last decadesthis view has gradually eroded. Modem artificial
intelligenceresearchersacceptstatisticaland control algorithms, for example,
as relevantcompetingmethodsor simply as tools of their trade. The previouslyignored
areaslying betweenartificial intelligence and conventionalengineeringare
now amongthe most active, including new fields such as neural networks, intelligent
control, and our topic, reinforcementlearning. In reinforcementlearning we
extendideasfrom optimal control theory and stochasticapproximationto address
the broaderandmore ambitiousgoalsof artificial intelligence.
A good way to understandreinforcementlearningis to considersomeof the examples
and possibleapplicationsthat haveguidedits development
. A masterchessplayer makesa move. The choiceis informedboth by planning- and by immediate, intuitive judgments
anticipatingpossiblereplies and counterreplies
of the desirability of particularpositionsand moves.
. An adaptivecontroller adjustsparametersof a petroleumrefinery' s operationin
real time. The controller optimizesthe yield/cost/quality trade-off on the basis of
specifiedmarginalcostswithout sticking strictly to the setpointsoriginally suggested
. A gazellecalf strugglesto its feet minutesafter beingborn. Half an hour later it is
. A mobile robot decideswhetherit shouldentera newroom in searchof moretrash
to collect or starttrying to find its way backto its batteryrechargingstation. It makes
its decisionbasedon how quickly andeasily it hasbeenableto find the rechargerin
. Phil prepareshis breakfast. Closely examined, eventhis apparentlymundaneactivity
revealsa complexweb of conditionalbehaviorand interlockinggoal- subgoal
relationships:walking to the cupboard,openingit , selectinga cerealbox, thenreaching
for, grasping, andretrievingthe box. Othercomplex, tuned, interactivesequences
of behaviorare requiredto obtain a bowl, spoon, and milk jug . Eachstepinvolvesa
seriesof eyemovementsto obtaininformationandto guidereachingandlocomotion.
Rapidjudgmentsarecontinuallymadeabouthow to carry the objectsor whetherit is
betterto ferry someof themto the dining tablebeforeobtainingothers. Eachstepis
guidedby goals, suchasgraspinga spoonor gettingto the refrigerator, andis in service
of other goals, suchas havingthe spoonto eat with oncethe cerealis prepared
Theseexamplessharefeaturesthat are so basicthat they areeasyto overlook. All
involve interaction betweenaU active decision-making agentand its environment,
within which the agentseeksto achievea goal despiteuncertaintyaboutits environment
. The agent's actionsare pennittedto affect the future stateof the environment
(e.g., the next chessposition, the level of reservoirsof the refinery, the next location
of the robot), therebyaffecting the options and opportunitiesavailableto the agent
at later times. Correct choice requirestaking into accountindirect, delayedconsequences
of actions, andthus may requireforesightor planning.
At the sametime, in all theseexamplesthe effects of actions cannot be fully
predicted; thus the agentmust monitor its environmentfrequentlyand
it from overflowing. All theseexamplesinvolve goals
that the agentcanjudge progresstowardits goal basedon what it can sensedirectly.
The chessplayer knows whetheror not he wins, the refinery controller knows how
much petroleumis being produced, the mobile robot knows when its batteriesrun
down, andPhil knowswhetheror not he is enjoyinghis breakfast.
In all of theseexamplesthe agentcanuseits experienceto improveits performance
overtime. The chessplayerrefinesthe intuition he usesto evaluatepositions, thereby
improving his play; the gazellecalf improvesthe efficiency with which it can ;
Phil learnsto streamlinemaking his breakfast
the task at the start- either from previousexperiencewith relatedtasksor built into
it by designor evolution- influenceswhat is useful or easyto learn, but interaction
with the environmentis essentialfor adjustingbehaviorto exploit specificfeatures
Beyondthe agentand the environment, one can identify four main subelementsof
a reinforcementlearningsystem: a policy, a rewardfunction, a valuefunction, and,
learning agent way of behavingat a given time. Roughly
speaking, a policy is a mappingfrom perceivedstatesof the environmentto actions
to be taken when in those states. It correspondsto what in psychologywould be
called a setof stimulus- responserulesor associations
learningagentin the sensethat it aloneis sufficientto determinebehavior. In general,
A rewardfunction definesthe goal in a reinforcementlearningproblem. Roughly
speaking, it mapseachperceivedstate(or state action pair) of the environment
a single number, a reward, indicating the intrinsic desirability
learningagents soleobjectiveis to maximizethe total rewardit receives
in the long run. The rewardfunction defineswhat are the good and bad eventsfor
the agent. In a biological system, it would not be inappropriateto identify rewards
with pleasureandpain. They arethe immediateanddefiningfeaturesof the problem
facedby the agent. As such, the rewardfunction must necessarilybe unalterableby
the agent. It may, however, serveas a basisfor altering the policy. For example, if
an action selectedby the policy is followed by low reward, then the policy may be
changedto selectsomeother actionin that situationin the future. In general, reward
Whereasa rewardfunction indicateswhat is good in an immediatesense,a value
function specifieswhat is good in the long run. Roughly speaking, the value of
a state is the total amount of reward an agent can expectto accumulateover the
future, startingfrom that state. Whereasrewardsdetenninethe immediate, intrinsic
desirability of environmentalstates, values indicate the long-term desirability of
statesafter taking into accountthe statesthat are likely to follow, and the rewards
availablein thosestates. For example, a statemight alwaysyield a low immediate
reward but still have a high value becauseit is regularly followed by other states
that yield high rewards. Or the reversecould be true. To make a humananalogy,
rewardsare like pleasure(if high) and pain (if low), whereasvaluescorrespondto a
more refinedand farsightedjudgment of how pleasedor displeasedwe are that our
environmentis in a particularstate. Expressedthis way, we hopeit is clearthat value
functionsformalizea basicandfamiliar idea.
Rewardsare in a senseprimary, whereasvalues, as predictionsof rewards, are
. Without rewardstherecould be no values, and the only purposeof estimating
valuesis to achievemore reward. Nevertheless
aremostconcernedwhenmakingandevaluatingdecisions.Action choicesaremade
basedon valuejudgments. We seekactionsthat bring aboutstatesof highestvalue,
not highestreward, becausetheseactionsobtain the greatestamountof rewardfor
us over the long run. In decision-making and planning, the derivedquantity called
valueis the onewith which we are mostconcerned
to detenninevaluesthan it is to detenninerewards. Rewardsare basicallygiven directly
by the environment, but valuesmust be estimatedand reestimatedfrom the
sequencesof observationsan agentmakesover its entire lifetime. In fact, the most
important componentof almost all reinforcementlearning algorithmsis a method
for efficiently estimatingvalues. The centralrole of valueestimationis arguablythe
mostimportantthing we havelearnedaboutreinforcementlearningover the last few
Although all the reinforcementlearning methodswe considerin this book are
structuredaroundestimatingvalue functions, it is not strictly necessaryto do this
to solvereinforcementlearningproblems. For example, searchmethodssuchasgenetic
algorithms, geneticprogramming, simulatedannealing, and other function optimization
methodshavebeenusedto solvereinforcementlearningproblems. These
methodssearchdirectly in the spaceof policieswithout everappealingto valuefunctions
. We call theseevolutionarymethodsbecausetheir operationis analogousto the
way biological evolutionproducesorganismswith skilled behaviorevenwhen they
do not learn during their individual lifetimes. If the spaceof policies is sufficiently
small, or can be structuredso that good policies are commonor easyto find, then
evolutionarymethodscan be effective. In addition, evolutionarymethodshaveadvantages
on problemsin which the learningagentcannotaccuratelysensethe state
, what we meanby reinforcementlearning involves learning while
interactingwith the environment, which evolutionarymethodsdo not do. It is our
belief that methodsable to take advantageof the detailsof individual behavioralinteractions
canbe muchmoreefficientthanevolutionarymethodsin manycases.Evolutionary
methodsignore much of the useful structureof the reinforcementlearning
problem: they do not usethe fact that the policy they are searchingfor is a function
from statesto actions; they do not notice which statesan individual passesthrough
during its lifetime, or which actionsit selects. In somecasesthis information can
be misleading(e.g., when statesare misperceived
moreefficient search. Although evolutionand learningsharemanyfeaturesandcan
naturally work together, asthey do in nature, we do not considerevolutionarymethods
by themselvesto be especiallywell suitedto reinforcementlearningproblems.
For simplicity, in this book when we usethe term " reinforcementlearning" we do
The fourth andfinal elementof somereinforcementlearningsystemsis a modelof
the environment.This is somethingthat mimics the behaviorof the environment.For
example, givena stateandaction, the modelmight predictthe resultantnextstateand
next reward. Models are usedfor planning, by which we meanany way of deciding
on a courseof actionby consideringpossiblefuture situationsbeforetheyareactually
. The incorporationof modelsand planninginto reinforcementlearning
explicitly trial-and-error learners; what they did was viewed as almostthe opposite
, it gradually becameclear that reinforcementlearning
methodsarecloselyrelatedto dynamicprogrammingmethods,which do usemodels,
andthat they in turn arecloselyrelatedto state-spaceplanningmethods.In Chapter9
we explore reinforcementlearning systemsthat simultaneouslylearn by trial and
error, learn a model of the environment, and use the model for planning. Modem
reinforcementlearningspansthe spectrumfrom low-level, trial-and-error learningto
To illusttate the general idea of reinforcement learning and contrast it with other
approaches, we next consider a single example in more detail .
Consider the familiar child ' s game of tic - tac-toe. lWo players take turns playing
on a three- by -three board. One player plays Xs and the other Os until one player
wins by placing three marks in a row , horizontally , vertically , or diagonally , as the X
If the board fills up with neitherplayer getting three in a row, the gameis a draw.
Becausea skilled playercanplay soasneverto lose, let usassumethat we areplaying
againstan imperfectplayer, one whoseplay is sometimesincorrectand allows us to
win. For the moment, in fact, let us considerdrawsand lossesto be equally bad for
us. How might we constructa playerthat will find the imperfectionsin its opponent's
play and learnto maximizeits chancesof winning?
Although this is a simpleproblem, it cannotreadily be solvedin a satisfactoryway
. For example, the classical" minimax" solution from
gametheoryis not correctherebecauseit assumesa particularway of playing by the
opponent.For example, a minimaxplayerwould neverreacha gamestatefrom which
it could lose, evenif in fact it alwayswon from that statebecauseof incorrectplay by
the opponent.Classicaloptimizationmethodsfor sequentialdecisionproblems, such
as dynamic programming, can computean optimal solution for any opponent, but
requireasinput a completespecificationof that opponent,including the probabilities
with which the opponentmakeseachmove in eachboard state. Let us assumethat
this information is not availablea priori for this problem, as it is not for the vast
majority of problemsof practicalinterest. On the otherhand, suchinformationcanbe
, in this caseby playing manygamesagainstthe opponent.
About the bestonecando on this problemis first to learna modelof the opponent's
behavior, up to somelevel of confidence, and then apply dynamicprogrammingto
computean optimal solutiongiventhe approximateopponentmodel. In the end, this
is not that different from someof the reinforcementlearning methodswe examine
An evolutionary approachto this problem would directly searchthe spaceof
possiblepolicies for one with a high probability of winning againstthe opponent.
Here, a policy is a rule that tells the playerwhat moveto makefor everystateof the
game- every possibleconfigurationof Xs and Os on the three-by-threeboard. For
, an estimateof its winning probability would be obtainedby
of gamesagainstthe opponent. This evaluationwould then
direct which policy or policies would be considerednext. A typical evolutionary
method would hill -climb in policy space, successivelygeneratingand evaluating
. Or, perhaps, ageneticpolicies in an attemptto obtain incrementalimprovements
policies. Literally hundredsof different optimizationmethodscould be applied. By
directly searchingthe policy spacewe meanthat entire policies are proposedand
Here is how the tic -tac-toe problem would be approachedusing reinforcement
learning and approximatevalue functions. First we set up a table of numbers, one
for eachpossiblestateof the game. Each numberwill be the latestestimateof the
probability of our winning from that state. We treat this estimateasthe states value,
andthe whole table is the learnedvaluefunction. StateA hashigher valuethan state
B , or is considered" better" than stateB, if the currentestimateof the probability of
our winning from A is higher than it is from B. Assumingwe alwaysplay Xs, then
for all stateswith threeXs in a row the probability of winning is I , becausewe have
alreadywon. Similarly, for all stateswith threeOs in a row, or that are filled up,
the correct probability is 0, as we cannotwin from them. We set the initial values
of all the other statesto 0.5, representinga guessthat we have a 50% chanceof
We play many gamesagainstthe opponent. To selectour moveswe examinethe
statesthat would result from eachof our possiblemoves(one for eachblank space
on the board) andlook up their currentvaluesin the table. Most of the time we move
greedily, selectingthe movethat leadsto the statewith greatestvalue, that is, with the
highestestimatedprobability of winning. Occasionally,however,we selectrandomly
from amongthe other movesinstead. Theseare called exploratorymovesbecause
they causeus to experiencestatesthat we might otherwiseneversee. A sequenceof
movesmadeandconsideredduring a gamecanbe diagrammedasin Figure 1.1.
While we are playing, we changethe valuesof the statesin which we find ourselves
during the game. We attemptto make them more accurateestimatesof the
Figure 1.1 A sequenceof tic-tac-toe moves. The solid lines representthe moves taken
during a game; the dashedlines representmovesthat we (our reinforcementlearningplayer)
consideredbut did not make. Our secondmovewasan exploratorymove, meaningthat it was
takeneventhoughanothersibling move, the oneleadingto e* , wasrankedhigher. Exploratory
movesdo not result in any learning, but eachof our other movesdoes, causingbackupsas
suggestedby the curvedarrowsanddetailedin the text.
probabilitiesof winning. To do this, we back up the value of the stateafter each
greedymoveto the statebeforethe move, as suggestedby the arrowsin Figure 1.1.
More precisely, the current value of the earlier stateis adjustedto be closer to the
value of the later state. This can be doneby moving the earlier state's value a fraction
of the way towardthe valueof the later state. If we let 8 denotethe statebefore
the greedymove, and 8' the stateafter the move, then the updateto the estimated
valueof 8, denotedV (8), canbe written as
wherea is a small positivefraction called the step-sizeparameter, which influences
the rateof learning. This updaterule is an exampleof a temporal-differencelearning
method, so called becauseits changesare basedon a difference, V (Sf) - V (s),
The methoddescribedaboveperfonnsquite well on this task. For example, if the
step-size parameteris reducedproperly over time, this methodconverges
play by our player. Furthennore,the movesthentaken(excepton exploratorymoves)
are in fact the optimal moves againstthe opponent. In other words, the method
convergesto an optimal policy for playing the game. If the step-size parameteris
not reducedall the way to zero over time, then this player also plays well against
opponentsthat slowly changetheir way of playing.
This exampleillustratesthe differencesbetweenevolutionarymethodsand methods
that learn value functions. To evaluatea policy, an evolutionarymethodmust
hold it fixed and play many gamesagainstthe opponent, or simulatemany games
using a model of the opponent. The frequencyof wins gives an unbiasedestimate
of the probability of winning with that policy, and can be used to direct the next
policy selection. But eachpolicy changeis madeonly after many games, and only
the final outcomeof eachgameis used: what happensduring the gamesis ignored.
For example, if the player wins, then all of its behaviorin the gameis given credit,
independentlyof how specificmovesmight havebeencritical to the win. Credit is
evengiven to movesthat neveroccurred! Valuefunction methods, in contrast, allow
individual statesto be evaluated. In the end, both evolutionaryand value function
methodssearchthe spaceof policies, but learninga valuefunction takesadvantage
of informationavailableduring the courseof play.
This simpleexampleillustratessomeof the key featuresof reinforcementlearning
methods. First, thereis the emphasison learningwhile interactingwith an environment
, in this casewith an opponentplayer. Second, thereis a cleargoal, and correct
behaviorrequiresplanning or foresight that takes into accountdelayedeffects of
one' s choices. For example, the simplereinforcementlearningplayerwould learnto
setup multimovetrapsfor a shortsightedopponent. It is a striking featureof the reinforcement
learningsolutionthat it canachievethe effectsof planningandlookahead
without using a model of the opponentand without conductingan explicit search
While this exampleillustratessomeof the key featuresof reinforcementlearning,
it is so simple that it might give the impressionthat reinforcementlearningis more
limited than it really is. Although tic-tac-toe is a two- persongame, reinforcement
learning also appliesin the casein which there is no externaladversary
the caseof a " gameagainstnature." Reinforcementlearningalso is not restrictedto
problemsin which behaviorbreaksdown into separateepisodes
gamesof tic-tac-toe, with rewardonly at the end of eachepisode. It is just as applicable
whenbehaviorcontinuesindefinitely andwhenrewardsof variousmagnitudes
Tic-tac-toe hasa relatively small, finite stateset, whereasreinforcementlearning
can be usedwhen the stateset is very large, or even infinite. For example, Gerry
Tesauro( 1992, 1995) combined the algorithm describedabove with an artificial
neuralnetwork to learn to play backgammon
With this many statesit is impossibleeverto experiencemore than a small fraction
of them. Tesauro's programlearnedto play far betterthananypreviousprogram, and
now playsat the level of the world' s besthumanplayers(seeChapter11). The neural
network providesthe programwith the ability to generalizefrom its experience
that in new statesit selectsmovesbasedon information savedfrom similar states
facedin the past, as detenninedby its network. How well a reinforcementlearning
systemcan work in problemswith such large state sets is intimately tied to how
appropriatelyit cangeneralizefrom pastexperience
greatestneedfor supervisedlearning methodswith reinforcementlearning. Neural
networksare not the only, or necessarilythe best, way to do this.
In this tic-tac-toe example, learningstartedwith no prior knowledgebeyondthe
rules of the game, but reinforcementlearningby no meansentailsa tabuiarasaview
of learningand intelligence. On the contrary, prior informationcan be incorporated
into reinforcementlearning in a variety of ways that can be critical for efficient
learning. We also had accessto the true statein the tic-tac-toe example, whereas
reinforcementlearning can also be applied when part of the state is hidden, or
when different statesappearto the learnerto be the same. That case, however, is
substantiallymoredifficult , and we do not coverit significantlyin this book.
Finally, thetic-tac-toeplayerwasableto look aheadandknow the statesthat would
resultfrom eachof its possiblemoves. To do this, it hadto havea modelof the game
that allowed it to " think about" how its environmentwould changein responseto
movesthat it might nevermake. Many problemsare like this, but in othersevena
short-term modelof the effectsof actionsis lacking. Reinforcementlearningcan be
applied in either case. No model is required, but modelscan easily be usedif they
, insteadof playing againsta fixed opponent, the
reinforcementlearningalgorithmdescribedaboveplayedagainstitself. What do you
think would happenin this case? Would it learna different way of playing?
Exercise1.2: Symmetries Many tic-tac-toepositionsappeardifferentbut arereally
the samebecauseof symmetries. How might we amendthe reinforcementlearning
algorithm describedaboveto take advantageof this? In what ways would this improve
it? Now think again. Supposethe opponentdid not takeadvantageof symmetries
. In that case, shouldwe? Is it true, then, that symmetricallyequivalentpositions
Exercise1.3: GreedyPlay Supposethe reinforcementlearningplayerwasgreedy,
that is, it alwaysplayedthe movethat broughtit to the positionthat it ratedthe best.
Would it learn to play better, or worse, than a nongreedyplayer? What problems
Exercise 1.4: Learning from Exploration Supposelearningupdatesoccurredafter
all moves, including exploratorymoves. If the step-size parameteris appropriately
reducedovertime, thenthe statevalueswould convergeto a setof probabilities.
What are the two setsof probabilitiescomputedwhen we do, and when we do not,
learn from exploratorymoves? Assumingthat we do continueto makeexploratory
moves, which set of probabilitiesmight be better to learn? Which would result in
Exercise 1.5: Other Improvements Can you think of other ways to improve the
reinforcementlearningplayer? Can you think of any betterway to solvethe tic-tactoe problemasposed?
Reinforcementlearningis a computationalapproachto understandingand automating
goal-directedlearningand decision-making. It is distinguishedfrom other com
esby its emphasison learningby the individual from direct interaction
with its environment, without relying on exemplarysupervisionor complete
modelsof the environment.In our opinion, reinforcementlearningis the first field to
seriouslyaddressthe computationalissuesthat arisewhen learningfrom interaction
with an environmentin order to achievelong-term goals.
Reinforcementlearningusesa formal frameworkdefiningthe interactionbetween
a learning agentand its environmentin terms of states, actions, and rewards. This
framework is intendedto be a simple way of representingessentialfeaturesof the
artificial intelligenceproblem. Thesefeaturesinclude a senseof causeand effect, a
learning methodsthat we consider in this book. We take the position that value
functionsareessentialfor efficient searchin the spaceof policies. Their useof value
The history of reinforcementlearninghastwo main threads, both long andrich, that
were pursuedindependentlybefore intertwining in modemreinforcementlearning.
One thread concernslearning by trial and error and startedin the psychologyof
animal learning. This thread runs through some of the earliest work in artificial
intelligenceand led to the revival of reinforcementlearningin the early 1980s. The
other threadconcernsthe problem of optimal control and its solution using value
functionsand dynamicprogramming. For the most part, this threaddid not involve
learning. Although the two threadshave beenlargely independent
revolvearounda third, lessdistinct threadconcerningtemporal-differencemethods
suchasusedin the tic-tac-toeexamplein this chapter.All threethreadscametogether
in the late 1980sto producethe modemfield of reinforcementlearningaswe present
The threadfocusingon trial-and-error learningis the one with which we are most
familiar and aboutwhich we havethe most to say in this brief history. Beforedoing
that, however, we briefly discussthe optimal control thread.
The term " optimal control" cameinto usein the late 1950sto describethe problem
of designinga controller to minimize a measureof a dynamicalsystem's behavior
esto this problemwasdevelopedin the mid- 1950sby
-centurytheoryof HamilRichardBellmanandothersthroughextendinga nineteenth
ton andJacobi. This approachusesthe conceptsof a dynamicalsystem's stateandof
a value function, or " optimal return function," to define a functional equation, now
often called the Bellman equation. The classof methodsfor solving optimal control
problemsby solving this equationcameto be known as dynamicprogramming
, 1957a). Bellman( 1957b) alsointroducedthe discretestochasticversionof
the optimal control problemknown as Markovian decisionprocess
Ron Howard ( 1960) devisedthe policy iteration methodfor MD Ps. All of theseare
essentialelementsunderlying the theory and algorithmsof modem reinforcement
Dynamic programmingis widely consideredthe only feasible way of solving
general stochasticoptimal control problems. It suffers from what Bellman called
dimensionality, meaningthat its computationalrequirementsgrow
exponentiallywith the numberof statevariables, but it is still far moreefficient and
more widely applicablethan any other generalmethod. Dynamic programminghas
been extensivelydevelopedsince the late 1950s, including extensionsto partially
observableMD Ps (surveyedby Lovejoy, 1991), many applications(surveyedby
White, 1985, 1988, 1993), approximationmethods(surveyedby Rust, 1996), and
, 1982, 1983). Many excellentmodemtreatments
of dynamicprogrammingareavailable(e.g., Bertsekas
1983; and Whittle, 1982, 1983). Bryson ( 1996) providesan authoritativehistory of
In this book, we consider all of the work in optimal control also to be, in a
sense, work in reinforcementlearning. We define reinforcementlearning as any
effective way of solving reinforcementlearning problems, and it is now clear that
theseproblemsare closely related to optimal control problems, particularly those
formulatedasMD Ps. Accordingly, we mustconsiderthe solutionmethodsof optimal
control, suchas dynamicprogramming, also to be reinforcementlearningmethods.
Of course, almostall of thesemethodsrequirecompleteknowledgeof the systemto
be controlled, and for this reasonit feels a little unnaturalto say that they are part
of reinforcementlearning. On the otherhand, manydynamicprogrammingmethods
areincrementalanditerative. Like learningmethods,they graduallyreachthe correct
. As we showin the restof this book, these
similarities are far more than superficial. The theoriesand solution methodsfor the
casesof completeandincompleteknowledgeare so closelyrelatedthat we feel they
mustbe consideredtogetheraspart of the samesubjectmatter.
Let us return now to the other major threadleading to the modem field of reinforcement
learning, that centeredon the ideaof trial-and-error learning. This thread
began psychology, where reinforcement theoriesof learningare common. Perhaps
the first to succinctlyexpressthe essenceof trial-and-error learningwasEdward
Thorndike. We take this essenceto be the ideathat actionsfollowed by good or bad
outcomeshavetheir tendencyto be reselectedalteredaccordingly. In Thorndikes
, theywill bemorelikelyto recur,. thosewhichare
or closelyfollowedby discomfortto theanimalwill, otherthingsbeingequal,
Thorndikecalled this the " Law of Effect" becauseit describesthe effect of reinforcing
eventson the tendencyto selectactions. Although sometimescontroversial(e.g.,
seeKimble, 1961, 1967; Mazur, 1994), the Law of Effect is widely regardedas an
obviousbasic principle underlyingmuch behavior(e.g., Hilgard and Bower, 1975;
Dennett, 1978; Campbell, 1960; Cziko, 1995).
The Law of Effect includes the two most important aspectsof what we mean
by trial-and-error learning. First, it is selectional, meaningthat it involves trying
alternativesand selectingamongthem by comparingtheir consequences
it is associative, meaning that the alternativesfound by selectionare associated
with particular situations. Natural selectionin evolution is a prime exampleof a
selectionalprocess, but it is not associative
not selectional. It is the combinationof thesetwo that is essentialto the Law of
Effect and to trial-and-error learning. Another way of saying this is that the Law
of Effect is an elementaryway of combiningsearchandmemory: searchin the fonn
of trying and selectingamongmany actionsin each situation, and memory in the
fonn of rememberingwhat actionsworkedbest, associatingthemwith the situations
in which they were best. Combining searchand memoryin this way is essentialto
The ideaof programminga computerto learn by trial and error datesback to the
earliestspeculationsaboutcomputersand intelligence(e.g., Turing, 1950). The earliest computationalinvestigationsof trial-and-error learningto be publishedin detail
were perhapsthoseby Minsky and by Farley and Clark, both in 1954. In his PhiD.
dissertation, Minsky discussedcomputationalmodelsof reinforcementlearningand
describedhis constructionof an analogmachinecomposedof componentshe called
SNARCs (StochasticNeural-Analog ReinforcementCalculators). Farley and Clark
describedanotherneural-network learning machinedesignedto learn by trial and
error. In the 1960sthe tenns " reinforcement
widely usedin the engineeringliteraturefor the first time (e.g., Waltz and Fu, 1965;
Mendel, 1966; Fu, 1970; Mendel and McClaren, 1970). Particularlyinfluential was
Minsky s paper StepsTowardArtificial Intelligence (Minsky, 1961), which discussed
severalissuesrelevantto reinforcementlearning, including what he called
the credit assignmentproblem: How do you distributecredit for successamongthe
manydecisionsthat may havebeeninvolvedin producingit? All of the methodswe
discussin this book are, in a sense,directedtowardsolving this problem.
The interestsof Farley and Clark ( 1954; Clark and Farley, 1955) shifted from
trial-and-error learning to generalizationand patternrecognition, that is, from reinforcemen
learningto supervisedlearning. This begana patternof confusionabout
the relationshipbetweenthesetypesof learning. Many researchers
that they were studyingreinforcementlearningwhenthey wereactuallystudyingsupervised
learning. For example, neuralnetwork pioneerssuchas Rosenblatt( 1962)
and Widrow and Hoff ( 1960) were clearly motivatedby reinforcementlearning- but the systemsthey studied
they usedthe languageof rewardsand punishments
were supervisedlearning systemssuitable for pattern recognition and perceptual
learning. Even today, researchersand textbooksoften minimize or blur the distinction
betweenthesetypesof learning. Somemodemneural-networktextbooksusethe
term " trial-and-error" to describenetworksthat learnfrom trainingexamplesbecause
they useerror information to updateconnectionweights. This is an understandable
confusion, but it substantiallymissesthe essentialselectionalcharacterof trialand error learning.
Partly asa resultof theseconfusions,researchinto genuinetrial-and-error learning
becamerare in the 1960sand 1970s. In the next few paragraphswe discusssomeof
the exceptionsand partial exceptionsto this trend.
One of thesewas the work by a New ZealandresearchernamedJohn Andreae.
Andreae( 1963) developeda systemcalled STeLLA that learnedby trial and error
in interactionwith its environment. This systemincluded an internal model of the
world and, later, an " internalmonologue" to dealwith problemsof hiddenstate(Andreae, 1969a). Andreae's later work ( 1977) placedmoreemphasison learningfrom
a teacher, but still included trial and error. Unfortunately, his pioneeringresearch
was not well known, and did not greatly impact subsequentreinforcementlearning
More influential was the work of Donald Michie. In 1961and 1963he described
a simple trial-and-error learning system for learning how to play tic-tac-toe (or
) calledMENACE (for MatchboxEducableNaughtsandCrosses
Engine). It consistedof a matchboxfor eachpossiblegameposition, eachmatchbox
containinga numberof coloredbeads, a different color for eachpossiblemovefrom
that position. By drawinga beadat randomfrom the matchboxcorrespondingto the
current gameposition, one could determineMENACE' s move. When a gamewas
over, beadswere addedto or removedfrom the boxesusedduring play to reinforce
or punishMENACE' s decisions. Michie andChambers( 1968) describedanothertictac-toe reinforcementlearnercalledGLEE (GameLearningExpectimaxingEngine)
and a reinforcementlearningcontrollercalled BOXES. They appliedBOXES to the
task of learningto balancea pole hingedto a movablecart on the basisof a failure
signal occurringonly when the pole fell or the cart reachedthe end of a track. This
task was adaptedfrom the earlier work of Widrow and Smith ( 1964), who used
supervisedlearning methods, assuminginstruction from a teacheralready able to
bestearly examplesof a reinforcementlearningtask underconditionsof incomplete
knowledge. It influencedmuch later work in reinforcementlearning, beginningwith
someof our own studies(Barto, Sutton, and Anderson, 1983; Sutton, 1984). Michie
has consistentlyemphasizedthe role of trial and error and learning as essential
aspectsof artificial intelligence(Michie, 1974).
Widrow, Gupta, and Maitra ( 1973) modified the LMS algorithm of Widrow and
Hoff ( 1960) to producea reinforcementlearning rule that could learn from success
and failure signalsinsteadof from training examples.They called this form of
learning selectivebootstrapadaptation and describedit as learningwith a critic"
insteadof learningwith a teacher. They analyzedthis rule andshowedhow it could
learn to play blackjack. This was an isolatedforay into reinforcementlearningby
Widrow, whosecontributionsto supervisedlearningweremuch moreinfluential.
Researchon learning automatahad a more direct influenceon the trial-and-error
thread leading to modem reinforcementlearning research. Theseare methodsfor
, purely selectionallearningproblemknown asthe n -anned
bandit by analogyto a slot machine, or " one-armedbandit," exceptwith n levers(see
Chapter2). Learning automataare simple, low-memory machinesfor solving this
problem. Learning automataoriginated in Russiawith the work of Tsetlin ( 1973)
and have beenextensivelydevelopedsince then within engineering(seeNarendra
, 1974, 1989). Barto andAnandan( 1985) extendedthesemethodsto
John Holland ( 1975) outlined a generaltheory of adaptivesystemsbasedon selectional
principles. His early work concernedtrial anderror primarily in its nonassociative
form, asin evolutionarymethodsandthe n-armedbandit. In 1986he introduced
, true reinforcementlearningsystemsincluding association
and valuefunctions. A key componentof Holland' s classifiersystemswasalwaysa
geneticalgorithm, an evolutionarymethodwhoserole wasto evolveusefulrepresentations
. Classifiersystemshavebeenextensivelydevelopedby many researchers
form a major branchof reinforcementlearningresearch(e.g., seeGoldberg, 1989;
Wilson, 1994), but geneticalgorithms- which by themselvesare not reinforcement
The individual most responsiblefor reviving the trial-and-error thread to reinforcement
learningwithin artificial intelligencewasHarry Klopf ( 1972, 1975, 1982).
Klopf recognizedthat essentialaspectsof adaptivebehaviorwerebeinglost aslearning
cameto focus almostexclusivelyon supervisedlearning. What was
missing according to Klopf , were the hedonic aspectsof behavior, the drive to
achievesomeresult from the environment, to control the environmenttoward desired
endsandawayfrom undesiredends. This is the essentialideaof trial-and-error
learning. Klopf s ideaswereespeciallyinfluential on the authorsbecauseour assessment
of them (Barto and Sutton, 1981a) led to our appreciationof the distinction
betweensupervisedand reinforcementlearning, and to our eventualfocus on reinforcement
learning. Much of the early work that we and colleaguesaccomplished
was directed toward showing that reinforcementlearning and supervisedlearning
were indeeddifferent (Barto, Sutton, and Brouwer, 1981; Barto and Sutton, 1981b;
Barto andAnandan, 1985). Other studiesshowedhow reinforcementlearningcould
addressimportant problemsin neural network learning, in particular, how it could
producelearningalgorithmsfor multilayer networks(Barto, Anderson, and Sutton,
1982; Barto and Anderson, 1985; Barto and Anandan, 1985; Barto, 1985, 1986;
We turn now to the third thread of the history of reinforcementlearning, that
concerningtemporal-differencelearning. Temporaldifferencelearningmethodsare
distinctivein beingdrivenby the differencebetweentemporallysuccessive
of the samequantity- for example, of the probability of winning in the tic-tac-toe
example. This threadis smallerand lessdistinct thanthe othertwo, but it hasplayed
a particularlyimportantrole in the field, in partbecausetemporal-differencemethods
seemto be new and uniqueto reinforcementlearning.
The origins of temporal-differencelearningarein part in animallearningpsychology
, in particular, in the notion of secondaryreinforcers. A secondaryreinforceris
a stimulusthat hasbeenpaired with a primary reinforcersuchas food or pain and,
as a result, hascometo take on similar reinforcing properties. Minsky ( 1954) may
havebeenthe first to realizethat this psychologicalprinciple could be importantfor
artificial learning systems. Arthur Samuel( 1959) was the first to proposeand implement
a learning methodthat included temporal-differenceideas, as part of his
-playing program. Samuelmadeno referenceto Minsky' s work
or to possibleconnectionsto animal learning. His inspirationapparentlycamefrom
ClaudeShannons ( 1950) suggestionthat a computercould be programmedto use
an evaluationfunction to play chess, and that it might be able to improve its play
by modifying this function on line. (It is possiblethat theseideasof Shannons also
influencedBellman, but we know of no evidencefor this.) Minsky ( 1961) extensively
discussedSamuels work in his Steps paper, suggestingthe connectionto
secondaryreinforcementtheories, both naturaland artificial.
, in the decadefollowing the work of Minsky and Samuel,
little computationalwork was done on trial-and-error learning, and apparentlyno
computationalwork at all wasdoneon temporaldifferencelearning. In 1972, Klopf
differencelearning. Klopf was interestedin principlesthat would scaleto learning
in largesystems,andthus wasinbigued by notionsof local reinforcement,whereby
of an overall learningsystemcould reinforceoneanother. He developed
the ideaof " generalizedreinforcement," wherebyeverycomponent(nominally,
everyneuron) views all of its inputs in reinforcementterms: excitatoryinputs as rewards
know astemporal-differencelearning, and in retrospectit is fartherfrom it thanwas
Samuel's work. On the otherhand, Klopf linked the ideawith bialand -error learning
andrelatedit to the massiveempirical databaseof animallearningpsychology.
Sutton ( 1978a, 1978b, 1978c) developedKlopf ' s ideasfurther, particularly the
links to animallearningtheories, describinglearningrulesdrivenby changesin temporally
successivepredictions. He and Barto refined theseideas and developeda
psychologicalmodel of classicalconditioning basedon temporal-differencelearning
(Sutton and Barto, 1981a; Barto and Sutton, 1982). There followed several
other influential psychologicalmodelsof classicalconditioningbasedon temporaldifferencelearning (e.g., Klopf , 1988; Moore et al., 1986; Suttonand Barto, 1987,
modelsdevelopedat this time arewell interpretedin terms
of temporal-differencelearning (Hawkins and Kandel, 1984; Byrne, Gingrich, and
Baxter, 1990; Gelperin, Hopfield, and Tank, 1985; Tesauro, 1986; Friston et al.,
1994), althoughin mostcasestherewasno historicalconnection. A recentsummary
of links betweentemporal-differencelearningandneuroscienceideasis providedby
Our early work on temporal-differencelearningwas strongly influencedby ani'
mallearning theoriesandby Klopf s work. Relationshipsto Minsky s Steps paper
and to Samuels checkersplayers appearto havebeenrecognizedonly afterward.
By 1981, however, we were fully awareof all the prior work mentionedaboveas
part of the temporal-differenceandtrial-and-error threads.At this time we developed
a methodfor using temporal-differencelearning in trial-and-error learning, known
as the actor- critic architecture, and appliedthis methodto Michie and Chambers
pole balancingproblem(Barto, Sutton, and Anderson, 1983). This methodwasextensively
studiedin Sutton's ( 1984) Ph.D. dissertationandextendedto usebackprop'
agationneuralnetworksin Andersons ( 1986) Ph.D. dissertation. Around this time,
Holland ( 1986) incorporatedtemporal-differenceideasexplicitly into his classifier
systems.A key stepwas takenby Suttonin 1988by separatingtemporal-difference
learning from control, treating it as a generalprediction method. That paper also
introducedthe TD (A) algorithmandprovedsomeof its convergenceproperties.
As we were finalizing our work on the actor- critic architecturein 1981, we discovered
a paperby Ian Witten ( 1977) that containsthe earliestknown publication
of a temporal-difference learning rule. He proposedthe methodthat we now call
tabularTD (O) for useas part of an adaptivecontroller for solving MOPs. Witten s
of Andreaes early experimentswith STeLLA andothertrialand
-error learning systems.Thus, Witten' s 1977paperspannedboth major threads
- trial-and-error learning and optimal controlof reinforcementlearning research
while making a distinct early contributionto temporal-differencelearning.
Finally, the temporal-differenceandoptimal control threadswerefully broughttogether
in 1989with Chris Watkinss developmentof Q-learning. This work extended
andintegratedprior work in all threethreadsof reinforcementlearningresearch
Werbos( 1987) contributedto this integrationby arguingfor the convergence
-error learningand dynamicprogrammingsince 1977. By the time of Watkins's
, priwork there had been tremendousgrowth in reinforcementlearning research
marily in the machinelearning subfield of artificial intelligence,
networksandartificial intelligencemorebroadly. In 1992, the remarkablesuccessof
Gerry Tesauros backgammonplaying program, TD Gammon, brought additional
at the endof the individual chaptersin which they arise.
For additional generalcoverageof reinforcementlearning, we refer the readerto the books
by Bertsekasand Tsitsiklis ( 1996) and Kaelbling ( 1993a). Two specialissuesof the journal
Machine Learning focus on reinforcementlearning: Sutton ( 1992) and Kaelbling ( 1996).
Useful surveysare provided by Barto ( 1995b); Kaelbling, Littman, and Moore ( 1996); and
The exampleof Phil s breakfastin this chapterwasinspiredby Agre ( 1988). We direct the
referencesto the kind of temporal-differencemethodwe usedin the
Modem attemptsto relate the kinds of algorithmsused in reinforcementlearning to the
nervoussystemare madeby Hampson( 1989), Friston et al. ( 1994), Barto ( 1995a), Houk,
Adams, and Barto ( 1995), Montague, Dayan, and Sejnowski( 1996), and Schultz, Dayan, and
The most important featuredistinguishingreinforcementlearningfrom other types
of learningis that it usestraining informationthat evaluatesthe actionstakenrather
than instructs by giving correct actions. This is what createsthe need for active
exploration, for anexplicit trial-and-error searchfor goodbehavior. Purelyevaluative
feedbackindicateshow good the action taken is, but not whetherit is the best or
the worst action possible. Evaluativefeedbackis the basisof methodsfor function
optimization, including evolutionarymethods. Purely instructive feedback, on the
other hand, indicatesthe correctaction to take, independentlyof the action actually
taken. This kind of feedbackis thebasisof supervisedlearning, which includeslarge
partsof patternclassification, artificial neuralnetworks, andsystemidentification. In
their pure forms, thesetwo kinds of feedbackarequite distinct: evaluativefeedback
dependsentirely on the actiontaken, whereasinstructivefeedbackis independentof
the action taken. There are also interestingintermediatecasesin which evaluation
In this chapterwe studytheevaluativeaspectof reinforcementlearningin a simplified setting, onethat doesnot involve learningto act in morethanonesituation. This
nonassociativesettingis the onein which mostprior work involving evaluativefeedback
hasbeendone, and it avoidsmuch of the complexity of the full reinforcement
learningproblem. Studyingthis casewill enableusto seemostclearlyhow evaluative
feedbackdiffers from, and yet canbe combinedwith, instructivefeedback.
, evaluativefeedbackproblem that we explore is a
simple version of the n armed bandit problem. We use this problem to introduce
a numberof basic learning methodswhich we extendin later chaptersto apply to
the full reinforcementlearning problem. At the end of this chapter, we take a step
closerto the full reinforcementlearningproblemby discussingwhat happenswhen
, that is, whenactionsaretakenin morethan
Considerthe following learning problem. You are faced repeatedlywith a choice
amongn different options, or actions. After each choice you receivea numerical
rewardchosenfrom a stationaryprobability distribution that dependson the action
you selected.Yourobjectiveis to maximizetheexpectedtotal rewardover sometime
period, for example, over 1000action selections. Each action selectionis called a
This is the original form of the n -armedbanditproblem, so namedby analogyto a
slot machine, or " one-armedbandit," exceptthat it hasn leversinsteadof one. Each
action selectionis like a play of one of the slot machine's levers, and the rewards
are the payoffs for hitting thejackpot. Throughrepeatedplays you are to maximize
your winnings by concentratingyour plays on the best levers. Another analogyis
that of a doctor choosingbetweenexperimentaltreatmentsfor a seriesof seriously
ill patients. Each play is a treatmentselection, and eachreward is the survival or
well-beingof the patient. Todaythe term " n-armedbanditproblem" is often usedfor
a generalizationof the problemdescribedabove, but in this book we useit to refer
In our n-armedbanditproblem, eachactionhasan expectedor meanrewardgiven
that that action is selected; let us call this the value of that action. If you knew the
value of eachaction, then it would be trivial to solve the n-armedbandit problem:
you would alwaysselectthe action with highestvalue. We assumethat you do not
know the action valueswith certainty, althoughyou may haveestimates
If you maintainestimatesof the actionvalues, thenat anytime thereis at leastone
action whoseestimatedvalueis greatest.We call this a greedyaction. If you selecta
greedyaction, we say that you are exploiting your currentknowledgeof the values
of the actions. If insteadyou selectone of the nongreedyactions, then we say you
are exploring becausethis enablesyou to improve your estimateof the nongreedy
action' s value. Exploitation is the right thing to do to maximizethe expectedreward
on the oneplay, but explorationmay producethe greatertotal rewardin the long run.
For example, supposethegreedyaction' s valueis knownwith certainty, while several
other actions are estimatedto be nearly as good but with substantialuncertainty.
The uncertaintyis suchthat at leastone of theseother actionsprobably is actually
betterthanthe greedyaction, but you don' t know which one. If you havemanyplays
yet to make, then it may be better to explore the nongreedyactions and discover
which of them are better than the greedyaction. Rewardis lower in the short run,
during exploration, but higher in the long run becauseafter you havediscoveredthe
better actions, you can exploit them. Becauseit is not possibleboth to explore and
to exploit with any singleaction selection, one often refersto the conflict between
In any specificcase, whetherit is better to exploreor exploit dependsin acomplex
remaining plays. There are many sophisticatedmethods balancingexploration
andexploitationfor particularmathematicalformulationsof the n-armedbandit and
relatedproblems. However, most of thesemethodsmakestrongassumptionsabout
stationarityand prior knowledgethat are either violated or impossibleto verify in
applicationsand in the full reinforcementlearningproblemthat we considerin subsequent
of optimality or boundedlossfor thesemethodsare
of little comfort whenthe assumptionsof their theorydo not apply.
In this book we do not worry about balancingexplorationand exploitation in a
sophisticatedway; we worry only about balancingthem at all. In this chapterwe
presentseveralsimplebalancingmethodsfor the n armedbanditproblemand show
that they work much betterthan methodsthat alwaysexploit. In addition, we point
out that supervisedlearning methods(or rather the methodsclosestto supervised
learning methodswhen adaptedto this problem) perform poorly on this problem
becausethey do not balanceexplorationand exploitationat all. The needto balance
explorationand exploitation is a distinctive challengethat arisesin reinforcement
learning; the simplicity of the n-armedbandit problemenablesus to showthis in a
We beginby looking moreclosely at somesimplemethodsfor estimatingthe values
of actions and for using the estimatesto make action selectiondecisions. In this
chapter, we denotethe true (actual) value of action a as Q (a), and the estimated
value at the tth playas Qt(a ) . Recall that the true value of an action is the mean
reward receivedwhen that action is selected. One natural way to estimatethis is
by averagingthe rewardsactually receivedwhen the action was selected. In other
words, if at the tth play actiona hasbeenchosenka timesprior to t , yielding rewards
If ka = 0, then we define Q,(a ) insteadas somedefault value, suchas Qo(a ) = O.
As ka ~ 00, by the law of large numbersQ,(a) convergesto Q* (a) . We call this
the sample-averagemethodfor estimatingaction valuesbecauseeachestimateis a
simple averageof the sampleof relevantrewards. Of coursethis is just one way to
estimateaction values, and not necessarilythe bestone. Nevertheless
staywith this simpleestimationmethodandturn to the questionof how the estimates
The simplestactionselectionrule is to selectthe action(or oneof the actions) with
highestestimatedaction value, that is, to selecton play t one of the greedyactions,
a* , for which Q,(a *) = InaxaQ,(a ) . This methodalwaysexploitscurrentknowledge
to maximizeimmediatereward; it spendsno time at all samplingapparentlyinferior
actionsto seeif theymight really be better. A simplealternativeis to behavegreedily
most of the time, but every once in a while, say with small probability €, instead
selectan action at random, unifonnly, independentlyof the action-value estimates
We call methodsusing this near-greedyaction selectionrule €-greedymethods. An
advantageof thesemethodsis that, in the limit asthe numberof playsincreases
actionwill be sampledan infinite numberof times, guaranteeingthat ka ~ 00 for all
a , andthusensuringthat all the Q,(a) convergeto Q* (a) . This of courseimpliesthat
the probability of selectingthe optimal actionconvergesto greaterthan 1 - €, that is,
to nearcertainty. Thesearejust asymptoticguarantees
the practicaleffectivenessof the methods.
To roughly assessthe relativeeffectivenessof the greedyand €-greedymethods,
we comparedthemnumericallyon a suiteof testproblems. This is a setof 2000randomly
generatedn-armedbandit taskswith n = 10. For eachaction a , the rewards
) probability distribution with mean Q* (a )
and variance1. The 2000 n-armedbandit taskswere generatedby reselectingthe
Q (a ) 2000times, eachaccordingto a normaldistributionwith mean0 andvariance
1. Averagingover tasks, we can plot the performanceand behaviorof variousmethods
as they improvewith experienceover 1000plays, as in Figure 2.1. We call this
Figure 2.1 comparesa greedymethodwith two €-greedymethods(€ = 0.01 and
€ 0.1), as describedabove, on the 10-armedtestbed. Both methodsformed their
action-valueestimatesusing the sample-averagetechnique. The uppergraphshows
the increasein expectedreward with experience
slightly faster than the other methodsat the very beginning, but then leveled off
at a lower level. It achieveda rewardper stepof only about 1, comparedwith the
best possibleof about 1.55 on this testbed. The greedy method performs significantly
worse in the long run becauseit often gets stuck perfonning suboptimal
Figure 2.1 Average perfonnance of E greedy action value methods on the 10- armed testbed.
These data are averages over 2(xx) tasks. All methods used sample averages as their action value estimates.
actions. The lower graph showsthat the greedy method found the optimal action
in only approximatelyone-third of the tasks. In the other two-thirds, its initial
samplesof the optimal action were disappointing, and it neverreturnedto it. The
E-greedymethodseventuallyperform better becausethey continueto explore, and
to improvetheir chancesof recognizingthe optimal action. The E = 0.1 methodexplores
more, and usually finds the optimal action earlier, but neverselectsit more
than 91% of the time. The E = 0.01 method improves more slowly, but eventually
performs better than the E = 0.1 method on both perfonnancemeasures
is also possibleto reduceE over time to try to get the best of both high and low
The advantage of f: -greedy over greedy methods dependson the task. For example,
supposethe reward variance had been larger, say 10 instead of 1. With noisier rewards
it would take more exploration to find the optimal action , and f: -greedy methods
would fare even better relative to the greedy method. On the other hand, if the reward
variances were zero, then the greedy method would know the true value of each
action after trying it once. In this case the greedy method might actually perform
best because it would soon find the optimal action and then never explore . But even
in the deterministic case, there is a large advantage to exploring if we weaken some
of the other assumptions. For example, suppose the bandit task were nonstationary,
that is , that the true values of the actions changed over time . In this case exploration
is needed even in the deterministic case to make sure one of the nongreedy actions
has not changed to become better than the greedy one. As we will see in the next
few chapters, effective nonstationarity is the case most commonly encountered in
reinforcement learning . Even if the underlying task is stationary and deterministic ,
the learner faces a set of banditlike decision tasks each of which changes over time
due to the learning process itself . Reinforcement learning requires a balance between
Exercise 2.1 In the comparison shown in Figure 2.1, which method will perform
best in the long run in terms of cumulative reward and cumulative probability of
selecting the best action? How much better will it be?
Although E"-greedyaction selectionis an effectiveand popular meansof balancing
explorationandexploitationin reinforcementlearning, onedrawbackis that whenit
exploresit choosesequallyamongall actions. This meansthat it is aslikely to choose
the worst-appearingaction as it is to choosethe next-to-bestaction. In taskswhere
the worst actionsare very bad, this may be unsatisfactory
to vary the action probabilitiesas a gradedfunction of estimatedvalue. The greedy
actionis still given the highestselectionprobability, but all the othersarerankedand
rules. The mostcommonsoftmaxmethodusesa Gibbs, or Boltzmann, distribution.
It choosesactiona on the Ith play with probability
where'[' is a positiveparametercalled the temperature
. Low temperaturescausea greaterdifference
in selectionprobability for actionsthat differ in their value estimates
course, the softmax effect can be producedin a large number ways other than
by a Gibbs distribution. For example, one could simply add a randomnumberfrom
a long-tailed distribution to each Q,(a ) and then pick the action whose sum was
Whethersoftmaxaction selectionor E-greedyaction selectionis betteris unclear
and may dependon the task and on humanfactors. Both methodshave only one
parameterthat must be set. Most peoplefind it easierto set the E parameterwith
confidence; setting '[' requiresknowledgeof the likely action valuesand of powers
of e. We know of no carefulcomparativestudiesof thesetwo simpleaction-selection
Exercise2.2 (programming) How doesthe softmaxactionselectionmethodusing
the Gibbs distributionfare on the 10-armedtestbed? Implementthe methodand run
it at severaltemperaturesto producegraphssimilar to thosein Figure 2.1. To verify
your code, first implementthe E-greedymethodsandreproducesomespecificaspect
* Exercise 2.3 Show that in the caseof two actions, the softmaxoperationusing
the Gibbs distributionbecomesthe logistic, or sigmoid, function commonlyusedin
artificial neuralnetworks. What effect doesthe temperatureparameterhaveon the
The n -anned bandit problem we considered above is a case in which the feedback
is purely evaluative. The reward received after each action gives some information
about how good the action was, but it says nothing at all about whether the action was
correct or incorrect , that is , about whether or not it was best. Correctness is a relative
property of actions that can be detennined only by trying them all and comparing
their rewards. In this sense the problem is inherently one requiring explicit search
among the alternative actions. You have to perform some form of the generate and
test method whereby you try actions, observe the outcomes, and selectively retain
those that are the most effective . This is learning by selection, in contrast to learning
by insttuction, and all reinforcementlearningmethodshaveto useit in one form or
This conttastssharply with supervisedlearning, where the feedbackfrom the
environmentdirectly indicateswhat the action shouldhavebeen. In this casethereis
no needto search: whateveractionyou try, you will be told what the right onewould
havebeen. There is no needto try a variety of actions; the insttuctive" feedback" is
typically independentof the actionselected(so is not really feedbackat all). It might
still be necessaryto searchin the parameterspaceof the supervisedlearningsystem
(e.g., the weight spaceof a neuralnetwork), but searchingin the spaceof actionsis
Of course, supervisedlearningis usually appliedto problemsthat are much more
complexin someways than the n-armedbandit. In supervisedlearningthereis not
one situation in which action is taken, but a large set of different situations, each
of which must be respondedto correctly. The main problem facing a supervised
learning systemis to consttuct a mapping from situationsto actions that mimics
the correct actions specifiedby the environmentand that generalizescorrectly to
new situations. A supervisedlearning systemcannotbe said to learn to control its
environmentbecauseit follows, ratherthan influences, the insttuctiveinformationit
receives.Insteadof trying to makeits environmentbehavein a certainway, it tries to
makeitself behaveas insttuctedby its environment.
Focusingon the specialcaseof a single situationthat is encounteredrepeatedly
helps makeplain the distinction betweenevaluationand insttuction. Supposethere
are 100possibleactionsandyou selectactionnumber32. Evaluativefeedbackwould
give you a score, say 7.2, for that action, whereasinsttuctive training information
would saywhatotheraction, sayactionnumber67, would actuallyhavebeencorrect.
The latter is clearly much more informative training information. Even if insttuctional information is noisy, it is still more informative than evaluativefeedback. It
is alwaysttue that a single insttuctioncan be usedto advantageto direct changesin
the actionselectionrule, whereasevaluativefeedbackmustbe comparedwith that of
other actionsbeforeany inferencescanbe madeaboutaction selection.I
1. The differencebetweeninSb'Uctionandevaluationcanbe clarified by contrastingtwo types
of function optimizationalgorithms. Onetype is usedwheninfonnation aboutthe gradient of
the function being minimized (or maximized) is directly available. The gradientinsb' Uctsthe
algorithm as to how it shouldmove in the searchspace. The errorsusedby many supervised
learningalgorithmsare gradients(or approximategradients). The other type of optimization
algorithm usesonly function values, correspondingto evaluativeinfonnation, and has to
The differencebetweenevaluativefeedbackand instructiveinfonnation remains
significant even if there are only two actionsand two possiblerewards. For these
binary bandit tasks, let us call the two rewardssuccessandfailure . If you received
, then you might reasonably infer that whateveraction you selectedwas
correct, and if you receivedfailure, then you might infer that whateveraction you
did not selectwas correct. You could then keep a tally of how often each action
was (inferred to be) correct and selectthe action that was correct most often. Let
us call this the supervisedalgorithm becauseit correspondsmost closely to what
a supervisedlearning methodmight do in the caseof a single input pattern. If the
rewardsare deterministic, then the inferencesof the supervisedalgorithm are all
correctand it perfonnsexcellently. If the rewardsare stochastic, then the picture is
In the stochasticcase, a particularbinary bandit task is definedby two numbers,
the probabilitiesof successfor eachpossibleaction. The spaceof all possibletasksis
thus a unit square, as shownin Figure 2.2. The upper-left and lower-right quadrants
correspondto relatively easytasksfor which the supervisedalgorithm would work
well. For these, the probability of successfor the betteraction is greaterthan0.5 and
the probability of successfor the pooreraction is lessthan 0.5. For thesetasks, the
action inferred to be correct (as describedabove) will actuallybe the correctaction
However, binary bandit tasksin the other two quadrantsof Figure 2.2 are more
difficult andcannotbe solvedeffectively by the supervisedalgorithm. For example,
considera task with successprobabilities0.1 and 0.2, correspondingto point A in
the lower-left difficult quadrantof Figure 2.2. Becauseboth actionsproducefailure
at least80% of the time, any methodthat takesfailure asan indicationthat the other
action wascorrectwill oscillatebetweenthe two actions, neversettlingon the better
one. Now considera task with successprobabilities0.8 and 0.9, correspondingto
point B in the upper-right difficult quadrantof Figure 2.2. In this caseboth actions
producesuccessalmostall the time. Any methodthat takessuccessas an indication
of correctnesscaneasilybecomestuckselectingthe wrong action.
Figure 2.3 showsthe averagebehaviorof the supervisedalgorithm and several
other algorithmson the binary bandit taskscorrespondingto points A and B. For
actively probe the function at additional points in the search space in order to decide where to
go next. Classical examples of these types of algorithms are, respectively , the Robbins Monro
and the Kiefer - Wolfowitz stochastic approximation algorithms ( see, e.g ., Kashyap, Blaydon ,
Figure 2.2 The easyanddifficult regionsin the space of all binary bandit tasks.
comparison,alsoshownis the behaviorof an f.-greedyaction-valuemethod(f. = 0.1)
as describedin Section2.2. In both tasks, the supervisedalgorithmlearnedto select
the betteractiononly slightly morethan half the time.
The graphsin Figure 2.3 also showthe averagebehaviorof two other algorithms,
known as LR- I and LR- P' Theseare classicalmethodsfrom the field of learning
automatathat follow a logic similar to that of the supervisedalgorithm. Both methods
are stochastic,updatingthe probabilitiesof selectingeachaction, denotedn' , ( 1)
and n' ,(2) . The LR- P methodinfers the correct actionjust as the supervisedalgorithm
does, and then adjustsits probabilitiesas follows. If the action inferred to be
correcton play t wasd" thenn' ,(d,) is incrementeda fraction, a , of the way from its
n" + l (d,) = n' ,(d,) + a [ 1 - n' ,(d,)] .
The probability of the otheractionis adjustedinversely, so that the two probabilities
sumto 1. For the resultsshownin Figure2.3, a was0.1. The ideaof L R- P is similar
to that of the supervisedalgorithm, only it is stochastic. Rather than committing
totally to the action inferredto be best, LR- P graduallyincreasesits probability}
2. Our descriptionis actually a considerablesimplification of theselearningautomataalgorithms
. For example, they are defined as well for n > 2 and often use a different step- size
, the limitations identified in this section
parameteron successand on failure. Nevertheless
Figure 2.3 Performanceof selectedalgorithmson the binary bandit taskscorrespondingto
points A and B in Figure2.2. Thesedataare averagesover 2(xx) runs.
The nameLR- P standsfor " linear, reward- penalty, meaningthat the update(2.3)
is linear in the probabilitiesand that the updateis performedon both success(reward
) plays and failure (penalty) plays. The nameLR- l standsfor linear, reward"
inaction. This algorithmis identicalto LR- P exceptthat it updatesits probabilities
only upon successplays; failure plays are ignoredentirely. The resultsin Figure 2.3
show that LR- Pperforms little , if any, better than the supervisedalgorithm on the
binary bandit taskscorrespondingto points A and B in Figure 2.2. LR- l eventually
performsvery well on the A task, but not on the B task, and learnsslowly in both
Binary bandit tasksare an instructivespecialcaseblendingaspectsof supervised
and reinforcementlearningproblems. Becausethe rewardsare binary, it is possible
to infer somethingabout the correct action given just a single reward. In some
instancesof suchproblems, theseinferencesarequitereasonableandleadto effective
, however, suchinferencesarelessappropriateandlead
to poor behavior. In bandit taskswith nonbinaryrewards, suchas in the IO-armed
testbed, it is not at all clearhow the ideasbehindtheseinferencescould be appliedto
produceeffectivealgorithms. All of theseare very simpleproblems, but alreadywe
seethe needfor capabilitiesbeyondthoseof supervisedlearningmethods.
Exercise2.4 Considera classof simplifiedsupervisedlearningtasksin which there
is only onesituation(input pattern) andtwo actions. Oneaction, saya , is correctand
the other, b, is incorrect. The instructionsignalis noisy: it instructsthe wrong action
with probability p ; that is, with probability p it saysthat b is correct. You can think
of thesetasksas binary bandit tasksif you treat agreeingwith the (possiblywrong)
, anddisagreeingwith it asfailure . Discussthe resulting
class of binary bandit tasks. Is anything specialabout thesetasks? How doesthe
The action-value methodswe have discussedso far all estimateaction valuesas
sampleaveragesof observedrewards. The obvious implementationis to maintain,
for eachactiona , a recordof all the rewardsthat havefollowed the selectionof that
action. Then, whenthe estimateof the valueof actiona is neededat time t , it canbe
computedaccordingto (2.1), which we repeathere:
whererl , . . . , rkaareall therewardsreceivedfollowing all selectionsof actiona prior
to play t . A problemwith this sb'aightforwardimplementationis that its memoryand
computationalrequirementsgrow over time without bound. That is, eachadditional
rewardfollowing a selectionof actiona requiresmorememoryto storeit andresults
in morecomputationbeing requiredto detennineQ,(a ) .
As you might suspect,this is not really necessary
updatefonnulas for computingaverageswith small, constantcomputationrequired
to processeachnew reward. For someaction, let Qk denotethe averageof its first
k rewards(not to be confusedwith Qk(a ), the averagefor actiona at the kth play).
Giventhis averageanda (k + l ) st reward, rk+ l , thenthe averageof all k + 1 rewards
which holds even for k = 0 , obtaining Q 1 = rl for arbitrary Qo. This implementation
requires memory only for Qk and k , and only the small computation (2.4) for each
The update rule (2.4 ) is of a form that occurs frequently throughout this book . The
NewEstimate ~ Old Estimate + StepSize [ Target - Old Estimate] .
The expression [ Target - Old Estimate ] is an error in the estimate. It is reduced by
taking a step toward the Target . The target is presumed to indicate a desirable
direction in which to move , though it may be noisy . In the case above, for example,
Note that the step- size parameter (Step Size ) used in the incremental method
described above changes from time step to time step. In processing the kth reward
for action a , that method uses a step- size parameter of * . In this book we denote the
step- size parameter by the symbol a or , more generally , by ak(a ) . For example, the
above incremental implementation of the sample- average method is described by the
equation ak (a ) = t . Accordingly , we sometimes use the informal shorthand a
to refer to this case, leaving the action dependenceimplicit .
Exercise 2.5 Give pseudocode for a complete algorithm for the n - armed bandit
problem . Use greedy action selection and incremental computation of action values
with a = * step- size parameter. Assume a function bandit (a ) that takes an action
and returns a reward. Use arrays and variables; do not subscript anything by the time
index t . Indicate how the action values are initialized and updated after each reward.
The averagingmethodsdiscussedsofar areappropriatefor a stationaryenvironment,
but not if the bandit is changingover time. As noted earlier, we often encounter
reinforcementlearning problemsthat are effectively nonstationary
makessenseto weight recentrewardsmore heavily than long-pastones. One of the
mostpopularwaysof doing this is to usea constantstep- sizeparameter
the incrementalupdaterule (2.4) for updatingan averageQk of the k pastrewardsis
, a , 0 < a ::5: I , is constant. This resultsin Qk being a
weightedaverageof pastrewardsandthe initial estimateQo:
We call this a weighted averagebecausethe sum of the weights is ( 1 - a )k +
, you cancheckyourself. Notethatthe weight, a ( 1 - a )k- i ,
givento the rewardri dependson how manyrewardsago, k - i , it wasobserved.The
quantity 1 - a is lessthan 1, andthusthe weight givento ri decreases
. In fact, the weight decaysexponentiallyaccording
to the exponenton 1 - a . Accordingly, this is sometimescalled an exponential
Sometimesit is convenientto vary the step-size parameterfrom stepto step. Let
ak(a) denotethe step-sizeparameterusedto processthe rewardreceivedafterthe kth
selectionof actiona. As we havenoted, the choiceak(a) = resultsin the
averagemethod, which is guaranteedto convergeto the true actionvaluesby the law
of largenumbers. But of courseconvergenceis not guaranteedfor all choicesof the
sequence(ak(a)}. A well-known result in stochasticapproximationtheory givesus
the conditionsrequiredto assureconvergencewith probability I :
The first condition is requiredto guaranteethat the stepsare large enoughto eventually
overcomeany initial conditionsor randomfluctuations. The secondcondition
guaranteesthat eventuallythe stepsbecomesmall enoughto assureconvergence
ak(a ) = i , but not for the caseof constantstep size parameter
latter case, the secondcondition is not met, indicatingthat the estimatesnevercompletely
convergebut continue to vary in responseto the most recently received
rewards. As we mentionedabove, this is actually desirablein a nonstationaryenvironment, andproblemsthat areeffectivelynonstationaryarethe norm inreinforcement
of step-sizeparametersthat meettheconditions
(2. 8) often convergevery slowly or needconsiderabletuning in orderto obtaina satisfactory
convergencerate. Although sequencesof step- size parametersthat meet
theseconvergenceconditions are often used in theoreticalwork, they are seldom
, ak(a ), are not constant, thenthe estimate
Qk(a) is a weightedaverageof previouslyreceivedrewardswith a weighting different
from that given by (2. 7). What is the weighting on eachprior rewardfor the
Exercise 2.7 (programming) Design and conduct an experimentto demonstrate
the difficulties that sample-averagemethodshavefor nonstationaryproblems. Use
a modified versionof the to -armedtestbedin which all the Q (a) start out equal
andthentakeindependentrandomwalks. Prepareplots like Figure2. 1 for an action, incrementallycomputedby a = i , andanother
action valuemethodusinga constantstep-sizeparameter
All the methods we have discussed so far are dependent to some extent on the initial
action - value estimates, Qo(a ) . In the language of statistics, these methods are biased
. For the sample-averagemethods, the bias disappearsonce
all actionshavebeenselectedat leastonce, but for methodswith constanta , the bias
, thoughdecreasingover time asgivenby (2.7). In practice, this kind of
bias is usually not a problem, and can sometimesbe very helpful. The downsideis
that the initial estimatesbecome, in effect, a setof parametersthat mustbe pickedby
the user, if only to set them all to zero. The upsideis that they provide an easyway
to supply someprior knowledgeaboutwhat level of rewardscanbe expected.
Initial action valuescan alsobe usedasa simple way of encouragingexploration.
Supposethat insteadof setting the initial action valuesto zero, as we did in the
IO-armed testbed, we set them all to +5. Recall that the Q* (a) in this problem
are selectedfrom a normal distribution with mean 0 and variance I . An initial
estimateof +5 is thus wildly optimistic. But this optimism encouragesaction-value
methodsto explore. Whicheveractionsare initially selected
the starting estimates; the learner switches to other actions, being " disappointed
with the rewardsit is receiving. The result is that all actionsare tried severaltimes
before the value estimatesconverge. The systemdoesa fair amountof exploration
evenif greedyactionsare selectedall the time.
Figure 2.4 showsthe performanceon the 10-armedtestbedof a greedymethod
using Qo(a ) = + 5, for all a. For comparison, also shown is an E-greedy method
with Qo(a) = O. Both methodsuseda constantstep-sizeparameter
the optimistic methodperforms worse becauseit exploresmore, but eventuallyit
performsbetterbecauseits explorationdecreaseswith time. We call this technique
for encouragingexplorationoptimistic initial values. We regardit as a simple trick
that canbe quite effectiveon stationaryproblems, but it is far from beinga generally
useful approachto encouragingexploration. For example, it is not well suited to
nonstationaryproblemsbecauseits drive for explorationis inherentlytemporary. If
, creatinga renewedneedfor exploration, this methodcannothelp.
Indeed, any methodthat focuseson the initial statein any specialway is unlikely
to help with the general nonstationarycase. The beginning of time occurs only
once, and thus we should not focus on it too much. This criticism appliesas well
to the sample-averagemethods, which also treat the beginningof time as a special
methodsare very simple, and one of them or somesimple combinationof them is
often adequatein practice. In the rest of this book we makefrequentuseof several
Exercise2.8 The resultsshownin Figure 2.4 shouldbe quite reliablebecausethey
are averagesover 2000 individual, randomly chosen10-armedbandit tasks. Why,
Figure 2.4 The effect ofoptirnistic initial action valueestimateson the to- armedtestbed.
then, are there oscillations and spikes in the early part of the curve for the optimistic
method? What might make this method perform particularly better or worse, on
A centtal intuition underlying reinforcementlearning is that actions followed by
largerewardsshouldbe mademorelikely to recur, whereasactionsfollowed by small
rewardsshould be madeless likely to recur. But how is the learnerto know what
constitutesa largeor a smallreward? If anactionis takenandthe environmentreturns
a rewardof5 , is that largeor small? To makesuchajudgmentonemustcomparethe
rewardwith somestandardor referencelevel, calledthe referencereward. A natural
choicefor the referencerewardis an averageof previouslyreceivedrewards. In other
words, a rewardis interpretedas large if it is higher than average
comparisonmethods.Thesemethodsaresometimesmoreeffectivethanaction value
methods.They arealsothe precursorsto actor- critic methods, a classof
solving the full reinforcementlearningproblemthat we presentlater.
Reinforcementcomparisonmethodstypically do not maintainestimatesof action
values, but only of an overall reward level. In order to pick among the actions,
they maintain a separatemeasureof their preferencefor eachaction. Let us denote
the preferencefor action a on play t by p,(a) . The preferencesmight be usedto
detennine action - selection probabilities according to a softrnax relationship ~, such as
where 1l',(a) denotesthe probability of selectingaction a on the tth play. The reinforcement
comparisonidea is usedin updatingthe action preferences
differencebetweenthe reward, r " andthe referencereward, ;:, :
where .8 is a positive step-size parameter
probability of reselectingthe actiontaken, andlow
The referencerewardis an incrementalaverageof all recentlyreceivedrewards,
whicheveractionswere taken. After the update(2.10), the referencerewardis updated
wherea , 0 < a ~ I , is a step-size parameteras usual. The initial valueof the reference
reward, ;:0, canbe seteitheroptimistically, to encourageexploration, or according
to prior knowledge. The initial valuesof the action preferencescan all be set to
zero. Constanta is a goodchoiceherebecausethe distributionof rewardsis changing
over time as action selectionimproves. We seeherethe first casein which the
learningproblemis effectively nonstationaryeventhoughthe underlyingproblemis
Reinforcementcomparisonmethodscanbe very effective, sometimesperfonning
even better than action-value methods. Figure 2. 5 showsthe performanceof the
abovealgorithm (a = 0.1) on the to -armedtestbed. The performancesof €-greedy
(€ = 0.1) action-value methodswith a = 0.1 and a = I are also shown for comparison
Exercise2.9 The softmaxaction-selectionrule given for reinforcementcomparison
methods(2. 9) lacks the temperatureparameter
* Exercise2.10 The reinforcementcomparisonmethodsdescribedhere havetwo
, a and.8. Could we, in general, reducethis to oneparameterby
Figure 2.5 Reinforcementcomparisonmethodsversus action-value methodson the 10armedtestbed.
Exercise 2.11 (programming) Supposethe initial referencereward, r , is far too
low. Whateveractionis selectedfirst will thenprobablyincreasein its probability of
selection. Thus it is likely to be selectedagain, and increasedin probability again.
In this way an early action that is no betterthan any other could crowd out all other
actionsfor a long time. To counteractthis effect, it is commonto add a factor of
( 1 - 1l't(at to the incrementin (2.10). Design and implement an experimentto
determinewhetheror not this really improvesthe performanceof the algorithm.
Another class of effective learning methods for the n - anned bandit problem are
pursuit methods. Pursuit methods maintain both action value estimates and action
preferences, with the preferences continually pursuing the action that is greedy
according to the current action value estimates. In the simplest pursuit method , the
action preferences are the probabilities , 1l', (a ), with which each action , a , is selected
After each play , the probabilities are updated so as to make the greedy action
more likely to be selected. After the tth play, let a:+ l = arg maxa Q' + l (a ) denote the
greedy action (or a random sample from the greedy actions if there are more than one)
for the ( t + I ) st play . Then the probability of selecting a' + l = a: + l is incremented a
1l', + l (a:+ l ) = 1l', (a:+ l ) + ,8 [ I - 1l', (a: + l ) ] ,
Figure 2.6 Perfonnanceof the pursuit method vis-a-vis action-value and reinforcement
while the probabilitiesof selectingthe other actionsaredecrementedtowardzero:
irt + l (a ) = irt (a) + fJ [ 0 - irt (a)] ,
The actionvalues, Qt+ I (a), areupdatedin oneof the waysdiscussedin thepreceding
sections, for example, to be sampleaveragesof the observedrewards, using(2.1).
Figure 2.6 showsthe perfonnanceof the pursuit algorithmdescribedabovewhen
theactionvaluesareestimatedusingsampleaverages(incrementallycomputedusing
a = i ). In theseresults, the initial action probabilitieswere 1ro(a ) = k , for all a ,
and the parameterfJ was 0.01. For comparison, we also show the performance
of an E"-greedy method (E" = 0.1) with action valuesalso estimatedusing sample
. The performanceof the reinforcementcomparisonalgorithm from the
previous sectionis also shown. Although the pursuit algorithm performs the best
of thesethree on this task at theseparametersettings, the ordering could well be
different in other cases. All three of thesemethodsappearto have their usesand
Exercise2.12 An E"-greedymethodalwaysselectsa randomactionon a fraction of
the time steps. How aboutthe pursuitalgorithm? Will it eventuallyselectthe optimal
Exercise2.13 For many of the problemswe will encounterlater in this book it is
not feasibleto updateaction probabilitiesdirectly. To usepursuit methodsin these
casesit is necessaryto modify themto useaction preferencesthat arenot probabili-
ties but that detennineaction probabilitiesaccordingto a softmaxrelationshipsuch
as the Gibbs distribution (2.9). How can the pursuit algorithm describedabovebe
modified to be usedin this way? Specify a completealgorithm, including the equations
Exercise2.14 (programming) How well doesthe algorithmyou proposedin Exercise
2. 13perform? Designandrun an experimentassessingthe performanceof your
method. Discussthe role of parametersettingsin your experiment.
Exercise2.15 The pursuit algorithm describedaboveis suitedonly for stationary
environmentsbecausethe action probabilitiesconverge, albeit slowly, to certainty.
How could you combinethe pursuit idea with the E-greedyideato obtain a method
with performancecloseto that of the pursuit algorithm, but that alwayscontinuesto
So far in this chapter we have considered only nonassociative tasks, in which there
is no need to associate different actions with different situations. In these tasks the
learner either tries to find a single best action when the task is stationary, or tries to
track the best action as it changes over time when the task is nonstationary. However,
in a general reinforcement learning task there is more than one situation , and the
goal is to learn a policy : a mapping from situations to the actions that are best in
those situations. To set the stage for the full problem , we briefly discuss the simplest
way in which nonassociative tasks extend to the associative setting .
As an example, suppose there are several different n -armed bandit tasks, and
that on each play you confront one of these chosen at random. Thus , the bandit
task changes randomly from play to play . This would appear to you as a single ,
nonstationary n -armed bandit task whose true action values change randomly from
play to play . You could try using one of the methods described in this chapter that
can handle nonstationarity , but unless the true action values change slowly , these
methods will not work very well . Now suppose, however, that when a bandit task is
selected for you , you are given some distinctive clue about its identity ( but not its
action values) . Maybe you are facing an actual slot machine that changes the color
of its display as it changes its action values. Now you can learn a policy associating
each task, signaled by the color you see, with the best action to take when facing
that task- for instance, if red, play arm 1; if green, play arm 2. With the right policy
you can usually do much better than you could in the absenceof any information
distinguishingone bandittaskfrom another.
This is an exampleof anassociativesearchtask, socalledbecauseit involvesboth
trial-and-error learningin the form of searchfor the bestactionsand associationof
theseactionswith the situationsin which they are best. Associativesearchtasksare
intermediatebetweenthen-armedbanditproblemandthe full reinforcementlearning
problem. They are like the full reinforcementlearningproblemin that they involve
learning a policy, but like our version of the n-armedbandit problem in that each
action affectsonly the immediatereward. If actionsare allowed to affect the next
situationaswell asthereward, thenwe havethe full reinforcementlearningproblem.
Wepresentthis problemin the nextchapterandconsiderits ramificationsthroughout
Exercise 2.16 Supposeyou face a binary bandit task whose true action values
changerandomly from play to play. Specifically, supposethat for any play the true
valuesof actions 1 and 2 are respectively0.1 and 0.2 with probability 0.5 (caseA ),
and 0.9 and 0.8 with probability 0.5 (caseB). If you are not able to tell which case
you faceat anyplay, what is the bestexpectationof successyou canachieveandhow
shouldyou behaveto achieveit? Now supposethat on eachplay you are told if you
are facing caseA or caseB (althoughyou still don t know the true action values).
This is an associativesearchtask. What is the bestexpectationof successyou can
achievein this task, andhow shouldyou behaveto achieveit?
We havepresentedin this chaptersomesimple ways of balancingexplorationand
exploitation. The f -greedymethodschooserandomly a small fraction of the time,
the softmaxmethodsgradetheir action probabilitiesaccordingto the currentactionvalueestimates
, andthe pursuitmethodskeeptaking stepstowardthe currentgreedy
action. Are thesesimple methodsreally the best we can do in terms of practically
useful algorithms? So far, the answerappearsto be " yes. Despitetheir simplicity,
in our opinion the methodspresentedin this chaptercan fairly be consideredthe
state of the art. There are more sophisticatedmethods, but their complexity and
assumptionsmakethem impracticalfor the full reinforcementlearningproblemthat
is our real focus. Startingin Chapter5 we presentlearningmethodsfor solving the
full reinforcementlearningproblemthat usein part the simple methodsexploredin
Although the simple methodsexploredin this chaptermay be the bestwe can do
at present, they are far from a fully satisfactorysolutionto the problemof balancing
explorationand exploitation. We concludethis chapterwith a brief look at someof
the current ideasthat, while not yet practically useful, may point the way toward
One promising idea is to use estimatesof the uncertaintyof the action-value
estimatesto direct and encourageexploration. For example, supposethere are two
actionsestimatedto havevaluesslightly lessthan that of the greedyaction, but that
differ greatly in their degreeof uncertainty. One estimateis nearly certain; perhaps
that action has beentried many times and many rewardshavebeenobserved. The
uncertaintyfor this action s estimatedvalue is so low that its true value is very
unlikely to be higher than the valueof the greedyaction. The other action is known
lesswell, andthe estimateof its valueis very uncertain. The true valueof this action
could easily be betterthan that of the greedyaction. Obviously, it makesmoresense
to explorethe secondaction thanthe first.
This line of thoughtleadsto interval estimationmethods.Thesemethodsestimate
for eachactiona confidenceintervalof the action' s value. That is, ratherthanlearning
that the action' s value is approximately10, they learn that it is between9 and 11
with, say, 95% confidence.The action selectedis then the action whoseconfidence
interval hasthe highestupperlimit . This encouragesexplorationof actionsthat are
uncertainand havea chanceof ultimately being the bestaction. In somecasesone
thatthe optimal actionhasbeenfoundwith confidenceequalto
the confidencefactor (e.g., the 95%). Unfortunately, intervalestimationmethodsare
problematicin practicebecauseof the complexityof the statisticalmethodsusedto
estimatethe confidenceintervals. Moreover, the underlying statisticalassumptions
required by thesemethodsare often not satisfied. Nevertheless
confidenceintervals, or someother measureof uncertainty,to encourageexploration
of particularactionsis soundand appealing.
Thereis alsoa well-known algorithmfor computingthe Bayesoptimal way to balance
explorationandexploitation. This methodis computationallyintractablewhen
doneexactly, but theremay be efficient ways to approximateit. In this methodwe
assumethat we know the distribution of problem instances
of eachpossibleset of true action values. Given any action selection, we can then
computethe probability of eachpossibleimmediaterewardand the resultantposterior
probability distribution over action values. This evolving distribution becomes
the informationstateof the problem. Given a horizon, say 1()()() plays, one canconsider
all possibleactions, all possibleresultingrewards, all possiblenext actions, all
next rewards, and so on for all 1()()() plays. Given the assumptions
probabilitiesof eachpossiblechain of eventscan be determined,and one needonly
pick the best. But the tree of possibilitiesgrowsextremelyrapidly; evenif thereare
only two actionsand two rewards, the tree will have22 leaves. This approacheffectively
turnsthe banditprobleminto an instanceof the full reinforcementlearning
problem. In the end, we may be able to usereinforcementlearningmethodsto approximate
this optimal solution. But that is a topic for currentresearchand beyond
The classicalsolutionto balancingexplorationandexploitationin n-armedbandit
problemsis to computespecialfunctionscalledGinins indices. Theseprovideanoptimal
solution to a certainkind of bandit problemmoregeneralthan that considered
herebut that assumesthe prior distributionof possibleproblemsis known. Unfortunately
, neitherthe theory nor the computationaltractability of this methodappearto
generalizeto the full reinforcementlearningproblemthat we considerin the rest of
2.12 Bibliographicaland HistoricalRemarks
Bandit problemshavebeenstudiedin statistics, engineering,andpsychology.Instatis"
tics, bandit problemsfall underthe heading" sequentialdesignof experiments
by Thompson( 1933, 1934) andRobbins( 1952), andstudiedby Bellman( 1956).
Berry and Fristedt( 1985) provide an extensivetreatmentof bandit problemsfrom the
perspectiveof statistics. Narendraand Thathachar( 1989) treat bandit problemsfrom
, providing a gooddiscussionof the varioustheoreticaltraditions
that havefocusedon them. In psychology, bandit problemshaveplayedroles
in statisticallearningtheory (e.g., Bushand Mosteller, 1955; Estes, 1950).
The term greedyis often usedin the heuristic searchliterature (e.g., Pearl, 1984).
The conflict betweenexplorationand exploitationis known in control engineeringas
the conflict betweenidentification (or estimation) and connol (e.g., Witten, 1976).
Feldbaum( 1965) called it the dual control problem, referring to the needto solvethe
two problemsof identification and control simultaneouslywhen trying to control a
systemunderuncertainty. In discussingaspectsof geneticalgorithms, Holland ( 1975)
emphasizedthe importanceof this conflict, referring to it as the conflict betweenthe
needto exploit and the needfor new information.
Action-valuemethodsfor our n-armedbandit problemwerefirst proposedby Thathachar and Sastry( 1985). Theseare often called estimatoralgorithms in the learning
automataliterature. The term action valueis due to Watkins( 1989). The first to use~greedymethodsmay also havebeenWatkins( 1989, p. 187), but the idea is so simple
2.12 Bibliographical and Historical Remarks
The main argumentand resultsin this sectionwere first presentedby Sutton( 1984).
Furtheranalysisof the relationshipbetweenevaluationand instructionhasbeenpresented
by Barto ( 1985, 1991, 1992), and Barto and Anandan( 1985). The unit-square
of a binary bandittaskusedin Figure2.2 hasbeencalleda contingency
spacein experimentalpsychology(e.g., Staddon, 1983).
Narendraand Thathachar( 1989) provide a comprehensivetreatmentof modem
learning automatatheory and its applications. They also discusssimilar algorithms
from the statisticallearningtheoryof psychology. Other methodsbasedon converting
reinforcementlearning experienceinto target actions were developedby Widrow,
Gupta, and Maitra ( 1973) andby Gallmo and Asplund ( 1995) .
2.5-6 This materialfalls underthe generalheadingof stochasticiterativealgorithms, which
is well coveredby BertsekasandTsitsiklis ( 1996).
Reinforcementcomparisonmethodswere extensivelydevelopedby Sutton ( 1984)
and further refinedby Williams ( 1986, 1992), Kaelbling ( 1993a), and Dayan( 1991).
Theseauthorsanalyzedmany variationsof the idea, including other eligibility terms
The pursuit algorithm is due to Thathacharand Sastry( 1985).
2.10 The termassociativesearchandthe correspondingproblemwereintroducedby Barto,
Sutton, and Brouwer ( 1981) . The term associativereinforcementlearning has also
beenusedfor associativesearch(Barto and Anandan, 1985), but we prefer to reserve
that termasa synonymfor the full reinforcementlearningproblem(asin Sutton, 1984).
We note that Thorndike's Law of Effect (quotedin Chapter I ) describesassociative
searchby referringto the formationof associativelinks betweensituations(states) and
actions. According to the terminologyof operant, or instrumental, conditioning(e.g.,
Skinner, 1938), a discriminativestimulus is a stimulus that signalsthe presenceof
a particularreinforcementcontingency. In our terms, different discriminativestimuli
Interval estimationmethodsare due to Lai ( 1987) and Kaelbling ( 1993a). Bellman
( 1956) wasthe first to showhow dynamicprogrammingcould be usedto computethe
optimal balancebetweenexplorationand exploitation within a Bayesianformulation
of the problem. The surveyby Kumar ( 1985) providesa good discussionof Bayesian
es to theseproblems. The term infonnation state comes
from the literatureon partially observableMD Ps; see, e.g., Lovejoy ( 1991). TheGittins
index approachis due to Gittins and Jones( 1974). Duff ( 1995) showedhow it is
possibleto learnGittins indicesfor banditproblemsthroughreinforcementlearning.
In this chapter we introduce the problem that we try to solve in the rest of the book.
For us, this problem defines the field of reinforcement learning : any method that is
suited to solving this problem we consider to be a reinforcement learning method.
Our objective in this chapter is to describe the reinforcement learning problem in
a broad sense. We try to convey the wide range of possible applications that can be
framed as reinforcement learning tasks. We also describe mathematically idealized
forms of the reinforcement learning problem for which precise theoretical statements
can be made. We introduce key elements of the problem ' s mathematical structure,
such as value functions and Bellman equations. As in all of artificial intelligence ,
there is a tension between breadth of applicability and mathematical tractability .
In this chapter we introduce this tension and discuss some of the trade-offs and
The reinforcementlearningproblemis meantto be a straightforwardframing of the
problem of learning from interactionto achievea goal. The learnerand decisionmakeris called the agent. The thing it interactswith, comprisingeverythingoutside
. Theseinteractcontinually, the agentselecting
actionsand the environmentrespondingto thoseactionsand presentingnew situations
to the agent.I The environmentalso gives rise to rewards, specialnumerical
, andaction insteadof the engineersterniscontroller,
controlled system(or plant), and control signal becausethey are meaningful to a wider
Figure 3.1 The agent- environmentinteractionin reinforcementlearning.
valuesthat the agent tries to maximize over time. A completespecificationof an
environmentdefinesa task, one instanceof the reinforcementlearningproblem.
More specifically, the agent and environmentinteract at eachof a sequenceof
discretetime steps, t = 0, I , 2, 3, . . . .2 At eachtime stept , the agentreceivessome
representationof the environments state, StE 8, where8 is the setof possiblestates,
and on that basis selectsan action, at E A (st), where A (st) is the set of actions
availablein stateSt. One time steplater, in part as a consequence
agentreceivesa numericalreward, rt+1 E 91, and finds itself in a new state, St+ I .
Figure 3.1 diagramsthe agent- environmentinteraction.
At eachtime step, the agentimplementsa mappingfrom statesto probabilities
of selectingeach possibleaction. This mappingis called the agent's policy and is
denoted7ft, where 7ft(S, a ) is the probability that at = a if St = s. Reinforcement
learningmethodsspecify how the agentchangesits policy as a result of its experience
. The agent's goal, roughly speaking, is to maximizethe total amountof reward
This framework is abstractand flexible and can be applied to many different
problemsin manydifferent ways. For example, the time stepsneednot refer to fixed
intervalsof realtime; theycanreferto arbitrarysuccessive
andacting. The actionscanbe low level controls, suchasthe voltagesappliedto the
motorsof a robot arm, or high-level decisions, suchas whetheror not to havelunch
or to go to graduateschool. Similarly, the statescan take a wide variety of forms.
2. Werestrictattentionto discretetime to keepthingsassimpleaspossible, eventhoughmany
of the ideascan be extendedto the continuous-time case(e.g., seeBertsekasand Tsitsiklis,
3. We user ' +1 insteadof r , to denotethe immediaterewarddue to the action takenat time t
becauseit emphasizesthat the next rewardand the next state, S' +I , arejointly detennined.
They can be completelydeterminedby low-level sensations
of objects in a room. Someof what makesup a statecould be basedon memory
of past sensationsor evenbe entirely mental or subjective. For example, an agent
could be in " the state" of not being surewherean object is, or of havingjust been
surprised in someclearly definedsense. Similarly, someactionsmight be totally
mental or computational. For example, someactionsmight control what an agent
choosesto think about, or whereit focusesits attention. In general, actionscan be
any decisionswe want to learn how to make, and the statescan be anythingwe can
In particular, theboundarybetweenagentandenvironmentis not oftenthe sameas
the physicalboundaryof a robot' s or animal' s body. Usually, the boundaryis drawn
closerto the agentthan that. For example, the motorsand mechanicallinkagesof a
robot andits sensinghardwareshouldusuallybe consideredpartsof the environment
rather than partsof the agent. Similarly, if we apply the frameworkto a personor
animal, the muscles, skeleton, and sensoryorgansshouldbe consideredpart of the
environment. Rewards, too, presumablyare computedinsidethe physicalbodiesof
naturaland artificial learningsystems,but areconsideredexternalto the agent.
The generalrule we follow is that anythingthat cannotbe changedarbitrarily by
the agentis consideredto be outsideof it and thus part of its environment. We do
not assumethat everythingin the environmentis unknownto the agent. For example,
the agentoften knowsquite a bit abouthow its rewardsare computedas a function
of its actions and the statesin which they are taken. But we always considerthe
rewardcomputationto be externalto the agentbecauseit definesthe task facing the
agentandthus mustbe beyondits ability to changearbitrarily. In fact, in somecases
the agent may know everythingabout how its environmentworks and still face a
difficult reinforcementlearningtask, just aswe may know exactlyhow a puzzlelike
Rubik' s cubeworks, but still be unableto solveit. The agent- environmentboundary
representsthe limit of the agents absolutecontrol, not of its knowledge.
The agent- environmentboundarycan be locatedat different placesfor different
. In a complicatedrobot, many different agentsmay be operatingat once,
eachwith its own boundary. For example, one agentmay makehigh-level decisions
which form part of the statesfacedby a lower-level agentthat implementsthe highlevel decisions. In practice, the agent- environmentboundaryis determinedonceone
hasselectedparticularstates, actions, and rewards, andthus hasidentifieda specific
The reinforcementlearningframeworkis a considerableabstractionof the problem
of goal-directedlearningfrom interaction. It proposesthat whateverthe details
of the sensory,memory, andconttol apparatus
to achieve, any problemof learninggoal directedbehaviorcan be reducedto three
signalspassingback and forth betweenan agentand its environment: one signal to
representthechoicesmadeby the agent(the actions), onesignalto representthebasis
on which the choicesare made(the states), andone signalto definethe agent's goal
(therewards). This frameworkmay not be sufficientto representall decision-learning
problemsusefully, but it hasprovedto be widely usefuland applicable.
Of course, the particular statesand actionsvary greatly from applicationto application
, and how they are representedcan sttongly affect performance
learning, as in other kinds of learning, suchrepresentationalchoicesare
at presentmore art than science. In this book we offer someadvice and examples
regardinggood ways of representingstatesand actions, but our primary focus is on
generalprinciples for learning how to behaveonce the representationshave been
Example 3.1: Bioreactor Supposereinforcementlearningis being appliedto detennine moment-by-momenttemperaturesand stirring ratesfor a bioreactor(a large
vat of nutrientsand bacteriausedto produceuseful chemicals). The actionsin such
an applicationmight be targettemperatures
lower-level control systemsthat, in turn, directly activateheatingelementsand motors
to attain the targets. The statesare likely to be thermocoupleand other sensory
readings, perhapsfiltered and delayed, plus symbolic inputs representingthe ingredients
in the vat andthe targetchemical. The rewardsmight be moment-by-moment
measuresof the rate at which the usefulchemicalis producedby the bioreactor. Notice
that hereeachstateis a list, or vector, of sensorreadingsand symbolic inputs,
andeachaction is a vectorconsistingof a targettemperatureanda stirring rate. It is
typical of reinforcementlearningtasksto havestatesandactionswith suchstructured
. Rewards,on the other hand, are alwayssinglenumbers.
Example 3.2: Pick-and-Place Robot Considerusing reinforcementlearning to
control the motion of a robot arm in a repetitive pick-and-place task. If we want
to learnmovementsthat are fast and smooth, the learningagentwill haveto control
the motorsdirectly andhavelow-latencyinformationaboutthe currentpositionsand
velocitiesof the mechanicallinkages. The actionsin this casemight be the voltages
applied to each motor at eachjoint , and the statesmight be the latest readingsof
joint anglesand velocities. The reward might be + 1 for each object success
Example 3.3: Recycling Robot A mobile robot has the job of collecting empty
sodacansin an office environment. It has sensorsfor detectingcans, and an arm
and gripper that can pick them up and place them in an onboardbin; it runs on
a rechargeablebattery. The robot' s control systemhas componentsforinterpreting
sensoryinformation, for navigating, and for controlling the arm and gripper.
High-level decisionsabouthow to searchfor cansaremadeby a reinforcementlearning
agentbasedon the currentchargelevel of the battery. This agenthasto decide
whetherthe robot should( 1) actively searchfor a can for a certain period of time,
(2) remainstationaryand wait for someoneto bring it a can, or (3) headback to its
homebaseto rechargeits battery. This decisionhasto be madeeither periodically
or whenevercertain eventsoccur, such as finding an empty can. The agenttherefore
has three actions, and its state is determinedby the stateof the battery. The
rewardsmight be zero most of the time, but then becomepositive when the robot
securesan empty can, or largeand negativeif the batteryruns all the way down. In
this example, the reinforcementlearning agentis not the entire robot. The statesit
monitorsdescribeconditionswithin the robot itself, not conditionsof the robot' sexternalenvironment.The agent's environmentthereforeincludesthe restof the robot,
which might containother complexdecision-making systems,as well asthe robot' s
Exercise3.1 Devisethreeexampletasksof your own that fit into the reinforcement
learningframework, identifying for eachits states, actions, and rewards. Make the
threeexamplesas different from eachother as possible. The frameworkis abstract
andflexible andcanbe appliedin manydifferentways. Stretchits limits in someway
Exercise3.2 Is the reinforcementlearningframeworkadequateto usefully represent
all goal-directedlearningtasks? Can you think of any clearexceptions?
Exercise3.3 Considertheproblemof driving. Youcould definetheactionsin terms
, steeringwheel, and brake, that is, where your body meetsthe
machine. Or you could define them farther out- say, where the rubber meetsthe
road, consideringyour actionsto be tire torques. Or you could define them farther
in- say, where your brain meetsyour body, the actionsbeing muscletwitches to
control your limbs. Or you could go to a really high level and saythat your actions
areyour choicesof whereto drive. What is the right level, the right placeto draw the
line betweenagentandenvironment? On what basisis one locationof the line to be
preferredover another? Is thereany fundamentalreasonfor preferringone location
In reinforcementlearning, the purposeor goal of the agentis formalizedin termsof
a specialrewardsignalpassingfrom the environmentto the agent. At eachtime step,
the rewardis a simplenumber, rt E ffl. Informally, the agent's goal is to maximizethe
total amountof rewardit receives.This meansmaximizingnot immediatereward, but
The use of a reward signal to formalize the idea of a goal is one of the most
distinctive featuresof reinforcementlearning. Although this way of formulating
goalsmight at first appearlimiting , in practiceit hasprovedto be flexible andwidely
applicable. The bestway to seethis is to considerexamplesof how it hasbeen, or
couldbe, used. For example, to makea robot learnto walk, researchers
rewardon eachtime step proportionalto the robot' s forward motion. In making a
robot learnhow to escapefrom a maze, the rewardis often zerountil it escapes
it becomes+ 1. Another commonapproachin mazelearningis to give a rewardof
- 1 for everytime stepthat passesprior to
asquickly aspossible. To makea robot learnto find andcollect empty sodacansfor
recycling, one might give it a rewardof zero most of the time, and then a rewardof
+ 1 for eachcancollected(andconfirmedasempty). Onemight alsowantto give the
robot negativerewardswhen it bumpsinto things or when somebodyyells at it. For
an agentto learn to play checkersor chess, the naturalrewardsare + 1 for winning,
- 1 for losing, and0 for drawingandfor all nontenninal
Youcanseewhat is happeningin all of theseexamples.The agentalwayslearnsto
maximizeits reward. If we want it to do somethingfor us, we must providerewards
to it in sucha way that in maximizingthemthe agentwill alsoachieveour goals. It is
thuscritical that the rewardswe setup truly indicatewhat we want accomplished
particular, the rewardsignal is not the placeto impart to the agentprior knowledge
about how to achievewhat we want it to do.4 For example, a chess-playing agent
should be rewardedonly for actually winning, not for achievingsubgoalssuch as
taking its opponents piecesor gainingcontrol of the centerof the board. If achieving
thesesorts of subgoalswere rewarded, then the agentmight find a way to achieve
them without achievingthe real goal. For example, it might find a way to take the
opponents piecesevenat the costof losing the game. The rewardsignalis your way
4. Betterplacesfor impartingthis kind of prior knowledgearethe initial policyor value
of communicatingto the agentwhat you want it to achieve, not how you want it
Newcomersto reinforcementlearningare sometimessurprisedthat therewardswhich defineof the goal of learning- arecomputedin the environmentratherthanin
the agent. Certainly mostultimategoalsfor animalsarerecognizedby computations
occurringinside their bodies, for example, by sensorsfor recognizingfood, hunger,
, as we discussedin the previoussection, one can
redraw the agent- environmentinterfacein sucha way that thesepartsof the body
are consideredto be outsideof the agent(and thus part of the agent's environment).
For example, if the goal concernsa robot' s internalenergyreservoirs, then theseare
consideredto be part of the environment; if the goal concernsthe positionsof the
robot' s limbs, then thesetoo are consideredto be part of the environment
the agents boundaryis drawn at the interfacebetweenthe limbs and their control
systems.Thesethingsareconsideredinternalto the robot but externalto the learning
, it is convenientto placethe boundaryof the learningagent
not at the limit of its physicalbody, but at the limit of its control.
The reasonwe do this is that the agent's ultimate goal should be something
over which it has imperfect control: it should not be able, for example, to simply
decreethat the reward has beenreceivedin the sameway that it might arbitrarily
changeits actions. Therefore, we placethe rewardsourceoutsideof the agent. This
does not precludethe agent from defining for itself a kind of internal reward, or
a sequenceof internal rewards. Indeed, this is exactly what many reinforcement
So far we havebeenimpreciseregardingthe objectiveof learning. We havesaidthat
the agent's goal is to maximize the reward it receivesin the long run. How might
this be formally defined? If the sequenceof rewardsreceivedafter time step t is
rt + l , rt + 2, rt + 3, . . . , then what
Rt , is defined as some specific function of the reward sequence . In the simplest case
whereT is a final time step. This approachmakessensein applicationsin which there
is a naturalnotion of final time step, that is, whenthe agent- environmentinteraction
trips through a maze, or any sort of repeatedinteractions. Each episodeends in a
specialstatecalled the terminal state, followed by a resetto a standardstartingstate
or to a samplefrom a standarddistributionof startingstates. Taskswith episodesof
this kind arecalledepisodictasks. In episodictaskswe sometimesneedto distinguish
the setof all nonterminalstates,denotedS, from the setof all statesplus the terminal
On the other hand, in many casesthe agent- environmentinteraction does not
, but goeson continually without limit . For
example, this would be the naturalway to formulatea continualprocess
or an applicationto a robot with a long life span. We call thesecontinuingtasks. The
return formulation (3.1) is problematicfor continuing tasksbecausethe final time
step would be T = 00, and the return, which is what we are trying to maximize,
could itself easily be infinite. (For example, supposethe agentreceivesa rewardof
+ I at eachtime step.) Thus, in this book we usuallyusea definition of return that is
slightly morecomplexconceptuallybut much simplermathematically
The additional concept that we need is that of discounting. According to this
approach,the; agenttries to selectactionsso that the sum of the discountedrewards
it receivesover the future is maximized. In particular, it choosesat to maximizethe
The discountratedeterminesthepresentvalueof futurerewards: a rewardreceived
k time stepsin the future is worth only yk l times what it would be worth if it were
receivedimmediately. If y < 1, the infinite sum has a finite value as long as the
rewardsequence{rk} is bounded. If y = 0, the agentis " myopic in beingconcerned
only with maximizing immediaterewards: its objectivein this caseis to learn how
to chooseat so as to maximize only rt + l . If eachof the agents actionshappened
to influenceonly the immediatereward, not future rewardsas well, then a myopic
agentcould maximize(3.2) by separatelymaximizing eachimmediatereward. But
in general, actingto maximizeimmediaterewardcanreduceaccessto future rewards
so that the return may actually be reduced. As y approach
future rewardsinto accountmore strongly: the agentbecomesmorefarsighted.
5. Episodesareoften called "trials " in the literature.
Example 3.4: Pole-Balancing Figure 3.2 shows a task that servedas an early
illustration of reinforcementlearning. The objectivehereis to apply forcesto a cart
moving along a track so as to keep a pole hinged to the cart from falling over. A
failure is said to occur if the pole falls pasta given anglefrom vertical or if the cart
reachesan end of the track. The pole is resetto vertical after eachfailure. This task
could be treatedas episodic, wherethe naturalepisodesarethe repeatedattemptsto
balancethe Poi.e. The rewardin this casecould be + I for everytime stepon which
failure did not occur, so that the return at eachtime would be the numberof steps
until failure. Alternatively, we could treat pole-balancingas a continuingtask, using
discounting. In this casethe rewardwould be - Ion eachfailure andzeroat all other
times. The return at eachtime would then be relatedto _ yk , wherek is the number
of time stepsbefore failure. In either case, the return is maximizedby keepingthe
Exercise3.4 Supposeyou treatedpole-balancingasan episodictaskbut also used
discounting, with all rewardszero except for - I upon failure. What then would
the return be at eachtime? How doesthis return differ from that in the discounted,
Exercise 3.5 Imagine that you are designinga robot to run a maze. You decide
to give it a reward of + I for escapingfrom the mazeand a reward of zero at all
other times. The task seemsto break down naturally into episodes
runs throughthe maze so you decideto treat it asan episodictask, wherethe goal
is to maximize expectedtotal reward (3.1). After running the learning agentfor a
while, you find that it is showingno improvementin escapingfrom the maze. What
is going wrong? Haveyou effectively communicatedto the agentwhat you want it
3.4 Unified Notation for Episodic and Continuing Tasks
In the precedingsectionwe describedtwo kinds of reinforcementlearningtasks, one
in which the agent- environmentinteractionnaturally breaksdown into a sequence
of separateepisodes(episodic tasks), and one in which it does not (continuing
tasks). The former caseis mathematicallyeasierbecauseeachaction affects only
the finite numberof rewardssubsequentlyreceivedduring the episode. In this book
we considersometimesonekind of problemandsometimesthe other, but often both.
It is thereforeuseful to establishone notationthat enablesus to talk preciselyabout
To be preciseaboutepisodictasksrequiressomeadditionalnotation. Ratherthan
one long sequenceof time steps, we need to considera seriesof episodes
of which consistsof a finite sequenceof time steps. We numberthe time stepsof
each episodestarting anew from zero. Therefore, we have to refer not just to St,
the stateat time t , but to St,;, the stateat time t of episodei (and similarly for at,;,
rt ,;, 7ft,;, T; , etc.). However, it turns out that when we discussepisodictaskswe will
almostneverhaveto distinguishbetweendifferent episodes
be consideringa particular single episode, or stating somethingthat is true for all
. Accordingly, in practicewe will almostalwaysabusenotationslightly by
droppingthe explicit referenceto episodenumber. That is, we will write St to refer
We needoneotherconventionto obtaina singlenotationthat coversboth episodic
and continuing tasks. We havedefinedthe return as a sum over a finite numberof
terms in one case(3.1) and as a sum over an infinite numberof terms in the other
(3.2). Thesecanbe unified by consideringepisodetenninationto be the enteringof a
specialabsorbingstatethat transitionsonly to itself andthat generatesonly rewards
of zero. For example, considerthe statetransitiondiagram:
Herethe solid squarerepresentsthe specialabsorbingstatecorrespondingto the end
of an episode. Startingfrom so, we get the rewardsequence+ 1, + 1, + 1, 0, 0, 0, . . . .
Summingthese, we get the samereturn whetherwe sum over the first Trewards
(hereT = 3) or overthe full infinite sequence
discounting. Thus, we can definethe return, in general, accordingto (3.2), usingthe
conventionof omitting episodenumberswhenthey arenot needed,andincluding the
possibility that y = 1 if the sumremainsfinite (e.g., becauseall episodesterminate).
Alternatively, we canalso write the return as
including the possibility that T = 00 or y = 1 (but not both6). We usetheseconventions
throughoutthe rest of the book to simplify notation and to exoressthe close
parallelsbetweenepisodicandcontinuingtasks.
In the reinforcement learning framework , the agent makes its decisions as a function
of a signal from the environment called the environment s state. In this section we
discuss what is required of the state signal , and what kind of information we should
and should not expect it to provide . In particular , we formally define a property of
environments and their state signals that is of particular interest, called the Markov
In this book , by "the state we mean whatever information is available to the agent.
We assume that the state is given by some preprocessing system that is nominally
part of the environment . We do not address the issues of constructing , changing , or
learning the state signal in this book . We take this approach not becausewe consider
state representation to be unimportant , but in order to focus fully on the decision making issues. In other words , our main concern is not with designing the state
signal , but with deciding what action to take as a function of whatever state signal is
Certainly the state signal should include immediate sensations such as sensory
measurements, but it can contain much more than that. State representations can be
highly processed versions of original sensations, or they can be complex structures
6. Waysto formulatetasksthat are both continuingand undiscountedare subjectsof current
, 1996; Schwartz, 1993; TadepalliandOk, 1994). Someof the ideas
built up over time from the sequenceof sensations
eyesover a scene, with only a tiny spot correspondingto the fovea visible in detail
at anyone time, yet build up a rich and detailedrepresentationof a scene. Or, more
obviously, we can look at an object, then look away, and know that it is still there.
We can hearthe word " yes" and considerourselvesto be in totally different states
dependingon the questionthat came before and which is no longer audible. At a
more mundanelevel, a control systemcan measureposition at two different times
to producea staterepresentationincluding informationaboutvelocity. In all of these
casesthe stateis constructedand maintainedon the basisof immediatesensations
togetherwith the previousstateor someother memory of past sensations
book, we do not explorehow that is done, but certainly it canbe andhasbeendone.
There is no reasonto restrict the state representationto immediatesensations
typical applicationswe shouldexpectthe staterepresentationto be able to inform
On the other hand, the statesignal shouldnot be expectedto inform the
everythingabout the environment, or eveneverythingthat would be useful to it in
makingdecisions. If the agentis playing blackjack, we shouldnot expectit to know
what the next card in the deck is. If the agentis answeringthe phone, we shouldnot
expectit to know in advancewho the caller is. If the agentis a paramediccalled to
a road accident, we should not expectit to know immediatelythe internal
of an unconsciousvictim. In all of thesecasesthere is hidden stateinformation in
the environment, and that information would be useful if the agentknew it , but the
agentcannotknow it becauseit hasneverreceivedany relevantsensations
we don' t fault an agentfor not knowing somethingthat matters, but
What we would like, ideally, is a state signal that summarizespast sensations
compactly, yet in sucha way that all relevantinformationis retained. This normally
requires more than the immediate sensations
. A statesignal that succeedsin retaining all relevant
information is said to be Markov, or to havethe Markov property (we define this
formally below). For example, a checkersposition- the current configurationof
all the pieceson the board- would serveas a Markov statebecauseit summarizes
everythingimportantabout the completesequenceof positionsthat led to it. Much
of the informationaboutthe sequenceis lost, but all that really mattersfor the future
of the gameis retained. Similarly, the currentposition and velocity of a cannonball
is all that mattersfor its future flight. It doesn't matterhow that positionand
cameabout. This is sometimesalsoreferredto asan " independence
becauseall that mattersis in the current statesignal; its meaningis independentof
the " path," or history, of signalsthat haveled up to it.
We now fonnally definethe Markov propertyfor the reinforcementlearningproblem
. To keepthe mathematicssimple, we assumeherethat therearea finite number
of statesand rewardvalues. This enablesus to work in tenns of sumsand probabilities rather than integralsand probability densities, but the argumentcan easily be
extendedto includecontinuousstatesandrewards. Considerhow a generalenvironment
might respondat time t + 1 to the action takenat time t . In the most general,
causalcasethis responsemay dependon everythingthat has happenedearlier. In
Pr {S'+1= S' , r ' +l = r I s" a" r " s' - l , a' - l , . . . , rl , So, ao} ,
for all S , r , and all possiblevaluesof the past eventss " a" r " . . . , rl , So, QQ
the statesignal hasthe Markov property, on the other hand, then the environment's
responseat t + I dependsonly on the stateand action representations
casethe environment's dynamicscanbe definedby specifyingonly
for all S' , r , s" and a, . In other words, a statesignal hasthe Markov property, and
is a Markov state, if and only if (3.5) is equal to (3.4) for all S' , r , and histories,
. In this case, the environmentand task as a whole are also
If anenvironmenthasthe Markov property, thenits one-stepdynamics(3.5) enable
us to predict the next state and expectednext reward given the current state and
action. Onecanshowthat, by iteratingthis equation, onecanpredictall future states
and expectedrewardsfrom knowledgeonly of the currentstateas well as would be
possiblegiventhecompletehistory up to the currenttime. It alsofollows thatMarkov
statesprovidethe bestpossiblebasisfor choosingactions. That is, the bestpolicy for
choosingactionsasa function of a Markov stateis just asgoodasthe bestpolicy for
choosingactionsasa function of completehistories.
Even when the state signal is non-Markov, it is still appropriateto think of the
statein reinforcementlearningas an approximationto a Markov state. In particular,
we always want the stateto be a good basisfor predicting future rewardsand for
selecting actions. In casesin which a model of the environmentis learned (see
Chapter9), we alsowant the stateto be a goodbasisfor predictingsubsequent
basisfor doing all of thesethings. To theextent
es the ability of Markov statesin theseways, one will obtain
betterperformancefrom reinforcementlearningsystems.For all of thesereasons
is usefulto think of the stateat eachtime stepasan approximationto a Markov state,
althoughone shouldrememberthat it may not fully satisfythe Markov property.
The Markov propertyis importantin reinforcementlearningbecausedecisionsand
valuesare assumedto be functionsonly of the currentstate. In order for theseto be
effective and informative, the staterepresentationmust be informative. All of the
theory presentedin this book assumesMarkov statesignals. This meansthat not all
the theory strictly appliesto casesin which the Markov property doesnot strictly
apply. However, thetheorydevelopedfor the Markov casestill helpsusto understand
the behaviorof the algorithms, and the algorithmscan be success
of the Markov caseis an essentialfoundationfor extendingit to the more complex
andrealistic non-Markov case. Finally, we notethat the assumptionof Markov state
is not uniqueto reinforcementlearningbut is also presentin most if
Example 3.5: Pole-Balancing State In the pole-balancingtaskintroducedearlier,
a statesignal would be Markov if it specifiedexactly, or madeit possibleto reconstruct
exactly, the positionandvelocity of the cart alongthe track, the anglebetween
the cart andthe pole, andthe rateat which this angleis changing(the angularvelocity
). In an idealizedcart- pole system, this informationwould be sufficientto exactly
predict the future behaviorof the cart and pole, given the actionstakenby the controller
. In practice, however, it is never possibleto know this information exactly
becauseany real sensorwould introducesomedistortion and delay in its measurements
. Furthermore,in any real cart- pole systemtherearealwaysothereffects, such
as the bending of the pole, the temperaturesof the wheel and pole bearings, and
variousforms of backlash, that slightly affect the behaviorof the system. Thesefactors
would causeviolations of the Markov propertyif the statesignal wereonly the
positionsandvelocitiesof the cart andthe pole.
However, often the positionsand velocitiesservequite well as states. Someearly
studiesof learning to solve the pole-balancingtask useda coarsestatesignal that
divided cart positionsinto three regions: right, left, and middle (and similar rough
quantizationsof the otherthreeintrinsic statevariables). This distinctly non-Markov
statewas sufficient to allow the task to be solvedeasily by reinforcementlearning
methods. In fact, this coarserepresentationmay have facilitated rapid learning by
forcing the learningagentto ignorefine distinctionsthat would not havebeenuseful
Example 3.6: Draw Poker In drawpoker, eachplayeris dealta handof five cards.
There is a round of betting, in which eachplayer exchangessomeof his cardsfor
new ones, andthenthereis a final roundof betting. At eachround, eachplayermust
matchor exceedthe highestbetsof the other players, or elsedrop out (fold). After
the secondround of betting, the player with the besthandwho hasnot folded is the
The statesignal in draw poker is different for eachplayer. Eachplayerknowsthe
cards in his own hand, but can only guessat thosein the other players' hands. A
commonmistakeis to think that a Markov statesignalshouldincludethe contentsof
all the players' handsand the cardsremainingin the deck. In a fair game, however,
we assumethat the playersare in principle unableto determinethesethings from
. If a playerdid know them, thenshecould predictsomefuture
events(suchasthecardsonecouldexchangefor) better thanby rememberingall past
In additionto knowledgeof one' s own cards, the statein drawpokershouldinclude
the bets and the numbersof cardsdrawn by the other players. For example, if one
of the other playersdrew three new cards, you may suspecthe retaineda pair and
adjust your guessof the strengthof his hand accordingly. The players' bets also
of their hands. In fact, much of your past history with
theseparticularplayersis part of the Markov state. DoesEllen like to bluff , or does
? Doesher faceor demeanorprovidecluesto the strengthof
herhand? How doesJoe's play changewhenit is lateat night, or whenhe hasalready
Although everythingeverobservedaboutthe other playersmay havean effect on
the probabilitiesthat they are holding variouskinds of hands, in practicethis is far
too muchto rememberand analyze, and mostof it will haveno cleareffect on one' s
predictionsanddecisions.Very goodpokerplayersareadeptat rememberingjust the
key clues, and at sizing up new playersquickly, but no one rememberseverything
that is relevant. As a result, the staterepresentations
decisionsareundoubtedlynon-Markov, andthedecisionsthemselvesarepresumably
, people still make very good decisionsin such tasks. We
concludethat the inability to haveaccessto a perfect Markov staterepresentation
is probablynot a severeproblemfor a reinforcementlearningagent.
Exercise3.6: Broken VISionSystem Imaginethat you are a vision system. When
you are first turnedon, an imagefloodsinto your camera. You can seelots of things,
but not all things. You can' t seeobjectsthat are occluded, and of courseyou can' t
seeobjectsthat are behindyou. After seeingthat first scene, do you haveaccessto
the Markov stateof the environment? Supposeyour camerawere broken and you
receivedno imagesat all, all day. Would you haveaccessto the Markov statethen?
A reinforcementlearningtask that satisfiesthe Markov propertyis called a Markov
decisionprocess, or MDP. If the stateand action spacesare finite, then it is called a
finite Markov decisionprocess(finite MDP) . Finite MD Psareparticularly important
to the theory of reinforcementlearning. We treat them extensivelythroughoutthis
book; they are all you needto understand90% of modemreinforcementlearning.
A particularfinite MOP is definedby its stateand action setsandby the one-step
dynamicsof the environment.Given any stateand actions anda , the probability of
!/':S' = Pr {S'+l = S Is , = s, a, = a } .
Thesequantitiesarecalled transitionprobabilities. Similarly, givenany currentstate
and actions and a , togetherwith any next state, S , the expectedvalue of the next
/ R:s' = E { r '+ ll s, = s, a, = a , s'+l = S } .
Thesequantities, !/':s' and/ R:s' , completelyspecifythe mostimportantaspectsof the
dynamicsof a finite MOP (only informationaboutthe distributionof rewardsaround
the expectedvalue is lost). Most of the theory we presentin the rest of this book
implicitly assumesthe environmentis a finite MOP.
Example 3.7: Recycling Robot MDP The recycling robot ( Example3.3) can be
turnedinto a simpleexampleof an MDP by simplifying it andproviding somemore
details. (Our aim is to producea simple example, not a particularly realistic one.)
Recall that the agentmakesa decisionat times determinedby externalevents(or
by other parts of the robot s control system). At each suchtime the robot decides
whetherit should ( 1) actively searchfor a can, (2) remain stationaryand wait for
someoneto bring it a can, or (3) go backto homebaseto rechargeits battery. Suppose
the environmentworks asfollows. The bestway to find cansis to actively searchfor
them, but this runsdown the robot s battery, whereaswaiting doesnot. Wheneverthe
robot is searching,the possibility existsthat its batterywill becomedepleted. In this
casethe robot must shutdown and wait to be rescued(producinga low reward).
The agent makesits decisionssolely as a function of the energy level of the
battery. It can distinguish two levels, high and low , so' that the state set is 8
{high , low }. Let uscall the possibledecisions the agents actions wait , search ,
andrecharge . When the energylevel is high , rechargingwould alwaysbe foolish,
Note: There is a row for each possible combination
possible in the current state, a E A (s ) .
so we do not include it in the action setfor this state. The agent's action setsare
If the energylevel is high , thena periodof activesearchcanalwaysbe completed
without risk of depletingthe battery. A period of searchingthat beginswith a high
energylevel leavesthe energylevel high with probability a and reducesit to low
with probability 1 - a . On the other hand, a period of searchingundertakenwhen
the energylevel is low leavesit low with probability .Banddepletesthe batterywith
probability 1 - .B. In the latter case, the robot must be rescued, and the battery is
thenrechargedbackto high . Eachcancollectedby the robot countsasa unit reward,
whereasa rewardof - 3 resultswheneverthe robot hasto be rescued. Let ~ 8earcb
and ~ vait , with ~ 8earcb> ~ vait , respectivelydenotethe expectednumberof cans
the robot will collect (and hencethe expectedreward) while searchingand while
waiting. Finally, to keepthings simple, supposethat no canscanbe collectedduring
a run homefor recharging, and that no canscan be collectedon a stepin which the
battery is depleted. This systemis then a finite MDP, and we can write down the
transitionprobabilitiesandthe expectedrewards, asin Table3.1.
A transition graph is a useful way to summarizethe dynamicsof a finite MDP.
Figure3.3 showsthe transitiongraphfor the recyclingrobot example. Therearetwo
Figure 3.3 Transitiongraphfor the recyclingrobot example.
kinds of nodes: state nodes and action nodes. There is a state node for each possible
state (a large open circle labeled by the name of the state), and an action node for each
state- action pair (a small solid circle labeled by the name of the action and connected
by a line to the state node) . Starting in state s and taking action a moves you along
the line from state node s to action node (s , a ) . Then the environment responds with
a transition to the next state s node via one of the arrows leaving action node (s , a ) .
Each arrow corresponds to atriples , Sf, a ) , where Sf is the next state, and we label the
arrow with the transition probability , ! Ps~" and the expected reward for that transition ,
~ :s" Note that the transition probabilities labeling the arrows leaving an action node
Exercise 3.7 Assuming a finite MDP with a finite number of reward values, write
an equation for the transition probabilities and the expected rewards in terms of the
joint conditional distribution in (3.5) .
Almost all reinforcementlearning algorithms are basedon estimatingvaluefunctions
- functionsof states(or of state- action pairs) that estimatehow good it is for
the agentto be in a given state(or how good it is to perform a given action in a
given state). The notion of how good here is definedin terms of future rewards
that can be expected, or, to be precise, in terms of expectedreturn. Of coursethe
rewardsthe agentcan expectto receivein the future dependon what actionsit will
take. Accordingly, valuefunctionsaredefinedwith respectto particularpolicies.
Recall that a policy, 7r, is a mappingfrom eachstate, Se 8 , and action, a e A (s) ,
to the probability 7r(s, a) of taking actiona when in states. Informally, the valueof
a states undera policy 7r, denotedV7r(s), is the expectedreturn when startingin s
andfollowing 7r thereafter.For MOPs, we candefine V7r(s) formally as
{ } denotesthe expectedvaluegiven that the agentfollows policy if . Note
that the valueof thetenninal state, if any, is alwayszero. We call the function V1Cthe
Similarly, we define the value of taking action a in state S under a policy if ,
(S, a ), as the expectedreturn starting from staking the action a , and
{ Rt 1St= s, at = a } = E1Cf : ykrt+k+l St s, at a .
The valuefunctions V1Cand Q1Ccan be estimatedfrom experience
if an agentfollows policy if andmaintainsan average
the actualreturnsthat havefollowed that state, then the averagewill convergeto the
(S), as the numberof times that state is encounteredapproach
infinity . If separateaveragesare kept for each
averageswill similarly convergeto the action values, Q1C
methodsof this kind Monte Carlo methodsbecausethey involve averagingover
randomsamplesof actualreturns. Thesekindsof methodsarepresentedin Chapter5.
Of course, if thereare very manystates, thenit may not be practicalto keepseparate
averagesfor eachstateindividually. Instead, the agentwould haveto maintain V1C
and Q1Cas parameterizedfunctions and adjust the parametersto better match the
observedreturns. This can alsoproduceaccurateestimates
A fundamentalpropertyof valuefunctionsusedthroughoutreinforcementlearning
and dynamicprogrammingis that they satisfyparticularrecursiverelationships.For
any policy if and any states, the following consistencyconditionholdsbetweenthe
valueof S and the valueof its possiblesuccessorstates:
whereit is implicit thatthe actions, a , aretakenfrom the setA (s), andthe next states,
s' , aretakenfrom the set8, or from 8+ in the caseof an episodicproblem. Equation
(3.10) is the Bellmanequationfor V7r. It express
of a stateand the valuesof its successorstates. Think of looking aheadfrom one
stateto its possiblesuccessorstates, as suggestedby Figure 3.4a. Eachopencircle
representsa stateand eachsolid circle representsa state- action pair. Startingfrom
states, the root nodeat the top, theagentcould takeanyof somesetof actions- three
are shownin Figure 3.4a. From eachof these, the environmentcould respondwith
one of severalnext states, s' , along with a reward, r . The Bellman equation(3.10)
averagesover all the possibilities, weighting eachby its probability of occurring. It
statesthat the valueof the startstatemustequalthe (discounted)valueof theexpected
next state, plus the rewardexpectedalongthe way.
The value function V7r is the unique solution to its Bellman equation. We show
in subsequentchaptershow this Bellman equationforms the basisof a numberof
Figure 3.4 Backup,diagramsfor (a) V and (b) Q .
waysto compute, approximate,andlearn V7r. We call diagramslike thoseshown
Figure 3.4 backupdiagramsbecausethey diagramrelationshipsthat form the basis
of the updateor backupoperationsthat are at the heart of reinforcementlearning
methods. Theseoperationstransfer value information back to a state (or a state
actionpair) from its successorstates(or state- actionpairs). We usebackupdiagrams
throughoutthe book to provide graphicalsummariesof the algorithmswe discuss.
(Note that unlike transition graphs, the statenodesof backupdiagramsdo not nec
essarily representdistinct states; for example, a statemight be its own successor
We also omit explicit arrowheadsbecausetime alwaysflows downwardin a backup
Example 3.8: Gridworld Figure 3.5a usesa rectangulargrid to illustrate value
functions for a simple finite MDP. The cells of the grid correspondto the statesof
the environment. At eachcell, four actionsare possible: north , south , east , and
west , which deterministicallycausethe agentto moveonecell in the corresponding
directionon the grid. Actions that would takethe agentoff the grid leaveits location
, but also result in a rewardof - 1. Other actionsresult in a rewardof 0,
exceptthosethat movethe agentout of the specialstatesA
four actionsyield a rewardof + 10 andtakethe agentto A . From stateB, all actions
yield a rewardof + 5 andtakethe agentto B .
Supposethe agent selectsall four actions with equal probability in all states.
Figure 3.5b showsthe valuefunction, V7r, for this policy, for the discountedreward
case with y = 0.9. This value function was computedby solving the systemof
equations(3.10). Notice the negativevaluesnearthe lower edge; thesearethe result
of the high probability of hitting the edgeof the grid thereunderthe randompolicy.
StateA is the beststateto be in underthis policy, but its expectedreturn is lessthan
Figure 3.5 Grid example: (a) exceptionalreward
10, its immediatereward, becausefrom A the agentis takento A' , from which it is
likely to run into the edgeof the grid. StateB, on the otherhand, is valuedmorethan
5, its immediatereward, becausefrom B the agentis takento B' , which hasa positive
value. From B' the expectedpenalty (negativereward) for possiblyrunning into an
for by the expectedgain for possiblystumblingonto
Example 3.9: Golf To formulateplaying a holeof golf asa reinforcementlearning
task, we count a penalty (negativereward) of - 1 for eachstrokeuntil the ball is in
the hole. The stateis the location of the ball. The valueof a stateis the negativeof
the numberof strokesto the hole from that location. Our actionsare how we aim
and swing at the ball, of course, and which club we select. Let us takethe former as
given and considerjust the choice of club, which we assumeis either a putter or a
driver. The upperpart of Figure 3.6 showsa possiblestate-valuefunction, vputt (s),
for the policy that always usesthe putter. The terminal state in - the - hole has a
valueof O. From anywhereon the greenwe assumewe can makea putt; thesestates
havevalue - 1. Off the greenwe cannotreachthe hole by putting, and the value is
greater. If we can reachthe greenfrom a stateby putting, then that statemust have
value one less than the green's value, that is, - 2. For simplicity, let us assumewe
can putt very preciselyanddeterministically, but with a limited range. This givesus
the sharpcontourline labeled- 2 in the figure; all locationsbetweenthat line andthe
greenrequireexactlytwo strokesto completethe hole. Similarly, any locationwithin
putting rangeof the - 2 contourline must havea value of - 3, and so on to get all
the contourlines shownin the figure. Puttingdoesn't get us out of sandtraps, so they
havea valueof - 00. Overall, it takesus six strokesto get from the teeto the hole by
Exercise 3.8 What is the Bellman equationfor action values, that is, for Q7r? It
must give the action value Q7r(s, a) in terms of the action values, Q7r(s' , a' ), of
to the state- actionpair (s, a ) . As a hint, the backupdiagramcorresponding
to this equationis given in Figure 3.4b. Showthe sequenceof equations
analogousto (3.10), but for action values.
Exercise 3.9 The Bellman equation(3.10) must hold for eachstatefor the value
function V7rshownin Figure 3.5b. As an example, shownumericallythat this equation
holds for the centerstate, valuedat + 0.7, with respectto its four neighboring
states, valuedat + 2.3, + 0.4, - 0.4, and + 0.7. (Thesenumbersare accurateonly to
Exercise 3.10 In the gridworld example, rewardsare positive for goals, negative
for runninginto the edgeof the world, and zerothe restof the time. Are the signsof
theserewardsimportant, or only the intervalsbetweenthem? Prove, using(3.2), that
addinga constantC to all the rewardsaddsa constant, K , to the valuesof all states,
andthus doesnot affect the relativevaluesof any statesunderany policies. What is
Exercise3.11 Now consideraddinga constantC to all the rewardsin an episodic
task, such as mazerunning. Would this haveany effect, or would it leavethe task
unchangedas in the continuingtaskabove? Why or why not? Give an example.
Exercise3.12 The valueof a statedependson the valuesof the actionspossiblein
that stateandon how likely eachactionis to be takenunderthecurrentpolicy. Wecan
think of this in tenD Sof a small backupdiagramrootedat the stateand considering
Give the equationcorrespondingto this intuition anddiagramfor the valueat the root
node, V7r(s), in tenDSof the valueat the expectedleaf node, Q7r(St, at), givenSt= S.
the expectedvalueis written out explicitly in tenDSof 11
Exercise3.13 The valueof an action, Q7r(s, a ), can be divided into two parts, the
expectednext reward, which doesnot dependon the policy 11
of the remainingrewards, which dependson the next stateandthe policy. Again we
can think of this in tenDS of a small backupdiagram, this one rooted at an action
(state-actionpair) andbranchingto the possiblenext states:
Give the equationcorrespondingto this intuition and diagramfor the action value,
Q7r(s, a ), in tenDs of the expectednext reward, rt+l, and the expectednext state
value, V7r(St+ I), given that St= S andat = a. Then give a secondequation, writing
out the expected value explicitly in terms of : P:S, and oR:s" defined respectively by
( 3.6) and ( 3.7), such that no expected value notation appears in the equation .
Solving a reinforcementlearningtask means, roughly, finding a policy that achieves
a lot of rewardoverthe long run. For finite MOPs, we canpreciselydefineanoptimal
policy in the following way. Valuefunctionsdefinea partial orderingover policies.
A policy 1Cis definedto be betterthan or equalto a policy 1C
is greaterthan or equalto that of 1Cfor all states. In other words, 1C~ 1C
if VIr(s) ~ VIr (s) for all se 8. Thereis alwaysat leastone policy that is betterthan
or equalto all otherpolicies. This is an optimalpolicy. Although theremay be more
than one, we denoteall the optimal policies by 1C
function, called the optimal state valuefunction, denotedV * , and definedas
Optimal policies also sharethe sameoptimal action-valuefunction, denotedQ ,
for all se 8 and a e A (s) . For the state- action pair (s, a), this function gives the
expectedreturn for taking action a in states and thereafterfollowing an optimal
policy. Thus, we canwrite Q in tenDSof V asfollows:
Q (s, a) = E { rt+l + yV (St+ l ) 1St= s, at = a } .
Example 3.10: Optimal Value Functions for Golf The lower part of Figure 3.6
showsthecontoursof a possibleoptimal action-valuefunction Q* (s, driver ) . These
arethevaluesof eachstateif we first playa strokewith thedriver andafterwardselect
eitherthe driver or the putter, whicheveris better. The driver enablesus to hit the ball
farther, but with lessaccuracy.Wecanreachthe hole in oneshotusingthe driver only
if we are alreadyvery close; thus the - 1 contour for Q* (s, driver ) coversonly a
small portion of the green. If we havetwo strokes, however, then we can reachthe
hole from much farther away, as shownby the - 2 contour. In this casewe don t
haveto drive all the way to within the small - 1 contour, but only to anywhereon
the green; from therewe can usethe putter. The optimal action-valuefunction gives
the valuesafter committing to a particularfirst action, in this case, to the driver, but
afterwardusing whicheveractionsare best. The - 3 contour is still farther out and
includesthe startingtee. From the tee, the bestsequenceof actionsis two drivesand
one putt, sinking the ball in threestrokes.
BecauseV * is the value function for a policy, it must satisfy the self-consistency
condition given by the Bellman equationfor statevalues(3.10). Becauseit is the
optimal value function, however, V s consistencycondition can be written in a
specialform without referenceto anyspecificpolicy. This is theBellmanequationfor
V * , or the Bellmanoptimality equation. Intuitively, the Bellmanoptimality equation
expresses the fact that the value of a stateunder an optimal policy must equal the
expectedreturn for the bestactionfrom that state:
Q*(S, a) = E{rt+1+ y ~ Q*(St+l , a ) 1St= s,at = a}
the sameas the backupdiagramsfor V7rand Q7rexceptthat arcshavebeenadded
at the agent's choicepoints to representthat the maximumover that choiceis taken
ratherthan the expectedvaluegiven somepolicy. Figure 3.7a graphicallyrepresents
For finite MD Ps, the Bellman optimality equation(3.15) has a unique solution
independentof the policy. The Bellman optimality equationis actually a systemof
equations, one for eachstate, so if thereare N states, then thereare N equationsin
N unknowns. If the dynamicsof the environmentareknown (~ :s' and 9' s~,), then in
principle onecan solvethis systemof equationsfor V usinganyone of a varietyof
methodsfor solving systemsof nonlinearequations. One can solve a relatedset of
Once one has V * , it is relatively easyto detenninean optimal policy. For each
state s, there will be one or more actions at which the maximum is attainedin
the Bellman optimality equation. Any policy that assignsnonzeroprobability only
to theseactions is an optimal policy. You can think of this as a one-step search.
If you have the optimal value function, V * , then the actions that appearbest after
a one-step searchwill be optimal actions. Another way of saying this is that
any policy that is greedywith respectto the optimal value function V is an optimal
policy. The term greedyis usedin computerscienceto describeany searchor
decisionprocedurethat selectsalternativesbasedonly on local or immediateconsiderations
, without consideringthe possibility that such a selectionmay prevent
actionsbasedonly on their short term consequences
if one usesit to evaluatethe short-term consequences
- then a greedypolicy is actually optimal in the long-term
sensein which we are interestedbecauseV * already takes into accountthe reward
of all possiblefuture behavior. By meansof V , the optimal
expectedlong-term return is turned into a quantity that is locally and immediately
available for each state. Hence, a one- step- ahead search yields the long -term optimal
Having Q makes choosing optimal actions still easier. With Q , the agent does
not even have to do a one step ahead search: for any state s , it can simply find any
action that maximizes Q * (s , a ) . The action -value function effectively caches the
results of all one- step-ahead searches. It provides the optimal expected long - term
return as a value that is locally and immediately available for each state- action pair .
Hence, at the cost of representing a function of state- action pairs, instead of just of
states, the optimal action - value function allows optimal actions to be selected without
having to know anything about possible successor states and their values, that is,
without having to know anything about the environment ' s dynamics .
Example 3.11: Bellman Optimality Equations for the Recyc Ung Robot Using
( 3.15), we can explicitly give the Bellman optimality equation for the recycling robot
example. To make things more compact, we abbreviate the states high and low , and
the actions search , wait , and recharge respectively by h , 1, s , w, and reo Since
there are only two states, the Bellman optimality equation consists of two equations.
The equation for V * (h ) can be written as follows :
V*(h) = max:P~[~~ + yV*(h)] + :P:1[~:1+ yV*(l )],
{ :P~[~~ + yV (h)] + :P:1[~~1+ yV (l )] }
,8~ 8 - 3( 1 - ,8) + y [( 1 - ,8) V*(h) + ,8V*(l )],
For anychoiceof ~ 8, ~ w, a , ,8, and y , with 0 ~ y < 1, 0 ~ a , ,8 ~ 1, thereis
Example3.12: Solvingthe Gridworld Suppose
equationfor V for thesimplegridtaskintroduced
in Figure3.8a. RecallthatstateA is followedby a rewardof + 10andtransitionto
stateA' , while state8 is followed by a rewardof + 5 and transitionto state8 ' . Figure
3.8b showsthe optimal valuefunction, andFigure3.8c showsthe corresponding
optimal policies. Wherethereare multiple arrowsin a cell, any of the corresponding
Explicitly solving the Bellman optimality equationprovidesone route to finding
an optimal policy, andthusto solving the reinforcementlearningproblem. However,
this solutionis rarely directly useful. It is akin to an exhaustivesearch,looking ahead
at all possibilities, computingtheir probabilitiesof occurrenceandtheir desirabilities
in termsof expectedrewards. This solutionrelies on at leastthreeassumptionsthat
are rarely true in practice: ( 1) we accuratelyknow the dynamicsof the environment;
(2) we have enoughcomputationalresourcesto completethe computationof the
solution; and (3) the Markov property. For the kinds of tasks in which we are
interested, one is generally not able to implement this solution exactly because
variouscombinationsof theseassumptionsare violated. For example, althoughthe
first and third assumptionspresentno problemsfor the gameof backgammon
secondis a major impediment. Sincethe gamehasabout 1020states, it would take
millions of yearson today' s fastestcomputersto solvethe Bellmanequationfor V * ,
and the sameis true for finding Q* . In reinforcementlearningone typically hasto
Many differentdecision-makingmethodscanbe viewedaswaysof approximately
solving the Bellmanoptimality equation. For example, heuristicsearchmethodscan
be viewedasexpandingthe right-handsideof (3.15) severaltimes, up to somedepth,
fonning a tree of possibilities, and then using a heuristic evaluationfunction to
approximateV at the leaf" nodes. (HeuristicsearchmethodssuchasA arealmost
alwaysbasedon the episodiccase.) The methodsof dynamicprogrammingcan be
relatedevenmore closely to the Bellman optimality equation. Many reinforcement
learning methodscan be clearly understoodas approximatelysolving the Bellman
optimality equation, using actual experiencedtransitionsin place of knowledgeof
the expectedtransitions. We considera variety of such methodsin the following
Exercise3.14 Draw or describethe optimal state-valuefunction for the golf exampIe.
Exercise3.15 Draw or describethe contoursof the optimal action-valuefunction
for putting, Q* (s, putter ), for the golf example.
Exercise3.16 Give the Bellmanequationfor Q* for the recyclingrobot.
Exercise3.17 Figure 3.8 givesthe optimal valueof the beststateof the gridworld
as24.4, to onedecimalplace. Useyour knowledgeof the optimal policy and(3.2) to
expressthis valuesymbolically, andthento computeit to threedecimalplaces.
We havedefinedoptimal valuefunctionsandoptimal policies. Clearly, an agentthat
learnsan optimal policy hasdonevery well, but in practicethis rarely happens
the kinds of tasksin which we areinterested,optimal policies
with extremecomputationalcost. A well-definednotion of optimality organizesthe
approachto learningwe describein this book and providesa way to understandthe
theoreticalpropertiesof various learning algorithms, but it is an ideal that agents
a completeand accuratemodel of the environment's dynamics, it is usually not
possibleto simply computean optimal policy by solving the Bellman optimality
equation. For example, board gamessuch as chessare a tiny fraction of human
, yet large, custom-designedcomputersstill cannotcomputethe optimal
moves. A critical aspectof the problemfacing the agentis alwaysthe computational
power availableto it , in particular, the amountof computationit can perform in a
The memoryavailableis alsoan importantconsttaint. A largeamountof memory
is often requiredto build up approximationsof valuefunctions, policies, andmodels.
In taskswith small, finite statesets, it is possibleto form theseapproximationsusing
arraysor tableswith one entry for eachstate(or state-action pair). This we call the
tabular case, andthe correspondingmethodswe call tabularmethods. In manycases
of practicalinterest, however, therearefar more statesthancould possiblybe entries
in a table. In thesecasesthe functions must be approximated
morecompactparameterizedfunction representation
Our framing of the reinforcementlearningproblemforcesus to settlefor approximations
. However, it alsopresentsus with someuniqueopportunitiesfor achieving
. For example, in approximatingoptimal behavior, theremay
be many statesthat the agentfaceswith sucha low probability that selectingsuboptimal
actionsfor them haslittle impact on the amountof rewardthe agentreceives.
Tesauro's backgammonplayer, for example, playswith exceptionalskill eventhough
it might makevery bad decisionson boardconfigurationsthat neveroccur in games
againstexperts. In fact, it is possiblethat TO-Gammonmakesbad decisionsfor a
large fraction of the games stateset. The on-line natureof reinforcementlearning
makesit possibleto approximateoptimal policies in ways that put more effort into
learning to make good decisionsfor frequently encounteredstates, at the expense
of lesseffort for infrequentlyencounteredstates. This is one key propertythat dises to approximatelysolving
tinguishesreinforcementlearning from other approach
Let us summarizethe elementsof the reinforcementlearningproblemthat we have
presentedin this chapter. Reinforcementlearningis aboutlearningfrom interaction
how to behavein order to achievea goal. The reinforcementlearning agent and
its environmentinteract over a sequenceof discretetime steps. The specification
of their interfacedefinesa particulartask: the actions are the choicesmadeby the
agent; the statesare the basisfor makingthe choices; and the rewardsare the basis
for evaluatingthe choices. Everything inside the agent is completely known and
controllableby the agent; everythingoutsideis incompletelycontrollablebut mayor
may not becompletelyknown. A policy is a stochasticrule by which the agentselects
actionsas a function of states. The agent's objectiveis to maximizethe amountof
The return is the function of future rewardsthat the agentseeksto maximize. It
has severaldifferent definitionsdependingupon the natureof the task and whether
one wishesto discount delayedreward. The undiscountedformulation is appropriate
for episodictasks, in which the agent- environmentinteractionbreaksnaturally
into episodes; the discountedformulation is appropriatefor continuing tasks, in
which the interactiondoesnot naturally break into episodesbut continueswithout
An environmentsatisfiestheMarkovproperty if its statesignalcompactlysummarizes
the pastwithout degradingthe ability to predictthe future. This is rarely exactly
true, but often nearly so; the statesignalshouldbe chosenor constructedso that the
Markov property holds as nearly as possible. In this book we assumethat this has
alreadybeendoneand focus on the decision-making problem: how to decidewhat
to do asa function of whateverstatesignalis available. If the Markov propertydoes
hold, thenthe environmentdefinesa Markov decisionprocess(MDP). Afinite MDP
is an MDP with finite stateand action sets. Most of the currenttheory of reinforcement
learning is restrictedto finite MD Ps, but the methodsand ideasapply more
A policy' s valuefunctions assignto eachstate, or state- action pair, the expected
return from that state, or state- action pair, given that the agentusesthe policy. The
optimal valuefunctions assignto eachstate, or state- actionpair, the largestexpected
return achievableby any policy. A policy whosevalue functionsare optimal is an
optimalpolicy. Whereasthe optimal valuefunctionsfor statesandstate actionpairs
is greedy with respectto the optimal value functions must be an optimal policy.
The Bellmanoptimality equationsarespecialconsistencyconditionsthat the optimal
valuefunctionsmustsatisfyandthatcan, in principle, be solvedfor the optimal value
functions, from which an optimal policy canbe determinedwith relativeease.
A reinforcementlearning problem can be posedin a variety of different ways
dependingon assumptionsabout the level of knowledgeinitially availableto the
agent. In problemsof completeknowledge, the agenthas a completeand accurate
model of the environment's dynamics. If the environmentis an MDP, then such a
model consistsof the one-steptransition probabilities and expectedrewardsfor all
statesandtheir allowableactions. In problemsof incompleteknowledge,a complete
andperfectmodelof the environmentis not available.
Even if the agenthas a completeand accurateenvironmentmodel, the agentis
typically unableto perform enoughcomputationper time step to fully use it. The
memoryavailableis also an importantconstraint. Memory may be requiredto build
up accurateapproximationsof valuefunctions, policies, and models. In most cases
of practicalinteresttherearefar morestatesthancould possiblybe entriesin a table,
A well-definednotionof optimality organizesthe approachto learningwe describe
in this book and providesa way to understandthe theoreticalpropertiesof various
learningalgorithms, but it is an ideal that reinforcementlearningagentscanonly approximate
. In reinforcementlearningwe arevery muchconcerned
with casesin which optimal solutionscannotbe found but must be approximatedin
3.11 Bibliographical and Historical Remarks
The earliestinstanceof which we are awarein which reinforcementlearningwas
discussedusingthe MDP formalismis Andreaes ( 1969b) descriptionof a unified view
of learning machines. Witten and Corbin ( 1973) experimentedwith a reinforcement
learningsystemlater analyzedby Witten ( 1977) usingthe MOP formalism. Although
he did not explicitly mentionMD Ps, Werbos( 1977) suggestedapproximatesolution
methodsfor stochasticoptimal control problemsthat are relatedto modemreinforcement
learning methods(seealso Werbos, 1982, 1987, 1988, 1989, 1992). Although
Werbos's ideaswerenot widely recognizedat the time, they wereprescientin emphasizing
the importanceof approximatelysolving optimal control problemsin a variety
of domains, including artificial intelligence. The most influential integrationof reinforcement
learningandMD Psis dueto Watkins( 1989). His treatmentof reinforcement
learningusingthe MOP formalismhasbeenwidely adopted.
Our characterizationof the rewarddynamicsof an MDP in termsof ~ ~s' is slightly
unusual. It is morecommonin the MOP literatureto describethe rewarddynamicsin
terms of the expectednext reward givenjust the current stateand action, that is, by
~ : = E {rt+List= S, at = a }. This quantity is relatedto our ~ ~s' asfollows:
In conventionalMDP theory, /R.~s' alwaysappearsin a sumlike this one, andtherefore
it is easierto use /R.: . In reinforcementlearning, however, we more often have to
refer to individual actualor sampleoutcomes.In teachingreinforcementlearning, we
have found the /R.~s' notation to be more straightforwardconceptuallyand easierto
3.7-8 Assigning value on the basis of what is good or bad in the long run has ancient
roots. In control theory, mapping statesto numerical valuesrepresentingthe longof control decisionsis a key part of optimal control theory, which
1950sby extendingnineteenthcenturystate-function theoriesof
classicalmechanics(see, e.g., SchultzandMelsa, 1967). In describinghow a computer
could be programmedto play chess, Shannon( 1950) suggestedusing an evaluation
function that took into accountthe long-term advantagesand disadvantages
Watkins's ( 1989) Q-learningalgorithmfor estimatingQ (Chapter6) madeactionvalue functions an important part of reinforcementlearning, and consequentlythese
functionsareoften calledQ-functions. But the ideaof an action-valuefunction is much
older than this. Shannon( 1950) suggestedthat a function h ( P , M ) could be usedby a
chess-playing programto decidewhethera move M in position P is worth exploring.
Michie' s ( 1961, 1963) MENACE systemand Michie and Chamberss ( 1968) BOXES
Hamiltons principal function is an action-value function; Newtoniandynamicsare
greedy with respectto this function'(e.g., Goldstein, 1957). Action value functions
also playeda centralrole in Denardos ( 1967) theoreticaltreatmentof DP in termsof
3.11 Bibliographical and Historical Remarks
What we call die Bellmanoptimalityequationfor V* wasfirst introduced
), whocalledit die "basicfunctionalequation
of die Bellmanoptimalityequationfor continuous
- Bellmanequation(or oftenjust die Hamilton-Jacobi
In this part of the book we describethreefundamentalclassesof methodsfor solving
the reinforcementlearningproblem: dynamicprogramming, Monte Carlo methods,
and temporal-differencelearning. All of thesemethodssolve the full versionof the
Eachclassof methodshasits strengthsand weaknesses
model of the environment. Monte Carlo methodsdon' t require a model and are
conceptuallysimple, but are not suited for step-by-step incrementalcomputation.
Finally, temporal-differencemethodsrequireno modelandarefully incremental,but
are more complexto analyze. The methodsalso differ in severalways with respect
to their efficiencyand speedof convergence
. In the third part of the book we explore
how thesemethodscanbe combinedso asto obtainthe bestfeaturesof eachof them.
The tenD dynamic programming(DP) refers to a collection of algorithmsthat can
be usedto computeoptimal policies given a perfectmodel of the environmentas a
Markov decisionprocess( MDP). ClassicalDP algorithmsare of limited utility in
reinforcementlearningboth becauseof their assumptionof a perfectmodeland because
, but they are still importanttheoretically.
DP providesan essentialfoundationfor the understandingof the methodspresented
in the rest of this book. In fact, all of thesemethodscan be viewed as attemptsto
achievemuchthe sameeffect asDP, only with lesscomputationandwithout assuming
Startingwith this chapter, we usuallyassumethat the environmentis a finite MDP.
That is, we assumethat its stateand action sets, 8 and A (s), for se 8, are finite,
and that its dynamicsare given by a setof ttansition probabilities, / P:S, = Pr{St+ l =
S' I St= S, at = a }, and expectedimmediaterewards, : R:s' = E {rt+l I at = a , St=
s, St+ l = s' }, for all Se 8 , a e A (s), andse 8+ (8+ is 8 plus a terminal stateif the
problemis episodic). Although DP ideascanbe appliedto problemswith continuous
, exactsolutionsarepossibleonly in specialcases.A common
way obtainingapproximatesolutionsfor taskswith continuousstatesand actions
is to quantizethe stateandactionspacesandthenapply finite-stateDP methods.The
methodswe explore in Chapter8 are applicableto continuousproblemsand are a
The key idea of DP, and of reinforcementlearninggenerally, is the useof value
functionsto organizeand structurethe searchfor good policies. In this chapterwe
showhow DP can be usedto computethe valuefunctionsdefinedin Chapter3. As
discussedthere, we caneasilyobtainoptimalpoliciesoncewe havefoundtheoptimal
valuefunctions, V * or Q* , which satisfythe Bellmanoptimality equations:
V*(s) = maxE {rt+l + yV * (St+l) 1St= S, at = a }
for all sE 8, a E A (s), andsE 8+ . As we shall see, OP algorithmsare obtainedby
turning Bellman equationssuchas theseinto assignments
for improving approximationsof the desiredvaluefunctions.
First we considerhow to computethe state-valuefunction V1Cfor an arbitrarypolicy
1r. This is called policy evaluationin the DP literature. We also refer to it as the
predictionproblem. Recallfrom Chapter3 that, for all sE 8,
{ rt+l + yrt +2 + y2rt+3 + . . . 1St= s }
where 7r(S, a ) is the probability of taking action a in stateS under policy 7r, and
the expectationsaresubscriptedby 7r to indicatethat they areconditionalon 7r being
followed. The existenceanduniquenessof V7rareguaranteedaslong aseithery < I
or eventualterminationis guaranteedfrom all statesunderthe policy 7r.
If the environment's dynamicsare completelyknown, then (4.4) is a systemof
ISI simultaneouslinear equationsin ISI unknowns(the V7r(S), Se a). In principle,
its solution is a straightforward, if tedious, computation. For our purposes
solutionmethodsaremost suitable. Considera sequenceof approximatevaluefunctions
Yo, VI, V2, . . ., eachmappingS+ to 9t. The initial approximation, Yo, is chosen
arbitrarily (exceptthat the terminal state, if any, must be given value 0), and each
is obtainedby usingtheBellmanequationfor V1r(4.3) as
for all sE 8. Clearly, Vk= V " is a fixed point for this updaterule becausethe Bellman
equationfor V assuresus of equality in this case. Indeed, the sequence{Vk}
canbe shownin generalto convergeto V " ask ~ 00 underthe sameconditionsthat
guaranteethe existenceof V . This algorithmis called iterativepolicy evaluation.
To produceeachsuccessiveapproximation, Vk+ 1 from Vk, iterativepolicy evaluation
appliesthe sameoperationto eachstates: it replacesthe old valueof s with a
new valueobtainedfrom the old valuesof the successorstatesof s, andthe expected
immediaterewards, alongall the one-steptransitionspossibleunderthe policy being
evaluated.Wecall this kind of operationafull backup. Eachiterationof iterativepolicy
evaluationbacksup the valueof everystateonceto producethe newapproximate
valuefunction Vk+ I . Thereare severaldifferent kinds of full backups, dependingon
whethera state(ashere) or a state-action pair is beingbackedup, anddependingon
the preciseway the estimatedvaluesof the successorstatesare combined. All the
backupsdonein DP algorithmsarecalledfull backupsbecausethey arebasedon all
possiblenext statesratherthanon a samplenext state. The natureof a backupcanbe
expressedin an equation, as above, or in a backupdiagramlike thoseintroducedin
Chapter3. For example, Figure 3.4a is the backupdiagramcorrespondingto the full
To write a sequentialcomputerprogramto implementiterativepolicy evaluation,
as given by (4.5), you would haveto usetwo arrays, one for the old values, Vk(S),
and one for the new values, Vk+ I(S) . This way, the new valuescan be computed
one by one from the old valueswithout the old valuesbeing changed. Of courseit
is easierto use one array and updatethe values in place, that is, with eachnew
backed-up valueimmediatelyoverwriting the old one. Then, dependingon the order
in which the statesare backedup, sometimesnew valuesare used insteadof old
oneson the right-handsideof (4.5). This slightly different algorithmalsoconverges
to V" ; in fact, it usually convergesfasterthan the two-array version, as you might
expect, sinceit usesnew dataas soonasthey are available. We think of the backups
asbeingdonein a sweepthroughthe statespace.For the in-placealgorithm, the order
in which statesarebackedup during the sweephasa significantinfluenceon the rate
. We usuallyhavethe in-placeversionin mind when we think of DP
Another implementationpoint concernsthe tennination of the algorithm. Formally
, iterativepolicy evaluationconvergesonly in the limit , but in practiceit mustbe
haltedshortof this. A typical stoppingcondition for iterativepolicy evaluationis to
test the quantity maxse8IVk+ l (S) - Vk(s) 1after eachsweepand stopwhen it is sufficiently
small. Figure4.1 givesa completealgorithm for iterativepolicy evaluation
Example 4.1 Considerthe 4 x 4 gridworld shownbelow.
The nonterminalstatesare 8 = { I , 2, . . . , 14}. There are four actions possiblein
eachstate, A = {up, down, right , left }, which deterministicallycausethe corresponding
statetransitions, exceptthat actionsthat would take the agentoff the grid
. Thus, for instance, 9' ~~ght = 1, 9' ~~rt = 0, and
, episodictask. The rewardis - Ion all transitions
until the terminal stateis reached. The terminal stateis shadedin the figure
(althoughit is shownin two places, it is fonnally one state). The expectedreward
:s' = 1 for all statess, s and actionsa. Supposethe agentfollows
the equiprobablerandompolicy (all actions equally likely ). The left side of
Figure4.2 showsthe sequenceof valuefunctions {Vk} computedby iterativepolicy
evaluation. The final estimateis in fact V7r, which in this casegivesfor eachstatethe
negationof the expectednumberof stepsfrom that stateuntil tennination.
Exercise4.1 If7r is the equiprobablerandompolicy, what is Q7r( II , down) ? What
Exercise 4.2 Supposea new state 15 is addedto the gridworld just below state
13, and its actions, left , up, right , and down, take the agent to states12, 13,
. Assumethat the transitionsfrom the original statesare
. What, then, is V7r( 15) for theequiprobablerandompolicy? Now suppose
the dynamicsof state13 arealsochanged, suchthat action downfrom state13takes
the agentto the new state15. What is V7r( 15) for the equiprobablerandompolicy in
Exercise 4.3 What are the equationsanalogousto (4.3), (4.4), and (4.5) for the
action-value function Q7rand its successiveapproximationby a sequenceof functions
Our reasonfor computingthevaluefunctionfor a policy is to help find betterpolicies.
of iterativepolicy evaluationon a smallgridworld. The left column
is the sequenceof approximationsof the state-value function for the random policy (all
actionsequal) . The right columnis the sequenceof greedypoliciescorrespondingto the value
function estimates(arrowsare shownfor all actionsachievingthe maximum). The last policy
is guaranteedonly to be an improvementover the randompolicy, but in this caseit, and all
policies after the third iteration, are optimal.
The key criterion is whetherthis is greaterthan or lessthan V1C
(s) . If it is greaterthat is, if it is better to selecta once in s and thereafterfollow n than it would be
to follow n aUthe time- then one would expectit to be betterstill to selecta every
, and that the new policy would in fact be a betterone overall.
That this is true is a specialcaseof a generalresultcalledthepolicy improvement
theorem. Let n andn ' be any pair of detenninisticpolicies suchthat, for all se 8,
Thenthepolicy n ' mustbeasgoodas, or betterthan, n . Thatis, it mustobtaingreater
or equalexpectedreturn from all statesse 8:
Moreover, if there is strict inequality of (4.7) at any state, then there must be strict
inequality of (4.8) at one or more states. This result applies in particular to the
two policies that we consideredin the previousparagraph
n (s) . Obviously, (4.7) holds at all statesother than s. Thus, if Q1C
then the changedpolicy is indeedbetterthann .
The idea behind the proof of the policy improvement theorem is easy to understand
. Starting from (4.7 ), we keep expanding the Q7rside using (4.6) and reapplying
~ E7f'{ rt+l + yrt +2 + y2rt+3 + y3rt+4 + . . . 1St= S}
So far we have seenhow, given a policy and its value function, we can easily
evaluatea changein the policy at a single stateto a particularaction. It is a natural
extensionto considerchangesat all statesand to all possibleactions, selectingat
each state the action that appearsbest accordingto Q1I
considerthe new greedypolicy, 7r , given by
= arg max E { rt+l + yV1I'(St+ l) 1St= S, at = a }
wherearg maxadenotesthe valueof a at which the expressionthat follows is maximized(with tiesbrokenarbitrarily). The greedypolicy takesthe actionthat looksbest
in the short term- after one stepof lookahead
the greedypolicy meetsthe conditionsof the policy improvementtheorem(4.7), so
we know that it is asgoodas, or betterthan, the original policy. The processof making
a new policy that improveson an original policy, by making it greedyor nearly
greedywith respectto the value function of the original policy, is calledpolicy improvemen
Supposethe newgreedypolicy, 7r , is asgoodas, but not betterthan, the old policy
7r. Then V1I= V1I, and from (4.9) it follows that for all Se 8:
But this is the sameastheBellmanoptimality equation(4.1), andtherefore, V1Cmust
" must be optimal policies. Policy improvementthus must
give us a strictly betterpolicy exceptwhenthe original policy is alreadyoptimal.
So far in this sectionwe haveconsideredthe specialcaseof deterministicpolicies.
' specifiesprobabilities, 1I'(S, a ), for taking
In the generalcase, a stochasticpolicy 11
not go through the details, but in fact all
the ideasof this sectionextendeasily to stochasticpolicies. In particular, the policy
improvementtheoremcarries through for the stochasticcaseas stated, under the
Q7r(s , 1f (S = L 1f (s , a ) Q7r(s , a ) .
In addition , if there are ties in policy improvement steps such as (4.9) - that is , if
there are several actions at which the maximum is achieved- then in the stochastic
case we need not select a single action from among them. Instead, each maximizing
action can be given a portion of the probability of being selected in the new greedy
policy . Any apportioning scheme is allowed as long as all submaximal actions are
The last row of Figure4.2 showsan exampleof policy improvementfor stochastic
policies. Here the original policy, if , is the equiprobablerandompolicy, andthe new
policy, 1f , is greedy with respectto V7r. The value function V7r is shown in the
bottom-left diagramandthe setof possible1f' is shownin the bottom-right diagram.
The stateswith multiple arrowsin the 1f' diagramare thosein which severalactions
achievethe maximumin (4.9); anyapportionmentof probability amongtheseactions
is permitted. The valuefunctionof anysuchpolicy, V7r(s), canbe seenby inspection
to be either - 1, - 2, or - 3 at all statesS e 8, whereasV7r(s) is at most - 14.
Thus, V7r(s) ~ V7r(s), for all se 8 , illustrating policy improvement. Although in
this casethe newpolicy 1f' happensto be optimal, in generalonly an improvementis
Once a policy , 7r, has been improved using y7r to yield a better policy , 7r , we can
then compute Y7r and improve it again to yield an even better 7r . We can thus obtain
a sequenceof monotonically improving policies and value functions :
a finite MOPhasonly a finite numberof policies, this
optimalpolicyandoptimalvaluefunctionin a finite
This way of findingan optimalpolicyis calledpolicy iteration. Acompleteal, itself an iterative
gorithmis givenin Figure4.3. Notethateachpolicyevaluation
, is startedwith thevaluefunctionfor thepreviouspolicy. Thistypically
theexamplein Figure4.2. Thebottomleft diagramshowsthevaluefunctionfor the
Figure 4.3 Policy iteration (using iterativepolicy evaluation) for V . In the arg max step
in 3, it is assumedthat ties arebrokenin a consistentorder.
equiprobablerandompolicy, andthe bottom-right diagramshowsa greedypolicy for
this value function. The policy improvementtheoremassuresus that thesepolicies
are better than the original randompolicy. In this case, however, thesepolicies are
notjust better, but optimal, proceedingto the tenninal statesin the minimum number
of steps. In this example, policy iterationwould find an optimal policy afterjust one
Example 4.2: Jack s Car Rental Jack managestwo locationsfor a nationwide
car rental company. Each day, somenumber of customersarrive at each location
to rent cars. If Jack has a car available, he rents it out and is credited$10 by the
national company. If he is out of cars at that location, then the businessis lost.
Cars becomeavailablefor renting the day after they are returned. To help ensure
that carsare availablewherethey are needed,Jackcan movethem betweenthe two
locationsovernight, at a cost of $2 per car moved. We assumethat the numberof
carsrequestedandreturnedat eachlocationare Poissonrandomvariables, meaning
that the probability that the numberis n is ~ e ).., whereA is the expectednumber.
SupposeA is 3 and 4 for rental requestsat the first and secondlocationsand 3 and
2 for returns. To simplify the problemslightly, we assumethat therecanbe no more
than 20 cars at each location (any additional cars are returnedto the nationwide
company, and thus disappearfrom the problem) and a maximum of five cars can
be movedfrom one locationto the otherin onenight. We takethe discountrateto be
y = 0.9 andformulatethis asa continuingfinite MDP, wherethe time stepsaredays,
the stateis the numberof carsat eachlocationat the end of the day, andthe actions
are the net numbersof carsmovedbetweenthe two locationsovernight. Figure4.4
showsthe sequenceof policiesfound by policy iterationstartingfrom the policy that
Figure 4.4 The sequenceof policies found by policy iterationon Jack s car rentalproblem,
and the final statevalue function. The first five diagramsshow, for eachnumberof cars at
each location at the end of the day, the numberof cars to be movedfrom the first location
to the second(negativenumbersindicate transfersfrom the secondlocation to the first).
Each successivepolicy is a strict improvementover the previouspolicy, and the last policy
Exercise 4.4 (programming) Write a program for policy iteration and re-solve
Jack's car rental problem with the following changes
the first locationrides a bushomeeachnight and lives nearthe secondlocation. She
is happyto shuttleone car to the secondlocation for free. Each additionalcar still
costs$2, as do all cars movedin the other direction. In addition, Jack has limited
parking spaceat eachlocation. If more than 10 carsare kept overnightat a location
(after any moving of cars), then an additional cost of $4 must be incurred to use
a secondparking lot (independentof how many cars are kept there). Thesesorts
of nonlinearitiesand arbittary dynamicsoften occur in real problemsand cannot
easily be handledby optimization methodsother than dynamic programming. To
check your program, first replicate the results given for the original problem. If
your computer is too slow for the full problem, cut all the numbersof cars in
Exercise 4.5 How would policy iteration be defined for action values? Give a
completealgorithm for computing Q , analogousto Figure 4.3 for computing V * .
Pleasepay specialattentionto this exercise, becausethe ideasinvolvedwill be used
Exercise 4.6 Supposeyou are restrictedto consideringonly policies that are E"soft, meaningthat the probability of selectingeachaction in eachstate, s, is at least
E"/ IA(s)l . Describequalitatively the changesthat would be requiredin eachof the
steps3, 2, and 1, in that order, of the policy iterationalgorithmfor V * (Figure4.3).
One drawback to policy iteration is that each of its iterations involves policy evaluation
, which may itself be a protracted iterative computation requiring multiple
sweeps through the state set. If policy evaluation is done iteratively , then convergence
exactly to V1I occurs only in the limit . Must we wait for exact convergence, or
can we stop short of that? The example in Figure 4.2 certainly suggests that it may
be possible to truncate policy evaluation. In that example, policy evaluation iterations
beyond the first three have no effect on the corresponding greedy policy .
In fact , the policy evaluation step of policy iteration can be truncated in several
ways without losing the convergence guarantees of policy iteration . One important
special case is when policy evaluation is stopped after just one sweep ( one backup of
each state) . This algorithm is called value iteration . It can be written as a particularly
for all SE 8. For arbittaryYo, the sequence
valueiterationis by referenceto the Bellman
valueiterationis obtainedsimplyby turningthe
updaterule. Also notehowthevalueiteration
. Anotherwayof seeingthiscloserelationship
is to comparethe backupdiagramsfor thesealgorithms
for valueiteration.Thesetwo arethenaturalbackupoperations
Finally, let us considerhow valueiterationtenninates
. Figure4.5 givesa completevalueiterationalgorithmwith this kind of
evaluationandonesweepof policy improvement
Example4.3: Gamblers Problem A gamblerhastheopportunityto makebets
asmanydollarsashe hasstakedon thatflip; if it is tails, he
gameendswhenthegamblerwinsby reachinghisgoalof $100, or losesby running
. On eachflip, thegamblermustdecidewhatportionof his capitalto
stake, in integer numbersof dollars. This problem can be formulatedas an undiscounted
, episodic, finite MOP. The stateis the gambler's capitalS E { I , 2, . . . , 99}
and the actionsare stakes, a E ( I , 2, . . . , min(s, 100- s)}. The reward is zero on
all transitionsexceptthoseon which the gamblerreacheshis goal, when it is + I . A
state-valuefunctionthengivesthe probability of winning from eachstate. A policy is
a mappingfrom levelsof capitalto stakes.The optimal policy maximizesthe probability
of reachingthe goal. Let p denotethe probability of the coin coming up heads.
If p is known, then the entire problemis known and it can be solved, for instance,
by valueiteration. Figure4.6 showsthe changein the valuefunction over successive
sweepsof valueiteration, andthe final policy found, for the caseof p = 0.4.
Exercise4.7 Why doesthe optimal policy for the gambler's problemhavesucha
curious form? In particular, for capital of 50 it betsit all on one flip , but for capital
of 51 it doesnot. Why is this a goodpolicy?
Exercise4.8 (programming) Implementvalueiterationfor the gambler's problem
and solveit for p = 0.25 and p = 0.55. In programming, you may find it convenient
to introducetwo dummy statescorrespondingto tennination with capital of 0 and
100, giving them valuesof 0 and I respectively
Exercise 4.9 What is the analog of the value iteration backup (4.10) for action
Figure4.6 Thesolutionto thegamblers problemfor p = 0.4. Theuppergraphshowsthe
A major drawbackto the DP methodsthat we have discussedso far is that they
involveoperationsoverthe entirestatesetof the MDP, that is, they requiresweepsof
the stateset. If the statesetis very large, thenevena singlesweepcanbe prohibitively
. For example, the gameof backgammonhasover 1020states. Even if we
could performthe valueiterationbackupon a million statesper second,it would take
over a million yearsto completea single sweep.
AsynchronousDP algorithms are in-place iterative DP algorithms that are not
organizedin termsof systematicsweepsof the stateset. Thesealgorithmsback up
the valuesof statesin any order whatsoever
before the values of others are backedup once. To convergecorrectly, however,
an asynchronousalgorithm must continue to backup the values of all the states:
it can' t ignore any state after some point in the computation. AsynchronousDP
algorithmsallow greatflexibility in selectingstatesto which backupoperationsare
For example, one versionof asynchronousvalue iteration backsup the value, in
place, of only one state, Sk, on eachstep, k , using the valueiteration backup(4.10).
If 0 ~ y < 1, asymptoticconvergenceto V * is guaranteedgiven only that all states
occurin the sequence{Sk} an infinite numberof times. (In the undiscountedepisodic
case, it is possiblethat there are someorderingsof backupsthat do not result in
, but it is relatively easyto avoidthese.) Similarly, it is possibleto intermix
policy evaluationandvalueiterationbackupsto producea kind of asynchronous
truncatedpolicy iteration. Although the details of this and other more unusualDP
algorithmsare beyondthe scopeof this book, it is clear that a few different backups
form building blocksthat canbe usedflexibly in a wide varietyof sweeplessDP
Of course, avoidingsweepsdoesnot necessarilymeanthat we can get awaywith
lesscomputation. It just meansthat an algorithmdoesnot needto get lockedinto any
hopelesslylong sweepbeforeit canmakeprogressimprovinga policy. Wecantry to
takeadvantageof this flexibility by selectingthe statesto which we applybackupsso
asto improvethe algorithm' s rateof progress.We cantry to orderthe backupsto let
valueinformationpropagatefrom stateto statein an efficient way. Somestatesmay
not needtheir valuesbackedup asoften asothers. We might eventry to skip backing
up somestatesentirely if they are not relevantto optimal behavior. Someideasfor
Asynchronousalgorithmsalso makeit easierto intermix computationwith realtime interaction. To solvea given MDP, we can run an iterativeDP algorithmat the
sametime that an agent is actually experiencingthe MDP. The agent's experience
can be usedto determinethe statesto which the DP algorithm appliesits backups.
At the sametime, the latestvalueandpolicy informationfrom the DP algorithmcan
guide the agents decision-making. For example, we can apply backupsto statesas
the agentvisits them. This makesit possibleto focus the DP algorithm' s backups
onto partsof the statesetthat aremostrelevantto the agent. This kind of focusingis
Policy iteration consistsof two simultaneous
value function consistentwith the current policy (policy
makingthe policy greedywith respectto the currentvaluefunction (policy improvement
. In value iteration, for example, only a
single iteration of policy evaluationis performedin betweeneachpolicy improvement
. In asynchronousOP methods, the evaluationand improvementprocess
interleavedat an even finer grain. In somecasesa single stateis updatedin one
processbeforereturning to the other. As long as both process
all states, the ultimateresultis typically
We usethe term generalizedpolicy iteration (GPI) to refer to the generalidea of
interactingpolicy evaluationand policy improvementprocess
methodsare well describedas GPI. That is, all haveidentifiablepolicies and value
functions, with the policy alwaysbeing improvedwith respectto the valuefunction
and the valuefunction alwaysbeing driven towardthe valuefunction for the policy.
This overall schemafor GPI is illustratedin Figure4.7.
Figure 4.7 Generalizedpolicy iteration: Value and policy functions interact until they are
It is easyto seethat if both the evaluationprocessand the improvementprocess
stabilize, that is, no longerproducechanges
be optimal. The valuefunction stabilizesonly when it is consistentwith the current
policy, and the policy stabilizesonly when it is greedywith respectto the current
esstabilizeonly whena policy hasbeenfoundthat
optimality equation(4.1) holds, and thus that the policy and the value function are
Making the policy greedywith respectto the valuefunctiontypically makesthe value
function incorrectfor the changedpolicy, and making the value function consistent
with the policy typically causesthat policy no longer to be greedy. In the long run,
esinteractto find a singlejoint solution: the optimal value
One might also think of the interactionbetweenthe evaluationand improvement
es in GPI in termsof two constraintsor goals- for example, as two lines in
Although the real geomebyis much more complicatedthan this, the diagramsuggests
what happensin the real case. Eachprocessdrivesthe valuefunction or policy
toward one of the lines representinga solution to one of the two goals. The goals
interactbecausethe two lines are not orthogonal. Driving directly toward one goal
causessome movementaway from the other goal. Inevitably, however, the joint
processis brought closer to the overall goal of optimality. The arrows in this diagram
correspondto the behaviorof policy iteration in that each takesthe system
all the way to achievingoneof the two goalscompletely. In OPI onecould alsotake
smaller, incompletestepstowardeachgoal. In eithercase, the two
achievethe overall goal of optimality eventhoughneitheris
areactuallyquiteefficient.If we ignorea fewtechnical
) timeOPmethodstaketo find anoptimalpolicy
is polynomialin thenumberof statesandactions
, this meansthata OPmethodtakesa numberof computational
that is lessthansomepolynomialfunctionof n andm. A OP method
to find an optimalpolicy in polynomialtime eventhoughthe total
thananydirectsearchin policyspacecouldbe, because
examineeachpolicyto providethesameguarantee
methodscanalsobe usedto solveMDPs, andin somecasestheirworst-case
methodsbecomeimpracticalat a muchsmallernumberof statesthando OP
methods( by a factorof about100). Forthelargestproblems
thoughtto be of limitedapplicabilitybecause
), thefactthatthenumberof statesoftengrowsex. Largestatesetsdocreatedifficulties
ponentiallywith thenumberof statevariables
buttheseareinherentdifficultiesof theproblem
. Bothpolicyiterationandvalueiterationarewidelyused, andit
is not clearwhich, if either, is betterin general
arestartedwith goodinitial valuefunctionsor policies.
, yet the problemis still potentiallysolvablebecause
relatively few statesoccur along optimal solution ttajectories. Asynchronousmethods
and other variationsof GPI can be appliedin suchcasesand may find good or
optimal policies much faster than synchronous methods can.
In this chapter we have becomefamiliar with the basic ideas and algorithms of
dynamicprogrammingasthey relateto solving finite MD Ps. Policy evaluationrefers
to the (typically) iterative computationof the value functions for a given policy.
Policy improvementrefersto the computationof an improvedpolicy given the value
function for that policy. Putting thesetwo computationstogether, we obtainpolicy
iteration and value iteration, the two most popularDP methods. Either of thesecan
be usedto reliably computeoptimal policies and value functions for finite MD Ps
ClassicalDP methodsoperatein sweepsthrough the stateset, performing afull
backupoperationon each state. Each backupupdatesthe value of one statebased
on the valuesof all possiblesuccessorstatesand their probabilitiesof occurring.
Full backupsarecloselyrelatedto Bellmanequations:they are little morethanthese
hasoccurredto valuesthat satisfythe corresponding
Bellman equation. Just as thereare four primary value functions( V7r, V * , Q7r,
and Q*), therearefour correspondingBellmanequationsandfour correspondingfull
backups.An intuitive view of the operationof backupsis givenby backupdiagrams.
Insight into DP methodsand, in fact, into almostall reinforcementlearningmethods
, canbe gainedby viewing themasgeneralizedpolicy iteration (GPI). GPI is the
esrevolvingaround8!1approximatepolicy and
an approximatevalue function. One processtakesthe policy as given and performs
someform of policy evaluation, changingthe valuefunction to be morelike the true
valuefunction for the policy. The otherprocesstakesthe valuefunction asgivenand
performssomeform of policy improvement, changingthe policy to makeit better,
assumingthat thevaluefunction is its valuefunction. Althougheachprocesschanges
thebasisfor the other, overallthey work togetherto find ajoint solution: a policy and
valuefunction that are unchangedby either processand, consequently
In somecases, GPI can be proved to converge, most notably for the classicalDP
methodsthat we havepresentedin this chapter. In other casesconvergencehasnot
beenproved, but the idea of GPI still improvesour understandingof the methods.
4.9 Bibliographical and Historical Remarks
It is not necessaryto perfonn OP methodsin completesweepsthroughthe state
set. AsynchronousDP methodsarein-placeiterativemethodsthatbackup statesin an
- arbitraryorder, perhapsstochasticallydetenninedandusingout of dateinfonnation.
Many of thesemethodscanbe viewedasfine grainedfonDSof OPI.
Finally, we note one last special property of OP methods. All of them update
estimatesof the valuesof statesbasedon estimatesof the valuesof successorstates.
That is, they updateestimateson the basisof other estimates.We call this general
idea bootstrapping. Many reinforcementlearning methodsperfonn bootstrapping,
eventhosethat do not require, asOP requires, a completeand accuratemodelof the
environment.In the next chapterwe explorereinforcementlearningmethodsthat do
not requirea modelanddo not bootstrap.In the chapterafterthat we exploremethods
that do not require a model but do bootstrap. Thesekey featuresand propertiesare
, yet canbe mixed in interestingcombinations.
4.9 Bibliographical and Historical Remarks
Theterm"dynamicprogrammingis dueto Bellman( 1957a
canbehandledin closedanalyticform. Thisremarkmayhavemisledartificialintelligence
DPcalled heuristicdynamicprogramming emphasizes
discussionof value iteration as a fonD of truncatedpolicy iteration is basedon the
approachof Putennanand Shin ( 1978), who presenteda classof algorithmscalled
modifiedpolicy iteration, which includespolicy iterationand valueiterationasspecial
cases.An analysisshowinghow valueiteration can be madeto find an optimal policy
in finite time is given by Bertsekas( 1987).
Iterative policy evaluationis an exampleof a classicalsuccessiveapproximation
algorithm for solving a systemof linear equations. The versionof the algorithm that
usestwo arrays, oneholding the old valueswhile the otheris updated, is often calleda
Jacobi-style algorithm, after Jacobi's classicaluseof this method. It is alsosometimes
called a synchronousalgorithm becauseit can be perfonnedin parallel, with separate
processorssimultaneouslyupdatingthe valuesof individual statesusing input from
. The secondarray is neededto simulate this parallel computation
sequentially. The in-place version of the algorithm is often called a Gauss- Seidelstylealgorithmafter the classicalGauss- Seidelalgorithmfor solving systemsof linear
equations.In additionto iterativepolicy evaluation, otherDP algorithmscanbe implemented
in thesedifferent versions. Bertsekasand Tsitsiklis ( 1989) provide excellent
coverageof thesevariationsand their perfonnancedifferences.
AsynchronousDP algorithmsaredueto Bertsekas( 1982, 1983), who alsocalledthem
distributedDP algorithms. The original motivationfor asynchronous
on a multiprocessorsystemwith communicationdelaysbetweenprocessors
and no global synchronizingclock. These algorithms are extensivelydiscussedby
Bertsekasand Tsitsiklis ( 1989). Jacobi-style and Gauss- Seidel-style DP algorithms
are specialcasesof the asynchronousversion. Williams and Baird ( 1990) presented
DP algorithmsthat are asynchronousat a finer grain than the oneswe havediscussed
the backupoperationsthemselvesare broken into stepsthat can be perfonnedasynchronously
This section, written with the help of Michael Littman, is basedon Littman, Dean, and
In this chapterwe considerour first learningmethodsfor estimatingvaluefunctions
anddiscoveringoptimalpolicies. Unlike the previouschapter, herewe do not assume
completeknowledgeof the environment. Monte Carlo methodsrequireonly experience
- samplesequencesof states, actions, and rewardsfrom on-line or simulated
interaction with an environment. Learning from on-line experienceis striking because
it requiresno prior knowledgeof the environments dynamics, yet can still
attain optimal behavior. Learning from simulatedexperienceis also powerful. Although
a modelis required, the modelneedonly generatesampletransitions, not the
completeprobability distributionsof all possibletransitionsthat are requiredby dynamic
programming(DP) methods. In surprisinglymanycasesit is easyto generate
experiencesampledaccordingto the desiredprobability distributions, but infeasible
to obtain the distributionsin explicit form.
Monte Carlo methodsare ways of solving the reinforcementlearning problem
basedon averagingsamplereturns. To ensurethat well-definedreturnsareavailable,
we define Monte Carlo methodsonly for episodictasks. That is, we assumeexperience
, and that all episodeseventuallyterminateno matter
what actionsare selected.It only uponthe completionof an episodethat valueestimates
and policies are changed. Monte Carlo methodsare thus incrementalin an
episode-by-episodesense, but not in a step-by stepsense.The term Monte Carlo
is often usedmore broadly for any estimationmethodwhoseoperationinvolvesa
significant random component. Here we use it specifically for methodsbasedon
averagingcompletereturns(as opposedto methodsthat learn from partial returns,
Despitethe differencesbetweenMonte Carlo andOP methods,the mostimportant
ideascarryoverfrom DP to the MonteCarlo case. Not only do theycomputethe same
valuefunctions, but they interactto attainoptimality in essentiallythe sameway. As
in the DP chapter, we considerfirst policy evaluation, the computationof V1f and
Q1ffor a fixed arbitrarypolicy 7r, thenpolicy improvement,and, finally, generalized
policy iteration. Eachof theseideastakenfrom DP is extendedto the Monte Carlo
casein which only sampleexperienceis available.
We begin by consideringMonte Carlo methodsfor learningthe state-valuefunction
for a given policy. Recall that the value of a stateis the expectedreturn- expected
cumulativefuture discountedreward- starting from that state. An obvious way to
, then, is simply to averagethe returns observedafter
visits to that state. As morereturnsare observed,the averageshouldconvergeto the
expectedvalue. This ideaunderliesall Monte Carlo methods.
In particular, supposewe wish to estimate V7r(s), the value of a statesunder
policy 7r, given a set of episodesobtainedby following 7r and passingthroughs .
Each occurrenceof states in an episodeis called a visit to s. The every-visit MC
methodestimatesV7r(s) as the averageof the returnsfollowing all the visits to s in
. Within a given episode, the first time s is visited is called thefirst
visit to s. Thefirst -visit MC method averagesjust the returnsfollowing first visits
to s. Thesetwo Monte Carlo methodsare very similar but have slightly different
theoreticalproperties. First-visit MC has beenmost widely studied, dating back to
the 1940s, and is the one we focus on in this chapter. We reconsiderevery-visit MC
in Chapter7. First-visit MC is shownin proceduralform in Figure5.1.
Both first-visit MC andevery-visit MC convergeto V7r(s) asthe numberof visits
(or first visits) to s goesto infinity . This is easyto seefor the caseof first-visit MC.
, identically distributedestimateof V7r(s) .
sequenceof averagesof theseestimatesconverges
to their expectedvalue. Eachaverageis itself an unbiasedestimate, andthe standard
deviationof its error falls as 1/ Jij , wheren is thenumberof returnsaveraged
. Everyvisit MC is less sb'aightforward, but its estimatesalso convergeasymptoticallyto
The useof Monte Carlo methodsis bestillustratedthroughan example.
Example 5.1 Blackjackis a popularcasinocardgame. The objectis to obtaincards
the sumof whosenumericalvaluesis asgreataspossiblewithout exceeding21. All
face cardscount as 10, and the acecan count either as 1 or as 11. We considerthe
versionin which eachplayer competesindependentlyagainstthe dealer. The game
Figure5.1 First-visit MC methodfor estimatingV .
beginswith two cardsdealt to both dealerand player. One of the dealers cardsis
faceupand the other is facedown. If the player has 21 immediately(an ace and a
10-card), it is called a natural. He then wins unlessthe dealeralso hasa natural, in
which casethe gameis a draw. If the player does not havea natural, then he can
requestadditionalcards, oneby one(hits), until he eitherstops(sticks) or exceeds21
(goesbust). If he goesbust, he loses; if he sticks, then it becomesthe dealers turn.
The dealerhits or sticksaccordingto a fixed strategywithout choice: he stickson any
sumof 17or greater, andhits otherwise. If the dealergoesbust, thenthe playerwins;
otherwise, the outcome- win, lose, or draw- is detenninedby whosefinal sum is
Playingblackjackis naturallyformulatedasan episodicfinite MDP. Eachgameof
blackjackis an episode.Rewardsof + 1, - 1, and0 aregivenfor winning, losing, and
. All rewardswithin a gameare zero, and we do not discounty
= 1); thereforethesetenninal rewardsare also the returns. The player' s actions
areto hit or to stick. The statesdependon the player s cardsandthe dealers showing
card. We assumethat cardsaredealtfrom an infinite deck(i.e., with replacement
that there is no advantageto keepingtrack of the cardsalreadydealt. If the player
holds an ace that he could count as 11 without going bust, then the ace is said to
be usable. In this caseit is always countedas 11 becausecounting it as 1 would
make the sum 11 or less, in which casethere is no decisionto be madebecause
of threevariables: his currentsum ( 12- 21), the dealer's one showingcard (ace- l0 ),
andwhetheror not he holds a usableace. This makesfor a total of 200 states.
Approximatestate-valuefunctionsfor the blackjackpolicy that sticksonly on 20
or 21, computedby Monte Carlo policy evaluation.
Considerthe policy that sticks if the player' s sum is 20 or 21, and otherwise
hits. To find the state-valuefunction for this policy by a Monte Carlo approach,one
simulatesmanyblackjackgamesusingthe policy andaveragesthe returnsfollowing
eachstate. Note that in this task the samestateneverrecurswithin one episode, so
thereis no differencebetweenfirst-visit andevery-visit MC methods.In this way, we
obtainedthe estimatesof the state-valuefunction shownin Figure5.2. The estimates
for stateswith a usableace are less certain and less regular becausethesestates
are lesscommon. In any event, after 500,000 gamesthe valuefunction is very well
Although we havecompleteknowledgeof the environmentin this task, it would
not be easyto apply DP policy evaluationto computethe valuefunction. DP methods
requirethe distributionof next events- in particular, they requirethe quantities/ Ps~,
.:sand it is not easyto detenninethesefor blackjack. For example, suppose
the player' s sum is 14 and he choosesto stick. What is his expectedreward as a
function of the dealer's showingcard? All of theseexpectedrewardsand transition
probabilitiesmustbe computedbeforeDP canbe applied, andsuchcomputationsare
often complexanderror-prone. In contrast, generatingthe samplegamesrequiredby
MonteCarlo methodsis easy. This is the casesurprisinglyoften; the ability of Monte
Carlo methodsto work with sampleepisodesalone can be a significant advantage
evenwhenone hascompleteknowledgeof the environment's dynamics.
Figure5.3 Thebackupdiagramfor MonteCarloestimation
Can we generalizethe idea of backupdiagramsto Monte Carlo algorithms? The
generalideaof a backupdiagramis to showat the top the root nodeto be updatedand
to showbelow all the transitionsand leaf nodeswhoserewardsandestimatedvalues
contributeto the update. For Monte Carlo estimationof VIr, the root is a statenode,
and below is the entire sequenceof transitionsalong a particularepisode, endingat
the terminal state, as in Figure 5.3. Whereasthe OP diagram(Figure 3.4a) shows
all possibletransitions, the Monte Carlo diagramshowsonly thosesampledon the
one episode. Whereasthe OP diagramincludesonly one-steptransitions, the Monte
Carlo diagramgoesall the way to the end of the episode. Thesedifferencesin the
diagramsaccuratelyreflect the fundamentaldifferencesbetweenthe algorithms.
An importantfact aboutMonte Carlo methodsis that the estimatesfor eachstate
. The estimatefor one state does not build upon the estimateof
as is the casein OP. In other words, Monte Carlo methodsdo not
In particular, note that the computationalexpenseof estimatingthe value of a
single state is independentof the number of states. This can make Monte Carlo
methodsparticularly attractivewhen one requiresthe value of only a subsetof the
states. Onecan generatemanysampleepisodesstartingfrom thesestates, averaging
returnsonly from thesestates, ignoring all others. This is a third advantageMonte
Carlo methodscan have over OP methods(after the ability to learn from actual
Example 5.2: SoapBubble Supposea wire framefonning a closedloop is dunked
in soapy water to form a soap surfaceor bubble confonning at its edgesto the
wire frame. If the geometryof the wire frame is irregular but known, how can you
computethe shapeof the surface? The shapehasthe propertythat the total force on
eachpoint exertedby neighboringpoints is zero (or elsethe shapewould change).
This meansthat the surface's height at any point is the averageof its heights at
points in a small circle aroundthat point. In addition, the surfacemust meet at its
boundarieswith the wire frame. The usual approachto problemsof this kind is
to put a grid over the area coveredby the surfaceand solve for its height at the
grid points by an iterative computation. Grid points at the boundaryare forced to
the wire frame, and all others are adjustedtoward the averageof the heights of
their four nearestneighbors. This processthen iterates, much like DP' s iterative
policy evaluation, and ultimately convergesto a closeapproximationto the desired
This is similar to the kind of problemfor which Monte Carlo methodswereoriginally
designed.Insteadof the iterativecomputationdescribedabove, imaginestanding
on the surfaceand taking a randomwalk, steppingrandomlyfrom grid point to
neighboringgrid point, with equalprobability, until you reachthe boundary. It turns
out that the expectedvalueof the heightat the boundaryis a closeapproximationto
the height of the desiredsurfaceat the startingpoint (in fact, it is exactly the value
computedby the iterativemethoddescribedabove). Thus, one can closely approximate
the heightof the surfaceat a point by simply averagingthe boundaryheightsof
manywalks startedat the point. If one is interestedin only the valueat onepoint, or
any fixed small setof points, thenthis Monte Carlo methodcanbe far moreefficient
than the iterativemethodbasedon local consistency
Exercise5.1 Considerthe diagramson the right in Figure5.2. Why doesthe value
functionjump up for the last two rows in the rear? Why doesit drop off for the whole
last row on the left? Why arethe frontmostvalueshigher in the upperdiagramsthan
If a model is not available, then it is particularly useful to estimateaction values
ratherthan statevalues. With a model, statevaluesaloneare sufficientto determine
a policy; one simply looks aheadone stepandchooseswhicheveraction leadsto the
bestcombinationof rewardandnext state, aswe did in the chapteron DP. Without a
5.2 Monte Carlo Estimationof Action Values
model, however, statevaluesalonearenot sufficient. Onemustexplicitly estimatethe
valueof eachaction in orderfor the valuesto be usefulin suggestinga policy. Thus,
oneof our primary goalsfor Monte Carlo methodsis to estimateQ* . To achievethis,
we first consideranotherpolicy evaluationproblem.
The policy evaluationproblem for action valuesis to estimateQ7r(s, a), the expected
returnwhenstartingin statestaking actiona, andthereafterfollowing policy
7r. The Monte Carlo methodshereareessentiallythe sameasjust presentedfor state
values. The every-visit MC methodestimatesthe valueof a state- action pair as the
averageof the returnsthat havefollowed visits to the statein which the action was
selected.The first-visit MC methodaveragesthe returnsfollowing the first time in
eachepisodethat the statewas visited and the action was selected.Thesemethods
convergequadratically, asbefore, to the true expectedvaluesasthe numberof visits
The only complicationis that many relevantstate- action pairs may neverbe visited
. If 7r is a deterministicpolicy, then in following 7r one will observereturns
only for one of the actionsfrom eachstate. With no returnsto average
Carlo estimatesof the other actionswill not improve with experience
problem becausethe purposeof learningaction v~ ues is to help in choosing
amongthe actionsavailablein eachstate. To comparealternativeswe needto estimate
the value of all the actions from each state, not just the one we currently
This is thegeneralproblemof maintainingexploration, asdiscussedin the context
of the n-armedbanditproblemin Chapter2. For policy evaluationto work for action
values, we must assurecontinual exploration. One way to do this is by specifying
that the first stepof eachepisodestartsat a state- action pair , and that every such
pair hasa nonzeroprobability of being selectedas the start. This guaranteesthat all
state- actionpairswill be visited an infinite numberof timesin the limit of an infinite
. We call this the assumptionof exploringstarts.
The assumptionof exploring startsis sometimesuseful, but of courseit cannot
be relied upon in general, particularly when learningdirectly from real interactions
with anenvironment.In that casethe startingconditionsareunlikely to be sohelpful.
The most common alternativeapproachto assuringthat all state-action pairs are
encounteredis to consideronly policiesthat arestochasticwith a nonzeroprobability
of selectingaUactions. We discusstwo importantvariantsof this approachin later
sections. For now, we retain the assumptionof exploring starts and completethe
presentationof a full Monte Carlo control method.
Exercise5.2 What is the backupdiagramfor Monte Carlo estimationof Q7r?
We are now ready to consider how Monte Carlo estimation can be used in control ,
that is , to approximate optimal policies . The overall idea is to proceed according to
the same pattern as in the DP chapter, that is , according to the idea of generalized
policy iteration (OPI ) . In OPI one maintains both an approximate policy and an
approximate value function . The value function is repeatedly altered to more closely
approximate the value function for the current policy , and the policy is repeatedly
improved with respect to the current value function :
Thesetwo kinds of changeswork againsteachother to someextent, as eachcreates
a moving targetfor the other, but togetherthey causeboth policy and valuefunction
To begin, let us considera Monte Carlo version of classicalpolicy iteration. In
this method, we perform alternatingcompletestepsof policy evaluationand policy
improvement, beginning with an arbitrary policy 1(0 and ending with the optimal
where ~ denotesa completepolicy evaluationand ~ denotesa completepolicy
improvement. Policy evaluationis done exactly as describedin the preceding
, with the approximateaction-valuefunction
approachingthe ttue function asymptotically.For the moment, let us assumethat we
do indeedobservean infinite numberof episodesandthat, in addition, the episodes
aregeneratedwith exploringstarts. Undertheseassumptions
Policy improvementis done by making the policy greedy with respectto the
currentvaluefunction. In this casewe haveanaction-valuefunction, andthereforeno
model is neededto consttuctthe greedypolicy. For any action-valuefunction Q, the
correspondinggreedypolicy is the onethat. for each sE 8 , deterministically chooses
As we discussedin the previouschapter, the theoremassuresus that each1l'k+ 1 is
unifonnly betterthan1l'k, unlessit is equalto 1l'k, in which casethey areboth optimal
policies. This in turn assuresus that the overall processconvergesto an optimal
policy andthe optimal valuefunction. In this way Monte Carlo methodscanbe used
to find optimal policies given only sampleepisodesand no other knowledgeof the
We madetwo unlikely assumptionsabovein order to easily obtain this guarantee
of convergencefor the Monte Carlo method. Onewasthat the episodeshaveexploring
starts, and the other was that policy evaluationcould be done with an infinite
. To obtain a practical algorithm we will have to removeboth
. We postponeconsiderationof the first assumptionuntil later in this
For now we focuson the assumptionthat policy evaluationoperateson an infinite
. This assumptionis relativelyeasyto remove. In fact, the sameissue
arisesevenin classicalDP methodssuchasiterativepolicy evaluation,which also
convergeonly asymptoticallyto the true valuefunction. In both DP andMonte Carlo
casesthereare two ways to solvethe problem. One is to hold firm to the ideaof approximating
Q7rkin eachpolicy evaluation.Measurements
to obtainboundson the magnitudeandprobability of error in the estimates
sufficient stepsare taken during eachpolicy evaluationto assurethat thesebounds
are sufficiently small. This approachcan probablybe madecompletelysatisfactory
in the senseof guaranteeingcorrectconvergence
However, it is also likely to requirefar too manyepisodesto be usefulin practiceon
The secondapproachto avoiding the infinite numberof episodesnominally required
for policy evaluationis to forgo trying to completepolicy evaluationbefore
returningto policy improvement.On eachevaluationstepwe movethe valuefunction
, but we do not expectto actually get closeexceptover many steps. We
usedthis idea when we first introducedthe ideaof OPI in Section4.6. One extreme
form of the ideais valueiteration, in which only oneiterationof iterativepolicy evaluation
is performedbetweeneachstepof policy improvement.The in-placeversion
of valueiterationis evenmoreextreme; therewe alternatebetweenimprovementand
For Monte Carlo policy evaluationit is natural to alternatebetweenevaluation
and improvementon an episode-by- episodebasis. After eachepisode, the observed
returnsareusedfor policy evaluation, andthenthe policy is improvedat all the states
visited in the episode. A completesimple algorithm along theselines is given in
Figure 5.4. ~ e call this algorithmMonte Carlo ES, for Monte Carlo with Exploring
In Monte Carlo ES, all the returnsfor eachstate-actionpair areaccumulatedand
, irrespectiveof what policy was in force when they were observed. It is
that Monte carlo ES cannot convergeto any suboptimalpolicy. If it
did, then the valuefunction would eventuallyconvergeto the valuefunction for that
policy, and that in turn would causethe policy to change. Stability is achievedonly
Figure 5.4 Monte Carlo ES: A Monte Carlo control algorithm assumingexploringstarts.
whenboth the policy andthe valuefunction areoptimal. Convergence
time, but has not yet beenfonnally proved. In our opinion, this is one of the most
fundamentalopenquestionsin reinforcementlearning.
Example 5.3: Solving Blackjack It is straightforwardto apply Monte Carlo ES
to blackjack. Since the episodesare all simulatedgames, it is easyto arrangefor
exploringstartsthat includeall possibilities. In this caseonesimply picksthe dealers
cards, the player' s sum, and whether or not the player has a usable ace, all at
randomwith equalprobability. As the initial policy we usethe policy evaluatedin the
previousblackjack example, that which sticks only on 20 or 21. The initial action
value function can be zero for all state-action pairs. Figure 5.5 showsthe optimal
policy for blackjackfound by Monte Carlo ES. This policy is the sameasthe basic
strategyof Thorp ( 1966) with the soleexceptionof the leftmostnotchin thepolicy for
a usableace, which is not presentin Thorp' s strategy.We areuncertainof the reason
, but confident that what is shown here is indeed the optimal
policy for the versionof blackjackwe havedescribed.
Figure 5.5 The optimal policy andstate-valuefunction for blackjack. foundby Monte Carlo
ES (Figure5.4) . The state-valuefunction shownwascomputedfrom the action-valuefunction
How canwe avoidthe unlikely assumptionof exploringstarts? The only generalway
to ensurethat all actionsare selectedinfinitely often is for the agentto continueto
es to ensuringthis, resulting in what we call
on-policy methodsand off-policy methods. On-policy methodsattemptto evaluate
or improve the policy that is usedto makedecisions. In this sectionwe presentan
on-policy Monte Carlo control methodin orderto illustratethe idea.
In on-policy control methodsthe policy is generallysoft, meaningthat 7r(s, a) > 0
for all sE 8 andall a E A (s) . Therearemanypossiblevariationson on-policy methods
. One possibility is to gradually shift the policy toward a deterministicoptimal
policy. Many of the methodsdiscussedin Chapter2 provide mechanismsfor this.
The on-policy methodwe presentin this section usesf.-greedypolicies, meaning
that mostof the time they choosean actionthat hasmaximalestimatedactionvalue,
but with probability f. they insteadselectan actionat random. That is, all nongreedy
actionsaregiven the minimal probability of selection,
>i , is givento the greedyaction. The f. greedypolicies
are examplesof f. soft policies, definedas policies for which 7r(s, a) ~
statesand actions, for somef. > O. Among f.-soft policies, f.-greedypolicies are in
The overall idea of on-policy Monte Carlo control is still that of GPI. As in
Monte Carlo ES, we usefirst-visit MC methodsto estimatethe action-valuefunction
for the current policy. Without the assumptionof exploring starts, however, we
cannot simply improve the policy by making it greedywith respectto the current
valuefunction, becausethat would preventfurther explorationof nongreedyactions.
Fortunately, GPI doesnot require that the policy be taken all the way to a greedy
policy, only that it be movedtoward a greedypolicy. In our on-policy methodwe
will moveit only to an f.-greedypolicy. For anyf.-soft policy, 7r, anyf.-greedypolicy
with respectto Q7ris guaranteedto be betterthanor equalto 7r.
That any f.-greedypolicy with respectto Q7ris an improvementover any f. -soft
policy 7r is assuredby the policy improvementtheorem. Let 7r be the f.-greedy
policy. The conditions of the policy improvementtheoremapply becausefor any
- with nonnegative weigh~ ~ummingto 1, andassuch
it must be less than or equal to the largest number averaged)
Thus, by the policy improvementtheorem, 7r ~ 7r (i.e., V7r(s) ~ V7r(s), for aU
se B). We now prove that equality can hold only when both 7r and 7r are optimal
amongtheE-soft policies, that is, whenthey arebetterthanor equalto all otherE soft
Considera newenvironmentthat is just like the original environment,exceptwith
the requirementthat policies be E-soft moved inside the environment. The new
environmenthasthe sameactionandstatesetsasthe original andbehavesasfollows.
If in states and taking action a , then with probability 1 - E the new environment
behavesexactly like the old environment. With probability E it repicksthe action at
random, with equal probabilities, and then behaveslike the old environmentwith
the new, randomaction. The bestone can do in this new environmentwith general
policiesis the sameasthe bestone could do in the original environmentwith E soft
policies. Let V denotethe optimal valuefunction for the new environment
policy 7r is optimal amongE SOftpoliciesif andonly if V7r= V . Fromthe definition
of V * we know that it is the uniquesolutionto
V*(s) = (1- E) m: x L 9':S, [~ :s' + yV*(s')]
When equality holds and the €- soft policy 7r is no longer improved , then we also
is the sameasthe previousone, exceptfor -the substitutionof
V7rfor V * . Since V * is the uniquesolution, it mustbe that V7r= V * .
, we have shown in the last few pagesthat policy iteration works for
€ soft policies. Using the natural notion of greedy policy for €-soft policies, one
is assuredof improvementon every step, except when the best policy has been
found amongthe €-soft policies. This analysisis independentof how the action-value
functions are detenninedat eachstage, but it doesassumethat they are computed
exactly. This brings us to roughly the samepoint as in the previoussection. Now
we only achievethe bestpolicy amongthe €-soft policies, but on the otherhand, we
haveeliminatedthe assumptionof exploring starts. The completealgorithmis given
s.s EvaluatingOnePolicyWblle FoUowingAnother
So far we haveconsideredmethodsfor estimatingthe value functionsfor a policy
givenan infinite supplyof episodesgeneratedusingthat policy. Supposenow that all
we haveareepisodesgeneratedfrom a different policy. That is, supposewe wish to
, but all we haveare episodesfollowing 7r , where7r :;C7r. Can
we learnthe valuefunction for a policy given only experience off" the policy?
Happily, in many case~ we can. Of course, in order to use episodesfrom 7r to
estimatevaluesfor 7r, we require that every action taken under7r is also taken, at
leastoccasionally,under7r' . That is, we requirethat 7r(S, a) > 0 implies 7r (S, a ) > O.
In the episodesgeneratedusing 7r , considerthe ith first visit to state S and the
completesequenceof statesand actions following that visit. Let Pi (s) and P; (s)
denotethe probabilitiesof thatcompletesequencehappeninggivenpolicies7r and7r
andstartingfrom s. Let Ri (s) denotethe correspondingobservedreturnfrom states.
Evaluating One Policy While Following Another
Figure 5.6 An E-SOfton-policy Monte Carlo control algorithm.
To averagetheseto obtainan unbiasedestimateof V1f(s), we needonly weight each
returnby its relativeprobability of occurringunder1( and1( , that is, by Pi (s) / P~(s) .
The desiredMonte Carlo estimateafter observingns returnsfrom states is then
This equationinvolvesthe probabilitiesp ;(s) and p~(s), which arenonnaily considered
unknownin applicab~ns of Monte Carlo methods. Fortunately, here we need
only their ratio, p;(s)/ p ~(s), which can be determinedwith no knowledgeof the
environment's dynamics. Let T;(s) be the time of tenninationof the idt episodeinvolving
Thustheweightneededin (5.3), Pi(S)/ p~(s), depends
not at all on the environment ' s dynamics .
Exercise 5.3 What is the Monte Carlo estimate analogous to (5.3) for action
We are now ready to presentan exampleof the secondclass of learning control
methodswe considerin this book: off -policy methods. Recallthat the distinguishing
featureof on-policy methodsis that they estimatethe valueof a policy while using
it for control. In off -policy methodsthesetwo functions are separated
usedto generatebehavior, calledthe behaviorpolicy, may in fact be unrelatedto the
policy that is evaluatedand improved, calledthe estimationpolicy. An advantageof
this separationis that theestimationpolicy may be deterministic(e.g., greedy), while
the behaviorpolicy cancontinueto sampleall possibleactions.
Off -policy Monte Carlocontrol methodsusethetechniquepresentedin thepreceding
sectionfor estimatingthe valuefunction for onepolicy while following another.
They follow the behaviorpolicy while learningaboutand improving the estimation
policy. This techniquerequiresthat the behaviorpolicy havea nonzeroprobability
of selectingall actionsthat might be selectedby the estimationpolicy. To exploreall
possibilities, we requirethat the behaviorpolicy be soft.
Figure5.7 showsan off -policy Monte Carlo method, basedon OPI, for computing
policy 1Cis maintainedas an arbitrary soft policy. The estimation
policy 1Cis the greedypolicy with respectto Q, an estimateof Q1C
optimal policy, an infinite numberof returnssuitablefor usein (c) mustbe obtained
for eachpair of stateandaction. This canbe assuredby carefulchoiceof thebehavior
policy. For example, any f -SOftbehaviorpolicy will suffice.
A potential problem is that this method learnsonly from the tails of episodes
after the last nongreedyaction. If nongreedyactionsare frequent, then learningwill
be slow, particularly for statesappearingin the early portions of long episodes
with off-policy Monte Carlo methodsto assesshow seriousthis problemis.
So, ao , rl , Sl , ai , r2 , . . . , ST - I , aT - IrT
Exercise 5.4: Racetrack ( programming ) Consider driving a race car around a turn
like those shown in Figure 5.8. You want to go as fast as possible, but not so fast as
to run off the track. In our simplified racetrack, the car is at one of a discrete set of
grid positions , the cells in the diagram . The velocity is also discrete, a number of grid
cells moved horizontally and vertically per time step. The actions are increments to
the velocity components. Each may be changed by + 1, - 1, or 0 in one step, for a
total of nine actions. Both velocity components are restricted to be nonnegative and
less than 5 , and they cannot both be zero. Each episode begins in one of the randomly
selected start states and ends when the car crossesthe finish line . The rewards are - 1
for each step that stays on the track , and - 5 if the agent tries to drive off the track.
Actually leaving the track is not allowed , but the position is always advanced by at
least one cell along either the horizontal or vertical axes. With these restrictions and
considering only right turns , such as shown in the figure , all episodes are guaranteed
to tenninate , yet the optimal policy is unlikely to be excluded. To make the task more
challenging , we assumethat on half of the time stepsthe position is displaced forward
A coupleof right turnsfor the raceb'acktask.
or to the right by one additionalcell beyondthat specifiedby the velocity. Apply the
on-policy Monte Carlo control methodto this task to computethe optimal policy
from eachstartingstate. Exhibit severaltrajectoriesfollowing the optimal policy.
Monte Carlo methods can be implemented incrementally , on an episode-by - episode
basis, using extensions of techniques described in Chapter 2. They use averages of
returns just as some of the methods for solving n -armed bandit tasks described in
Chapter 2 use averages of rewards. The techniques in Sections 2. 5 and 2. 6 extend
immediately to the Monte Carlo case. They enable Monte Carlo methods to process
each new return incrementally with no increase in computation or memory as the
There are two differences between the Monte Carlo and bandit cases. One is
that the Monte Carlo case typically involves multiple situations , that is, a different
averaging process for each state, whereas bandit problems involve just one state
(at least in the simple form treated in Chapter 2) . The other difference is that the
reward distributions in bandit problems are typically stationary, whereas in Monte
Carlo methods the return distributions are typically nonstationary. This is because
the returns depend on the policy , and the policy is typically changing and improving
The incremental implementation described in Section 2.5 handles the case of
simple or arithmetic averages, in which each return is weighted equally . Suppose we
instead want to implement a weighted average, in which each return Rn is weighted
For example, the methoddescribedfor estimatingonepolicy while following another
in Section5.5 usesweightsof Wn(S) = Pn(s)/ p~(s) . Weightedaveragesalsohavea
simpleincrementalupdaterule. In additionto keepingtrack of Vn, we mustmaintain
for eachstatethe cumulativesum Wn of the weightsgiven to the first n returns. The
Exercise5.5 Modify the algorithmfor first-visit MC policy evaluation(Figure5.1)
to use the incrementalimplementationfor stationaryaveragesdescribedin Section
Exercife 5.6 Derive the weighted-averageupdaterule (5.5) from (5.4). Follow the
patternof the derivationof the unweightedrule (2.4) from (2.1).
Exercife 5.7 Modify the algorithm for the off -policy Monte Carlo control algorithm
(Figure 5.7) to usethe methoddescribedabovefor incrementallycomputing
The Monte Carlo methodspresentedin this chapterlearn value functions and optimal
policies from experiencein the form of sampleepisodes
over DP methods. First, they canbe usedto learnoptimal
behaviordirectly from interactionwith the environment, with no modelof the
environment's dynamics. Second, they can be usedwith simulationor samplemodels
. For surprisinglymany applicationsit is easyto simulatesampleepisodeseven
thoughit is difficult to constructthe kind of explicit modelof transitionprobabilities
requiredby DP methods.Third, it is easyandefficient tofocus Monte Carlo methods
on a small subsetof the states. A region of specialinterestcan be accuratelyevaluated
without going to the expenseof accuratelyevaluatingthe rest of the stateset
A fourth advantageof Monte Carlo methods, which we discusslater in the book,
is that they may be lessharmedby violationsof the Markov property. This is because
they do not update their value estimateson the basis of the value estimatesof
successorstates. In other words, it is becausethey do not bootstrap.
In designingMonteCarlo control methodswe havefollowed the overallschemaof
generalizedpolicy iteration (GPI) introducedin Chapter4. GPI i ~volvesinteracting
es of policy evaluationand policy improvement. Monte Carlo methodsprovide
an alternativepolicy evaluationprocess.Ratherthanusea modelto computethe
valueof eachstate, they simply averagemanyreturnsthat startin the state. Becausea
state's valueis theexpectedreturn, this averagecanbecomea goodapproximationto
the value. In control methodswe areparticularlyinterestedin approximatingactionvalue functions, becausethesecan be usedto improvethe policy without requiring
a model of the environment's transition dynamics. Monte Carlo methodsinternlix
policy evaluationandpolicy improvementstepson an episode-by-episodebasis, and
canbe incrementallyimplementedon an episode-by-episodebasis.
Maintaining sufficientexplorationis an issuein Monte Carlo control methods. It
is not enoughjust to selectthe actionscurrently estimatedto be best, becausethen
no returnswill be obtainedfor alternativeactions, and it may neverbe learnedthat
they are actually better. One approachis to ignore this problem by assumingthat
episodesbegin with state- action pairs randomly selectedto cover all possibilities.
Such exploring starts can sometimesbe arrangedin applicationswith simulated
, but are unlikely in learning from real experience
es can be used. In on policy methods, the agentcommitsto always
to find the bestpolicy that still explores. In off-policy methods,the
agentalso explores, but learnsa deternlinisticoptimal policy that may be unrelated
to the policy followed. More instancesof both kinds of methodsare presentedin the
Monte Carlo methodsfor reinforcementlearning havebeenexplicitly identified
only recently. Their convergencepropertiesare not yet clear, andtheir effectiveness
in practicehasbeenlittle tested. At present, their primary significanceis their simplicity
Monte Carlo methodsdiffer from DP methodsin two ways. First, they operate
, and thus can be used for direct learning without a model.
5.9 Bibliographical and Historical Remarks
Second,theydo not bootstrap.That is, they do not updatetheir valueestimateson the
. Thesetwo differencesarenot tightly linked andcanbe
. In the next chapterwe considermethodsthat learn from experience
Monte Carlo methods, but alsobootstrap, like DP methods.
5.9 Bibliographical and Historical Remarks
The tenn " Monte Carlo" datesfrom the 1940s, whenphysicistsat Los Alamosdevisedgames
of chancethat theycould studyto help understandcomplexphysicalphenomenarelatingto the
atombomb. Coverageof Monte Carlo methodsin this sensecanbe found in severaltextbooks
(e.g., Kalos and Whitlock, 1986; Rubinstein, 1981).
An early useof Monte Carlo methodsto estimateactionvaluesin a reinforcementlearning
context was by Michie and Chambers( 1968). In pole balancing(Example 3.4), they used
averagesof episodedurationsto assessthe worth (expectedbalancing life ) of eachpossible
actionin eachstate, andthenusedtheseassessments
is similar in spirit to MonteCarlo ES. In our tenns, theyuseda fonn of every-visit MC method.
NarendraandWheeler( 1986) studieda Monte Carlo methodfor ergodicfinite Markov chains
that usedthe return accumulatedfrom one visit to a stateto the next as a rewardfor adjusting
a learningautomaton's actionprobabilities.
Barto and Duff ( 1994) discussedpolicy evaluationin the contextof classicalMonte Carlo
algorithmsfor solving systemsof linear equations.They usedthe analysisof Curtiss( 1954) to
point out the computationaladvantagesof Monte Carlo policy evaluationfor largeproblems.
Singh and Sutton ( 1996) distinguishedbetweenevery-visit and first visit MC methodsand
provedresultsrelatingthesemethodsto reinforcementlearningalgorithms.
The blackjackexampleis basedon an exampleusedby Widrow, Gupta, andMaitra ( 1973).
The soapbubble exampleis a classicalDirichlet problem whoseMonte Carlo solution was
first proposedby Kakutani ( 1945; seeHershand Griego, 1969; Doyle and Snell, 1984) . The
racetrackexerciseis adaptedfrom Barto, Bradtke, andSingh( 1995), andfrom Gardner( 1973).
If onehadto identify oneideaascenttalandnovelto reinforcementlearning, it would
undoubtedlybe temporal-difference( TO) learning. TD learning is a combination
of Monte Carlo ideas and dynamic programming(OP) ideas. Like Monte Carlo
methods, TD methodscan learn directly from raw experiencewithout a model of
the environment's dynamics. Like OP, TD methodsupdateestimatesbasedin part
, without waiting for a final outcome(they bootstrap). The
relationshipbetweenTD , OP, and Monte Carlo methodsis a recurringthemein the
theory of reinforcementlearning. This chapteris the beginningof our exploration
of it. Before we are done, we will seethat theseideasand methodsblend into each
other and can be combinedin many ways. In particular, in Chapter7 we inttoduce
the TD ()") algorithm, which seamlesslyintegratesTD andMonte Carlo methods.
As usual, we startby focusingon the policy evaluationor prediction problem, that
of estimatingthe value function V7rfor a given policy 1C
(finding an optimal policy), OP, TD , andMonte Carlo
of generalizedpolicy iteration (GPI). The differencesin the methodsare primarily
their estimateV (s,) basedon what happensafterthat visit. Roughlyspeaking,Monte
Carlo methodswait until the return following the visit is known, then use that
return as a target for V (s,) . A simple every-visit Monte Carlo methodsuitablefor
whereRt is the actualreturnfollowing time t anda is a constantstep- sizeparameter
(cf., Equation2. 5). Let us call this methodconstant-a MC. WhereasMonte Carlo
methodsmust wait until the end of the episodeto determinethe incrementto V (St)
(only thenis Rt known), TD methodsneedwait only until the next time step. At time
t + 1 they immediatelyform a targetand makea useful updateusing the observed
rewardrt+1andthe estimateV (St+ I) . The simplestTD method, known as TD(O), is
V (St) ~ V (St) + a [ rt+1+ yV (St+ l ) - V (St)] .
In effect, the targetfor the Monte Carlo updateis Rt, whereasthe targetfor the TD
Becausethe TD methodbasesits updatein part on an existing estimate, we say
that it is a bootstrapping method, like DP. We know from Chapter3 that
Roughlyspeaking,Monte Carlo methodsuseanestimateof (6.3) asa target, whereas
DP methodsuseanestimateof (6.4) asa target. The MonteCarlo targetis anestimate
becausethe expectedvaluein (6.3) is not known; a samplereturn is usedin placeof
the real expectedreturn. The DP target is an estimatenot becauseof the expected
value, which is assumedto be completelyprovidedby a model of the environment,
but becauseV7r(S'+I) is not knownandthe currentestimate, V,(S' + I), is usedinstead.
The TO targetis an estimatefor both reasons:it samplesthe expectedvaluein (6.4)
and it usesthecurrentestimateV, insteadof the true V7r. Thus, TO methodscombine
the samplingof Monte Carlo with the bootstrapping of DP. As we shall see, with care
andimaginationthis cantakeus a long way towardobtainingthe advantages
Figure 6.1 specifiesTO(O) completelyin proceduralform, and Figure 6.2 shows
its backupdiagram. The valueestimatefor the statenodeat the top of the backupdiagram
is updatedon the basisof the onesampletransitionfrom it to the immediately
following state. We refer to TO and Monte Carlo updates as sample backups because
they involve looking ahead to a sample successor state (or state- action pair ), using
the value of the successor and the reward along the way to compute a backed-up
value, and then changing the value of the original state (or state- action pair) accordingly
. Sample backups differ from the full backups of OP methods in that they are
based on a single sample successorrather than on a complete distribution of all possible
Example 6.1: Driving Home Eachday as you drive homefrom work, you try to
predict how long it will take to get home. When you leaveyour office, you note the
time, the day of week, and anythingelsethat might be relevant. Say on this Friday
you are leaving at exactly 6 o clock, and you estimatethat it will take 30 minutes
to get home. As you reachyour car it is 6:05, and you notice it is startingto rain.
Traffic is often slowerin the rain, so you reestimatethat it will take 35 minutesfrom
then, or a total of 40 minutes. Fifteenminuteslater you havecompletedthe highway
portion of your journey in good time. As you exit onto a secondaryroad you cut
your estimateof total travel time to 35 minutes. Unfortunately, at this point you get
stuck behind a slow truck, and the road is too narrow to pass. You end up having
to follow the truck until you turn onto the side streetwhereyou live at 6:40. Three
minutes later you are home. The sequence of states, times , and predictions is thus as
The rewardsin this exampleare the elapsedtimes on eachleg of the journey.} We
arenot discounting(y = 1), andthusthe returnfor eachstateis the actualtime to go
from that state. The valueof eachstateis the expectedtime to go. The secondcolumn
of numbersgivesthe currentestimatedvaluefor eachstateencountered
A simpleway to view theoperationof MonteCarlomethodsis to plot thepredicted
total time (the last column) over the sequence
Theseare exactly the errors betweenthe estimatedvalue (predictedtime to go) in
eachstateandthe actualreturn(actualtime to go). For example, whenyou exitedthe
highway you thoughtit would take only 15 minutesmore to get home, but in fact it
took 23 minutes. Equation6.1 appliesat this point and detenninesan incrementin
the estimateof time to go after exiting the highway. The error, R, - V,(s,), at this
time is eight minutes. Supposethe step-sizeparameter
time to go after exiting the highway would be revisedupwardby four minutesas a
. This is probablytoo large a changein this case; the truck
wasprobablyjust an unlucky break. In any event, the changecan only be madeoffline, that is, after you havereachedhome. Only at this point do you know any of the
Is it necessaryto wait until the final outcomeis known beforelearningcanbegin?
Supposeon anotherday you again estimatewhen leaving your office that it will
take 30 minutesto drive home, but then you becomestuck in a massivetraffic jam.
lWenty-five minutesafter leaving the office you are still bumper-to- bumperon the
highway. You now estimatethat it will take another25 minutesto get home, for a
1. If this werea control problemwith the objectiveof minimizing travel time, then we would
of coursemakethe rewardsthe negativeof the elapsedtime. But sincewe areconcernedhere
only with prediction(policy evaluation), we cankeepthingssimpleby usingpositivenumbers.
Figure6.3 Changesrecommendedby Monte Carlo methodsin the driving homeexample.
recommendedby m medlodsin die driving homeexample .
total of 50 minutes. As you wait in traffic, you alreadyknow that your initial estimate
of 30 minuteswastoo optimistic. Must you wait until you get homebeforeincreasing
your estimatefor the initial state? Accordingto the Monte Carlo approachyou must,
becauseyou don' t yet know the true return.
According to a TO approach, on the other hand, you would learn immediately,
shifting your initial estimatefrom 30 minutestoward50. In fact, eachestimatewould
be shiftedtowardthe estimatethat immediatelyfollows it. Returningto our first day
of driving, Figure 6.4 showsthe samepredictionsas Figure 6.3, except with the
changesrecommendedby the TO rule (6.2) (theseare the changesmadeby the rule
if a = 1). Eacherror is proportionalto the changeover time of the prediction, that is,
Besidesgiving you somethingto do while waiting in traffic, thereareseveralcomto learnbasedon your currentpredictions
rather than waiting until terminationwhen you know the actual return. We briefly
TD methodslearntheir estimatesin part on the basisof other estimates
a guessfrom a guess they bootstrap. Is this a good thing to do? What advantages
do TD methodshaveover Monte Carlo andDP methods? Developingandanswering
such questionswill take the rest of this book and more. In this sectionwe briefly
Obviously, TO methodshave an advantageover DP methodsin that they do
not require a model of the environment, of its reward and next-state probability
The next most obvious advantageof TO methodsover Monte Carlo methodsis
that they are naturally implementedin an on-line, fully incrementalfashion. With
Monte Carlo methodsone must wait until the end of an episode, becauseonly then
is the return known, whereaswith TO methodsone needwait only one time step.
Surprisinglyoftenthis turnsout to be a critical consideration
Other applicationsare continuing tasksand haveno episodesat all. Finally, as we
notedin the previouschapter, someMonte Carlo methodsmust ignore or discount
episodeson which experimentalactionsare taken, which can greatly slow learning.
TO methodsare much less susceptibleto theseproblemsbecausethey learn from
eachtransitionregardlessof what subsequentactionsaretaken.
But areTO methodssound? Certainly it is convenientto learnoneguessfrom the
next, without waiting for an actualoutcome, but can we still guaranteeconvergence
to the correct answer? Happily, the answeris yes. For any fixed policy 11
algorithm describedabove has been proved to convergeto V1C
a constantstep-size parameterif it is sufficiently small, and with probability 1 if
the step- size parameterdecreasesaccordingto the usual stochasticapproximation
conditions(2.8). Most convergenceproofs apply only to the table-basedcaseof the
algorithm presentedabove(6.2), but somealso apply to the caseof generallinear
function approximation. Theseresultsare discussedin a more generalsettingin the
If both ill and Monte Carlo methodsconvergeasymptotically to the correct
predictions, then a naturalnext questionis Which getstherefirst? In other words,
which method learnsfaster? Which makesthe more efficient use of limited data?
At the current time this is an open questionin the sensethat no one hasbeenable
to prove mathematicallythat one methodconvergesfasterthan the other. In fact, it
is not evenclear what is the most appropriateformal way to phrasethis question!
In practice, however, ill methodshaveusually beenfound to convergefasterthan
constant-a MC methodson stochastictasks, asillustratedin the following example.
Example 6.2: Random Walk In this examplewe empirically comparethe prediction
abilities of ill (O) and constant-a MC appliedto the small Markov process
shownin Figure 6.5. All episodesstart in the centerstate, C, andproceedeither left
or right by one stateon eachstep, with equalprobability. This behavioris presum'
ably dueto thecombinedeffectof a fixed policy andanenvironments statetransition
probabilities, but we do not care which; we are concernedonly with predicting
extremeright. When an episodetenninateson the right a rewardof + 1 occurs; all
other rewardsare zero. For example, a typical walk might consistof the following
: C, 0, B, 0, C, 0, D, 0, E, 1. Becausethis task is undiscounted
and episodic, the true value of each stateis the probability of tenninating
on the right if starting from that state. Thus, the true value of the center state is
V7r(C) = 0.5. The true valuesof all the states, A through E, are ~, g, ! ' ~, and i .
Figure 6.6 showsthe valueslearnedby ill (O) approachingthe true valuesas
the averageerror in the predictionsfound by ill (O) and constant-a MC , for a variety
of valuesof a , asa function of numberof episodes
Exercise 6.1 This is an exerciseto help develop your intuition about why ill
methodsare often more efficient than Monte Carlo methods. Considerthe driving
home exampleand how it is addressedby ill and Monte Carlo methods. Can you
Figure 6.6 Valueslearnedby m (O) after variousnumbersof episodesin the randomwalk
example. The final estimateis aboutascloseasthe estimateseverget to the true values. With
a constantstep- size parameter(a = 0.1 in this example), the valuesfluctuateindefinitely in
responseto the outcomesof the mostrecentepisodes
imaginea scenarioin which a TD updatewould be betteron averagethan an Monte
Carlo update? Give an examplescenario- a descriptionof past experienceand a
currentstate - in which you would expectthe TD updateto be better. Here s a hint:
Supposeyou have lots of experiencedriving home from work. Then you move to
a new building and a new parking lot (but you still enter the highway at the same
place). Now you are startingto learn predictionsfor the new building. Can you see
why TD updatesarelikely to be muchbetter, at leastinitially , in this case? Might the
samesort of thing happenin the original task?
Exercise 6.2 From Figure 6.6, it appearsthat the first episoderesultsin a change
in only V (A ) . What does this tell you about what happenedon the first episode?
Why wasonly the estimatefor this one statechanged? By exactlyhow much was it
Exercise6.3 Do you think that by choosingthe step- sizeparameter
but still leaving it a constant, either algorithm could
Exercise6.4 In Figure6.7, the RMS error of the TD methodseemsto go downand
then up again, particularly at high a ' s. What could havecausedthis? Do you think
this alwaysoccurs, or might it be a function of how the approximatevaluefunction
Figure6.7 Learningcurvesfor m (O) andconstant
on thepredictionproblemfor therandomwalk. Theperformance
) errorbetweenthevaluefunctionlearnedandthetruevaluefunction,
Exercise 6.5 Above we statedthat the true valuesfor the randomwalk task are
A, g, ~, i , and ~, for statesA through E. Describeat leasttwo different ways that
thesecould havebeencomputed. Which would you guesswe actuallyused? Why?
Supposethere is availableonly a finite amountof experience
is to presentthe experiencerepeatedlyuntil the methodconvergesupon an answer.
Given an approximatevalue function, V , the incrementsspecifiedby (6.1) or (6.2)
are computedfor every time step t at which a nonterminalstateis visited, but the
value function is changedonly once, by the sum of all the increments.Then all the
availableexperienceis processedagainwith the newvaluefunction to producea new
overall increment, and so on, until the valuefunction converges
Under batch updating, TD(O) convergesdeterministicallyto a single answerindependent
, a , as long as a is chosento be sufficiently
small. The constant-a MC methodalso convergesdeterministicallyunderthe same
conditions, but to a different answer. Understandingthesetwo answerswill help
us understandthe differencebetweenthe two methods. Under normal updatingthe
methodsdo not moveaUthe way to their respectivebatchanswers
they take stepsin thesedirections. Before trying to understandthe two answersin
general, for all possibletasks, we first look at a few examples.
Example 6.3: Random Walk under Batch Updating Batch-updatingversionsof
TO(O) and constant-a MC were applied as follows to the randomwalk prediction
example(Example6.2). After eachnewepisode,all episodesseenso far weretreated
asa batch. Theywererepeatedlypresentedto thealgorithm, eitherTO(O) or constanta MC , with a sufficiently small that the value function converged
value function was then comparedwith V7r, and the averageroot mean-squared
error acrossthe five states(and across 100 independentrepetitionsof the whole
experiment) wasplottedto obtainthe learningcurvesshownin Figure6.8. Note that
the batchTO methodwasconsistentlybetterthanthe batchMonte Carlo method. 8
Under batch training, constant-a MC convergesto values, V (s), that are sample
averagesof the actual returns experiencedafter visiting each state s. These are
optimal estimatesin the sensethat they minimize the mean-squarederror from the
actualreturnsin thetraining set. In this senseit is surprisingthatthebatchTO method
wasableto performbetteraccordingto the root mean-squarederror measureshown
in Figure 6.8. How is it that batch TD was able to perfonn better than this optimal
method? The answer is that the Monte Carlo method is optimal only in a limited
way, and that TD is optimal in a way that is more relevant to predicting returns. But
first let ' s develop our intuitions about different kinds of optimality through another
Place yourself now in the role of the predictor
of returns for an unknown Markov reward process. Suppose you observe the
This means that the first episode started in state A , transitioned to 8 with a reward
of 0 , and then ternrinated from 8 with a reward of O. The other seven episodes were
even shorter, starting from 8 and ternrinating immediately . Given this batch of data,
what would you say are the optimal predictions , the best values for the estimates
V (A ) and V ( 8 ) ? Everyone would probably agree that the optimal value for V ( 8 ) is
i , because six out of the eight times in state 8 the process ternrinated immediately
with a return of 1, and the other two times in 8 the process ternrinated immediately
But what is the optimal value for the estimate V (A ) given this data? Here there
are two reasonable answers. One is to observe that 100% of the times the process
was in state A it traversed immediately to 8 ( with a reward of 0 ); and since we have
already decided that 8 has value i , therefore A must have value i as well . One way
of viewing this answer is that it is based on first modeling the Markov process, in this
andthen computingthe correctestimatesgiven the model, which indeedin this case
gives V (A ) = i . This is alsothe answerthat batchTD(O) gives.
The otherreasonableansweris simply to observethat we haveseenA onceandthe
returnthat followed it was0; we thereforeestimateV (A ) asO. This is the answerthat
batchMonteCarlo methodsgive. Noticethat it is alsothe answerthatgivesminimum
squarederror on the training data. In fact, it giveszeroerror on the data. But still we
expectthe first answerto be better. If the processis Markov, we expectthat the first
answerwill producelower error onfuture data, eventhoughthe Monte Carlo answer
The aboveexampleillustratesa generaldifferencebetweenthe estimatesfound
by batch TD(O) and batch Monte Carlo methods. Batch Monte Carlo methods
always find the estimatesthat minimize mean-squarederror on the training set,
whereasbatch TD (O) always finds the estimatesthat would be exactly correct for
the maximum-likelihood model of the Markov process. In general, the maximumlikelihood estimate of a parameteris the parametervalue whose probability of
generatingthe datais greatest.In this case, the maximum-likelihood estimateis the
modelof the Markov processformedin the obviousway from the observedepisodes
the estimatedtransitionprobability from i to j is the fraction of observedtransitions
from i that went to j , and the associatedexpectedrewardis the averageof the rewards
observedon thosetransitions. Given this model, we cancomputethe estimate
of the valuefunction that would be exactlycorrectif the modelwereexactlycorrect.
This is called the certainty-equivalenceestimatebecauseit is equivalentto ass~ming that the estimateof the underlyingprocesswasknown with certaintyratherthan
. In general, batchTD(O) convergesto the certainty-equivalence
This helps explain why TD methodsconvergemore quickly than Monte Carlo
methods. In batchform, TD(O) is fasterthan Monte Carlo methodsbecauseit computes
the true certainty-equivalenceestimate. This explainsthe advantageof TD(O)
shownin the batchresultson the randomwalk task (Figure6.8). The relationshipto
the certainty-equivalenceestimatemay also explain in part the speedadvantageof
nonbatchTD (O) (e.g., Figure 6.7). Although the nonbatchmethodsdo not achieve
either the certainty-equivalenceor the minimum squared
be understoodas moving roughly in thesedirections. NonbatchTD(O) may be faster
than constant-a MC becauseit is moving towarda betterestimate, eventhoughit is
not getting all the way there. At the current time nothing more definite can be said
aboutthe relativeefficiencyof on-line TD andMonte Carlo methods.
Finally, it is worth noting that althoughthe certainty-equivalenceestimateis in
somesensean optimal solution, it is almostneverfeasibleto computeit directly. If
N is the number of states, then just forming the maximum - likelihood estimate of the
process may require N2 memory , and computing the corresponding value function
requires on the order of N3 computational steps if done conventionally . In these
terms it is indeed sbiking that TD methods can approximate the same solution using
memory no more than N and repeated computations over the ttaining set. On tasks
with large state spaces, TD methods may be the only feasible way of approximating
We turn now to the useof TD predictionmethodsfor the control problem. As usual,
we follow the patternof generalizedpolicy iteration (GPI), only this time usingTD
methodsfor the evaluationor predictionpart. As with Monte Carlo methods,we face
the needto tradeoff explorationandexploitation, andagainapproach
: on-policy and off -policy. In this sectionwe presentan on-policy TD
The first stepis to learnan action-valuefunctionratherthana state-valuefunction.
In particular, for an on-policy method we must estimate QIr(S, a) for the current
behaviorpolicy 7randfor all statessandactionsa. This canbe doneusingessentially
the sameTD methoddescribedabovefor learningVIrRecall that anepisodeconsists
of an alternatingsequenceof statesandstate- actionpairs:
In the previoussectionwe consideredttansitionsfrom stateto stateand learnedthe
valuesof states. Now we considerttansitionsfrom state-action pair to state-action
pair, and learn the value of state-action pairs. Formally thesecasesare identical:
they are both Markov chains with a reward process. The theoremsassuringthe
convergenceof statevaluesunderTD (O) also apply to the correspondingalgorithm
Q(St, at) ~ Q(St, at) + a [ rt+l + yQ (St+ l , at+l ) - Q(St, at)] .
This updateis done after every ttansition from a nontenninal state St. If St+ l is
tenninal, then Q(St+ l , at+ l ) is definedas zero. This rule useseveryelementof the
Figure 6.9 Sarsa: An on-policy m control algorithm.
quintupleof events, (St, at, rt +l , St+ l , at+I), that makeup a transitionfrom onestateactionpair to the next. This quintuplegivesrise to the nameSarsafor the algorithm.
It is straightforwardto designan on-policy control algorithm basedon the Sarsa
predictionmethod. As in all on-policy methods, we continuallyestimateQ7rfor the
behaviorpolicy 1l', andat the sametime change1l' towardgreedinesswith respectto
Q7r. The generalform of the Sarsacontrol algorithmis given in Figure6.9.
The convergencepropertiesof the Sarsaalgorithm dependon the natureof the
policy s dependenceon Q. For example, one could use€-greedyor €-soft policies.
Sarsaconvergeswith probability 1 to an optimal policy andaction-valuefunction as
long as all state- action pairs are visited an infinite numberof times and the policy
convergesin the limit to the greedypolicy (which canbe arranged,for example, with
Example 6.5: Windy Gridworld Figure 6.10 showsa standardgridworld, with
startandgoal states,but with onedifference: thereis a crosswindupwardthroughthe
middle of the grid. The actionsarethe standardfour- up, down, right , and left but in the middle regionthe resultantnext statesare shiftedupwardby a " wind," the
strengthof which variesfrom column to column. The strengthof the wind is given
below eachcolumn, in numberof cells shifted upward. For example, if you are one
cell to the right of the goal, then the action left takesyou to the cell just above
the goal. Let us treat this as an undiscountedepisodictask, with constantrewards
of - 1 until the goal stateis reached. Figure 6.11 showsthe result of applying €greedySarsato this task, with € = 0.1, a = 0.1, and the initial values Q(s, a) = 0
for all s, a. The increasingslopeof the graph showsthat the goal is reachedmore
and more quickly over time. By 8000 time steps, the greedypolicy (showninset)
Figure 6.10 Gridworld in which movementis altered by a location- dependent
-e6.11 Resultsof Sarsaappliedto the windy gridworld.
was long since optimal; continuedE"-greedyexplorationkept the averageepisode
length at about 17 steps, two lessthan the minimum of 15. Note that Monte Carlo
methodscannoteasilybe usedon this taskbecausetenninationis not guaranteedfor
all policies. If a policy waseverfound that causedthe agentto stayin the samestate,
thenthe next episodewould neverend. Step-by-steplearningmethodssuchas Sarsa
do not have this problem becausethey quickly learn during the episodethat such
policies arepoor, and switch to somethingelse.
Exercise 6.6: Windy Gridworld with King 's Moves (programming) Resolvethe
windy gridworld taskassumingeight possibleactions, including the diagonalmoves,
ratherthan the usualfour. How much bettercan you do with the extra actions? Can
you do evenbetterby including a ninth action that causesno movementat all other
Exercise 6.7: StochasticWind (programming) Resolvethe windy gridworld task
with King' s moves, assumingthat the effect of the wind, if thereis any, is stochastic,
sometimesvaryingby 1 from the meanvaluesgivenfor eachcolumn. That is, a third
of the time you moveexactly accordingto thesevalues, as in the previousexercise,
but also a third of the time you move one cell abovethat, and anotherthird of the
time you move one cell below that. For example, if you are one cell to the right of
the goal andyou moveleft , thenone-third of the time you moveonecell abovethe
goal, one-third of the time you movetwo cells abovethe goal, and one third of the
Exercise6.8 What is the backupdiagramfor Sarsa?
One of the most important breakthroughs in reinforcement learning was the development
of an off - policy TO control algorithm known as Q-leaming ( Watkins, 1989) .
Its simplest form , one-step Q-leaming , is defined by
Q (St, at) -+- Q(St, at ) + a " + 1 + y rn: x Q(St+ l , a )
In this case, the learnedaction-valuefunction, Q, directly approximatesQ , the optimal
action-valuefunction, independentof the policy being followed. This dramatically
simplifiesthe analysisof the algorithm and enabledearly convergenceproofs.
The policy still hasan effect in that it determineswhich state- actionpairsarevisited
and updated. However, all that is requiredfor correct convergenceis that all pairs
continueto be updated. As we observedin Chapter5, this is a minimal requirement
in the sensethat any methodguaranteedto find optimal behaviorin the generalcase
must requireit. Under this assumptionand a variantof the usualstochasticapproximation
conditionson the sequenceof step-size parameters
What is the backupdiagramfor Q-Iearning? The rule (6.6) updatesa state-action
pair, so the top node, the root of the backup, must be a small, filled action node.
The backup is alsofrom action nodes, maximizing over all those actions possible
in the next state. Thus the bottom nodesof the backupdiagram should be all
theseactionnodes. Finally, rememberthat we indicatetaking the maximumof these
Figure6.12 Q-leaming: An off-policy m control algorithm
the diagramis? If so, pleasedo makea guessbefore turning to the answerin Figure
Example 6.6: Qift' Walking This gridworld example comparesSalSa and Qlearning, highlighting the differencebetweenon-policy (Sarsa) and off -policy (Qlearning) methods. Considerthe gridworld shownin the upperpart of Figure 6.14.
, episodictask, with start and goal states, and the
usualactionscausingmovementup, down, right, and left. Rewardis - Ion all transitions
exceptthose into the region marked The Cliff . Steppinginto this region
incurs a rewardof 100 and sendsthe agentinstantly back to the start. The lower
part of the figure showsthe perfonnanceof the SalSaand Q-learningmethodswith
E"-greedyactionselection, E" = 0.1. After an initial transient, Q-learninglearnsvalues
for the optimal policy, that which travelsright along the edgeof the cliff . Unfortunately
, this resultsin its occasionallyfalling off the cliff becauseof the E"-greedy
action selection. SalSa, on the other hand, takesthe action selectioninto account
and learnsthe longer but saferpath throughthe upperpart of the grid. Although Qlearningactually learnsthe valuesof the optimal policy, its on-line perfonnanceis
worsethan that of SalSa, which learnsthe roundaboutpolicy. Of course, if E" were
graduallyreduced, thenboth methodswould asymptoticallyconvergeto the optimal
Exercire 6.9 Why is Q-learningconsideredan off-policy control method?
Exercire 6.10 Considerthe learning algorithm that is just like Q-learningexcept
that insteadof the maximumover next state-action pairs it usesthe expectedvalue,
taking into account how likely each action is under the current policy. That is,
considerthe algorithmotherwiselike Q-learningexceptwith the updaterule
Figure 6.13 The backupdiagramfor Q-leaming.
Figure 6.14 The cliff -walking task. The resultsare from a singlenm, but smoothed.
Q(s" a,) ~ Q(s" a,) + a [r ,+l + yE { Q(S,+l, a,+l) Is, } - Q(s" a,)]
Is this new method an on-policy or off -policy method? What is the backup diagram
for this algorithm ? Given the same amount of experience, would you expect this
method to work better or worse than Sarsa? What other considerations might impact
the comparison of this method with Sarsa?
Actor- critic methodsare TD methodsthat have a separatememory structureto
explicitly representthepolicy independentof the valuefunction. The policy structure
is known as the actor, becauseit is usedto selectactions, and the estimatedvalue
function is known as the critic , becauseit criticizes the actionsmadeby the actor.
Learningis alwayson-policy: the critic mustlearnaboutandcritique whateverpolicy
is currently being followed by the actor. The critique takesthe form of a TD error.
This scalarsignal is the soleoutput of the critic and drivesallleaming in both actor
Figure 6.15 The actor- critic architecture .
Actor- critic methodsare the naturalextensionof the idea of reinforcementcomparison
methods(Section2.8) to TO learningandto the full reinforcementlearning
problem. Typically, thecritic is a state-valuefunction. After eachactionselection, the
critic evaluatesthe new stateto determinewhetherthings havegonebetteror worse
thanexpected.That evaluationis the TO error:
where V is the currentvalue function implementedby the critic. This TO error can
be usedto evaluatethe actionjust selected
, the action at takenin stateSt. If the TO
error is positive, it suggeststhat the tendencyto selectat shouldbe strengthenedfor
the future, whereasif the TO error is negative, it suggeststhe tendencyshouldbe
. Supposeactionsaregeneratedby the Gibbs sofunaxmethod:
wherethe p (s, a ) are the valuesat time t of the modifiablepolicyparametersof the
actor, indicating the tendencyto select(preferencefor) eachaction a when in each
states. Then the strengtheningor weakeningdescribedabovecan be implemented
by increasingor decreasingp (St, at), for instance,by
where.8 is anotherpositivestep-sizeparameter
This is just one exampleof an actor- critic method. Other variationsselectthe
actions in different ways, or use eligibility traceslike thosedescribedin the next
chapter. Another commondimensionof variation, as in reinforcementcomparison
methods, is to include additional factors varying the amountof credit assignedto
the action taken, at. For example, one of the most commonsuchfactorsis inversely
relatedto the probability of selectingat, resultingin the updaterule:
Theseissueswere exploredearly on, primarily for the immediaterewardcase(Sutton, 1984; Williams, 1992) andhavenot beenbroughtfully up to date.
Many of the earliestreinforcementlearningsystemsthat usedTD methodswere
actor- critic methods( Witten, 1977; Barto, Sutton, andAnderson, 1983). Sincethen,
more attentionhas beendevotedto methodsthat learn action-value functions and
detenninea policy exclusively from the estimatedvalues (such as Sarsaand Qlearning). This divergencemay be just historical accident. For example, one could
imagine intermediatearchitecturesin which both an action-value function and an
independent policy would be learned. In any event, actor- critic methods are likely to
remain of current interest becauseof two significant apparent advantages:
. They require minimal computation in order to select actions. Consider a case where
there are an infinite number of possible actions- for example, a continuous - valued
action. Any method learning just action values must search through this infinite
set in order to pick an action. If the policy is explicitly stored, then this extensive
computation may not be needed for each action selection.
. They can learn an explicitly stochastic policy ; that is , they can learn the optimal
probabilities of selecting various actions. This ability turns out to be useful in competitive
and non- Markov cases(e.g ., see Singh , Jaakkola, and Jordan, 1994) .
In addition , the separate actor in actor- critic methods makes them more appealing
in some respects as psychological and biological models. In some cases it may also
make it easier to impose domain - specific constraints on the set of allowed policies .
R-learningis an off -policy control methodfor the advancedversionof the reinforcement
learning problem in which one neither discountsnor divides experienceinto
distinct episodeswith finite returns. In this caseone seeksto obtain the maximum
rewardper time step. The value functionsfor a policy, 7r, are definedrelativeto the
averageexpectedrewardper time stepunderthe policy:
assuming the process is ergodic (nonzero probability of reaching any state from any
other under any policy ) and thus that pic does not depend on the starting state. From
any state, in the long run the average reward is the same, but there is a transient. From
some states better- than-average rewards are received for a while , and from others
worse-than- average rewards are received. It is this transient that defines the value of
and the value of a state- action pair is similarly the tran ~lent
when starting in that state and taking that action:
( , ) L E1C{rt +k - pic 1St= S, at = a } .
We call theserelative valuesbecausethey are relativeto the averagerewardunder
There are subtle distinctions that needto be drawn betweendifferent kinds of
optimality in the undiscountedcontinuing case. Nevertheless
purposesit may be adequatesimply to order policies accordingto their average
rewardper time step, in other words, accordingto their p1f. For now let us consider
all policies that attainthe maximalvalueof p1fto be optimal.
Other than its useof relative values, R-learningis a standardTD control method
basedon off -policy GPI, muchlike Q-learning. It maintainstwo policies, a behavior
policy and an estimation policy, plus an action-value function and an estimated
averagereward. The behaviorpolicy is usedto generateexperience
E greedypolicy with respectto the action value function.
the one involved in GPI. It is typically the greedypolicy with respectto the actionvalue function. If 7r is the estimationpolicy, then the action-value function, Q, is
an approximationof Q1fand the averagereward, p, is an approximationof p1f. The
completealgorithmis givenin Figure6.16. Therehasbeenlittle experiencewith this
methodand it shouldbe consideredexperimental.
Example 6.7: An Access-Control Queuing Task This is a decisiontaskinvolving
accesscontrol to a setof n servers. Customersof four different priorities arrive at a
single queue. If given accessto a server, the customerspay a rewardof 1, 2, 4, or
8, dependingon their priority, with higher priority customerspaying more. In each
Initialize p and Q(s, a ), for all s, a , arbitrarily
Chooseactiona in s usingbehaviorpolicy (e.g., f:-greedY)
Q(s, a) ~ Q(s, a) + a [ r - p + maxa, Q(s , a ) - Q(s, a) ]
p ~ p + fJ [ r - p + maxa' Q(s , a ) - maxaQ(s, a)]
Figure 6.16 R-learning: An off -policy m control algorithm for undiscounted
tasks. The scalarsa andfJ are step- sizeparameters
Figure 6.17 The policy and valuefunction found by R learningon the accessconb' Olqueuing
taskafter 2 million steps. The drop on the right of the graphis probablydueto insufficient
data; manyof thesestateswere neverexperienced
time step, the customerat the headof the queueis either accepted(assignedto one
of the servers) or rejected(removedfrom the queue). In either case, on the next
time step the next customerin the queueis considered
and the proportion of (randomly distributed) high priority customersin the queue
is h. Of coursea customercan be servedonly if there is a free server. Each busy
serverbecomesfree with probability p on eachtime step. Although we havejust
, let us assumethe statisticsof arrivalsanddepartures
are unknown. The task is to decide on each step whether to acceptor reject the
next customer, on the basis of his priority and the number of free servers, so as
to maximizelong-term rewardwithout discounting. Figure 6.17 showsthe solution
foundby R-learningfor this taskwith n = 10, h = 0.5, andp = 0.06. The R-learning
parameterswerea = 0.1, .8= 0.01, andE = 0.1. The initial actionvaluesandp were
* Exercise6.11 Designan on-policy methodfor undiscounted
6.8 Games, After states, and Other Special Cases
In this book we tty to presenta uniform approachto a wide classof tasks, but of
coursetherearealwaysexceptionaltasksthat arebettertreatedin a specializedway.
For example, our generalapproachinvolveslearning an action-value function, but
in ChapterI we presenteda TO methodfor learningto play tic-tac-toe that learned
somethingmuchmorelike a state-valuefunction. If we look closelyat that example,
it becomesapparentthat thefunction learnedthereis neitheran action-valuefunction
nor a state-value function in the usual sense. A conventionalstate-value function
evaluatesstatesin which the agenthasthe option of selectingan action, but the statevaluefunction usedin tic-tac-toe evaluatesboardpositionsafter the agenthasmade
its move. Let uscall theseafterstates, andvaluefunctionsoverthese, afterstatevalue
functions. Afterstatesare useful when we haveknowledgeof an initial part of the
environment's dynamicsbut not necessarilyof the full dynamics. For example, in
gameswe typically know the immediateeffects of our moves. We know for each
possiblechessmove what the resultingposition will be, but not how our opponent
will reply. Afterstatevaluefunctionsarea naturalway to takeadvantageof this kind
of knowledgeandtherebyproducea moreefficient learningmethod.
The reasonit is more efficient to design algorithms in terms of afterstatesis
apparentfrom the tic-tac-toe example. A conventionalaction-value function would
mapfrom positionsand movesto an estimateof the value. But manyposition- move
pairsproducethe sameresultingposition, asin this example:
In such casesthe position - move pairs are different but produce the same afterposi "
tion , and thus must have the same value. A conventional action -value function would
have to separately assessboth pairs , whereas an afterstate value function would im -
mediatelyassessboth equally. Any learningaboutthe position- movepair on the left
would immediatelyttansferto the pair on the right.
Afterstatesarisein manytasks, notjust games.For example, in queuingtasksthere
areactionssuchasassigningcustomersto servers,rejectingcustomers
information. In suchcasesthe actionsarein fact definedin termsof their immediate
effects, which are completely known. For example, in the access
be obtainedby breaking the environments dynamicsinto the immediateeffect of
the action, which is detenninisticand completelyknown, and the unknownrandom
es having to do with the arrival and departureof customers
would be the numberof free serversafter the actionbut beforethe randomprocess
hadproducedthe next conventionalstate. Learningan afterstatevaluefunction over
theafterstateswould enableall actionsthatproducedthe samenumberof free servers
. This shouldresultin a significantreductionin learningtime.
It is impossibleto describeall the possiblekinds of specializedproblemsand
correspondingspecializedlearning algorithms. However, the principles developed
in this book should apply widely. For example, afterstatemethodsare still aptly
describedin termsof generalizedpolicy iteration, with a policy and(afterstate) value
function interactingin essentiallythe sameway. In many casesone will still face
the choice betweenon-policy and off -policy methodsfor managingthe need for
Exercise6.12 Describehow the task of Jack's Car Rental( Example4.2) could be
reformulatedin termsof afterstates. Why, in termsof this specifictask, would such
a reformulationbe likely to speedconvergence
In this chapterwe introduceda new kind of learning method, temporal-difference
(TD ) learning, and shownhow it can be appliedto the reinforcementlearningproblem
. As usual, we dividedtheoverallprobleminto a predictionproblemanda control
problem. TD methodsare alternativesto Monte Carlo methodsfor solving the prediction
problem. In both cases, the extensionto the control problemis via the idea
of generalizedpolicy iteration(GPI) that we abstractedfrom dynamicprogramming.
This is the ideathat approximatepolicy andvaluefunctionsshouldinteractin sucha
way that they both movetowardtheir optimal values.
es making up GPI drives the value function to accurately
policy; this is thepredictionproblem. The otherprocess
drivesthe policy to improvelocally (e.g., to be f. -greedy) with respectto the current
value function. When the first processis basedon experience
an on policy or off policy approach. Sarsa
methods, andQ IearningandR leamingare off policy methods.
The methodspresentedin this chapterare today the most widely usedreinforcement
learning methods. This is probably due to their great simplicity: they can be
applied on-line, with a minimal amount of computation, to experiencegenerated
from interactionwith an environment; they can be expressednearly completelyby
singleequationsthat canbe implementedwith small computerprograms. In the next
few chapterswe extendthesealgorithms, making them slightly more complicated
and significantly more powerful. All the new algorithmswill retain the essenceof
thoseintroducedhere: they will be ableto processexperienceon-line, with relatively
little computation, and they will be driven by TD errors. The specialcasesof TD
methodsintroducedin the presentchaptershould rightly be called one-step, tabulal; modelfreeTD methods. In the next threechapterswe extendthem to multistep
forms (a link to Monte Carlo methods), forms using function approximationrather
than tables(a link to artificial neural networks), and forms that include a model of
the environment(a link to planninganddynamicprogramming).
Finally, in this chapterwe havediscussedTD methodsentirely within the context
of reinforcementlearningproblems, but TD methodsareactuallymoregeneralthan
this. They are generalmethodsfor learning to make long-term predictionsabout
dynamicalsystems.For example, TD methodsmayberelevantto predictingfinancial
data, life spans, electionoutcomes, weatherpatterns, animal behavior, demandson
as pure prediction methods, independentof their use in reinforcementlearning,
that their theoreticalpropertiesfirst came to be well understood
6.10 Bibliographical and Historical Remarks
As we outlined in Chapter 1, the idea of m learning has its early roots in animal learning
psychologyand' artificial intelligence, most notably the work of Samuel( 1959) and Klopf
( 1972). Samuels work is describedasa casestudyin Section11.2. Also relatedto m learning
are Holland' s ( 1975, 1976) early ideas about consistencyamong value predictions. These
influencedone of the authors(Barto), who was a graduatestudentfrom 1970 to 1975 at
the University of Michigan, whereHolland wasteaching. Holland' s ideasled to a numberof
m -relatedsystems, including the work of Booker ( 1982) and the bucketbrigadeof Holland
( 1986), which is relatedto Sarsaasdiscussedbelow.
6.1- 2 Most of the specific material from thesesectionsis from Sutton ( 1988), including
the m (o) algorithm, the random walk example, and the term "temporal-difference
learning. The characterizationof the relationshipto dynamicprogrammingandMonte
Carlo methodswasinfluencedby Watkins( 1989), Werbos( 1987), andothers. The use
of backupdiagramshereand in otherchaptersis new to this book. Example6.4 is due
to Sutton, but hasnot beenpublishedbefore.
Tabularm (O) wasprovedto convergein the meanby Sutton( 1988) andwith probability
I by Dayan ( 1992), basedon the work of Watkinsand Dayan ( 1992). These
results were extendedand strengthenedby Jaakkola
Tsitsiklis ( 1994) by using extensionsof the powerful existing theory of stochastic
approximation.Otherextensionsandgeneralizationsarecoveredin the next two chapters
The optimality of the m algorithm under batch training was establishedby Sutton
( 1988) . The term certainty equivalenceis from the adaptiveconttolliterature (e.g.,
Goodwin and Sin, 1984) . illuminating this result is Barnard's ( 1993) derivationof the
m algorithm as a combinationof one stepof an incrementalmethodfor learning a
model of the Markov chain and one stepof a methodfor computingpredictionsfrom
The Sarsaalgorithm wasfirst exploredby Rummeryand Niranjan ( 1994), who called
it modified Q-leaming. The name " Sarsa" was introduced by Sutton ( 1996). The
of one-steptabularSarsa(theform treatedin this chapter) hasbeenproved
Holland' s ( 1986) bucketbrigadeidea evolvedinto an algorithm closely relatedto
Sarsa.The original ideaof the bucketbrigadeinvolvedchainsof rules triggeringeach
other; it focusedon passingcredit back from the currentrule to the rulesthat triggered
it. Over time, the bucketbrigadecameto be more like m learningin passingcredit
back to any temporally precedingrule, not just to the onesthat triggeredthe current
rule. The modemform of the bucketbrigade, whensimplified in variousnaturalways,
is nearly identicalto one-stepSarsa, asdetailedby Wilson ( 1994).
Q-learningwas introducedby Watkins( 1989), whoseoutline of a convergenceproof
was later made rigorous by Watkins and Dayan ( 1992). More generalconvergence
resultswereprovedby Jaakkola, Jordan, and Singh( 1994) andTsitsiklis ( 1994).
Actor- critic architecturesusing m learningwere first studiedby Witten ( 1977) and
then by Barto, Sutton, and Anderson( 1983; Sutton, 1984), who introducedthis use
of the terms " actor" and " critic." Sutton ( 1984) and Williams ( 1992) developedthe
eligibility termsmentionedin this section. Barto ( I 995a) andHook, Adams, andBarto
R-leaming is due to Schwartz( 1993) . Mahadevan( 1996), Tadepalliand Ok ( 1994),
and 8ertsekasand Tsitsiklis ( 1996) have studied reinforcementlearning forundis countedcontinuing tasks. In the literature, the undiscountedcontinuingcaseis often
called the caseof maximizing " averagerewardper time step" or the " average
case." The nameR-leaming was probablymeantto be the alphabeticsuccessorto Qlearning, but we preferto think of it asa referenceto the learningof relativevalues. The
- control queuingexamplewas suggestedby the work of Carlsb'Omand Nordsb' Om( 1997) .
So far we have discussedthree classesof methodsfor solving the reinforcement
learning problem: dynamic programming, Monte Carlo methods, and temporaldifferencelearning. Although each is different, theseare not really alternativesin
the sensethat one must pick one or another. It is perfectly sensibleand often desirable
to apply methodsof severaldifferent kinds simultaneously
may want to emphasizeonekind of methodover another,
but thesechoicescanbe madesmoothlyandat the time the methodsareused, rather
than the time at which they are designed.In Partill we presenta unified view of the
threekinds of elementarysolutionmethodsintroducedin Partll .
The unificationswe presentin this part of the book are not rough analogies. We
developspecificalgorithmsthat embodythe key ideasof oneor moreof the elementary
solutionmethods. First we presentthe mechanismof eligibility traces, unifying
Monte Carlo and temporal-differencemethods. Then we bring in function approximation
, enabling generalizationacrossstatesand actions. Finally we reintroduce
models of the environmentto obtain the strengthsof dynamic programmingand
heuristicsearch. All of thesecanbe usedsynergisticallyaspartsof joint methods.
Eligibility tracesare one of the basic mechanismsof reinforcementlearning. For
example, in the popular TO()") algorithm, the ).. refers to the use of an eligibility
trace. Almost any temporal-difference(TO) method, such as Q-Iearningor Sarsa,
can be combinedwith eligibility tracesto obtain a more generalmethodthat may
There are two ways to view eligibility traces. The more theoreticalview, which
we emphasizehere, is that they are a bridge from TO to Monte Carlo methods.
When TO methodsare augmentedwith eligibility traces, they producea family of
methodsspanninga spectrumthat has Monte Carlo methodsat one end and onestep TO methodsat the other. In betweenare intermediatemethodsthat are often
betterthaneitherextrememethod. In this senseeligibility tracesunify TO andMonte
Carlo methodsin a valuableandrevealingway.
Theotherway to view eligibility tracesis moremechanistic.Fromthis perspective
an eligibility trace is a temporaryrecordof the occurrenceof an event, suchas the
visiting of a stateor the taking of an action. The tracemarksthe memoryparameters
associatedwith the eventas eligible for undergoinglearningchanges
error occurs, only the eligible statesor actions are assignedcredit or blame for
the error. Thus, eligibility traceshelp bridge the gap betweeneventsand training
For reasonsthat will becomeapparentshortly, the more theoreticalview of eligibility
tracesis called theforward view, and the more mechanisticview is called the
backwardview. The forward view is mostusefulfor understandingwhat is computed
by methodsusing eligibility traces, whereasthe backwardview is more appropriate
for developingintuition aboutthe algorithmsthemselves
both viewsandthenestablishthe sensesin which theyareequivalent,that is, in which
they describethe samealgorithmsfrom two points of view. As usual, we first consider
the predictionproblemand then the control problem. That is, we first consider
how eligibility tracesare usedto help in predictingreturnsas a function of statefor
a fixed policy (i.e., in estimatingV7r). Only after exploringthe two viewsof eligibility
traceswithin this prediction settingdo we extendthe ideasto action valuesand
What is the spaceof methodslying betweenMonte Carlo and TD methods? Consider
estimatingV1Cfrom sampleepisodesgeneratedusing1C
from that stateuntil the end of the episode. The backupof simple TD methods, on
the other hand, is basedon just the one next reward, using the valueof the stateone
steplater as a proxy for the remainingrewards. One kind of intermediatemethod,
then, would perform a backupbasedon an intermediatenumberof rewards: more
thanone, but lessthan all of themuntil termination. For example, a two- stepbackup
would be basedon the first two rewardsandthe estimatedvalueof the statetwo steps
later. Similarly, we could havethree-stepbackups, four-stepbackups, andsoon. Figure
7.1 diagramsthe specttumof n-stepbackupsfor V1C
backupson the left and up- until terminationMonte Carlo backupson the right.
The methodsthat use n-step backupsare still TD methodsbecausethey still
changean earlier estimatebasedon how it differs from a later estimate. Now the
later estimateis not one steplater, but n stepslater. Methodsin which the temporal
difference extendsover n stepsare called n-step TD methods. The TD methods
inttoducedin the previouschapteraUuseone-stepbackups, and henceforthwe call
More formally, considerthe backup applied to states, as a result of the staterewardsequences
" r '+I, S' + I, r ' +2, . . . , rT, ST(omitting the actionsfor simplicity).
We know that in Monte Carlo backupsthe estimateV,(s,) of V1C
- Rt = rt + l + yrt + 2 + Y 2rt +3 + . . . + Y T t l rT ,
where T is the last time step of the episode. Let us call this quantity the target of the
backup . Whereas in Monte Carlo backups the target is the complete return , in onestep backups the target is the first reward plus the discounted estimated value of the
Figure 7.1 The spectrumrangingfrom the one stepbackupsof simple m methodsto the
upbasedon n stepsof real rewardsandthe estimatedvalueof the ndl next state, all appropriately
This makessensebecauseyVt (St+ l ) takesthe placeof the remainingtenDS yrt +2 +
- tlrr , as we discussedin the previouschapter. Our point now
is that this idea makesjust as much senseafter two stepsas it doesafter one. The
(St+V takes the placeof the tenDS Y2r ,+3 + Y3r ' +4 + . . . +
R~n) = r ' + l + yr '+2 + y2r'+3 + . . . + yn lr ,+n + ynV,(s' +n) .
This quantity is sometimescalled the correctedn-steptruncatedreturn becauseit
is a returntruncatedafter n stepsandthenapproximatelycorrectedfor the truncation
by addingthe estimatedvalueof thennth next state. That terminologyis descriptive,
but a bit long. We insteadrefer to R~ ) simply asthe n-stepReturnat time t .
Of course, if the episodeendsin lessthann steps, then the truncationin an n-step
return occursat the episodes end, resultingin the conventionalcompletereturn. In
other words, if T - t ~ n, then R: ) = R: ) = R, . Thus, the last nn -stepreturns
of any episodeare always completereturns, and an infinite-step return is always
a completereturn. This definition enablesus to treat Monte Carlo methodsas the
specialcaseof infinite-stepreturns. All of this is consistentwith the tricks for treating
episodicand continuingtasksequivalentlythat we introducedin Section3.4. There
we choseto treatthetenninal stateasa statethat alwaystransitionsto itself with zero
reward. Underthis trick , all n-stepreturnsthat last up to or pasttenninationhavethe
An n-stepbackupis definedto be a backuptowardthe n-stepreturn. In the tabular,
state-valuecase, the incrementto V,(s,) (the estimatedvalueof V7r(s,) at time I ), due
wherea is a positive step-size parameteras usual. The incrementsto the estimated
valuesof theotherstatesareof course.6.V,(s) = 0, for all s # s, . Wedefinethen-step
backupin termsof an increment, ratherthan as a direct updaterule as we did in the
previouschapter, in orderto distinguishtwo different waysof makingthe updates.In
on-line updating, the updatesare doneduring the episode, as soonasthe increment
is computed. In this casewe haveV' + I(S) = V,(s) + .6. V,(s) for all sE 8. This is the
caseconsideredin the previouschapter. In off-line updating, on the other hand, the
incrementsareaccumulated" on the side" andarenot usedto changevalueestimates
until the end of the episode. In this case, V,(s) is constantwithin an episode, for all
s. If its value in this episodeis V (s) , then its new value in the next episodewill be
The expectedvalueof all n-stepreturnsis guaranteedto improvein a certainway
over the current value function as an approximationto the true value function. For
any V , the expectedvalue of the n-stepreturn using V is guaranteedto be a better
estimateof V7rthan V is, in a worst-statesense.That is, the worsterrorunderthe new
estimateis guaranteedto be lessthan or equalto yn timesthe worst error under V:
This is called the error reductionproperty of n-step returns. Becauseof the error
reductionproperty, one can show formally that on-line and off -line ill prediction
methodsusingn-stepbackupsconvergeto the correctpredictionsunderappropriate
technicalconditions. The n-stepill methodsthus form a family of valid methods,
with one-stepill methodsandMonte Carlo methodsasextrememembers.
Example 7.1: n-Step TO Methods on the Random Walk Consider using nstep ill methodson the random walk task describedin Example 6.2 and shown
in Figure 6.5. Supposethe first episodeprogresseddirectly from the center state,
C, to the right, throughD and E, and then terminatedon the right with a return of
1. Recall that the estimatedvaluesof all the statesstartedat an intermediatevalue,
Vo(s) = 0.5. As a resultof this experience
estimatefor the last state, V (E), which would be incrementedtoward 1, the observed
return. A two-stepmethod, on the otherhand, would incrementthe valuesof the two
statesprecedingtermination: V (D) and V (E) would both be incrementedtoward 1.
A three-stepmethod, or any n-stepmethodfor n > 2, would incrementthe valuesof
all three of the visited statestoward 1, all by the sameamount. Which n is better?
Figure 7.2 showsthe resultsof a simple empirical assessment
walk process, with 19 states(and with a - 1 outcomeon the left, all valuesinitial ized to 0). Shownis the root mean-squarederror in the predictionsat the end of an
episode, averagedover states, over the first 10 episodes
the whole experiment(the samesetsof walkswereusedfor all methods). Resultsare
shownfor on-line and off -line n-stepTO methodswith a rangeof valuesfor n and
a . Empirically, on-line methodswith an intermediatevalueof n seemto work best
on this task. This illustrateshow the generalizationof TO and Monte Carlo methods
to n-stepmethodscan potentially perform betterthan either of the two extreme
, n-stepTD methodsare rarely usedbecausethey are inconvenient
to implement. Computing n-step returns requires waiting n stepsto observethe
resultantrewardsandstates.For largen , this canbecomeproblematic, particularlyin
control applications. The significanceof n-stepTD methodsis primarily for theory
and for understandingrelatedmethodsthat are more convenientlyimplemented. In
the next few sectionswe use the idea of n-stepTD methodsto explain andjustify
Exercise 7.1 Why do you think a largerrandomwalk task ( 19 statesinsteadof 5)
was used in the examplesof this chapter? Would a smaller walk have shifted the
advantageto a different valueof n? How aboutthe changein left-sideoutcomefrom
0 to - I ? Would that havemadeany differencein the bestvalueof n?
Exercise 7.2 Why do you think on-line methodsworkedbetterthanoff -line methods
* Exercise 7.3 In the lower part of Figure 7.2, notice that the plot for n = 3 is
different from the others, dropping to low performanceat a much lower value of
a than similar methods. In fact, the samewasobservedfor n = 5, n = 7, andn = 9.
Can you explainwhy this might havebeenso? In fact, we arenot sureourselves.
Figure 7.2 Perfonnanceof n-stepmmed Iods as a function of a , for variousvaluesof n ,
on a 19-staterandomwalk task. The performancemeasureshownis dIe root mean-squared
( RMS) error betweendIe true valuesof statesand dIe valuesfound by dIe learningmedIods,
averagedover dIe 19 states, over dIe first 10trials, andover 100different sequences
Backupscan be done not just toward any n-stepreturn, but toward any averageof
n-step returns. For example, a backup can be done toward a return that is half of
a two-stepreturn and half of a four-stepreturn: Rfve = ! R: ) + ! R: ). Any set of
returnscan be averagedin this way, evenan infinite set, as long as the weightson
the componentreturns are positive and sum to 1. The overall return possess
error reductionproperty similar to that of individual n-step returns (7.2) and thus
canbe usedto constructbackupswith guaranteedconvergence
one step and infinite step backupsto obtain anotherway of interrelatingTD and
Monte Carlo methods. One could evenaverageexperience
backupsto get a simplecombinationof experience
A backupthat averagessimplercomponentbackupsin this way is calleda complex
backup. The backupdiagramfor a complexbackupconsistsof the backupdiagrams
for eachof the componentbackupswith a horizontalline abovethemandthe weighting
fractionsbelow. For example, the complexbackupmentionedabove, mixing half
of a two- stepbackupandhalf of a four-stepbackup, hasthe diagram:
The TD (A) algorithmcanbe understoodasoneparticularway of averagingn-step
backups. This averagecontainsall the n-stepbackups, eachweightedproportional
Figure7.3 Thebackupdigramfor m (A). If A = 0, thentheoverallbackupreduces
to An I , where0 ~ A ~ 1 (Figure 7.3). A nonnalizationfactor of 1 - A ensuresthat
the weights sum to 1. The resulting backupis toward a return, called the A-return,
Figure7.4 illustratesthis weightingsequence
weight, 1 )..; the two step return is given the next largestweight, ( 1 ).. )..; the
three-step return is given the weight ( 1 ).. ).. ; and so on. The weight fadesby )..
with eachadditionalstep. After a terminal statehasbeenreached,all subsequentnstepreturnsare equalto Rt. If we want, we can separatethesetermsfrom the main
This equationmakesit clearerwhat happenswhenA = 10In this casethe summation
term goesto zero, and the remainingterm reducesto the conventionalreturn, R, o
Thus, for A = 1, backingup accordingto the A-returnis the sameasthe Monte Carlo
Figure 7.4 Weightinggiven in the A return to eachof the n step
Figure 7.5 The forward or theoreticalview. We decidehow update
algorithmthatwe calledconstanta MC (6.1)l )in thepreviouschapter
hand,if A = 0, thentheA returnreduces : ,
WedefinetheA-returnalgorithmasthealgorithmthatperformsbackups
for otherstatesareof course6. V,(s) = 0, for all s # s,.) As withthe
, theupdatingcanbeeitheron-line or off-line.
thatwe havebeentakingsofar is whatwe call thetheoretical
to all thefuturerewardsanddecidehowbestto combinethem. Wemightimagine
, lookingforwardfromeachstateto determine
by Figure7.5. After lookingforwardfromandupdatingone
state, we moveon to the next andneverhaveto work with the precedingstateagain.
Future states, on the other hand, are viewed and processedrepeatedly
Example 7.2: The A-return Algorithm on the Random Walk Task Figure 7.6
showsthe perfonnanceof the off -line A-return algorithm on the 19-state random
walk task usedwith n-stepm methodsin Example7.1. The experimentwasjust
as in the n-stepcaseexceptthat herewe variedA insteadofn . Note that we get best
perfonnancewith an intermediatevalueof A.
The A-return algorithm is the basisfor the forward view of eligibility tracesas
usedin the m (A) algorithm. In fact, we showin a later sectionthat, in the off -line
case, the A-returnalgorithmis them (A) algorithm. The A-returnandm (A) methods
use the A parameterto shift from one-step m methodsto Monte Carlo methods.
The specificway this shift is done is interesting, but not obviously better or worse
than the way it is done with simple n-step methodsby varying n. illtimately , the
most compellingmotivationfor the A way of mixing n-stepbackupsis that thereis
a simplealgorithm- m (A)- for achievingit. This is a mechanismissueratherthan
a theoreticalone. In the next few sectionswe developthe mechanistic,or backward,
view of eligibility tracesas usedin m (A) .
Exercise 7.4 The parameterA characterizeshow fast the exponentialweightingin
Figure 7.4 falls off , and thus how far into the future the A-return algorithm looks in
detenniningits backup. But a rate factor suchasA is sometimesan awkwardway of
Figure 7.6 Performanceof the off -line A-return algorithmon a 19-staterandomwalk task.
characterizingthe speedof the decay. For somepurposesit is betterto specifya time
constant, or half-life. What is the equationrelatingA andthe half-life , 1')., the time by
which the weightingsequencewill havefallen to half of its initial value?
In the previoussectionwe presentedthe forward or theoreticalview of the tabular
TD (A) algorithm as a way of mixing backupsthat parametricallyshifts from a
TD method to a Monte Carlo method. In this section we instead define TD (A)
, and in the next section we show that this mechanismcorrectly
implements forwardview. The mechanistic,or backward, view of T D(A) is useful
becauseit is simple conceptuallyand computationally. In particular, the forward
view itself is not directly implementablebecauseit is acausal, using at each step
knowledgeof what will happenmany stepslater. The backwardview provides a
causal, incrementalmechanismfor approximatingthe forward view and, in the off line case, for achievingit exactly.
In the backwardview of T D(A), thereis an additionalmemoryvariableassociated
with eachstate, its eligibility trace. The eligibility tracefor states at time t is denoted
e, (s) em + . On each step, the eligibility ttacesfor all statesdecayby YA, and the
eligibility ttacefor the one statevisited on the stepis incrementedby 1:
for all se 8 , where Y is the discountrate and A is the parameterinb' oducedin the
previoussection. Henceforthwe refer to A as the trace-decayparameter. This kind
of eligibility traceis calledanaccumulatingttacebecauseit accumulateseachtime a
stateis visited, then fadesawaygraduallywhenthe stateis not visited, as illustrated
which eachstateis eligible for undergoinglearning changes
event occur. The reinforcing eventswe are concernedwith are the moment-bymomentone-stepm errors. For example, the m error for state-valuepredictionis
In the backwardview of m (A), the global m error signal triggers proportional
updatesto all recentlyvisited states, as signaledby their nonzerotraces:
As always, theseincrementscouldbe doneon eachstepto form an on-line algorithm,
or saveduntil the end of the episodeto producean off -line algorithm. In either
case, equations(7.5- 7.7) providethe mechanisticdefinition of the m (A) algorithm.
for on-line m (A) is given in Figure 7.7.
The backwardview ofm (A) is orientedbackwardin time. At eachmomentwe
look at the currentm error and assignit backwardto eachprior stateaccordingto
the state's eligibility traceat that time. We might imagineourselvesriding alongthe
streamof states, computingm errors, and shoutingthem back to the previously
visited states, as suggestedby Figure 7.8. Where the m error and traces come
together, we get the updategiven by (7.7).
To betterunderstandthe backwardview, considerwhat happensat variousvalues
of A. If A = 0, thenby (7.5) all tracesarezeroat t exceptfor thetracecorrespondingto
St. Thusthe m (A) update(7.7) reducesto the simplem rule (6.2), m (O). In terms
view. Eachupdatedependson thecurrentm error
of Figure 7.8, TD(O) is the casein which only the one stateprecedingthe current
one is changedby the TD error. For larger valuesof A, but still A < 1, more of the
precedingstatesarechanged,but eachmore temporallydistantstateis changedless
becauseits eligibility trace is smaller, as suggestedin the figure. We say that the
earlier statesaregiven lesscredit for the TD error.
If A = 1, then the credit given to earlier statesfalls only by y per step. This turns
out to be just the right thing to do to achieveMonte Carlo behavior. For example,
rememberthatthe TD error, ~t, includesanundiscountedtenDof rt+l . in passingthis
back k stepsit needsto be discounted, like any rewardin a return, by yk , which is
. If A = 1 and y = 1, thenthe eligibility
just what the falling eligibility traceachieves
tracesdo not decayat all with time. In this casethe methodbehaveslike a Monte
, episodic task. If A = I , the algorithm is also
TD( 1) is a way of implementingMonte Carlo algorithmsthat is moregeneralthan
thosepresentedearlier and that significantly increasestheir rangeof applicability.
Whereasthe earlier Monte Carlo methodswere limited to episodic tasks, TD ( 1)
can be applied to discountedcontinuing tasks as well. Moreover, TD( I ) can be
perfonnedincrementallyand on-line. One disadvantageof Monte Carlo methodsis
that they learnnothingfrom an episodeuntil it is over. For example, if a Monte Carlo
control methoddoessomethingthat producesa very poorrewardbut doesnot endthe
episode,thenthe agents tendencyto do thatwill be undiminishedduring the episode.
On-line TD ( l ), on the other hand, learnsin an n-stepTD way from the incomplete
ongoingepisode,wherethen stepsareall the way up to thecurrentstep. If something
unusuallygood or bad happensduring an episode, control methodsbasedon TD ( 1)
can learnimmediatelyand alter their behavioron that sameepisode.
In this section we show that off -line TD (A), as defined mechanisticallyabove,
achievesthe sameweight updatesas the off -line A-return algorithm. In this sense
we align the forward (theoretical) and backward(mechanistic) views of TD (A) . Let
A Vt (s,) denotethe updateat time t of V (s,) accordingto the A-return algorithm
(7.4), andlet AV,TD(s) denotethe updateat time t of states accordingto the mechanistic
definition of TD (A) as given by (7.7). Then our goal is to showthat the sum
of all the updatesover an episodeis the samefor the two algorithms:
wherelSStis an identity-indicatorfunction, equal to 1 if s = s, and equal to 0
First note that an accumulatingeligibility tracecan be written explicitly (nonrecursively) as
Now we turn to the right-handsideof (7.8). Consider
Equivalence of Forward and Backward Views
= - Vt(St) + ( 1 - ).,)}.,O[rt+l + yVt(St+l)]
+ ( 1 - ).,)}.,1[rt+l + yrt+2 + y2Vt(St+V]
+ ( 1 - }.,)}.,2[rt+l + yrt+2 + y2rt+3 + y3Vt(St+3)]
Examine the first column inside the brackets-all the rt+l ' s with their weighting
factors of 1 - A times powersof A. It turns out that all the weighting factors sum
to 1. Thus we can pull out the first column and get an unweightedterm of rt+l . A
similar trick pulls out the secondcolumn in brackets, startingfrom the secondrow,
which sumsto YArt +2. Repeatingthis for eachcolumn, we get
+ (y }..)O[rt+l + yVt(St+l) - y }..Vt(St+l)]
+ (y }..) 1[ rt+2 + y Vt(St+V - y }..Vt(St+V ]
+ (y }..)2 [rt+3 + y Vt(St+3) - y }..Vt(St+3)]
The approximationaboveis exactin the caseof off -line updating, in which caseV,
is the samefor all t . The last stepis exact(not an approximation) becauseall the c5k
termsomitted are due to fictitious steps" after" the terminal statehasbeenentered.
All thesestepshavezerorewardsand zero values; thus all their c5
Thus, we have shown that in the off -line casethe right-hand side of (7.8) can be
which is the sameas (7.9). This proves(7.8).
In the caseof on-line updating, the approximationmadeabovewill be close as
long as a is small and thus Vt changeslittle within an episode. Even in the on-line
casewe canexpectthe updatesof T D(A) andof the A-returnalgorithmto be similar.
For the moment let us assumethat the incrementsare small enoughduring an
episodethat on-line TD (A) gives essentiallythe sameupdateover the courseof
an episodeas doesthe A-return algorithm. There still remain interestingquestions
about what happensduring an episode. Considerthe updatingof the value of state
St in midepisode, at time t + k. Under on-line TD (A), the effect at t + k is just as
if we had done a A-return updatetreating the last observedstate as the tenninal
state of the episodewith a nonzerotenninal value equal to its current estimated
value. This relationshipis maintainedfrom step to step as each new state is observed
Example 7.3: Random Walk with m (A) Becauseoff -line TD (A) is equivalentto
the A-returnalgorithm, we alreadyhavethe resultsfor off -line TD (A) on the 19-state
randomwalk task; they are shownin Figure 7.6. The comparableresultsfor on-line
TD (A) are shownin Figure 7.9. Note that the on-line algorithm works betterover a
. This is often found to be the casefor on-line methods.
* Exercise 7.5 Although TD (A) only approximatesthe A-return algorithm when
done on-line, perhapsthere is a slightly different TD methodthat would maintain
the equivalenceevenin the on-line case. One idea is to definethe TD error instead
as fJt= rt + l + yVt (St+ l ) - Vt- l (St) and the n-step return as R~n) = rt+l + . . . +
yn Irt +n + yn Vt+n- l (St+n) . Show that in this casethe modified TD (A) algorithm
of on-linem (J..) onthe19-staterandomwalktask.
evenin the caseof on-line updatingwith largea . In what ways might this modified
TD (A) be betteror worsethan the conventionalone describedin the text? Describe
an experimentto assessthe relativemeritsof the two algorithms.
How caneligibility tracesbe usednotjust for prediction, asin TD (A), but for control?
As usual, the main idea of one popular approachis simply to learn action values,
Qt(s, a ), rather than state values, Vt(s) . In this section we show how eligibility
tracescanbe combinedwith Sarsain a straightforwardway to producean on-policy
TD control method. The eligibility traceversionof Sarsawe call Sarsa(A), and the
original versionpresentedin the previouschapterwe henceforthcall one-stepSarsa.
The ideain Sarsa(A) is to apply the TD (A) predictionmethodto state- actionpairs
rather than to states. Obviously, then, we needa trace not just for each state, but
for each state- action pair. Let et(s, a) denotethe trace for state- action pair s, a.
Otherwisethe methodis just like TD (A), substitutingstate- action variablesfor state
variables- Q(s, a) for V (s) andet(s, a ) for et(s) :
cSt= rt + l + yQt (St+ l , at+ l ) - Qt (St, at)
Figure 7.10 shows the backup diagram for Sarsa(}..). Notice the similarity to the
diagramof the TD (}") algorithm ( Figure7.3). The first backuplooks aheadone full
stepto the next state-actionpair, the secondlooks aheadtwo steps, andsoon. A final
backupis basedon the completereturn. The weighting of eachbackupis just as in
One-stepSarsaand Sarsa(}..) are on-policy algorithms, meaningthat they approximate
(s, a), the action valuesfor the currentpolicy, 1r, then improvethe policy
gradually basedon the approximatevaluesfor the current policy. The policy improvement
canbedonein manydifferentways, aswe haveseenthroughoutthis book.
For example, the simplestapproachis to usethe E"-greedypolicy with respectto the
current action-value estimates.Figure 7.11 showsthe completeSarsa(}..) algorithm
Example7.4: Tracesin Gridworld Theuseof eligibilityb' acescansubstantially
Figure7.12 Gridworld exampleof die speedupof policy learningdueto die useof eligibility
gridworld examplein Figure7.12. The first panelshowsthe pathtakenby an agentin
a singleepisode,endingat a locationof high reward, markedby the * . In this example
the valueswere all initially 0, and all rewardswerezeroexceptfor a positivereward
at the * location. The arrowsin the other two panelsshowwhich action valueswere
asa resultof this pathby one-stepSarsaandSarsa(A) methods.Theonestrengthened
strengthensonly the last actionof the sequenceof actionsthat led to the
, whereasthe trace method strengthensmany actionsof the sequence
The degreeof strengthening(indicatedby the sizeof the arrows) falls off (according
to YA) with stepsfrom the reward. In this exampley = I andA = 0.9.
1\1/0 different methodshave beenproposedthat combineeligibility tracesand Q'
learning; we call them Watkinss Q(). ) and Pengs Q().), after the researcherswho
first proposedthem. First we describeWatkinss Q(). ).
Recall that Q-Iearningis an off -policy method, meaningthat the policy learned
about need not be the same as the one used to select actions. In particular, Qlearninglearnsaboutthe greedypolicy while it typically follows a policy involving
exploratoryactions- occasional selectionsof actionsthat are suboptimalaccording
to Q, . Becauseof this, specialcareis requiredwhenintroducingeligibility traces.
Supposewe are backingup the state action pair s" a, at time t . Supposethat on
the next two time stepsthe agentselectsthe greedyaction, but on the third, at time
t + 3, the agentselectsan exploratory, nongreedyaction. In learningaboutthe value
of the greedypolicy at s" a, we can usesubsequentexperienceonly as long as the
greedypolicy is being followed. Thus, we canusethe one stepandtwo stepreturns,
but not, in this case, the three stepreturn. The n stepreturnsfor all n ~ 3 no longer
haveany necessaryrelationshipto the greedypolicy.
Thus, unlike TD ()' ) or Sarsa(). ), Watkinss Q().) doesnot look aheadall the way to
the endof the episodein its backup. It only looks aheadasfar asthe nextexploratory
action. Aside from this difference, however, Watkinss Q(). ) is much like TD ()' )
and Sarsa(). ). Their lookaheadstops at episodes end, whereasQ(). ) s lookahead
stopsat the first exploratoryaction, or at episode
actionsbefore that. Actually, to be more precise, Watkinss Q(). ) looks one action
past the first exploration, using its knowledgeof the
supposethe first action, a,+l , is exploratory. Watkinss Q(). ) would still do the one
stepupdateof Q,(s" a,) toward" +1+ y maxa Q,(S' +I , a) . In general, ifa ' +n is the
first exploratoryaction, then the longestbackupis toward
" + 1+ Y" +2 + . . . + Yn l ' ,+n + Y max
whereherewe assumeoff -line updating. The backupdiagramin Figure 7.13 illustrates
the forward view of Watkinss Q(). ), showingall the componentbackups.
The mechanisticor backwardview of Watkinss Q(). ) is also very simple. Eligibility
tracesare usedjust asin Sarsa().), exceptthat they are setto zerowheneveran
exploratory(nongreedy) actionis taken. The traceupdateis bestthoughtof asoccur
ring in two steps. First, the tracesfor all state-actionpairs areeither decayedby y ).
or, if anexploratoryactionwastaken, setto O. Second,the tracecorrespondingto the
currentstateandaction is incrementedby 1. The overall result is
Figure 7.13 The backupdiagramfor Watkinss Q()..) . The seriesof componentbackupsends
if Q, - l (S" a,) = In8Xa Q, - l (S" a ) ;
where, as before , lxy is an identity - indicator function , equal to 1 if x = y and 0
otherwise . The rest of the algorithm is defined by
Q' + l (S, a ) = Q, (s , a ) + a8 , e,(s , a ),
Figure 7 .14 shows the complete algorithm in pseudocode.
Unfortunately , cutting off traces every time an exploratory action is taken loses
much of the advantage of using eligibility traces. If exploratory actions are frequent ,
as they often are early in learning , then only rarely will backups of more than one or
two steps be done, and learning may be little faster than one- step Q- learning . Peng s
Q(A) is an alternate version of Q(A) meant to remedy this. Peng s Q( A) can be thought
of as a hybrid of Sarsa(A) and Watkins Q( )
Q ( s , a ) arbib ' arily and e ( s , a ) = 0 , for all s , a
Choose a from s using policy derived from Q ( e.g ., ~ - greedY )
a +- arg Jnaxb Q ( s , b ) ( if a ties for the max , then a +- a )
Q ( s , a ) +- Q ( s , a ) + acSe( s , a )
If a = a * , then e ( s , a ) +- y )..e ( s , a )
Conceptually, Pengs Q()..) usesthe mixture of backupsshown in Figure 7.15.
Unlike Q-learning, there is no distinction betweenexploratoryand greedyactions.
Each componentbackupis over many stepsof actual experiences
are neither on policy nor off policy. The earlier transitionsof each are on-policy,
whereasthe last (fictitious) transition usesthe greedy policy. As a consequence
for a fixed nongreedypolicy, Q, convergesto neither Q1I
Q()..), but to somehybrid of the two. However, if the policy is graduallymademore
greedy, then the methodmay still convergeto Q . As of this writing this hasnot yet
, the methodperformswell empirically. Most studieshave
shownit performing significantly better than Watkinss Q()..) and almostas well as
On the other hand, Pengs Q()..) cannotbe implementedas simply as Watkinss
Q()..). For a complete description of the neededimplementation, see Peng and
Williams ( 1994, 1996). One could imagine yet a third version of Q()..), let us call
it naive Q()..), that is just like Watkinss Q()..) exceptthat the tracesarenot setto zero
on exploratoryactions. This methodmight have someof the advantagesof Pengs
Q()..), but without the compleximplementation.We know of no experiencewith this
method, but perhapsit is not asnaiveasone might at first suppose
* 7.7 Eligibility Tracesfor Actor- Critic Methods
Figure7.15 The backupdiagramIforPengsQ()..).
In this sectionwe describehow to extendthe actor-critic methodsintroducedin
anactor-critic methodis simplyon-policylearningof V7r.TheTD(A) algorithmcan
beusedfor that, with oneeligibilitytracefor eachstate.Theactorpartneedsto use
aneligibilitytracefor eachstate-actionpair. Thus, anactor-critic methodneedstwo
, onefor eachstateandonefor eachstate-actionpair.
Recall theone-stepactor-critic methodupdates
is theTD(A) error(7.6), andPt(s, a) is thepreference
theaboveequationto useeligibility tracesas
where et(s , a ) denotes the trace at time t for state- action pair s , a . For the simplest
case mentioned above, the trace can be updated as in Sarsa( )..) .
In Section 6.6 we also discussed a more sophisticated actor- critic method that uses
To generalize this equation to eligibility traces we can use the same update (7.11)
with a slightly different trace. Rather than incrementing the trace by 1 each time a
state- action pair occurs, it is updated by 1 - 1l't (St, at ) :
In somecasessignificantly better performancecan be obtainedby using a slightly
modified kind of trace known as a replacing trace. Supposea stateis visited and
then revisitedbefore the tracedue to the first visit has fully decayedto zero. With
accumulatingtraces(7.5), the revisit causesa further incrementin the trace, driving
it greaterthan 1, whereaswith replacingtraces, the trace is resetto 1. Figure 7.16
contraststhesetwo kinds of traces. Formally, a replacingtracefor a discretestates
Predictionor control algorithmsusing replacingtracesare often called replacetrace methods.Although replacingtracesareonly slightly different from accumulating
traces, they can producea significantimprovementin learningrate. Figure 7.17
comparesthe performanceof conventionaland replace-trace versionsof TD ()") on
the 19-staterandomwalk predictiontask. Otherexamplesfor a slightly moregeneral
casearegiven in Figure 8.15 in the next chapter.
Example 7.S Figure 7.18 showsan exampleof the kind of task that is difficult for
control methodsusingaccumulatingeligibility traces. All rewardsarezeroexcepton
enteringthetenninal state, which producesa rewardof + 1. Fromeachstate, selecting
the right actionbringsthe agentonestepcloserto the tenninal reward, whereasthe
- -_t~ ~~~~~ ,:,:..~-~- _J~"~~ ..~_._ Figure7.16
Figure 7.17 Error as a function of A on a 19-staterandomwalk predictiontask. Thesedata
are using the bestvalueof a for eachvalue of A. The error is averagedover all 19 statesand
over the first 20 trials of 100different runs.
Figure 7.18 A simple task that causesproblemsfor control methodsusing accumulating
wrong (upper) action leavesit in the samestateto try again. The full sequenceof
statesis long enoughthat onewould like to uselong tracesto get the fastestlearning.
However, problemsoccur if long accumulatingtracesare used. Suppose
episode, at somestate, s, the agentby chancetakesthe wrong action a few times
before taking the right action. As the agentcontinues, the trace e(s, wrong) is
likely to be larger than the trace e(s, right ) . The right action was more recent,
but the wrong actionwasselectedmoretimes. Whenrewardis finally received, then,
the value for the wrong action is likely to go up more than the value for the right
action. On the next episodethe agentwill be evenmore likely to go the wrong way
many times beforegoing right , making it evenmore likely that the wrong action
will have the larger trace. Eventually, all of this will be corrected, but learning is
significantly slowed. With replacing traces, on the other hand, this problem never
occurs. No matterhow many times the wrong action is taken, its eligibility traceis
alwayslessthan that for the right actionafter the right action hasbeentaken. 8
There is an interestingrelationshipbetweenreplace-trace methodsand Monte
Carlo methodsin the undiscountedcase. Just as conventionalm ( l ) is related to
the every-visit MC algorithm, so replace-trace m ( l ) is related to the first-visit
MC algorithm. In particular, the off -line versionof replace-tracem ( l ) is formally
identical to first-visit MC (Singh and Sutton, 1996). How, or even whether, these
methodsandresultsextendto the discountedcaseis unknown.
Thereare severalpossiblewaysto generalizereplacingeligibility tracesfor usein
control methods. Obviously, when a stateis revisitedand a new action is selected
the trace for that action should be resetto I . But what of the tracesfor the other
actionsfor that state? The approachrecommendedby Singhand Sutton( 1996) is to
set the tracesof all the other actionsfrom the revisited stateto O. In this case, the
state- actiontracesare updatedby the following insteadof (7.10):
Note that this variant of replacing tracesworks out even better than the original
replacingtracesin the exampletask. Oncethe right action hasbeenselected
trace at all. The results shown in Figure 8.15 were
obtainedusingthis kind of replacingtrace.
Exercise 7.6 In Example7.5, supposefrom states the wrong actionis takentwice
before the right action is taken. If accumulatingtracesare used, then how big
must the traceparameter}., be in order for the wrong action to end up with a larger
Exercise7.7 (programming) ProgramExample7.5 andcompareaccumulate
and replace-ttace versionsof Sarsa().,) on it , for ),, = 0.9 and a range of a values.
Can you empirically demonsttatethe claimed advantageof replacingttaceson this
* Exercise 7.8 Draw a backupdiagramfor Sarsa().,) with replacingtraces.
It might at first appearthat methodsusing eligibility ttaces are much more complex
than one-step methods. A naive implementationwould require every state(or
state-action pair) to updateboth its valueestimateand its eligibility traceon every
time step. This would not be a problem for implementationson single-instruction,
multiple-data parallel computersor in plausibleneural implementations
valuesof ).., and y the eligibility tracesof almost all statesare almost always
nearly zero; only thosethat haverecentlybeenvisited will havetracessignificantly
greaterthanzero. Only thesefew statesreally needto be updatedbecausethe updates
at the otherswill haveessentiallyno effect.
In practice, then, implementationson conventionalcomputerskeep ttack of and
updateonly the few stateswith nonzerottaces. Using this trick, the computational
expenseof usingtracesis typically a few timesthat of a one-stepmethod. The exact
multiple of coursedependson ).., andy andon the expenseof the othercomputations
Cichosz( 1995) hasdemonstrateda further implementationtechniquethat further reduces
complexity to a constantindependentof )", and y . Finally, it shouldbe noted
that the tabularcaseis in somesensea worst casefor the computationalcomplexity
of traces. When function approximationis used(Chapter8), the computational
advantagesof not using ttacesgenerallydecrease
networksandbackpropagationare used, then ttacesgenerallycauseonly a doubling
of the requiredmemoryandcomputationper step.
Exercise 7.9 Write pseudocodefor an implementationof TD ()"') that updatesonly
valueestimatesfor stateswhosettacesaregreaterthan somesmall positiveconstant.
The A-return can be significantlygeneralizedbeyondwhat we havedescribedso far
by allowing A to vary from stepto step, that is, by redefiningthe ttace updateas
where A, denotesthe value of A at time t . This is an advancedtopic becausethe
addedgeneralityhas neverbeenusedin practical applications, but it is interesting
theoretically and may yet prove useful. For example, one idea is to vary A as a
function of state: A, = A(S,) . If a state's valueestimateis believedto be known with
high certainty, thenit makessenseto usethat estimatefully , ignoring whateverstates
and rewardsare receivedafter it. This correspondsto cutting off all the tracesonce
this statehasbeenreached, that is, to choosingthe A for the certainstateto be zero
or very small. Similarly, stateswhosevalueestimatesare highly uncertain, perhaps
becauseeventhe stateestimateis unreliable, canbe givenASnear 1. This causestheir
estimatedvaluesto havelittle effect on any updates.They are " skippedover" until a
The eligibility trace equation above is the backwardview of variable AS. The
correspondingforward view is a moregeneraldefinition of the A-return:
* Exercise 7.10 Prove that the forward and backward views of off -line TD (A)
remain equivalentundertheir new definitions with variableA given in this section.
Eligibility tracesin conjunctionwith TD errorsprovidean efficient, incrementalway
of shifting andchoosingbetweenMonte Carlo andTD methods. Tracescanbe used
without TD errorsto achievea similar effect, but only awkwardly. A methodsuchas
TD (A) enablesthis to be donefrom partial experiencesand with little memoryand
little nonmeaningfulvariationin predictions.
As we mentionedin Chapter5, Monte Carlo methodsmay have advantagesin
non-Markov tasksbecausethey do not bootstrap. Becauseeligibility tracesmakeTD
methodsmore like Monte Carlo methods, they also can have advantagesin these
7.12 Bibliographical and Historical Remarks
cases. If one wants to use TO methodsbecauseof their other advantages
indicated. Eligibility tracesare the first line of defenseagainst
By adjustingA, we canplaceeligibility tracemethodsanywherealonga continuum
from Monte Carlo to one-stepTO methods. Whereshall we placethem? We do not
yet have a good theoreticalanswerto this question, but a clear empirical
appearsto be emerging. On taskswith manystepsper episode, or manysteps
thehalf-life of discounting, it appearssignificantlybetterto useeligibility tracesthan
not to (e.g., seeFigure8.15). On theotherhand, if the tracesaresolong asto produce
a pure Monte Carlo method, or nearly so, then performancedegradessharply. An
intermediatemixture appearsto be the bestchoice. Eligibility tracesshouldbe used
to bring us toward Monte Carlo methods, but not all the way there. In the future it
may be possibleto vary the trade off betweenTO and Monte Carlo methods
finely by using variableA, but at presentit is not
Methodsusingeligibility tracesrequiremorecomputationthanone-stepmethods,
but in return they offer significantly faster learning, particularly when rewardsare
delayedby many steps. Thus it often makessenseto use eligibility traces
data are scarceand cannot be repeatedlyprocessed
applications. On the other hand, in off line applicationsin which
pay to useeligibility traces. In thesecases objective
limited amountof data, but simply to processasmuchdataaspossible quickly as
possible. In thesecasesthe speedupper datum due to tracesis typically
7.1- 2 The forward view of eligibility tracesin terms of n-step returnsand the A-return is
due to Watkins ( 1989), who also first discussedthe error reduction property of n
stepreturns, Our presentationis basedon the slightly modified
this text basedon work by Sutton( 1988) and by Singh and Sutton( 1996) , The useof
theseand other algorithmsin this chapteris new, as are
siklis ( 1994), Jaakkola,Jordan, andSingh( 1994), in addition, first provedconvergence
We have so far assumedthat our estimatesof value functions are representedas a
table with oneenb'y for eachstateor for eachstate-actionpair. This is a particularly
clear and instructivecase, but of courseit is limited to tasks with small numbers
of statesand actions. The problem is not just the memory neededfor large tables,
but the time and data neededto fill them accurately. In other words, the key issue
is that of generalization. How can experiencewith a limited subsetof the state
spacebe usefully generalizedto producea good approximationover a much larger
This is a severeproblem. In manytasksto which we would like to apply reinforcement
learning, most statesencounteredwill never have been experiencedexactly
before. This will almostalwaysbe the casewhen the stateor action spacesinclude
to learn anythingat all on thesetasksis to generalizefrom previouslyexperienced
Fortunately, generalizationfrom exampleshas alreadybeenextensivelystudied,
andwe do not needto invent totally new methodsfor usein reinforcementlearning.
To a large extent we needonly combinereinforcementlearning methodswith existing
generalizationmethods. The kind of generalizationwe requireis often called
function approximationbecauseit takesexamplesfrom a desiredfunction (e.g., a
value function) and attemptsto generalizefrom them to constructan approximation
of the entire function. Function approximationis an instanceof supervised
learning, the primary topic studiedin machinelearning, artificial neural networks,
pattern recognition, and statistical curve fitting . In principle, any of the methods
studied in thesefields can be used in reinforcementlearning as describedin this
As usual, we beginwith the predictionproblemof estimatingthe state-valuefunction
VIr from experiencegeneratedusing policy 1C
the approximatevalue function at time t , V" is representednot as a table but as a
parameterizedfunctional form...with parametervector 9, . This meansthat the
function V, dependstotally on (J" varyingfrom time stepto time steponly as(J, varies.
For example, V, might be the function computedby an artificial neural network,
with 9, the vector of connectionweights. By adjustingthe weights, any of a wide
rangeof different functions V, can be implementedby the network. Or V, might be
the function computedby a decision tree, where 9, is all the parametersdefining
the split points and leaf valuesof the tree. Typically, the numberof parameters(the
numberof componentsof 9,) is much lessthan the numberof states, and changing
one parameterchangesthe estimatedvalue of many states. Consequently
All of the predictionmethodscoveredin this bookhavebeendescribedasbackups,
that is, asupdatesto an estimatedvaluefunctionthat shift its valueat particularstates
toward a " backed-up value for that state. Let us refer to an individual backupby
the notations ~ v, wheres is the statebackedup and v is the backed-up value, or
target, that s s estimatedvalue is shifted toward. For example, the DP backupfor
value prediction is s ~ EIr{ r ,+1+ yV,(S'+I) Is , = s } , the Monte Carlo backupis
s, ~ R" the TD (O) backupis s, ~ r ' +l + yV,(S'+I) , andthe generalTD ()") backup
is s, ~ Rt . In the DP case, an arbitrary states is backedup, whereasin the other
casesthe state, s" encounteredin (possiblysimulated) experienceis backedup.
It is natural to interpret each backup as specifying an exampleof the desired
input- output behaviorof the estimatedvalue function. In a sense, the backups ~
v meansthat the estimatedvalue for state s should be more like v. Up to now,
the actual updateimplementingthe backuphas beentrivial: the table entry for s s
estimatedvalue has simply been shifted a fraction of the way toward v. Now we
pennit arbitrarily complex and sophisticatedfunction approximationmethodsto
implement the backup. The normal inputs to thesemethodsare examplesof the
desiredinput- output behaviorof the function they are trying to approximate. We
use thesemethodsfor value prediction simply by passingto them the s ~ v of
eachbackupas a training example. We then interpretthe approximatefunction they
Viewing eachbackupasa conventionaltraining examplein this way enablesus to
useany of a wide rangeof existing function approximationmethodsfor value pre-
Value Prediction with Function Approximation
diction. In principle, we can useany methodfor supervisedlearningfrom examples,
including artificial neuralnetworks, decisiontrees, and variouskinds of multivariate
. However, not all function approximationmethodsareequallywell suited
for usein reinforcementlearning. The most sophisticatedneuralnetworkand statistical
methodsall assumea static training set over which multiple passesare made.
In reinforcementlearning, however, it is importantthat learningbe ableto occuronline
, while interactingwith the environmentor with a model of the environment.To
do this requiresmethodsthat areableto learnefficiently from incrementallyacquired
data. In addition, reinforcementlearninggenerallyrequiresfunction approximation
methodsable to handlenonstationarytargetfunctions(targetfunctionsthat change
over time). For example, in GPI control methodswe often seekto learn Q1fwhile
. Even if the policy remainsthe same, the targetvaluesof training examples
are nonstationaryif they are generatedby bootstrapping methods(suchas OP
and TO methods). Methodsthat cannoteasily handlesuch nonstationarity are less
What performancemeasuresare appropriatefor evaluatingfunction approximation
methods? Most supervisedlearningmethodsseekto minimizethe mean-squared
error (MSE) over somedistribution, P , of the inputs. In our valuepredictionproblem
, the inputsare statesandthe targetfunction-is the true valuefunction V1f, so the
MSE for an approximationV" usingparameter9" is
where P is a distribution weighting the errors of different states. This distribution is
important because it is usually not possible to reduce the error to zero at all states.
After all , there are generally far more statesthan there are components to 8, . The flexibility
of the function approximator is thus a scarce resource. Better approximation
at some states can be gained, generally , only at the expense of worse approximation
at other states. The distribution P specifies how these trade-offs should be made.
The distribution P is also usually the distribution from which the states in the
training examples are drawn , and thus the distribution of states at which backups are
done. If we wish to minimize error over a certain distribution of states, then it makes
senseto train the function approximatorwith examplesfrom that samedistribution.
For example, if you want a uniform level of error over the entire stateset, then it
makessenseto train with backupsdistributeduniformly overthe entirestateset, such
as in the exhaustivesweepsof someDP methods. Henceforth, let us assumethat the
distribution of statesat which backupsare done and the distribution that weights
A distributionof particularinterestis the onedescribingthe frequencywith which
statesareencounteredwhile the agentis interactingwith the environmentandselecting
' , the policy whosevalue function we are approximating.
We call this the on policy distribution, in part becauseit is the distributionof backups
in on-policy control methods. Minimizing error over the on-policy distribution
focusesfunction approximationresourceson the statesthat actuallyoccurwhile following
the policy, ignoring thosethat neveroccur. The on-policy distribution is also
the one for which it is easiestto get training examplesusing Monte Carlo or m
methods. Thesemethodsgeneratebackupsfrom sampleexperienceusing the policy
' . Becausea backupis generatedfor eachstateencounteredin the experience
the training examplesavailableare naturally distributedaccordingto the on-policy
resultsareavailablefor the on-policy distribution
than for otherdistributions, as we discusslater.
It is not completelyclearthat we shouldcareaboutminimizing the MSE. Our goal
in valuepredictionis potentiallydifferent becauseour ultimatepurposeis to usethe
predictionsto aid in finding a betterpolicy. The bestpredictionsfor that purposeare
not necessarilythe bestfor minimizing MSE. However, it is not yet clearwhata more
useful alternativegoal for valuepredictionmight be. For now, we continueto focus
tore for which MSE(e ) ~ MSE(e) for all possiblee. Reachingthis goal is sometimes
possiblefor simple function approximatorssuchas linear ones, but is rarely
possiblefor complexfunction approximatorssuchas artificial neuralnetworksand
decisiontrees. Shortof this, complexfunction..approximatorsmay..seekto converge
aile in someneighborhoodof e* . Although this guaranteeis only slightly reassuring
it is typically the bestthatcanbe hadfor nonlinearfunctionapproximators
casesof interestin reinforcementlearning, convergenceto an optimum, or eventrue
an optimum may still be achievedwith somemethods. Other methodsmay in fact
diverge, with their MSE approachinginfinity in the limit .
In this sectionwe haveoutlined a frameworkfor combining a wide rangeof reinforceme
learning methodsfor value prediction with a wide range of function
approximationmethods, usingthe backupsof the former to generatetraining examples
for the latter. We havealso outlined a rangeof MSE perfonnancemeasuresto
which thesemethodsmay aspire. The rangeof possiblemethodsis far too large to
coverall, andanywaytoo little is known aboutmostof themto makea reliableevaluation
, we consideronly a few possibilities. In the
rest of this chapterwe focus on function approximationmethodsbasedon gradient
principles , and on linear gradient- descent methods in particular . We focus on these
methods in part because we consider them to be particularly promising and because
they reveal key theoretical issues, but also because they are simple and our space is
limited . If we had another chapter devoted to function approximation , we would also
cover at least memory -based and decision -tree methods.
We now developin detail one classof learningmethodsfor function approximation
in value prediction, thosebasedon gradientdescent. Gradient-descentmethodsare
amongthe mostwidely usedof all function approximationmethodsandareparticularly
In gradient-descentmethods, the parametervectoris a column vectorwith a fixed
T the There denumberof real valuedcomponents
For now, let us assumethat on eachstept , we observea new exampleSt~ V7r(St) .
Thesestatesmight be successivestatesfrom an interactionwith the environment,but
for now we do not assumeso. Even though we are given the exact, correct values,
V7r(St) for eachSf, thereis still a difficult problembecauseour function approximator
..haslimited resourcesandthus limited resolution particular,
no 8 that getsall the states, or evenall the examples,exactlycorrect. In addition, we
must generalizeto all the other statesthat havenot appearedin examples.
We assumethat statesappearin exampleswith the samedistribution, P , over
which we are trying to minimize the MSE as given by (8.1). A good strategyin this
caseis to try to minimize error on the observedexamples.Gradient-descentmethods
do this by adjustingthe parametervector after eachexampleby a small amountin
the directionthat would mostreducethe error on that example:
= 8t + a [ V1I'(St) - Vt(St)] V9 Vt(St) ,
, and V9, ! (9t), for any function ! , denotes
Generalization and Function Approximation
This derivativevectoris the gradient of f with respectto 6, . This kind of methodis
called gradient descentbecausethe overall stepin 6, is proportionalto the negative
gradientof the examples squarederror. This is the direction in which the error falls
It may not be immediatelyapparentwhy only a small stepis takenin the direction
of the gradient. Could we not move all the way in this direction and completely
eliminate the error on the example? In many casesthis could be done, but usually
it is not desirable. Rememberthat we do not seekor expectto find a valuefunction
that haszeroerror on all states,but only an approximationthat balancesthe errorsin
different states. If we completelycorrectedeachexamplein onestep, thenwe would
not find sucha balance. In fact, the convergence
that the step size parameterdecreasesover time. If it decreasesin such a way as
to satisfy the standardstochasticapproximationconditions(2.8), then the gradientdescentmethod(8.2) is guaranteedto convergeto a local optimum.
We turn now to the casein which the targetoutput, v" of the tth training examples
v" is not the true value, V7r(s,), but someapproximationof it. For example, v,
might be a noise-corruptedversionof V7r(s,), or it might be one of the backed-up
valuesmentionedin the previoussection. In suchcaseswe cannotperformthe exact
update(8.2) becauseV7r(s,) is unknown, but we can approximateit by substituting
v, in placeof V7r(s,) . This yields the generalgradient-descentmethodfor state-value
6'+1= 6, + a [ v, - V,(s,) ] V8, V,(s,) .
If v, is an unbiased estimate, that is, if E {v,} = V7r(s,), for each t , then 6, is
guaranteedto convergeto a local optimumunderthe usualstochasticapproximation
conditions(2.8) for decreasingthe step-sizeparametera .
For example, supposethe statesin the examplesarethe statesgeneratedby interaction
(or simulatedinteraction) with the environmentusingpolicy 1C
the return following eachstate, s, . Becausethe true value of a stateis the expected
value of the return following it , the Monte Carlo target v, = R, is by definition an
unbiasedestimateof V7r(s,) . With this choice, the generalgradient-descentmethod
(8.3) convergesto a locally optimal approximationto V7r(s,) . Thus, the gradientdescentversionof Monte Carlo state-valuepredictionis guaranteedto find a locally
Similarly, we canusen-stepill returnsandtheir averagesfor v, . For example, the
gradient-descentform of ill (A) usesthe A-return, v, = Rt , as its approximationto
V7r(s,), yielding the forward-view update:
Unfortunately, for A < 1, Rt is not an unbiasedestimateof V1C
methoddoesnot convergeto a local optimum. The situationis the samewhenDP targets
{ r ' + 1+ yV,(S,+I) I s, } . Nevertheless
for important specialcases, as we discusslater in this chapter. For now we emphasize
the relationshipof thesemethodsto the generalgradient-descentform (8.3).
Although incrementsas in (8.4) are not themselvesgradients, it is useful to view
this methodas a gradient-descentmethod(8.3) with a bootstrapping approximation
As (8.4) providesthe forward view of gradient-descentm (A), so the backward
ande, is a column vectorof eligibility traces, onefor eachcomponentof 8" updated
with eo= O. A completealgorithm for on-line gradient-descentm (A) is given in
Two methodsfor gradient-basedfunction approximationhavebeenusedwidely in
reinforcementlearning. One is multilayer artificial neural networksusing the error
backpropagationalgorithm. This maps immediately onto the equationsand algorithms
just given, wherethe backpropagationprocessis the way of computingthe
gradients. The secondpopularform is the linear form, which we discussextensively
Exercise 8.1 Showthat table-lookup m (A) is a specialcaseof generalm (A) as
Exercise 8.2 Stateaggregationis a simple form of generalizingfunction approximation
in which statesare groupedtogether, with one table entry (value estimate)
usedfor eachgroup. Whenevera statein a groupis encountered
usedto detenninethe states value, and when the stateis updated, the group s entry
V1r. The approximatevaluefuncm (>..) for estimating
is updated. Show that this kind of stateaggregationis a specialcaseof a gradient
Exercise 8.3 The equationsgiven in this section are for the on-line version of
gradient-descentTD (A) . What are the equationsfor the off-line version? Give a
completedescriptionspecifying the new approximatevalue function at the end of
an episode, V' , in termsof the approximatevaluefunction usedduring the episode,
V. Startby modifying a forward-view equationfor TD (A), suchas (8.4).
* Exercise 8.4 For off -line updating, show that equations(8.5- 8.7) produceupdates
Oneof the mostimportantspecialcasesof gradient-descentfunction approximation
is that in which the approximatefunction, V" is a linear function of the parameter
, 8, . Correspondingto every state s, there is a column vector of features
<1>$ 1>$( 1), <1>$(2), . . . , <I>$(n , with the samenumberof componentsas 6, . The
featuresmay be constructedfrom the statesin many different ways; we cover a
few possibilitiesbelow. Howeverthe featuresareconstructed
, the approximatestatevaluefunction is givenby
In this casethe approximatevaluefunction is saidto be linear in theparameters, or
It is natural to use gradient-descentupdateswith linear function approximation.
The gradientof the approximatevaluefunction with respectto 9, in this caseis
Thus, the generalgradient-descentupdate(8.3) reducesto a particularlysimpleform
in the linear case. In addition, in the linear casethere is only one optimum 9 (or,
in degeneratecases,one set of equally good optima). Thus, any methodguaranteed
to convergeto or neara local optimum is automaticallyguaranteedto convergeto or
near the global optimum. Becauseit is simple in theseways, the linear, gradientdescentcase is one of the most favorablefor mathematicalanalysis. Almost all
resultsfor learningsystemsof all kinds arefor linear (or simpler)
In particular, the gradient-descentTD (J...) algorithmdiscussedin the previoussection
(Figure 8.1) has been proved to convergein the linear case if the step size
parameteris reducedover time accordingto the usualconditions(2.8). Convergence
is not to the minimum-error parametervector, 9 , but to a nearbyparametervector,
That is, the asymptoticerror is no more than ~
bound appliesto other on-policy bootstrapping methods. For example, linear
gradient-descentDP backups(8.3), with the on policy distribution, will convergeto
the sameresult asTD(O). Technically, this boundappliesonly to discountedcontinuing
tasks, but a relatedresult presumablyholds for episodictasks. Thereare also a
conditionson the rewards, features, and decreasein the step-size parameter
, which we are omitting here. The full details can be found in the original
Critical to the aboveresult is that statesarebackedup accordingto the on-policy
distribution. For other backupdistributions, bootstrapping methodsusing function
approximationmay actuallydivergeto infinity. Examplesof this anda discussionof
possiblesolutionmethodsaregiven in Section8.5.
Generalization and Function Approximation
Beyondthesetheoreticalresults, linear learningmethodsare also of interestbecause
in practicethey can be very efficient in tenDs of both data and computation.
Whether or not this is so dependscritically on how the statesare representedin
tenDsof the features. Choosingfeaturesappropriateto the task is an importantway
of addingprior domainknowledgeto reinforcementlearningsystems.Intuitively, the
featuresshouldcorrespondto the naturalfeaturesof the task, thosealongwhich generalizationis mostappropriate.If we are valuinggeometricobjects, for example, we
might wantto havefeaturesfor eachpossibleshape,color, size, or function. If we are
valuing statesof a mobile robot, then we might want to havefeaturesfor locations,
degreesof remainingbatterypower, recentsonarreadings, and so on.
In general, we also need featuresfor combinationsof these natural qualities.
This is becausethe linear fonD prohibits the representationof interactionsbetween
features, suchasthe presenceof featurei beinggoodonly in the absenceof featurej .
For example, in the pole-balancingtask (Example3.4), a high angularvelocity may
be either good or bad dependingon the angularposition. If the angle is high, then
high angularvelocity meansan imminentdangerof falling, a badstate, whereasif the
angleis low, thenhigh angularvelocity meansthe pole is righting itself, a goodstate.
In caseswith suchinteractionsone needsto inttoduce featuresfor conjunctionsof
featurevalueswhenusing linear function approximationmethods.We next consider
How could we reproducethe tabular casewithin the linear framework
Exercise8.6 How could we reproducethe stateaggregationcase(seeExercise8.2)
Considera task in which the stateset is continuousandtwo-dimensional. A statein
this caseis a point in 2-space,a vectorwith two real components
for this caseis thosecorrespondingto circles in statespace, as shownin Figure 8.2.
If the stateis inside a circle, then the correspondingfeaturehas the value 1 and is
said to be present; otherwisethe featureis 0 and is said to be absent. This kind of
lO -valuedfeatureis called a binary feature. Given a state, which binary features
are presentindicatewithin which circlesthe statelies, andthus coarselycodefor its
location. Representinga statewith featuresthat overlapin this way (althoughthey
neednot be circlesor binary) is known ascoarsecoding.
Figure 8. 2 Coarse coding . Generalization from state X to state depends
feature in common , so there will be slight generalization between them.
function approximationmethodsis detenninedby the
sizesand shapesof the features receptivefields. All three of thesecaseshave roughly the
thesizeanddensityof thecircles.Corresponding
X, asshownin Figure8.2. If thecirclesaresmall, thenthe generalization
largedistance, as in Figure 8.3b. Moreover, the shapeof the featureswill determine
. For example, if they are not strictly circular, but
are elongatedin one direction, then generalizationwill be similarly affected, as in
Featureswith largereceptivefields give broadgeneralization
to limit the learnedfunction to a coarseapproximation, unableto makediscriminations
much finer than the width of the receptivefields. Happily, this is not the case.
Initial generalizationfrom one point to anotheris indeedcontrolled by the size and
shapeof the receptivefields, but acuity, the finestdiscriminationultimately possible,
is controlled moreby the total numberof features.
Example 8.1: Coarsenessof Coarse Coding This exampleillustratesthe effect
on learning of the size of the receptivefields in coarsecoding. Linear function
approximationbasedon coarsecodingand(8.3) wasusedto learna one-dimensional
square-wave function (shownat the top of Figure 8.4). The valuesof this function
were used as the targets, v, . With just one dimension, the receptivefields were
intervalsratherthan circles. Learningwas repeatedwith threedifferent sizesof the
intervals: narrow, medium, andbroad, asshownat the bottomof the figure. All three
caseshadthe samedensityof features, about50 overthe extentof the function being
learned. Trainingexamplesweregenerateduniformly at randomoverthis extent. The
step-sizeparameterwasa = ~ , wherem is the numberof featuresthat werepresent
at onetime. Figure 8.4 showsthe functionslearnedin all threecasesover the course
of learning. Note that the width of the featureshad a strongeffect early in learning.
With broadfeatures,thegeneralizationtendedto bebroad; with narrowfeatures,only
the closeneighborsof eachtrainedpoint werechanged,causingthe function learned
to be more bumpy. However, the final function learnedwas affectedonly slightly
by the width of the features. Receptivefield shapetendsto havea strongeffect on
generalizationbut little effect on asymptoticsolutionquality.
Tile coding is a form of coarsecoding that is particularly well suited for use on
sequentialdigital computersand for efficient on-line learning. In tile coding the
receptivefields of the featuresare groupedinto exhaustivepartitions of the input
space.Eachsuchpartition is calleda tiling , andeachelementof the partition is called
a tile. Eachtile is the receptivefield for onebinary feature.
An immediateadvantageof tile coding is that the overall numberof featuresthat
are presentat one time is strictly controlled and independentof the input state.
Figure8.4 Exampleof featurewidths strongeffecton initial generalization
Generalization and Function Approximation
Given the x and y coordinatesof a point in the space, it is computationally
to determinethe index of the tile it is in. When multiple tilings are used, each is
offset by a different amount, so that eachcuts the spacein a different way. In the
exampleshownin Figure 8.5, an extra row and an extra column of tiles havebeen
addedto the grid so that no points are left uncovered
thosethat are presentin the stateindicatedby the X. The different tilings may be
offset by randomamounts, or by cleverly designeddeterministicstrategies(simply
offsetting eachdimensionby the sameincrementis known not to be a good idea).
The effectson generalizationand asymptoticaccuracyillustratedin Figures8.3 and
8.4 apply hereas well. The width and shapeof the tiles shouldbe chosento match
the width of generalizationthat oneexpectsto be appropriate.The numberof
should be chosento influencethe density of tiles. The denserthe tiling , the finer
and more accuratelythe desiredfunction can be approximated
It is important to note that the tilings can be arbitrary and neednot be uniform
grids. Not only can the tiles be strangelyshaped, as in Figure 8.6a, but they can be
shapedand distributedto give particular kinds of generalization
Figure 8.5 Multiple, overlappinggridtilings
stripetiling in Figure 8.6b will promotegeneralizationalongthe vertical dimension
and discriminationalong the horizontaldimension, particularly on the left. The diagonal
stripetiling in Figure8.6c will promotegeneralizationalongonediagonal
in someof the tilings, that is, to hyperplanarslices.
Anotherimportanttrick for reducingmemoryrequirementsis hashing- a consistent
pseudorandomcollapsingof a largetiling into a muchsmallersetof
consisting noncontiguous disjoint regions
tile might consistof the four subtilesshownbelow:
Throughhashing, memoryrequirementsareoften reducedby largefactors
loss of perfonnance. This is possiblebecausehigh resolution is neededin only a
small fraction of the statespace. Hashingfreesus from the curseof dimensionality
in the sensethat memory requirementsneednot be exponentialin the number of
dimensions, but need merely match the real demandsof the task. Good public
domainimplementationsof tile coding, including hashing, are widely
Generalization and Function Approximation
Exercise 8.7 Supposewe believethat one of two statedimensionsis more likely
to havean effect on the value function than is the other, that generalizationshould
be primarily acrossthis dimensionratherthanalongit. What kind of tilings could be
usedto takeadvantageof this prior knowledge?
Radial basis functions (RBFs) are the natural generalizationof coarsecoding to
continuous-valued features. Rather than each feature being either 0 or 1, it can
be anything in the interval [0, 1], reflecting various degreesto which the feature
is present. A typical RBF feature, i , has a Gaussian(bell-shaped) response<I>$(i )
dependentonly on the distancebetweenthe state, s, andthe features prototypicalor
centerstate, c;, andrelativeto the features width, 0'; :
The nonnor distancemetric of coursecan be chosenin whateverway seemsmost
appropriateto the statesandtaskat hand. Figure 8.7 showsa I -dimensionalexample
An RBF network is a linear function approximatorusing RBFs for its features.
Learningis definedby equations(8.3) and (8.8), exactly as in other linear function
. The primary advantageof RBFsoverbinary featuresis that they produce
approximatefunctions that vary smoothly and are differentiable. In addition,
somelearningmethodsfor RBF networkschangethe centersand widths of the features
as well. Such nonlinearmethodsmay be able to fit the target function much
moreprecisely. The downsideto RBF networks, and to nonlinearRBF networksespecially
, is greatercomputationalcomplexityand, often, more manualtuning before
Figure8.7 One- dimensionalradial basisfunctions.
On tasks with very high dimensionality, say hundredsof dimensions, tile coding
and RBF networksbecomeimpractical. If we take either methodat face value, its
computationalcomplexity increasesexponentiallywith the numberof dimensions.
Thereare a numberof tricks that can reducethis growth (suchashashing), but even
thesebecomeimpracticalafter a few tensof dimensions.
On the other hand, someof the generalideasunderlying thesemethodscan be
practical for high-dimensionaltasks. In particular, the idea of representingstates
by a list of the featurespresentand then mapping those featureslinearly to an
approximationmay scale well to large tasks. The key is to keep the number of
featuresfrom scalingexplosively. Is thereanyreasonto think this might be possible?
First we needto establishsomerealistic expectations
functionsof comparablecomplexity. But asdimensionalityincreases
statespaceinherentlyincreasesexponentially. It is reasonableto assumethat in the
worstcasethe complexityof the targetfunction scaleslike the sizeof the statespace.
Thus, if we focuson the worst case, thenthereis no solution, no way to get goodapproximations
for high-dimensionaltaskswithout using resourcesexponentialin the
A moreusefulway to think aboutthe problemis to focuson the complexityof the
targetfunction as separateand distinct from the size anddimensionalityof the state
space.The sizeof the statespacemaygive anupperboundon complexity, but shortof
thathigh bound, complexityanddimensioncanbe unrelated.For example, onemight
havea l000-dimensionaltask whereonly one of the dimensionshappensto matter.
Givena certainlevelof complexity, we thenseekto beableto accuratelyapproximate
any target function of that complexity or less. As the target level of complexity
, we would like to get by with a proportionateincreasein computational
From this point of view, the real sourceof the problem is the complexity of the
target function, or of a reasonableapproximationof it , not the dimensionalityof
the state space. Thus, adding dimensionsto a task, such as new sensorsor new
features, should be almost without consequenceif the complexity of the needed
approximationsremainsthe same. The newdimensionsmayevenmakethingseasier
if the target function can be simply expressedin terms of them. Unfortunately,
methodslike tile coding and RBF coding do not work this way. Their complexity
increasesexponentially with dimensionality even if the complexity of the target
functiondoesnot. For thesemethods,dimensionalityitself is still a problem. Weneed
methodswhosecomplexity is unaffectedby dimensionalityper se, methodsthat are
limited only by, and scalewell with, the complexityof what they approximate.
Onesimpleapproachthat meetsthesecriteria, which we call Kanervacoding, is to
choosebinaryfeaturesthatcorrespondto particularprototypestates. For definiteness
let us say that the prototypesare randomly selectedfrom the entire state space.
The receptivefield of sucha featureis all statessufficiently closeto the prototype.
Kanervacodingusesa differentkind of distancemetricthanis usedin tile codingand
, considera binary statespaceandthe hammingdistance, the
numberof bits at which two statesdiffer. Statesareconsideredsimilar if they agree
on enoughdimensions,evenif they are totally different on others.
The strengthof Kanervacoding is that the complexity of the functions that can
be learneddependsentirely on the numberof features, which bearsno necessary
relationshipto the dimensionalityof the task. The numberof featurescanbe moreor
lessthan the numberof dimensions. Only in the worst casemust it be exponential
in the number of dimensions. Dimensionality itself is thus no longer a problem.
Complexfunctionsare still a problem, as they haveto be. To handlemore complex
tasks, a Kanervacoding approachsimply needsmore features. There is not a great
deal of experiencewith suchsystems, but what there is suggeststhat their abilities
increasein proportion to their computationalresources
, and significantimprovementsin existingmethodscanstill easilybe found.
We now extendvalue prediction methodsusing function approximationto control
methods, following the pattern of OPI. First we extendthe state-value prediction
methodsto action-value prediction methods, then we combine them with policy
improvementand action selectiontechniques
The extensionto action-value prediction is straightforward. In this caseit is the
, that is representedas a parameterizedfunctional
form with parametervector8, . Whereasbeforewe consideredtraining examplesof
the form s, ~ v" now we considerexamplesof the form Sf, a, ~ v, . The target
output, v" can be any approximationof Q1C
valuessuch as the full Monte Carlo return, R" or the one-step Sarsa-style return,
r ,+l + Y Q' (S' + l , a' + l ) . The generalgradient-descentupdatefor action-value predictionis
9'+1= 9, + a [ v, - Q,(s" a,)] VetQ,(s" a,) .
For example, the backwardview of the action-valuemethodanalogousto TD ()") is
h', = r ' + 1+ Y Q' (S'+ I, a,+ I) - Q,(s" a,),
with eo= O. We call this methodgradient-descentSarsa()..), particularly when it is
elaboratedto form a full control method. For a constantpolicy, this methodconverges
in the sameway that TD ()") does, with the samekind of error bound(8.9).
To form control methods, we needto couplesuchaction-valuepredictionmethods
with techniquesfor policy improvementand action selection. Suitabletechniques
applicableto continuousactions, or to actionsfrom large discretesets, are a topic
of ongoingresearchwith as yet no clear resolution. On the other hand, if the action
setis discreteandnot too large, thenwe canusethe techniquesalreadydevelopedin
previouschapters.That is, for eachpossibleactiona , availablein the currentstate, s"
we can computeQ,(s" a) and then find the greedyaction a7 = arg maxa Q,(s" a ) .
Policy improvementis doneby changingthe estimationpolicy to the greedypolicy
(in off -policy methods) or to a soft approximationof the greedypolicy suchas the
E-greedypolicy (in on-policy methods). Actions are selectedaccordingto this same
policy in on-policy methods, or by an arbitrarypolicy in off -policy methods.
Figures8.8 and 8.9 show examplesof on-policy (Sarsa().. and off -policy ( Watkins' s Q().. control methodsusingfunction approximation.Both methodsuselinear,
gradient-descentfunction approximationwith binary features, suchas in tile coding
and Kanervacoding. Both methodsusean E-greedypolicy for action selection, and
the Sarsamethodusesit for OPI as well. Both computethe setsof presentfeatures,
: Fa,correspondingto thecurrentstateandall possibleactions, a. lfthe valuefunction
for eachaction is a separatelinear function of the samefeatures(a commoncase),
then the indicesof the : Fafor eachaction are essentiallythe same, simplifying the
All the methodswe have discussedabove have used accumulatingeligibility
traces. Although replacing traces (Section 7.8) are known to have advantagesin
tabular methods, replacingtracesdo not directly extendto the use of function approximation
. Recall that the idea of replacingtracesis to reseta state's trace to 1
eachtime it is visited insteadof incrementingit by 1. But with function approximation
thereis no singlettacecorrespondingto a state, just a ttace for eachcomponent
of 9" which correspondsto many states. One approachthat seemsto work well
for linear, gradient-descentfunction approximationmethodswith binary features
is to treat the featuresas if they were statesfor the purposesof replacing
That is, each time a stateis encounteredthat has featurei , the trace for featurei
Figure 8.9 A linear, gradient-descentversion of Watkinss Q(.)..) with binary features, E
is set to 1 rather than being incrementedby 1, as it would be with accumulating
When working with state-actiontraces, it may alsobe usefulto clear (setto zero)
the b' acesof all nonselectedactionsin the statesencountered(seeSection7.8). This
idea can also be extendedto the caseof linear function approximationwith binary
, we first clear the tracesof all featuresfor the
stateand the actionsnot selected, then we set to 1 the tracesof the featuresfor the
stateand the action that was selected. As we noted for the tabular case, this may
or may not be the best way to proceedwhen using replacingb' aces
specificationof both kinds of traces, including the optionalclearing nonselected
actions, is given for the Sarsaalgorithmin Figure 8.8.
Generalization and Function Approximatio1i
Figure 8.10 The mountain- car task (upper left panel) and die cost-ta- go function
Example 8.2: Mountain - CarTask Considerthe taskof driving an underpowered
car up a steepmountain road, as suggestedby the diagram in the upper left of
Figure8.10. The difficulty is that gravity is strongerthanthe car s engine, andevenat
full throttle the car cannotaccelerateup the steepslope. The only solution is to first
move away from the goal and up the oppositeslopeon the left. Then, by applying
full throttle the car can build up enoughinertia to carry it up the steepslopeeven
thoughit is slowing down the whole way. This is a simple exampleof a continuous
control task wherethings haveto get worsein a sense(farther from the goal) before
they canget better. Many control methodologieshavegreatdifficulties with tasksof
this kind unlessexplicitly aidedby a humandesigner.
The rewardin this problemis - Ion all time stepsuntil the car movespastits goal
positionat the top of the mountain, which endsthe episode. Therearethreepossible
actions: full throttle forward (+ 1), full throttle reverse( - 1), and zero throttle (0).
The car movesaccordingto a simplified physics. Its position, Xt, andvelocity, it , are
Xt+ 1= bound[Xt + 0.00lat + - 0.0025cos(3Xt)] ,
The effect of a . A. and the kind of b' aceson early perfonnance on the mountain -
car task. This studyusedfive 9 x 9 tilings.
where the bound operationenforces- 1.2 ~ Xt+ 1 ~ 0.5 and - 0.07 ~ Xt+ 1 ~ 0.07.
When Xt+ 1 reachedthe left bound, Xt+ 1 was reset to zero. When it reachedthe
right bound, the goal was reachedand the episodewas terminated. Each episode
startedfrom a randomposition andvelocity unifonnly chosenfrom theseranges.To
convertthe two continuousstatevariablesto binary features, we usedgridtilings as
in Figure 8.5. We usedten 9 x 9 tilings, eachoffset by a randomfraction of a tile
The Sarsaalgorithmin Figure 8.8 (usingreplacetracesandthe optionalclearing)
readily solved this task, learning a near optimal policy within 100 episodes
on one run, using the parametersA = 0.9, f. = 0, and a = 0.05 ( ~ ). The initial action
valueswere all zero, which was optimistic (all true valuesare negativein this
task), causingextensiveexplorationto occureventhough
At this time not evenoneepisodehadbeencompleted,but the car hasoscillatedback
andforth in the valley, following circular trajectoriesin statespace.All the statesvisited
frequentlyare valuedworsethan unexploredstates, becausethe actualrewards
havebeenworsethanwhatwas(unrealistically) expected.This continuallydrivesthe
agentawayfrom whereverit hasbeen, to explorenewstates,until a solutionis found.
Figure8.11showsthe resultsof a detailedstudyof the effectof the parametersa and
A, andof the kind of traces, on the rateof learningon this task.
* Exercise8.8 Describehow the actor- critic control methodcanbe combinedwith
We return now to the predictioncaseto take a closerlook at the interactionbetween
bootstrapping, function approximation,andthe on-policy distribution. By bootstrap.
ping we meanthe updatingof a valueestimateon the basisof other valueestimates
TD methodsinvolve bootstrapping, as do OP methods, whereasMonte Carlo methods
do not. TD (A) is a bootstrapping method for A < I , and by conventionwe
considerit not to be a bootstrapping method for A = I . Although TD ( I ) involves
bootstrapping within an episode, the net effect over a completeepisodeis the same
Bootstrapping methodsare more difficult to combinewith function approximation
than are nonbootstrapping methods. For example, considerthe caseof value
prediction with linear, gradient-descentfunction approximation. In this case, nonbootstrapping methodsfind minimal MSE (8.1) solutions for any distribution of
training examples, P , whereasbootstrapping methodsfind only near-minimal MSE
(8.9) solutions, and only for the on-policy distribution. Moreover, the quality of the
MSE boundfor TD (A) getsworsethe farther A straysfrom I , that is, the fartherthe
methodmovesfrom its nonbootstrapping form.
The restriction of the convergenceresultsfor bootstrapping methodsto the onpolicy distribution is of greatestconcern. This is not a problemfor on-policy methods
suchas Sarsaand actor--critic methods, but it is for off -policy methodssuchas
Q learning and OP methods. Off-policy control methodsdo not backup states(or
state-actionpairs) with exactlythe samedistributionwith which the stateswould be
encounteredfollowing the estimationpolicy (the policy whosevalue function they
are estimating). Many OP methods, for example, backup all statesunifonnly. Qlearning may backup statesaccordingto an arbitrary distribution, but typically it
backs them up accordingto the distribution generatedby interactingwith the environment
and following a soft policy close to a greedyestimationpolicy. We use
the term off-policy bootstrapping for any kind of bootstrapping using a distribution
of backupsdifferent from the on-policy distribution. Surprisingly, off -policy bootstrappingcombinedwith function approximationcan leadto divergenceand infinite
Example 8.3: Baird s Counterexample Considerthe six-state, episodicMarkov
processshownin Figure8.12. Episodesbeginin oneof the five upperstates,proceed
immediately to the lower state, and then cycle there for some number of steps
beforeterminating. The rewardis zero on all transitions, so the true value function
(s) = 0, for all s. The form of the approximatevalue function is shownby
. The approximatevalue function for this Markov
expressionsinsideeachstate. The rewardis always
the equationsinset in each state. Note that.. the overall function is linear and that
there are fewer statesthan componentsof (), . Moreover, the set of featurevectors,
{c/>s: se a }, correspondingto this function is a.. linearly
true value function is easily formed by setting(), = O. In all ways, this task seems
a favorablecasefor linear function approximation.
The predictionmethodwe apply to this task is a linear, gradient-descentform of
DP policy evaluation.Theparametervector, 6k, is updatedin sweepsthroughthe state
, gradient-descentbackupat everystate, s, usingthe
6k+l = 6k + a L [ E { r ' +1+ yV' (S' +I) Is , = s } - Vk(S)] V6kVk(S) .
Like most DP methods, this one usesa uniform backup distribution, one of the
simplestoff -policy distributions. Otherwisethis is an ideal case. There is no randomness
. Eachstateis updatedexactlyonceper sweepaccording
to a classicalDP backup. The methodis entirely conventionalexceptin its use of
gradient-descentfunction approximation. Yet for someinitial valuesof the parameters
, the systembecomesunstable, asshowncomputationallyin Figure 8.13.
If we alterjust the distributionof D Pbackupsin Baird' s counterexample
uniform distributionto theon-policy distribution(which generallyrequiresasynchronous
updating), then convergenceis guaranteedto a solutionwith error boundedby
(8.9) for A = O. This exampleis striking becausethe DP methodusedis arguablythe
simplestandbest-understoodbootstrapping method, andthe linear, gradient-descent
of the instabilityof DP valuepredictionwith
a = 0.01, and80= ( I , I , I , I , I , 10, I )T.
methodusedis arguablythe simplestand best-understoodkind of function approximation
. The exampleshowsthat eventhe simplestcombinationof bootstrapping and
function approximationcanbe unstableif the backupsare not doneaccordingto the
There are also counterexamplessimilar to Baird' s showing divergencefor Qlearning. This is causefor concernbecauseotherwiseQ-Iearninghasthe bestconvergence
guaranteesof all control methods. Considerableeffort hasgoneinto trying to
find a remedyto this problemor to obtain someweaker, but still workable, guarantee
. For example, it may be possibleto guaranteeconvergence
as the behaviorpolicy (the policy usedto selectactions) is sufficiently closeto the
estimationpolicy (the policy usedin OPI), for example, whenit is the e-greedypolicy
. To the bestof our knowledge, Q-Iearninghasneverbeenfound to divergein this
case, but therehasbeenno theoreticalanalysis. In the restof this sectionwe present
Supposethat insteadof takingjust a steptowardthe expectedone-stepreturn on
eachiteration, as in Baird' s counterexample
all the way to the best, least-squaresapproximation. Would this solvethe instability
problem? Of courseit would if thefeaturevectors, { i>s: SE S}, formeda linearly independent
is possibleon eachiteration and the methodreducesto standardtabularOP. But of
coursethe point hereis to considerthe casewhen an exactsolution is not possible.
Figure 8.14 Tsitsiklis and Van Roy s counterexampleto DP policy evaluationwith least
eachiteration,asshownby thefollowingexample
Example8.4: Tsiuiklis and Van Roy s CounterexampleThe simplestcounterexample
DP is shownin Figure8.14. Therearejust two
= argmin [8 - y28k] 2 + [28 - ( 1 - E) y28kf
whereV9 denotesthe valuefunctiongiven6. The sequencey
Oneway to try to preventinstability is to usespecialmethodsfor function approximation
. In particular, stability is guaranteedfor function approximationmethods
that do not extrapolatefrom the observedtargets. Thesemethods, called averagers,
include nearestneighbor methodsand local weighted regression
methodssuchastile coding andbackpropagation
Another approachis to attemptto minimize not the mean-squarederror from the
true value function (8.1), but the mean-squarederror from the expectedone-step
return. It is naturalto call this error measurethe mean-squaredBellmanerror:
LP (s) [ E7r{rt+l + yVt(St+l) 1St= S} Vt(S)]
a V6' [ E7r{ rt+l + yVt (St+ l ) } - Vt(St) ]
= 9t + a [ E7r{ rt+l + yVt (St+ l ) } - Vt(St)] [V6 Vt(St) - E7r{V6 Vt(St+ l )}] ,
where the expectedvaluesare implicitly conditional on St. This updateis guaranteed
to convergeto a minimum of the mean-squaredBellman error underthe usual
. However, this method is feasible only for
deterministicsystemsor when a model is available. The problemis that the update
aboveinvolvesthe next state, St+ l , appearingin two expectedvaluesthat are multiplied together. To get an unbiasedsampleof the product, oneneedstwo independent
samplesof the next state, but during normal interactionwith the environmentonly
oneis obtained. Becauseof this, the methodis probablylimited in practiceto casesin
which a model is available(to producea secondsample). In practice, this methodis
alsosometimesslow to converge.To handlethat problem, Baird ( 1995) hasproposed
combiningthis methodparametricallywith conventionalTO methods.
Exercise 8.9 (programming) Look up the paperby Baird ( 1995) on the Internet
and obtain his counterexamplefor Q-Ieaming. Implement it and demonstratethe
At this point you may be wonderingwhy we bother with bootstrapping methods
at all. Nonbootstrapping methodscan be used with function approximationmore
reliably and over a broaderrangeof conditionsthan bootstrapping methods. Nonbootstrapping methodsachievea lower asymptoticerrorthanbootstrapping methods,
evenwhenbackupsaredoneaccordingto the on-policy distribution. By usingeligibility
tracesand A = 1, it is evenpossibleto implementnonbootstrapping methods
on-line, in a step-by-stepincrementalmanner. Despiteall this, in practicebootstrapping methodsare usuallythe methodsof choice.
, bootstrapping methodsusually perform much better
than nonbootstrapping methods. A convenientway to make suchcomparisonsis to
usea m methodwith eligibility tracesand vary A from 0 (purebootstrapping) to 1
Figure8.15 Theeffectof ).. on reinforcement
, the lower the curve. The two left panelsare applications
()..) algorithmandtile coding,witheitherreplacing
task(Example3.4) fromanearlierstudy(Sutton
). Figure 8.15 summarizesa collection of suchresults. In all
cases, performancebecamemuch worse as )., approached1, the nonbootsttapping
case. The examplein the upperright of the figure is particularly significant in this
regard. This is a policy evaluation(prediction) task and the performancemeasure
usedis root MSE (at the end of eachepisode, averagedover the first 20 episodes
Asymptotically, the )., = 1casemustbebestaccordingto this measure
of the asymptote,we seeit perfonning muchworse.
At this time it is unclearwhy methodsthat involve somebootsttappingperform
so much betterthan pure nonbootsttappingmethods. It could be that bootsttapping
methodslearnfaster, or it could be thatthey actuallylearnsomethingbetterthannonbootstrapping methods.The availableresultsindicatethat nonbootstrapping methods
arebetterthanbootstrapping methodsat reducingMSE from the true valuefunction,
but reducingMSE is not necessarilythe most important goal. For example, if you
add 1000to the true action-valuefunction at all state- action pairs, then it will have
very poor MSE, but you will still get the optimal policy. Nothing quite that simple
is going on with bootstrapping methods, but they do seemto do somethingright. We
expectthe understandingof theseissuesto improveasresearchcontinues.
Reinforcementlearningsystemsmust be capableof generalizationif they are to be
applicableto artificial intelligenceor to large engineeringapplications. To achieve
this, any of a broadrangeof existing methodsfor supervised
can be used simply by tteating each backup as a training example.
Gradient-descentmethods, in particular, allow a natural extensionto function approximati
of all the techniquesdevelopedin previouschapters,including eligibility
traces. Linear gradient-descentmethodsare particularly appealingtheoretically
and work well in practicewhen provided with appropriatefeatures. Choosingthe
featuresis one of the most important ways of adding prior domain knowledgeto
reinforcementlearning systems. Linear methodsinclude radial basisfunctions, tile
coding, and Kanervacoding. Backpropagationmethodsfor multilayer neural networks
aremethodsfor nonlinear gradient-descentfunction approximation.
For the most part, the extensionof reinforcementlearning prediction and control
methodsto gradient-descentforms is straightforward. However, there is an interesting
interaction betweenfunction approximation, bootstrapping, and the onpolicy/off -policy distinction. Bootstrapping methods, such as OP and TD (A) for
A < I , work reliably in conjunction with function approximationover a narrower
range of conditions than do nonbootstrappingmethods. Becausethe control case
hasnot yet yieldedto theoreticalanalysis, researchhasfocusedon the valueprediction
problem. In this case, on-policy bootstrapping methodsconvergereliably with
linear gradient-descentfunction approximationto a solution with mean-squarederror
times the minimum possibleerror. Off -policy bootstrapping
methods, on the other hand, may divergeto infinite error. Severalapproach
beenexploredto making off -policy bootstrapping methodswork with function approximati
, but this is still an open researchissue. Bootstrapping methodsare of
persistentinterestin reinforcementlearning, despitetheir limited theoreticalguarantees
, becausein practicetheyusuallywork significantlybetterthannonbootstrapping
Despiteour tteabnentof generalizationandfunction approximationlate in the book, they
the field hasfocusedon the tabularcase, aswe haveherefor the first sevenchapters.Bertsekas
and Tsitsiklis ( 1996) presentthe stateof the art in function approximationin reinforcement
learning, and the collection of papersby Boyan, Moore, and Sutton ( 1995) is also useful
Gradient-descentmethodsfor minimizing mean-squarederror in supervisedlearning
are well known. Widrow and Hoff ( 1960) introducedthe least-mean-square(LMS)
algorithm, which is the prototypical incrementalgradient-descentalgorithm. Details
of this and relatedalgorithmsare provided in many texts (e.g., Widrow and Stearns,
1985; Bishop, 1995; Duda andHart, 1973) .
Gradient- descentanalysesof ill learningdatebackat leastto Sutton( 1988). Methods
more sophisticatedthan the simple gradient-descentmethodscoveredin this section
have also been studiedin the context of reinforcementlearning, such as quasiNewton
, 1990) andrecursive-least-squaresmethods(Bradtke, 1993,
; Bradtke, Y dstie, andBarto, 1994) . BertsekasandTsit
siklis ( 1996) providea good discussionof thesemethods.
The earliest use of state aggregationin reinforcementlearning may have been
Michie and Chamberss BOXES system( 1968) . The theory of state aggregationin
reinforcementlearninghasbeendevelopedby Singh, Jaakkola, andJordan( 1995) and
ill ()") with linear gradient-descentfunction approximationwasfirst exploredby Sutton ( 1984, 1988), who provedconvergenceof ill (O) in the meanto the minimal MSE
solution for the casein which the featurevectors, {~s: se 8 }, are linearly independent
. Convergencewith probability 1 for general)., wasprovedby severalresearchers
at about the sametime ( peng, 1993; Dayan and Sejnowski, 1994; Tsitsiklis, 1994;
Gurvits, Lin , and Hanson, 1994). In addition, Jaakkola, Jordan, and Singh ( 1994)
underon-line updating. All of theseresultsassumedlinearly independent
featurevectors, which implies at leastas manycomponentsto 9, as thereare
states. Convergenceof linear ill ()") for the more interestingcaseof general(dependent
) featurevectorswasfirst shownby Dayan( 1992). A significantgeneralizationand
strengtheningof Dayans result was provedby Tsitsiklis and Van Roy ( 1997a). They
provedthe main result presentedin Section8.3, the boundon the asymptoticerror
Generalization and Function Approximation
m ()..) and other bootstrapping methods. Recentlythey extendedtheir analysisto the
undiscountedcontinuingcase(Tsitsiklis and VanRoy, 1997b).
Our presentationof the rangeof possibilitiesfor linear function approximationis
basedon that by Barto ( 1990). The term coarsecoding is due to Hinton ( 1984), and
our Figure 8.2 is basedon one of his figures. Waltz and Fu ( 1965) provide an early
exampleof this type of function approximationin a reinforcementlearningsystem.
Tile coding, includinghashing, wasintroducedby Albus ( 1971, 1981). He described
it in termsof his " cerebellarmodel articulatorcontroller," or CMAC, as tile coding is
known in the literature. The term "tile coding" is new to this book, thoughthe ideaof
describingCMAC in thesetermsis takenfrom Watkins( 1989). Tile coding hasbeen
usedin many reinforcementlearning systems(e.g., Shewchukand Dean, 1990; Lin
, and Kim , 1994; SofgeandWhite, 1992; Tham, 1994;
Sutton, 1996; Watkins, 1989) aswell asin othertypesof learningcontrol systems(e.g.,
, 1990; Kraft, Miller , and Dietz, 1992).
Functionapproximationusing radial basisfunctions(RBFs) has receivedwide attention
ever sincebeing relatedto neuralnetworksby Broomheadand Lowe ( 1988).
Powell ( 1987) reviewedearlier usesof RBFs, and Poggio and Girosi ( 1989, 1990)
extensivelydevelopedand appliedthis approach.
What we call " Kanervacoding" was introducedby Kanerva( 1988) as part of his
more generalidea of sparsedistributed memory. A good review of this and related
memory modelsis provided by Kanerva( 1993). This approachhas beenpursuedby
Gallant ( 1993) and by SuttonandWhitehead( 1993), amongothers.
Q()..) with function approximationwasfirst exploredby Watkins( 1989). Sarsa()..) with
function approximationwas first explored by Rummery and Niranjan ( 1994). The
mountain- car exampleis basedon a similar task studiedby Moore ( 1990). The results
on it presentedhereare from Sutton( 1996) and Singh and Sutton( 1996).
Convergenceof the control methodspresentedin this sectionhasnot beenproved
(and seemsunlikely for Q()..) given the resultspresentedin Section8.5). Convergence
resultsfor control methodswith stateaggregationand other specialkinds of function
approximationare proved by Tsitsiklis and Van Roy ( 1996), Singh, Jaakkola, and
Baird' s counterexampleis due to Baird ( 1995). Tsitsiklis and Van Roy' s counterexample
is due to Tsitsiklis and Van Roy ( 1997a). Averagingmethodsfor function approximation
are developedby Gordon ( 1995, 1996). Gradient-descentmethodsfor
minimizing the Bellman error are due to Baird, who called them residual-gradient
methods.Otherexamplesof instability with off-policy DP methodsandmorecomplex
methodsof function approximationare given by Boyan and Moore ( 1995). Bradtke
( 1993) givesan examplein which Q-learningusinglinear function approximationin a
linear quadraticregulationproblemconvergesto a destabilizingpolicy.
The use of function approximationin reinforcementlearninggoes back to the early neural
networks of Farley and Clark ( 1954; Clark and Farley, 1955), who used reinforcement
learning to adjust the parametersof linear threshold functions representingpolicies. The
8.8 Bibliographical and Historical Remarks
earliestexamplewe know of in which function approximationmethodswereusedfor learning
value functions was Samuels checkersplayer ( 1959, 1967). Samuelfollowed Shannons
( 1950) suggestionthat a value function did not have to be exact to be a useful guide
selectingmovesin a gameandthat it might beapproximatedby linearcombinationof features.
In addition to linear function approximation, Samuelexperimentedwith lookup tables and
hierarchicallookup tablescalledsignaturetables(Griffith , 1966, 1974; Page, 1977; Biermann,
At about the sametime as Samuels work, Bellman and Dreyfus ( 1959) proposedusing
function approximationmethodswith DP. (It is temptingto think that BellmanandSamuelhad
someinfluenceon oneanother, but we know of no referenceto the otherin the work of either.)
Thereis now a fairly extensiveliteratureon function approximationmethodsand DP, suchas
multigrid methodsand methodsusing splinesandorthogonalpolynomials(e.g., Bellmanand
Dreyfus, 1959; Bellman, Kalaba, and Kotkin, 1973; Daniel, 1976; Whitt, 1978; Reetz, 1977;
SchweitzerandSeidmann, 1985; Chow andTsitsiklis, 1991; KushnerandDupuis, 1992; Rust,
Holland' s ( 1986) classifiersystemuseda selectivefeature-match techniqueto generalize
evaluationinformation acrossstate- action pairs. Each classifier matcheda subsetof states
having specifiedvaluesfor a subsetof features, with the remainingfeatureshaving arbitrary
values(" wild cards" ). Thesesubsetswere then usedin a conventionalstate-aggregationapproach
to function approximation. Holland s idea was to usea geneticalgorithm to evolvea
set of classifiersthat collectively would implementa useful action-value function. Holland s
ideasinfluencedthe early researchof the authorson reinforcementlearning, but we focused
es to function approximation. As function approximators
in scalingand in representingsmoothfunctionsefficiently. In addition, the matchingrules of
classifierscan implementonly aggregationboundariesthat are parallel to the feature axes.
Perhapsthe most importantlimitation of conventionalclassifiersystemsis that the classifiers
are learnedvia the geneticalgorithm, an evolutionarymethod. As we discussedin ChapterI ,
thereis availableduring learningmuchmoredetailedinformationabouthow to learnthancan
be usedby evolutionarymethods.This perspectiveled us to insteadadaptsupervisedlearning
methodsfor use in reinforcementlearning, specificallygradient-descentand neuralnetwork
methods. ThesedifferencesbetweenHolland s approachand ours are not surprisingbecause
Holland s ideasweredevelopedduring a periodwhenneuralnetworksweregenerallyregarded
as being too weak in computationalpower to be useful, whereasour work was at the beginning
of theperiodthat sawwidespreadquestioningof that conventionalwisdom. Thereremain
manyopportunitiesfor combiningaspectsof thesedifferent approach
A numberof reinforcementlearningstudiesusingfunction approximationmethodsthat we
have not coveredpreviously should be mentioned. Barto, Sutton, and Brouwer ( 1981) and
Barto and Sutton ( 198Ib) extendedthe idea of an associativememorynetwork (e.g., Kohonen, 1977; Anderson, Silverstein, Ritz, and Jones, 1977) to reinforcementlearning. Hampson ( 1983, 1989) was an early proponentof multilayer neural networksfor learning value
functions. Anderson( 1986, 1987) coupled a m algorithm with the error backpropagation
algorithm to learn a value function.' Barto and Anandan( 1985) introduceda stochasticversion
of Widrow, Gupta, and Maitra s ( 1973) selectivebootstrapalgorithm, which they called
Generalization and Function Approximation
the associativereward-penalty (AR- P) algorithm. Williams ( 1986, 1987, 1988, 1992) extended
this type of algorithm to a generalclass of REINFORCEalgorithms, showing that
they perform stochasticgradientascenton the expectedreinforcement.Gullapalli ( 1990) and
Williams devisedalgorithmsfor learninggeneralizingpolicies for the caseof continuousactions
. Phansalkarand Thathachar( 1995) provedboth local and global convergencetheorems
for modifiedversionsof REINFORCEalgorithms. Christensenand Korf ( 1986) experimented
with regressionmethodsfor modifying coefficientsof linear value function approximations
in the gameof chess. Chapmanand Kaelbling ( 1991) and Tan ( 1991) adapteddecision-tree
methodsfor learning value functions. Explanation-basedlearning methodshave also been
adaptedfor learningvalue functions, yielding compactrepresentations
and Barto, 1990; Dietterichand Flann, 1995).
In this chapterwe developa unified view of methodsthat requirea modelof the environment
, suchas dynamicprogrammingand heuristicsearch, and methodsthat can
be usedwithout a model, such as Monte Carlo and temporal-differencemethods.
We think of the former as planning methodsand of the latter as learning methods
. Although thereare real differencesbetweenthesetwo kinds of methods, there
are also great similarities. In particular, the heart of both kinds of methodsis the
computationof value functions. Moreover, all the methodsare basedon looking
aheadto future events, computing a backed-up value, and then using it to update
an approximatevalue function. Earlier in this book we presentedMonte Carlo and
temporal-differencemethodsas distinct alternatives
chapteris a similar integrationof planningandlearningmethods.Havingestablished
theseas distinct in earlier chapters, we now explorethe extentto which they can be
By a model of the environment we mean anything that an agent can use to predict
how the environment will respond to its actions. Given a state and an action , a model
produces a prediction of the resultant next state and next reward. If the model is
stochastic, then there are several possible next states and next rewards, each with
some probability of occurring . Some models produce a description of all possibilities
and their probabilities ; these we call distribution models. Other models produce just
one of the possibilities , sampled according to the probabilities ; these we call sample
models. For example, consider modeling the sum of a dozen dice. A distribution
generates all possible transitions weighted by their probabilities
policy or path to a goal . Actions cause transitions from state to state, and value
plan into another, and value functions , if any, are defined over the space of plans.
popular kind of planning in artificial intelligence in which the ordering of steps is
presented in this book . It takes the rest of the chapter to develop this view , but
valuefunctionsas a key intennediatesteptowardimproving the policy, and(2) they
computetheir valuefunctionsby backupoperationsappliedto simulatedexperience
This commonstructurecanbe diagrammedasfollows:
Dynamicprogrammingmethodsclearly fit this structure: they makesweepsthrough
the spaceof states, generatingfor eachstatethe distribution of possibletransitions.
Eachdistribution is then usedto computea backed-up value and updatethe states
estimatedvalue. In this chapterwe argue that various other state-spaceplanning
methodsalso fit this structure, with individual methodsdiffering only in the kinds
of backupsthey do, the order in which they do them, andin how long the backed-up
Viewing planningmethodsin this way emphasizestheir relationshipto the learning
methodsthat we have describedin this book. The heart of both learning and
planningmethodsis the estimationof valuefunctionsby backupoperations.The difference
is that whereasplanning usessimulatedexperiencegeneratedby a model,
learning methodsusereal experiencegeneratedby the environment. Of coursethis
differenceleadsto a numberof other differences, for example, in how performance
and in how flexibly experiencecanbe generated
meansthat manyideasand algorithmscanbe transferredbetweenplanningand
learning. In particular, in manycasesa learningalgorithmcan be substitutedfor the
key backupstepof a planningmethod. Learningmethodsrequireonly experienceas
input, and in manycasesthey can be appliedto simulatedexperiencejust as well as
. Figure 9.1 showsa simple exampleof a planningmethodbased
on one-steptabularQ-Iearningand on randomsamplesfrom a samplemodel. This
method, which we call random-sampleone-step tabular Q-planning, convergesto
Figure9.1 Random-sampleone-steptabularQ-planning.
the optimal policy for the model underthe sameconditionsthat one-steptabularQlearningconvergesto the optimal policy for the real environment(eachstate- action
pair must be selectedan infinite numberof times in Step 1, and a must decrease
In additionto the unified view of planningand learningmethods, a secondtheme
in this chapteris the benefitsof planning in small, incrementalsteps. This enables
planningto be interruptedor redirectedat any time with little wastedcomputation,
which appearsto be a key requirementfor efficiently intermixing planning with
acting and with learning of the model. More surprisingly, later in this chapterwe
presentevidencethat planningin very small stepsmay be the mostefficientapproach
evenon pureplanningproblemsif the problemis too largeto be solvedexactly.
When planning is done on-line, while interactingwith the environment, a number
of interestingissuesarise. New informationgainedfrom the interactionmay change
the model and therebyinteractwith planning. It may be desirableto customizethe
planningprocessin someway to the statesor decisionscurrently underconsideration
, or expectedin the nearfuture. If decision-making and model-learningare both
es, then the availablecomputationalresourcesmay
needto be divided betweenthem. To beginexploringtheseissues, in this sectionwe
presentDyna-Q, a simple architectureintegratingthe major functionsneededin an
on-line planningagent. Eachfunction appearsin Dyna-Q in a simple, almosttrivial ,
form. In subsequentsectionswe elaboratesomeof the alternateways of achieving
eachfunction andthe trade-offs betweenthem. For now, we seekmerelyto illustrate
Within a planningagent, thereare at leasttwo roles for real experience
usedto improvethe model (to makeit more accuratelymatchthe real environment)
andit canbe usedto directly improvethe valuefunctionandpolicy usingthe kindsof
reinforcementlearningmethodswe havediscussedin previouschapters.The former
we call model-learning, and the latter we call direct reinforcementlearning (direct
RL). The possiblerelationshipsbetweenexperience
summarizedin Figure 9.2. Each arrow showsa relationshipof influenceand presumed
improvement. Note how experiencecan improve value and policy functions
either directly or indirectly via the model. It is the latter, which is sometimescalled
indirect reinforcementlearning, that is involvedin planning.
9.2 IntegratingPlanning, Acting, and Learning
Figure 9.2 Relationshipsamonglearning, planning, andacting.
Both direct and indirect methodshave advantagesand disadvantages
methodsoften makefuller useof a limited amountof experienceand thus achieve
a better policy with fewer environmentalinteractions. On the other hand, direct
methodsare muchsimplerandarenot affectedby blasesin the designof the model.
Somehavearguedthat indirect methodsare always superiorto direct ones, while
othershavearguedthat direct methodsare responsiblefor most humanand animal
learning. Relateddebatesin psychology and AI concern the relative importance
of cognition as opposedto trial-and-error learning, and of deliberativeplanning
as opposedto reactivedecision-making. Our view is that the contrastbetweenthe
alternativesin all thesedebateshasbeenexaggerated
emphasized deepsimilarities betweendynamic
programmingand temporal-differencemethods, eventhoughone was designedfor
planningandthe other for modelfreelearning.
es shown in Figure 9.2- planning, acting,
model-learning, and direct RL - all occurring continually. The planning method
is the random-sample one-step tabular Q-planning method given in Figure 9.1.
The direct RL methodis one-step tabular Q-Iearning. The model-learning method
is also table-basedand assumesthe world is detenninistic. After each transition
S" a, - + S' +l , r '+l , the model recordsin its table entry for s" a, the predictionthat
S'+l , r ' +l will detenninisticallyfollow. Thus, if the model is querledwith a stateactionpair that hasbeenexperiencedbefore, it simply returnsthe last-observednext
stateand next rewardas its prediction. During planning, the Q-planning algorithm
randomly samplesonly from state-action pairs that have previously been experienced
(in Step 1), so the model is neverquerledwith a pair about which it has no
The overall architectureof Dyna agents, of which the Dyna-Q algorithm is one
example, is shownin Figure 9.3. The centralcolumnrepresentsthe basicinteraction
betweenagentand environment, giving rise to a trajectoryof real experience
arrow on the left of the figure representsdirect reinforcementlearning operating
on real experienceto improve the value function and the policy. On the right are
es. The model is learnedfrom real experienceand gives rise
. We usethe term searchcontrol to refer to the processthat
selectsthe startingstatesandactionsfor the simulatedexperiencesgeneratedby the
model. Finally, planning is achievedby applying reinforcementlearning methods
to the simulatedexperiencesjust as if they had really happened
Dyna Q, the samereinforcementlearningmethodis usedboth learningfrom real
experienceand for planningfrom simulatedexperience
methodis thus the " final commonpath" for both learning and planning. Learning
and planning are deeply integratedin the sensethat they sharealmostall the same
machinery,differing only in the sourceof their experience
Conceptually, planning, acting, model learning, and direct RL occur simultaneously
andin parallelin Dynaagents.For concreteness
9.2 IntegratingPlanning, Acting, and Learning
Initialize Q (s , a ) and Model (s , a ) for all sE 8 and a E A (s )
(c ) Execute action a ; observe resultant state, S , and reward , r
(d ) Q (s , a ) +- Q (s , a ) + a [ r + y maxa' Q (s , a )
Q (s , a ) +- Q (s , a ) + a [ r + y maxa, Q (S , a )
Figure 9.4 Dyna Q algorithm . Model (s , a ) denotes the contents of the model (predicted
state and reward ) for state- action pairs , a . Direct reinforcement learning , model - learning , and
planning are implemented by steps (d ), (e), and (f), respectively . If (e) and (f)
the remaining algorithm would be ne step tabular Q learning
In Dyna-Q, the acting, model-learning, anddirect RL process
in eachstepcanbe devotedto theplanningprocess,
inherentlycomputationintensive. Let us assumethat thereis time in eachstep, after acting, model-learning,
and direct RL, to completeNiterations (Steps 1- 3) of the Q-planning algorithm.
Figure9.4 showsthe completealgorithmfor Dyna Q.
Example 9.1: Dyna Maze Considerthe simple mazeshowninset in Figure 9.5.
In eachof the 46 statesthere are four actions, up, down, right , and left , which
takethe agentdeterministicallyto the correspondingneighboringstates,exceptwhen
movementis blockedby an obstacleor the edgeof the maze, in which casethe agent
remainswhereit is. Rewardis zeroon all transitions, exceptthoseinto the goal state,
on which it is + 1. After reachingthe goal state(G), the agentreturnsto the startstate
(8 ) to begina new episode. This is a discounted,episodictaskwith y = 0.95.
The main part of Figure 9.5 showsaveragelearning curvesfrom an experiment
in which Dyna-Q agentswere applied to the mazetask. The initial action values
were zero, the step-size parameterwas a = 0.1, and the explorationparameterwas
f = 0.1. When selectinggreedily amongactions, ties were broken randomly. The
agentsvariedin the numberof planningsteps, N , they performedper real step. For
each N , the curvesshow the numberof stepstaken by the agentin eachepisode,
Figure 9.5 A simplemaze(inset) andthe averagelearningcurvesfor Dyna-Q agentsvarying
in their numberof planningstepsper real step. The task is to travel from S to G asquickly as
averagedover 30 repetitionsof the experiment.In eachrepetition, the initial seedfor
the randomnumbergeneratorwasheld constantacrossalgorithms. Becauseof this,
the first episodewasexactlythe same(about 1700steps) for all valuesof N , and its
dataarenot shownin the figure. After the first episode,perfonnanceimprovedfor all
valuesof N , but muchmorerapidly for largervalues. Recallthat the N = 0 agentis a
nonplanningagent, utilizing only direct reinforcementlearning(one-steptabularQlearning). This wasby far the slowestagenton this problem, despitethe fact that the
parametervalues(a andf. ) wereoptimizedfor it. The nonplanningagenttook about
25 episodesto reach(f.-)optimal performance
, andthe N 50 agenttook only threeepisodes
Figure9.6 showswhy the planningagentsfound the solution so much fasterthan
thenonplanningagent. Shownarethe policiesfoundby the N = 0 andN = 50 agents
halfway throughthe secondepisode. Without planning (N = 0), eachepisodeadds
only oneadditionalstepto the policy, andsoonly onestep(the last) hasbeenlearned
so far. With planning, again only one step is learnedduring the first episode, but
here during the secondepisodean extensivepolicy hasbeendevelopedthat by the
Figure 9.6 Policiesfound by planningandnonplanningDyna Q agentshalfway throughthe
secondepisode. The arrowsindicatethe greedyaction
episodes end will reach almost back to the start state. This policy is built by the
planning processwhile the agentis still wanderingnearthe start state. By the end
of the third episodea completeoptimal policy will have been found and perfect
In Dyna-Q, learning and planning are accomplishedby exactly the samealgorithm
, operatingon real experiencefor learning and on simulatedexperiencefor
planning. Becauseplanningproceedsincrementally, it is trivial to intermix planning
andacting. Both proceedasfast astheycan. The agentis alwaysreactiveandalways
deliberative, respondinginstantly to the latest sensoryinformation and yet always
planningin the background. Also ongoingin the backgroundis the model learning
process. As new information is gained, the model is updatedto better match reality
, the ongoing planning processwill gradually computea
different way of behavingto matchthe new model.
Exercise 9.1 The nonplanningmethod looks particularly poor in Figure 9.6 because
it is a one-stepmethod; a methodusing eligibility traceswould do better. Do
you think an eligibility tracemethodcould do aswell asthe Dyna method? Explain
In the mazeexamplepresentedin the previoussection, the changesin the modelwere
relatively modest. The modelstartedout empty, andwasthenfilled only with exactly
correctinformation. In general, we cannotexpectto be so fortunate. Models may be
incorrectbecausethe environmentis stochasticandonly a limited numberof samples
havebeenobserved, becausethe model was learnedusing function approximation
that has generalizedimperfectly, or simply becausethe environmenthas changed
and its new behaviorhas not yet beenobserved. When the model is incorrect, the
planningprocesswill computea suboptimalpolicy.
In somecases, the suboptimalpolicy computedby planningquickly leadsto the
discoveryandcorrectionof the modelingerror. This tendsto happenwhenthe model
is optimistic in the senseof predictinggreaterrewardor betterstatetransitionsthan
areactually possible. The plannedpolicy attemptsto exploit theseopportunitiesand
in doing so discoversthat they do not exist.
Example 9.2: Blocking Maze A mazeexampleillustrating this relatively minor
kind of modelingerror and recoveryfrom it is shownin Figure 9.7. Initially , there
is a short path from start to goal, to the right of the barrier, as shownin the upper
left of the figure. After 1000time steps, the short path is " blocked," and a longer
path is openedup along the left-handside of the barrier, as shownin upperright of
the figure. The graph showsaveragecumulativerewardfor Dyna-Q and two other
Dyna agents. The first part of the graphshowsthat all threeDyna agentsfound the
short path within 1000steps. When the environmentchanged, the graphsbecome
flat, indicating a period during which the agentsobtainedno rewardbecausethey
werewanderingaroundbehindthe barrier. After a while, however, they wereableto
find the new openingandthe new optimal behavior.
Greaterdifficulties arise when the environmentchangesto becomebetter than it
wasbefore, and yet the formerly correctpolicy doesnot revealthe improvement. In
thesecasesthe modelingerror may not be detectedfor a long time, if ever, aswe see
Example 9. 3: Shortcut Maze The problemcausedby this kind of environmental
changeis illustratedby the mazeexampleshownin Figure9.8. Initially , the optimal
pathis to go aroundthe left sideof the barrier(upperleft). After 3000steps, however,
a shorterpath is openedup along the right side, without disturbingthe longer path
(upperright). The graphshowsthat two of the threeDyna agentsneverswitchedto
the shortcut. In fact, they neverrealizedthat it existed. Their modelssaid that there
was no shortcut, so the more they planned, the less likely they were to stepto the
right and discoverit. Even with an f.-greedypolicy, it is very unlikely that an agent
will takeso manyexploratoryactionsasto discoverthe shortcut.
The general problem here is another version of the conflict between exploration
andexploitation. In a planningcontext, explorationmeanstrying actionsthat
improvethe model, whereasexploitationmeansbehavingin the optimal way given
the currentmodel. We want the agentto exploreto find changesin the environment,
of Dynaagentson a blockingtask. Theleft environment
. DynaACis aDynaagentthatusesanactorbonusthatencourages
Figure 9.8 Averageperfonnanceof Dyna agentson a shortcut
but not so much that performanceis greatly degraded
/ exploitation conflict, there probably is no solution that is both perfect and
practical, but simpleheuristicsareoften effective.
The Dyna-Q+ agentthat did solvethe shortcutmazeusesone suchheuristic. This
agentkeepstrack for each state- action pair of how many time stepshaveelapsed
sincethe pair waslast tried in a real interactionwith the environment.The moretime
that has elapsed, the greater(we might presume) the chancethat the dynamicsof
this pair haschangedandthat the modelof it is incorrect. To encouragebehaviorthat
testslong-untriedactions, a special" bonusreward" is givenon simulatedexperiences
involving theseactions. In particular, if the modeledrewardfor a transitionis r , and
the transitionhasnot beentried in n time steps, thenplanningbackupsaredoneasif
that transitionproduceda rewardof r + KIn , for somesmallK. This encouragesthe
agentto keeptestingall accessiblestatetransitionsandevento plan long sequences
of actionsin order to carry out suchtests. Of courseall this testinghasits cost, but
in many cases, as in the shortcutmaze, this kind of computationalcuriosity is well
Exercise9.2 Why did the Dyna agentwith explorationbonus, Dyna-Q+, perform
better in the first phaseas well as in the secondphaseof the blocking and shortcut
Exercise 9.3 Careful inspectionof Figure 9.8 revealsthat the differencebetween
Dyna-Q+ and Dyna-Q narrowedslightly over the first part of the experiment. What
Exercise 9.4 (programming) The exploration bonus describedabove actually
changesthe estimatedvaluesof statesand actions. Is this necessary
bonusKIn was usednot in backups, but solely in action selection. That is, suppose
the action selectedwasalwaysthat for which Q(s, a) + K, Jn; ; wasmaximal. Carry
out a gridworld experimentthat testsand illustratesthe strengthsand weaknesses
In the Dyna agentspresentedin the precedingsections, simulatedttansitions are
startedin state- action pairs selectedunifonnly at randomfrom all previouslyexperienced
pairs. But a uniform selectionis usually not the best; planningcan be much
more efficient if simulatedttansitionsand backupsare focusedon particularstateaction pairs. For example, considerwhat happensduring the secondepisodeof the
first mazetask (Figure 9.6). At the beginningof the secondepisode, only the stateaction pair leadingdirectly into the goal hasa positivevalue; the valuesof all other
pairs are still zero. This meansthat it is pointlessto back up along almostall transitions
, becausethey take the agentfrom one zero-valuedstateto another, and thus
the backupswould have no effect. Only a backupalong a transition into the state
just prior to the goal, or from it into the goal, will changeany values. If simulated
transitionsaregenerateduniformly, thenmanywastefulbackupswill be madebefore
stumblingonto oneof the two usefulones. As planningprogress
whereit would do the most good. In the much largerproblemsthat are our real objective
, the numberof statesis so largethat an unfocusedsearchwould be extremely
This examplesuggeststhat searchmight be usefullyfocusedby working backward
from goal states.Of course, we do not really want to useany methodsspecificto the
ideaof " goal state." We want methodsthat work for generalrewardfunctions. Goal
statesarejust a specialcase, convenientfor stimulatingintuition. In general, we want
to work back not just from goal statesbut from any statewhosevaluehaschanged.
Assumethat the valuesareinitially correctgiventhe model, asthey werein the maze
exampleprior to discoveringthe goal. Supposenow thatthe agentdiscoversa change
in the environmentand changesits estimatedvalueof one state. Typically, this will
imply that the valuesof manyotherstatesshouldalsobe changed,but the only useful
one-stepbackupsarethoseof actionsthat leaddirectly into the onestatewhosevalue
hasalreadybeenchanged.If the valuesof theseactionsare updated, then the values
of the predecessorstatesmay changein turn. If so, then actionsleading into them
needto be backedup, and then their predecessorstatesmay havechanged. In this
way one can work backwardfrom arbitrary statesthat havechangedin value, either
performingusefulbackupsor terminatingthe propagation.
As the frontier of useful backupspropagatesbackward, it often grows rapidly,
producingmany state- action pairs that could usefully be backedup. But not all of
thesewill be equally useful. The valuesof some statesmay have changeda lot,
whereasothershavechangedlittle. The predecessor
a lot are more likely to also changea lot. In a stochasticenvironment, variationsin
estimatedtransitionprobabilitiesalsocontributeto variationsin the sizesof changes
and in the urgencywith which pairs needto be backedup. It is naturalto prioritize
the backupsaccordingto a measureof their urgency, and perform them in order
of priority. This is the idea behindprioritized sweeping. A queueis maintainedof
every state- action pair whoseestimatedvalue would changenontrivially if backed
up, prioritized by the size of the change. When the top pair in the queueis backed
pairsis computed.If the effect is greaterthan
( e ) p ~ Ir + y maxa ' Q ( s , a ) - Q ( s , a ) l .
Q ( s , a ) + a [ r + y maxa , Q ( s , a ) to lead to s :
somesmall threshold, then the pair is insertedin the queuewith the new priority (if
there is a previousenb'y of the pair in the queue, then insertionresultsin only the
higher priority enb'y remainingin the queue). In this way the effectsof changesare
efficiently propagatedbackwarduntil quiescence
deterministicenvironmentsis given in Figure9.9.
Example 9.4: Prioritized Sweeping on Mazes Prioritized sweepinghas been
found to dramatically increasethe speedat which optimal solutionsare found in
mazetasks, often by a factor of 5 to 10. A typical exampleis shownin Figure 9.10.
Thesedataare for a sequenceof mazetasksof exactlythe samestructureasthe one
shownin Figure9.5, exceptthat they vary in the grid resolution. Prioritizedsweeping
maintaineda decisiveadvantageover unprioritizedDyna-Q. Both systemsmadeat
most N = 5 backupsper environmentalinteraction.
Example 9.5: Rod Maneuvering The objectivein this task is to maneuvera rod
aroundsomeawkwardlyplacedobstaclesto a goal position in the fewestnumberof
steps( Figure9.11). The rod canbe translatedalong its long axis or perpendicularto
that axis, or it canberotatedin eitherdirectionaroundits center. The distanceof each
movementis approximately1/20 of the work space, andthe rotationincrementis 10
. Translationsare deterministicand quantizedto one of 20 x 20 positions.
The figure showsthe obstaclesand the shortestsolution from start to goal, found
Figure 9.11 A rod-maneuveringtask and its solution by prioritized sweeping Reprinted
by prioritized sweeping. This problemis still deterministic, but hasfour actionsand
14,400 potentialstates(someof theseareunreachable
problemis probablytoo largeto be solvedwith unprioritizedmethods.
Prioritized sweepingis clearly a powerful idea, but the algorithmsthat havebeen
developedso far appearnot to extendeasily to more interestingcases.The greatest
problem is that the algorithmsappearto rely on the assumptionof discretestates.
When a changeoccursat one state, thesemethodsperform a computationon all the
predecessorstatesthat may havebeenaffected. If function approximationis usedto
learn the model or the value function, then a single backupcould influencea great
manyother states. It is not apparenthow thesestatescould be identifiedor processed
efficiently. On the other hand, the generalidea of focusing searchon the states
believedto havechangedin value, and then on their predecessors
to be valid in general. Additional researchmay producemore generalversionsof
Extensions of prioritized sweeping to stochasticenvironmentsare relatively
straightforward.The model is maintainedby keepingcountsof the numberof times
eachstate- action pair has beenexperiencedand of what the next stateswere. It is
naturalthen to backupeachpair not with a samplebackup, as we havebeenusing
so far, but with a full backup, taking into accountall possiblenext statesand their
The examplesin the previoussectionsgive someideaof the rangeof possibilitiesfor
combiningmethodsof learningand planning. In the rest of this chapter, we analyze
someof the componentideasinvolved, startingwith the relative advantagesof full
Much of this book hasbeenaboutdifferentkinds of backups,andwe haveconsidered
a greatmanyvarieties. Focusingfor the momenton one-stepbackups,they vary
primarily along threebinary dimensions.The first two dimensionsare whetherthey
back up statevaluesor action valuesand whetherthey estimatethe valuefor the optimal
policy or for an arbitrary given policy. Thesetwo dimensionsgive rise to four
classesof backupsfor approximatingthe four valuefunctions, Q* , V * , Q7r, and V7r.
The other binary dimensionis whetherthe backupsarefull backups, consideringall
possibleeventsthat might happen,or samplebackups,consideringa singlesampleof
what might happen. Thesethreebinary dimensionsgive rise to eight cases,sevenof
which correspondto specificalgorithms, as shownin Figure 9.12. (The eighth case
doesnot seemto correspondto any useful backup.) Any of theseone-stepbackups
canbe usedin planningmethods. The Dyna-Q agentsdiscussedearlier use Q* sample
backups, but they couldjust aswell use Q full backups, or eitherfull or sample
QIr backups.The Dyna-AC systemusesVIrsamplebackupstogetherwith a learning
policy structure. For stochasticproblems, prioritized sweepingis alwaysdoneusing
When we introducedone-step samplebackupsin Chapter6, we presentedthem
as substitutesfor full backups. In the absenceof a distribution model, full backups
are not possible, but samplebackupscan be doneusing sampletransitionsfrom the
environmentor a samplemodel. Implicit in that point of view is that full backups,
if possible, are preferableto samplebackups. But are they? Full backupscertainly
yield a better estimatebecausethey are uncorruptedby samplingerror, but they
also require more computation, and computationis often the limiting resourcein
planning. To properly assessthe relative merits of full and samplebackupsfor
planningwe must conttol for their different computationalrequirements
the specialcaseof discretestatesand actions, a table-lookup representationof the
approximatevaluefunction, Q, anda modelin the form of estimatedstate-transition
~/' and expectedrewards, ~ :sl. The full backupfor a state-action
Q(s, a) +- L fj>s~' [ ~ :s' + y ~~ Q(s , a )] .
The corresponding sample backup for s, a , given a sample next state, s' , is the Qlearning - like update:
where a is the usual positive step- size parameter and the model s expected value
of the reward , ~ :s" is used in place of the sample reward that is used in applying
The difference between these full and sample backups is significant to the extent
that the environment is stochastic, specifically , to the extent that , given a state and
action , many possible next states may occur with various probabilities . If only one
next state is possible, then the full and sample backups given above are identical
(taking a = 1) . If there are many possible next states, then there may be significant
differences. In favor of the full backup is that it is an exact computation , resulting in
a new Q(s , a ) whose correctness is limited only by the correctness of the Q(s' , a ' ) at
successorstates. The sample backup is in addition affected by sampling error. On the
other hand, the sample backup is cheaper computationally because it considers only
one next state, not all possible next states. In practice , the computation required by
backup operations is usually dominated by the number of state- action pairs at which
Q is evaluated. For a particular starting pair s , a , let b be the branching factor , the
number of possible next states, s' , for which Ii>S~I > O. Then a full backup of this pair
requires roughly b times as much computation as a sample backup .
H there is enough time to complete a full backup, then the resulting estimate is
generally better than that of b sample backups because of the absence of sampling
error. But if there is insufficient time to complete a full backup , then sample backups
are always preferable because they at least make some improvement in the value
estimate with fewer than b backups. In a large problem with many state- action pairs,
we are often in the latter situation . With so many state- action pairs, full backups of
all of them would take a very long time . Before that we may be much better off with a
few sample backups at many state- action pairs than with full backups at a few pairs .
Given a unit of computational effort , is it better devoted to a few full backups or to
Figure 9.13 shows the results of an analysis that suggests an answer to this question
. It shows the estimation error as a function of computation time for full and
sample backups for a variety of branching factors, b. The case considered is that in
which all b successor states are equally likely and in which the error in the initial
Figure 9.13 Comparisonof efficiencyof full and samplebackups.
estimateis 1. The valuesat the next statesareassumedcorrect, so the full backupreduces
the error to zero upon its completion. In this case, samplebackupsreducethe
wheret is the numberof samplebackupsthat havebeen
, i.e., a = I / t ). The key observationis that
for moderatelylargeb the error falls dramaticallywith a tiny fraction of b backups.
For thesecases, many state- action pairs could havetheir valuesimproveddramatically
, to within a few percentof the effect of a full backup, in the sametime that one
state- actionpair could be backedup fully .
The advantageof samplebackupsshownin Figure 9.13 is probablyan underestimate
of the real effect. In a real problem, the valuesof the successorstateswould
themselvesbe estimatesupdatedby backups. By causingestimatesto be moreaccurate
sooner, samplebackupswill havea secondadvantagein that the valuesbacked
the successorstateswill be moreaccurate.Theseresultssuggestthat sample
backupsare likely to be superiorto full backupson problemswith large stochastic
branchingfactorsandtoo manystatesto be solvedexactly.
Exercise9.5 The analysisaboveassumedthat all of the b possiblenext stateswere
equally likely to occur. Supposeinsteadthat the distributionwashighly skewed,that
someof theb statesweremuchmorelikely to occurthanmost. Wouldthis strengthen
or weakenthe casefor samplebackupsover full backups? Supportyour answer.
In this sectionwe comparetwo waysof distributingbackups.The classicalapproach,
from dynamicprogramming, is to perform sweepsthroughthe entire state(or stateaction) space, backing up eachstate(or state- action pair) once per sweep. This is
problematicon large tasks becausethere may not be time to completeeven one
sweep. In many tasksthe vast majority of the statesare irrelevantbecausethey are
visitedonly undervery poorpoliciesor with very low probability. Exhaustivesweeps
implicitly devoteequaltime to all partsof the statespaceratherthanfocusingwhere
it is needed.As we discussedin Chapter4, exhaustivesweepsandtheequaltreatment
of all statesthat they imply are not necessarypropertiesof dynamicprogramming.
In principle, backupscanbe distributedany way onelikes (to assureconvergence
statesor state action pairs must be visited in the limit an infinite numberof times),
but in practiceexhaustivesweepsareoften used.
The secondapproachis to samplefrom the stateor state- action spaceaccording
to somedistribution. One could sampleuniformly, as in the Dyna-Q agent, but this
would sufferfrom someof the sameproblemsasexhaustivesweeps.More appealing
is to distribute backupsaccordingto the on-policy distribution, that is, according
to the distribution observedwhen following the current policy. One advantageof
this distribution is that it is easily generated
anywhere just keepssimulating. In eithercase, sample
statetransitionsandrewardsaregivenby the model, andsampleactionsaregivenby
the currentpolicy. In other words, one simulatesexplicit individual trajectoriesand
performsbackupsat the stateor state- action pairs encounteredalong the way. We
call this way of generatingexperienceandbackupstrajectory sampling.
It is hard to imagine any efficient way of distributing backupsaccordingto the
on-policy distribution other than by trajectory sampling. If one had an explicit representation
of the on-policy distribution, then one could sweepthrough all states,
weighting backupof eachaccordingto the on-policy distribution, but this leaves
us againwith all the computationalcostsof exhaustivesweeps
sampleandupdateindividual state actionpairsfrom the distribution, but evenif this
could be doneefficiently, what benefit would this provide over simulatingtrajectories
? Even knowing the on-policy distribution in an explicit form is unlikely. The
distribution changeswheneverthe policy changes
requirescomputationcomparableto a completepolicy evaluation. Considerationof
suchother possibilitiesmakestrajectorysamplingseemboth efficient andelegant.
Is the on-policy distribution of backupsa good one? Intuitively it seemslike a
good choice, at least better than the uniform distribution. For example, if you are
learningto play chess, you studypositionsthat might arisein real games,not random
positionsof chesspieces. The latter may be valid states, but to be able to accurately
value them is a different skill from evaluatingpositions in real games. We also
know from Chapter8 that the on-policy distributionhassignificantadvantages
function approximationis used. At the currenttime this is the only distribution for
which we can guaranteeconvergencewith generallinear function approximation.
Whetheror not function approximationis used, onemight expecton-policy focusing
to significantlyimprovethe speedof planning.
Focusingon the on-policy distributioncould be beneficialbecauseit causesvast,
uninterestingpartsof the spaceto be ignored, or it could be detrimentalbecauseit
causesthe sameold partsof the spaceto be backedup over andover. Weconducteda
small experimentto assessthe effect empirically. To isolatethe effect of the backup
distribution, we usedentirely one-stepfull tabularbackups,asdefinedby (9.1). In the
uniform casewe cycledthroughall state- actionpairs, backingup eachin place, and
in the on-policy casewe simulatedepisodes
occurredunderthe current f.-greedypolicy (f. = 0.1). The taskswere undiscounted
episodic tasks, generatedrandomly as follows. From each of the 181states, two
actionswerepossible, eachof which resultedin oneof b (the branchingfactor ) next
states, all equally likely, with a different randomselectionof b statesfor eachstateaction pair. In addition, on all transitionstherewasa 0.1 probability of transitionto
the tenninal state, ending the episode. The expectedrewardon eachtransitionwas
selectedfrom a Gaussiandistribution with mean0 and variance1. At any point in
the planningprocessone can stop and exhaustivelycomputeV1f(so), the true value
of the start stateunderthe greedypolicy, 1r, given the currentaction-valuefunction,
Q, as an indication of how well the agentwould do on a new episodeon which it
actedgreedily(all the while assumingthe modelis correct).
The upper part of Figure 9.14 shows results averagedover 200 sample tasks
with 1000statesand branchingfactors of 1, 3, and 10. The quality of the policies
found is plotted as a function of the numberof full backupscompleted. In all cases,
samplingaccordingto the on-policy distribution resultedin fasterplanninginitially
andretardedplanningin the long run. The effect was stronger, andthe initial period
of fasterplanningwaslonger, at smallerbranchingfactors. In otherexperiments
found that theseeffectsalso becamestrongeras the numberof statesincreased
example, the lower part of Figure 9.14 showsresultsfor a branchingfactor of 1 for
taskswith 10,000 states.In this casethe advantageof on-policy focusingis largeand
All of theseresults make sense. In the short term, sampling accordingto the
on-policy distribution helps by focusing on statesthat are near descendants
start state. If there are many statesand a small branchingfactor, this effect
large and long lasting. In the long run, focusing on the on policy distribution may
hurt becausethe commonly occurring statesall alreadyhave their correct values.
, whereassamplingother statesmay actuallyperform some
usefulwork. This presumablyis why the exhaustive
in the long run, at leastfor small problems. Theseresultsarenot conclusivebecause
they areonly for problemsgeneratedin a particular, randomway, but theydo suggest
that samplingaccordingto the on-policy distribution can be a great advantagefor
largeproblems, in particular, for problemsin which a small subsetof the state- action
spaceis visited underthe on-policy distribution.
Exercise9.6 Someof the graphsin Figure9.14 seemto be scallopedin their early
portions, particularlythe uppergraphfor b = I andthe uniform distribution. Why do
you think this is? What aspectsof the datashownsupportyour hypothesis?
Figure 9.14 Relative efficiency of backupsdistributed uniformly acrossthe state space
versusfocusedon simulatedon-policy trajectories. Resultsare for randomlygeneratedtasks
of two sizesand variousbranchingfactors, b.
Exercise9.7 (programming) If you haveaccessto a moderatelylargecomputer,try
replicatingthe experimentwhoseresultsare shownin the lower part of Figure9.14.
Then try the sameexperimentbut with b = 3. Discussthe meaningof your results.
The predominantstate-spaceplanning methodsin artificial intelligenceare collectively
known asheuristic search. Although superficiallydifferent from the planning
methodswe have discussedso far in this chapter, heuristic searchand some of
its componentideascan be combinedwith thesemethodsin useful ways. Unlike
thesemethods, heuristicsearchis not concernedwith changingthe approximate,or
only with making improvedaction selectionsgiven
the current value function. In other words, heuristic searchis planningas part of a
In heuristic search, for eachstateencountered
. The approximatevaluefunction is appliedto the leaf nodesand
thenbackedup towardthe currentstateat the root. The backingup within the search
treeis just the sameasin the max-backups(thosefor V and Q ) discussedthroughout
this book. The backing up stopsat the state action nodesfor the current state.
Oncethe backed-up valuesof thesenodesare computed, the bestof them is chosen
asthe currentaction, andthen all backed-up valuesarediscarded.
In conventionalheuristic searchno effort is madeto savethe backed-up values
by changingthe approximatevaluefunction. In fact, the valuefunction is generally
designedby peopleand neverchangedas a result of search. However, it is natural
to considerallowing the value function to be improvedover time, using either the
backed-up values computedduring heuristic searchor any of the other methods
presentedthroughoutthis book. In a sensewe havetakenthis approachall along. Our
greedyand f. -greedyaction-selectionmethodsare not unlike heuristicsearch, albeit
on a smallerscale. For example, to computethe greedyaction given a model and a
state-valuefunction, we must look aheadfrom eachpossibleactionto eachpossible
next state, backupthe rewardsand estimatedvalues, and then pick the best action.
Just as in conventionalheuristic search, this processcomputesbacked-up valuesof
the possibleactions, but doesnot attemptto savethem. Thus, heuristicsearchcanbe
viewedasan extensionof the ideaof a greedypolicy beyonda singlestep.
The point of searchingdeeperthan one stepis to obtain better action selections.
If one has a perfect model and an imperfect action-value function, then in fact
deepersearchwill usually yield better policies. Certainly, if the searchis all the
way to the end of the episode, then the effect of the imperfect value function is
eliminated, andthe actiondeterminedin this way mustbe optimal. If the searchis of
sufficientdepthk suchthat yk is very small, thenthe actionswill be correspondingly
near optimal. On the other hand, the deeperthe search, the more computationis
required, usually resulting in a slower responsetime. A good exampleis provided
-level backgammonplayer, m -Gammon(Section 11.1).
This systemusedm (A) to learn an afterstatevalue function through many games
of self-play, using a form of heuristic searchto make its moves. As a model, m Gammonuseda priori knowledgeof the probabilitiesof dice rolls andthe assumption
that the opponentalwaysselectedthe actionsthat m -Gammonratedas bestfor it.
Tesaurofound that the deeperthe heuristic search, the better the movesmadeby
m -Gammon, but the longer it took to makeeachmove. Backgammonhasa large
branchingfactor, yet movesmustbe madewithin a few seconds
to searchaheadselectivelya few steps, but evenso
So far we haveemphasizedheuristic searchas an action-selectiontechnique, but
this may not be its most important aspect. Heuristic searchalso suggestsways of
selectivelydistributing backupsthat may lead to betterand fasterapproximationof
the optimal value function. A great deal of researchon heuristic searchhas been
devotedto making the searchas efficient as possible. The searchttee is grown
selectively, deeperalong somelines and shalloweralong others. For example, the
searchttee is often deeperfor the actions that seemmost likely to be best, and
shallowerfor thosethat the agentwill probably not want to take anyway. Can we
use a similar idea to improve the distribution of backups? Perhapsit can be done
by preferentiallyupdatingstate- action pairs whosevaluesappearto be closeto the
maximumavailablefrom the state. To our knowledge, this andotherpossibilitiesfor
distributingbackupsbasedon ideasborrowedfrom heuristicsearchhavenot yet been
We shouldnot overlook the most obviousway in which heuristic searchfocuses
backups: on the current state. Much of the effectivenessof heuristic searchis due
to its searchttee being tightly focusedon the statesand actionsthat might immediately
follow the current state. You may spendmore of your life playing chessthan
checkers,but whenyou play checkers,it paysto think aboutcheckersandaboutyour
I . Thereare interestingexceptionsto this. See, e.g., Pearl( 1984).
particularcheckersposition, your likely next moves, and successorpositions. However
you selectactions, it is thesestatesand actionsthat are of highestpriority for
backupsand whereyou most urgently want your approximatevalue function to be
accurate. Not only shouldyour computationbe preferentiallydevotedto imminent
events, but so should your limited memoryresources
arefar too manypossiblepositionsto storedistinct valueestimatesfor eachof them,
but chessprogramsbasedon heuristic searchcan easily storedistinct estimatesfor
the millions of positionsthey encounterlooking aheadfrom a single position. This
great focusing of memory and computationalresourceson the current decision is
presumablythe reasonwhy heuristicsearchcanbe so effective.
The distributionof backupscanbe alteredin similar waysto focus on the current
. As a limiting casewe might useexactly the methods
a searchtree, and then perform the individual, oneto
in this way and a table-lookup representationis used, then exactly the samebackup
would be achievedas in heuristic search. Any state-spacesearchcan be viewed in
this way as the piecing togetherof a large numberof individual one-stepbackups.
Thus, the performanceimprovementobservedwith deepersearches is not due to
the use of multistep backupsas such. Instead, it is due to the focus and concentration
of backupson statesand actionsimmediatelydownstreamfrom the current
state. By devoting a large amountof computationspecificallyrelevantto the candidate
actions, a much better decisioncan be madethan by relying on unfocused
We have presenteda perspectiveemphasizingthe surprisingly close relationships
betweenplanningoptimal behaviorandlearningoptimal behavior. Both involveestimating
the samevaluefunctions, andin bothcasesit is naturalto updatethe estimates
incrementally, in a long seriesof small backupoperations. This makesit straightforward
to integratelearning and planning process
can be convertedinto planning methodssimply by applying them to simulated
planningbecomeevenmoresimilar; they arepossiblyidenticalalgorithmsoperating
Figure 9.15 The deepbackupsof heuristicsearchcanbe implementedasa sequenceof onestepbackups(shownhereoutlined). The orderingshownis for a selectivedepth-first search.
It is straightforwardto integrateincrementalplanning methodswith acting and
model-learning. Planning, acting, and model-learning interact in a circular fashion
( Figure9.2), eachproducingwhat the other needsto improve; no other interaction
amongthem is either requiredor prohibited. The most natural approachis for all
es to proceedasynchronouslyand in parallel. If the process
, then the division can be handledalmost arbitrarily- by
whateverorganizationis mostconvenientandefficient for the taskat hand.
In this chapterwe havetouchedupona numberof dimensionsof variationamong
state-spaceplanningmethods. Oneof the most importantof theseis the disbibution
of backups, that is, the focus of the search. Prioritized sweepingfocuseson the
of stateswhosevalueshaverecentlychanged.Heuristic searchapplied
to reinforcementlearningfocuses, inter alia, on the successors
Trajectory samplingis a convenientway of focusing on the on policy disbibution.
es can significantly speedplanning and are current topics of
Another interestingdimensionof variationis the sizeof backups. The smallerthe
backups, the more incrementalthe planning methodscan be. Among the smallest
backupsare one-stepsamplebackups. We presentedone study suggestingthat onestepsamplebackupsmay be preferableon very largeproblems. A relatedissueis the
depthof backups. In manycasesdeepbackupscan be implementedas sequences
9.9 Bibliographical and Historical Remarks
The overall view of planningandlearningpresentedherehasdevelopedgraduallyover
a numberof years, in part by the authors(Sutton, 1990, 1991a, 1991b; Barto, Bradtke,
andSingh, 1991, 1995; SuttonandPinette, 1985; SuttonandBarto, 198Ib); it hasbeen
stronglyinfluencedby Agre andChapman( 1990; Agre 1988), BertsekasandTsitsiklis
( 1989), Singh ( 1993), and others. The authorswere also strongly influencedby psychological
studiesof latent learning ( Tolman, 1932) and by psychologicalviews of
the natureof thought(e.g., Galanterand Gerstenhaber
9.2- 3 The termsdirect and indirect, which we useto describedifferent kinds of reinforcement
learning, are from the adaptivecontrol literature(e.g., Goodwin and Sin, 1984),
wherethey are usedto makethe samekind of distinction. The term systemidentification
is usedin adaptivecontrol for what we call model-learning (e.g., Goodwin and
, 1983; Young, 1984) . The Oyna architectureis due
to Sutton( 1990), and the resultsin thesesectionsarebasedon resultsreportedthere.
Prioritized sweepingwasdevelopedsimultaneouslyand independentlyby Moore and
Atkeson( 1993) and Pengand Williams ( 1993) . The resultsin Figure 9.10 are due to
PengandWilliams ( 1993). The resultsin Figure9.11aredue to Moore andAtkeson.
This sectionwasstronglyinfluencedby the experimentsof Singh( 1993).
The utility of trajectorysamplinghasimplicitly beena part of reinforcementlearning
from the outset, but it was most explicitly emphasizedby Barto, Bradtke, and Singh
( 1991, 1995). They pointed out the utility of focusing planning computationby ordering
the backupsin an asynchronousOP algorithm by following real or simulated
trajectoriesof the MOP. They called an algorithm that doesthis using full value iteration
backupsreal-time DP after a deterministicanalogcalled real-time heuristic
searchdue to Korf ( 1990) . This view was also takenby Jalali and Ferguson( 1989),
who called the resultingalgorithm transientDP.
For further reading on heuristic search, the reader is encouragedto consult texts
and surveyssuch as thoseby Russelland Norvig ( 1995) and Korf ( 1988). Pengand
Williams ( 1993) exploreda forward focusingof backupsmuchas is suggestedin this
In this book we havehied to presentreinforcementlearningnot as a collection of
individual methods, but asa coherentsetof ideascutting acrossmethods. Eachidea
canbe viewedasa dimensionalongwhich methodsvary. The setof suchdimensions
spansa large spaceof possiblemethods. By exploring this spaceat the level of
dimensionswe hopeto obtain the broadestand most lasting understanding
of reinforcementlearningwe havedevelopedin this book andto identify someof the
more importantgapsin our coverageof the field.
All of the reinforcementlearningmethodswe haveexploredin this book havethree
key ideasin common. First, the objectiveof all of them is the estimationof value
functions. Second, all operateby backing up valuesalong actual or possiblestate
trajectories. Third, all follow the general strategyof generalizedpolicy iteration
(GPI), meaningthattheymaintainan approximatevaluefunctionandanapproximate
policy, andthey continuallytry to improveeachon the basisof the other. Thesethree
ideasthatthe methodshavein commoncircumscribethe subjectcoveredin this book.
Wesuggestthatvaluefunctions, backups,andGPI arepowerfulorganizingprinciples
potentiallyrelevantto any modelof intelligence.
Two of the most importantdimensionsalong which the methodsvary are shown
in Figure 10.1. Thesedimensionshaveto do with the kind of backupusedto improve
the valuefunction. The vertical dimensionis whetherthey aresamplebackups
(basedon a sampletrajectory) or full backups(basedon a distributionof possibletrajectories
). Full backupsof courserequirea model, whereassamplebackupscan be
Figure 10.1 A slice of the spaceof reinforcementlearningmethods.
doneeitherwith or without a model(anotherdimensionof variation). The horizontal
dimensioncorrespondsto the depth of backups, that is, to the degreeof bootstrapping. At three of the four comersof the spaceare the three primary methodsfor
estimatingvalues: DP, ill , andMonte Carlo. Along the lower edgeof the spaceare
the sample-backupmethods,rangingfrom one-stepill backupsto full -returnMonte
Carlobackups.Betweentheseis a spectrumincludingmethodsbasedon n-stepbackups
andmixturesof n-stepbackupssuchasthe A-backupsimplementedby eligibility
DP methodsare shownin the extremeupper-left comer of the spacebecausethey
involveone-stepfull backups.The upper-right comeris theextremecaseof full backups
so deepthat they run all the way to terminalstates(or, in a continuingtask, until
discountinghasreducedthe contributionof anyfurtherrewardsto a negligiblelevel).
This is the caseof exhaustivesearch. Intermediatemethodsalongthis dimensioninclude
heuristic searchand relatedmethodsthat searchand backupup to a limited
depth perhapsselectively.Therearealsomethodsthat areintermediatealongthe vertical
dimension. Theseincludemethodsthat mix full and samplebackups, aswell as
the possibility of methodsthat mix samplesanddistributionswithin a singlebackup.
The interior of the squareis filled in to representthe spaceof all suchintermediate
A third importantdimensionis that of function approximation. Functionapproximation
canbe viewedasanorthogonalspectrumof possibilitiesrangingfrom tabular
methodsat one extremethroughstateaggregation
then a diversesetof nonlinearmethods. This third dimensionmight be visualizedas
perpendicularto the planeof the pagein Figure 10.1.
Another dimensionthat we emphasizedin this book is the binary distinction between
on-policy andoff -policy methods.In theformercase, the agentlearnsthe value
function for the policy it is currently following, whereasin the latter caseit learns
the value function for the policy that it currently thinks is best. Thesetwo policies
are often different becauseof the needto explore. The interactionbetweenthis dimension
and the bootstrapping andfunction approximationdimensionsdiscussedin
Chapter8 illustratesthe advantagesof analyzingthe spaceof methodsin termsof
dimensions. Even thoughthis did involve an interactionbetweenthreedimensions,
manyother dimensionswere found to be irrelevant, greatly simplifying the analysis
In addition to the four dimensionsjust discussed
Is the task episodic or continuing, discountedor undiscounted
Action valuesVI. state valuesVI. afterstate values What kind of valuesshouldbe
estimated? If only statevaluesareestimated,theneithera modelor a separatepolicy
(as in actor- critic methods) is requiredfor action selection.
Action selection/exploration How are actionsselectedto ensurea suitabletradeoff
betweenexplorationandexploitation? Wehaveconsideredonly the simplestways
to do this: f -greedy and softmax action selection, and optimistic initialization of
SynchronousVI. asynchronous Are the backupsfor all statesperformedsimultaneously
Replacing VI. accumulating traces If eligibility tracesare used, which kind is
Real VI. simulated Shouldonebackupreal experienceor simulatedexperience
Location of backups What statesor state- actionpairsshouldbe backedup? Modelfree methodscan chooseonly among the statesand state- action pairs actually
, but model-basedmethodscanchoosearbitrarily. Therearemanypotent
Timing of backups Shouldbackupsbe doneas part of selectingactions, or only
Memory for backups How long shouldbacked-up valuesberetained? Shouldthey
Of course, thesedimensionsareneitherexhaustivenor mutuallyexclusive. Individual
algorithmsdiffer in many other ways as well, and many algorithms lie in several
placesalong severaldimensions. For example, Dyna methodsuse both real and
simulatedexperienceto affectthe samevaluefunction. It is alsoperfectlysensibleto
maintainmultiple valuefunctionscomputedin different waysor over different state
. Thesedimensionsdo, however, constitutea coherentset
of ideasfor describingand exploringa wide spaceof possiblemethods.
Much researchremainsto be donewithin this spaceof reinforcementlearningmethods
. For example, evenfor the tabularcaseno control methodusing multistepbackups
has beenproved to convergeto an optimal policy. Among planning methods,
basic ideas such as trajectory sampling and focusing samplebackupsare almost
completely unexplored. On closer inspection, parts of the spacewill undoubtedly
turn out to havefar greatercomplexityand greaterinternal structurethan is now apparent
. There are also other dimensionsalong which reinforcementlearningcan be
, which we havenot yet mentioned,that leadto a muchlargerspaceof methods
. Herewe identify someof thesedimensionsandnotesomeof the openquestions
and frontiersthat havebeenleft out of the precedingchapters.
One of the most importantextensionsof reinforcementlearningbeyondwhat we
havetreatedin this book is to eliminatethe requirementthat the staterepresentation
es to the nonhavethe Markov property. Thereare a numberof interestingapproach
Markov case. Most strive to constructfrom the given statesignal and its pastvalues
a new signal that is Markov, or more nearly Markov. For example, one approachis
basedon the theory of partially observableMD Ps (POMDPs). POMDPsare finite
MD Ps in which the stateis not observable
, but another sensation signal stochastically
studiedfor the caseof completeknowledgeof the dynamicsof the POMDP. In this
case, Bayesianmethodscan be usedto computeat each time step the probability
of the environments being in each stateof the underlying MDP. This probability
distribution can then be used as a new state signal for the original problem. The
downsidefor the BayesianPOMDP approachis its computationalexpenseand its
strong relianceon completeenvironmentmodels. Someof the recentwork pursuing
, and Kaelbling ( 1995), Parr and Russell
a POMD P s dynamics, thenexistingtheory seemsto offer little guidance. Nevertheless
, one can still attemptto constructa Markov statesignal from the sequenceof
. Variousstatisticaland ad hoc methodsalong theselines havebeenexplored
( g., McCallum, 1992, 1993, 1995; Lin and Mitchell, 1992; Chapmanand
Kaelbling, 1991; Moore, 1994; Rivestand Schapire, 1987; Colombettiand Dorigo,
1994; WhiteheadandBallard, 1991; HochreiterandSchmidhuber
All of the abovemethodsinvolve constructingan improved staterepresentation
from the non-Markov one provided by the environment. Another approachis to
leavethe staterepresentationunchangedand usemethodsthat are not too adversely
affectedby its being non-Markov (e.g., Singh, Jaakkola, and Jordan, 1994, 1995;
Jaakkola, Singh and Jordan, 1995). In fact, most function approximationmethods
can be viewed in this way. For example, stateaggregationmethodsfor function approximation
are in effect equivalentto a non-Markov representationin which all
membersof a set of statesare mappedinto a common sensation
parallelsbetweenthe issuesof function approximationand non Markov representations
. In both casesthe overall problem divides into two parts: constructingan
, and making do with the current representation
casesthe making do part is relatively well understood
theseparallelspoint to any commonsolutionmethodsfor the two problems.
Another important direction for extendingreinforcementlearning beyondwhat
we have coveredin this book is to incorporateideasof modularity and hierarchy.
Introductoryreinforcementlearning is about learning value functionsand one step
modelsof the dynamicsof the environment.But muchof what peoplelearndoesnot
seemto fall exactly into either of thesecategories
know about tying our shoes, making a phonecall, or traveling to London. Having
learnedhow to do such things, we are then able to chooseamongthem and plan
as if they were primitive actions. What we have learnedin order to do this are not
conventionalvaluefunctionsor one-stepmodels. We are able to plan and learn at a
variety of levels and flexibly interrelatethem. Much of our learningappearsnot to
be aboutlearningvaluesdirectly, but aboutpreparingus to quickly estimatevalues
later in responseto new situationsor new information. Considerablereinforcement
learningresearchhasbeendirectedat capturingsuchabilities (e.g., Watkins, 1989;
Dayan and Hinton, 1993; Singh, 1992a, 1992b; Ring, 1994; Kaelbling, 1993b;
havealso exploredways of using the structureof particulartasksto
. For example, manyproblemshavestaterepresentations
lists of variables, like the readingsof multiple sensorsor actionsthat are lists of
otherscansometimesbe exploitedto obtainmore
learningalgorithms. It is sometimesevenpossibleto decomposea probleminto
severalindependentsubproblemsthat can be solvedby separatelearningagents. A
reinforcementlearning problem can usually be structuredin many different ways,
somereflectingnaturalaspectsof the problem, suchasthe existenceof physicalsensors
, and othersbeing the result of explicit attemptsto decomposethe probleminto
. Possibilitiesfor exploiting structurein reinforcementlearning
andrelatedplanningproblemshavebeenstudiedby manyresearchers
of multiagentor distributedreinforcementlearning(e.g., Littman, 1994; Markey,
Finally, we wantto emphasizethat reinforcementlearningis meantto be a general
approachto learning from interaction. It is generalenoughnot to require special
purposeteachersand domain knowledge, but also generalenough
things if they are available. For example, it is often possibleto acceleratereinforcement
learning by giving advice or hints to the agent (Clouse and Utgoff, 1992;
Maclin and Shavlik, 1994) or by demonstratinginstructivebehavioraltrajectories
(Lin , 1992). Another way to makelearningeasier, relatedto shaping in psychology
, is to give the learning agenta seriesof relatively easy problemsbuilding up
to the harderproblemof ultimate interest(e.g., Selfridge, Sutton, and Barto, 1985).
, havethe potentialto give the machinelearningtermstraining andteachingnew meaningsthat arecloserto their meanings
In this final chapterwe presenta few casestudiesof reinforcementlearning. Several
of theseare substantialapplicationsof potential economicsignificance. One,
Samuel's checkersplayer, is primarily of historicalinterest. Our presentations
to illustratesomeof the trade-offs andissuesthatarisein realapplications.For
example, we emphasizehow domainknowledgeis incorporatedinto the formulation
and solution of the problem. We also highlight the representationissuesthat are so
often critical to successfulapplications. The algorithmsusedin someof thesecase
studiesaresubstantiallymorecomplexthanthosewe havepresentedin the restof the
book. Applicationsof reinforcementlearningare still far from routine and typically
requireasmuchart as science. Making applicationseasierandmore straightforward
is one of the goalsof currentresearchin reinforcementlearning.
One of the most impressiveapplicationsof reinforcementlearningto dateis that by
Gerry Tesauroto the gameof backgammon( Tesauro
program, TD-Gammon, requiredlittle backgammonknowledge, yet learnedto play
. The learningalextremelywell, nearthe level of the world s strongestgrandmasters
gorithm in m Gammonwasa sttaightforwardcombinationof the m (A) algorithm
and nonlinearfunction approximationusing a multilayer neuralnetwork ttained by
Backgammonis a major gamein the sensethat it is playedthroughoutthe world,
with numeroustournamentsand regularworld championshipmatches.It is in part a
gameof chance, and it is a popular vehicle for waging significant sumsof money.
There are probably more professionalbackgammonplayersthan there are professional
chessplayers. The gameis played with 15 white and 15 black pieceson a
boardof 24 locations, calledpoints. Figure 11.1 showsa typical positionearly in the
game, seenfrom the perspectiveof the white player.
In this figure, white hasjust rolled the dice and obtaineda 5 and a 2. This means
that he can moveone of his pieces5 stepsandone (possiblythe samepiece) 2 steps.
For example, he could movetwo piecesfrom the 12 point, one to the 17 point, and
one to the 14 point. White' s objective is to advanceall of his piecesinto the last
quadrant(points 19- 24) and then off the board. The first player to removeall his
pieceswins. One complication is that the piecesinteract as they passeach other
going in different directions. For example, if it were black s move in Figure 11.1,
he could use the dice roll of 2 to move a piece from the 24 point to the 22 point,
hitting the white piecethere. Piecesthat havebeenhit are placedon the bar" in
the middle of the board(wherewe alreadyseeone previouslyhit black piece), from
whencethey reenterthe race from the start. However, if there are two pieceson a
point, then the opponentcannotmove to that point; the piecesare protectedfrom
being hit. Thus, white cannotusehis 5- 2 dice roll to move either of his pieceson
the I point, becausetheir possibleresultingpoints are occupiedby groupsof black
pieces. Fonning contiguousblocks of occupiedpoints to block the opponentis one
Backgammoninvolves severalfurther complications, but the abovedescription
gives the basic idea. With 30 piecesand 24 possiblelocations (26, counting the
bar and off -the-board) it shouldbe clear that the numberof possiblebackgammon
positions is enormous, far more than the number of memory elementsone could
have in any physically realizablecomputer. The number of movespossiblefrom
eachposition is also large. For a typical dice roll there might be 20 different ways
Figure11.2 Theneuralnetworkusedin m -Gammon
of playing. In consideringfuture moves, suchas the responseof the opponent, one
must considerthe possibledice rolls as well. The result is that the gametree hasan
effectivebranchingfactor of about400. This is far too largeto pennit effectiveuse
of the conventionalheuristicsearchmethodsthat haveprovedso effectivein games
On the other hand, the gameis a good matchto the capabilitiesof TD learning
methods. Although the game is highly stochastic, a complete description of the
games stateis availableat all times. The gameevolvesover a sequence
hand, the theoreticalresultswe havedescribedso far cannotbe usefully appliedto
this task. The numberof statesis so largethat a lookup tablecannotbe used, andthe
opponentis a sourceof uncertaintyandtime variation.
TD-Gammonuseda nonlinearform of TD (}") . The estimatedvalue, V,(s), of any
state( boardpositions was meant to estimatethe probability of winning starting
from states. To achievethis, rewardswere definedas zero for all time stepsexcept
thoseon which the gameis won. To implementthe value function, TD Gammon
used a standardmultilayer neural network, much as shown in Figure 11.2. ( The
real networkhad two additionalunits in its final layer to estimatethe probability of
eachplayer s winning in a specialway called a gammon or backgammon
networkconsistedof a layer of input units, a layer of hiddenunits, anda final output
unit. The input to the network was a representationof a backgammonposition, and
the output wasan estimateof the valueof that position.
In the first version of TD-Gammon, TD-Gammon0.0, backgammonpositions
were representedto the network in a relatively direct way that involved little backgammon
knowledge. It did, however, involve substantialknowledgeof how neural
networkswork and how infonnation is best presentedto them. It is instructiveto
note the exact representationTesaurochose. There were a total of 198 input units
to the network. For eachpoint on the backgammonboard, four units indicatedthe
numberof white pieceson the point. If therewereno white pieces, thenall four units
took on the value zero. If there was one piece, then the first unit took on the value
I . If there were two pieces, then both the first and the secondunit were I . If there
werethreeor morepieceson the point, thenall of the first threeunits were I . If there
were more than three pieces, the fourth unit also cameon, to a degreeindicating
the numberof additionalpiecesbeyondthree. Letting n denotethe total numberof
pieceson the point, if n > 3, then the fourth unit took on the value (n - 3)/ 2. With
four units for white and four for black at eachof the 24 points, that madea total of
192units. lWo additionalunits encodedthe numberof white andblack pieceson the
bar (eachtook the valuen12, wheren is the numberof pieceson the bar), and two
more encodedthe numberof black and white piecesalreadysuccess
from the board(thesetook the valuenil S, wheren is the numberof piecesalready
borneoff) . Finally, two units indicatedin a binary fashionwhetherit waswhite' s or
black' s turn to move. The generallogic behindthesechoicesshouldbe clear. Basically
, Tesaurotried to representthe position in a straightforwardway, making little
attemptto minimize the numberof units. He providedoneunit for eachconceptually
distinct possibility that seemedlikely to be relevant, and he scaledthem to roughly
the samerange, in this casebetween0 and I .
Given a representationof a backgammonposition, the network computedits estimated
value in the standardway. Correspondingto eachconnectionfrom an input
unit to a hidden unit was a real-valued weight. Signals from each input unit
were multiplied by their correspondingweightsand summedat the hiddenunit. The
output, h (j ), of hidden unit j was a nonlinear sigmoid function of the weighted
where tf>(i ) is the value of the ith input unit and Wij is the weight of its connection
to the jth hidden unit. The output of the sigmoid is alwaysbetween0 and 1,
and hasa naturalinterpretationas a probability basedon a summationof evidence.
The computationfrom hiddenunits to the output unit was entirely analogous
connectionfrom a hidden unit to the output unit had a separateweight. The output
unit formed the weightedsum and then passedit through the samesigmoid nonlinearity
TO-Gammonusedthe gradient-descentform of the TO()") algorithm described
in Section8.2, with the gradientscomputedby the error backpropagationalgorithm
(Rumelhart, Hinton, andWilliams, 1986). Recallthat the generalupdaterule for this
8' + 1= 8, + a [ r '+1+ y V,(S'+I) - V,(s,)] e, ,
where8, is the vector of all modifiableparameters(in this case, the weightsof the
network) ande, is a vectorof eligibility traces, onefor eachcomponentof 8" updated
with eo= O. The gradientin this equationcan be computedefficiently by the backpropagationprocedure. For the backgammonapplication, in which Y = I and the
rewardis alwayszeroexceptuponwinning, the TO error portion of the learningrule
is usuallyjust V,(S'+I) - V,(s,), as suggestedin Figure 11.2.
To apply the learning rule we need a sourceof backgammongames. Tesauro
of gamesby playing his learningbackgammonplayer
againstitself. To chooseits moves, TO-Gammonconsideredeachof the 20 or so
ways it could play its dice roll and the correspondingpositionsthat would result.
The resultingpositionsare afterstatesas discussedin Section6.8. The network was
consultedto estimateeachof their values. The move was then selectedthat would
lead to the position with the highestestimatedvalue. Continuing in this way, with
TO-Gammonmaking the movesfor both sides, it was possibleto easily generate
largenumbersof backgammongames.Eachgamewastreatedasanepisode,with the
sequenceof positionsactingasthe states,so, Sl, S2, . . . . Tesauroappliedthe nonlinear
TO rule ( 11.1) fully incrementally,that is, after eachindividual move.
The weightsof the network were set initially to small randomvalues. The initial
evaluationswerethusentirely arbitrary. Sincethe moveswereselectedon thebasisof
, the initial moveswereinevitably poor, and the initial gamesoften
lastedhundredsor thousandsof movesbeforeone side or the other won, almostby
accident. After a few dozengameshowever, performanceimprovedrapidly.
After playing about 300,000 gamesagainstitself, TD-Gammon0.0 as described
abovelearnedto play approximatelyaswell asthe bestpreviousbackgammoncomputer
programs.This wasa striking resultbecauseall the previoushigh performance
computerprogramshad usedextensivebackgammonknowledge. For example, the
reigning championprogramat the time was, arguably, Neurogammon
written by Tesaurothat used a neural network but not TO learning. Neu'
rogammons network was trained on a large training corpus of exemplarymoves
provided by backgammonexperts, and, in addition, startedwith a set of features
. Neurogammonwas a highly tuned, highlyef specially crafted for backgammon
fective backgammonprogramthat decisivelywon the World BackgammonOlympiad
in 1989. TO-Gammon0.0, on the other hand, wasconstructedwith essentially
zero backgammonknowledge. That it was able to do as well as Neurogammon
es is sbiking testimonyto the potentialof self-play learning
The tournamentsuccessof TO-Gammon 0.0 with zero backgammonknowledge
suggestedan obviousmodification: add the specializedbackgammonfeatures
but keep the self-play TO learning method. This producedTO-Gammon 1.0. TOGammon 1.0 was clearly substantiallybetter than all previousbackgammonprograms
and found seriouscompetitiononly amonghumanexperts. Later versionsof
the program, TO-Gammon2.0 (40 hidden units) and TO-Gammon2.1 (80 hidden
units), were augmentedwith a selectivetwo-ply searchprocedure.To selectmoves,
theseprogramslookedaheadnotjust to the positionsthat would immediatelyresult,
but also to the opponents possibledice rolls and moves. Assumingthe opponent
always took the move that appearedimmediatelybest for him, the expectedvalue
of eachcandidatemove wascomputedand the bestwas selected.To savecomputer
time, the secondply of searchwas conductedonly for candidatemovesthat were
rankedhighly after the first ply, aboutfour or five moveson average
affectedonly the movesselected; the learningprocessproceededexactly as before.
The most recentversion of the program, TO-Gammon3.0, uses 160 hidden units
and a selectivethree-ply search.TO-Gammonillustratesthe combinationof learned
value functionsand decide-time searchas in heuristic searchmethods. In more recent
work, Tesauroand Galperin ( 1997) havebegunexploring trajectory sampling
Tesaurowas able to play his programsin a significantnumberof gamesagainst
world-classhumanplayers. A summaryof the resultsis given in Table 11.1. Based
on theseresults and analysesby backgammongrandmasters(Robertie, 1992; see
Tesauro, 1995), TO-Gammon3.0 appearsto be at, or very near, the playing strength
of the besthumanplayersin the world. It may alreadybe the world champion. These
programshavealreadychangedthe way the besthumanplayersplay the game. For
example, TO-Gammonlearnedto play certainopeningpositionsdifferently thanwas
the conventionamongthe besthumanplayers. Basedon TO-Gammons successand
further analysis, the best human players now play these positions as TD -Gammon
An importantprecursorto Tesauro's TD-Gammonwas the seminalwork of Arthur
Samuel( 1959, 1967) in constructingprogramsfor learningto play checkers.Samuel
was one of the first to makeeffective useof heuristic searchmethodsand of what
we would now call temporal-differencelearning. His checkersplayersareinstructive
casestudiesin additionto beingof historicalinterest. We emphasizethe relationship
of Samuel's methodsto modemreinforcementlearning methodsand try to convey
Samuelfirst wrote a checkers-playing programfor the mM 701 in 1952. His first
learning program was completedin 1955 and was demonstratedon television in
1956. Later versionsof the programachievedgood, thoughnot expert, playing skill.
Samuelwas attractedto game-playing as a domain for studying machinelearning
becausegamesare less complicatedthan problems "taken from life " while still
allowing fruitful studyof how heuristicproceduresandlearningcanbe usedtogether.
He choseto study checkersinsteadof chessbecauseits relative simplicity madeit
possibleto focus morestronglyon learning.
Samuel's programsplayed by perfonning a lookaheadsearchfrom eachcurrent
position. They usedwhat we now call heuristic searchmethodsto detenninehow
to expandthe searchtree and when to stop searching. The tenninal board positions
, or scored, by a value function, or " scoring
polynomial, using linear function approximation. In this and other respects
Samuel's work seemsto havebeeninspiredby the suggestionsof Shannon( 1950). In
particular, Samuels program was basedon Shannons minimax procedureto find
the bestmovefrom the currentposition. Working backwardthroughthe searchtree
from the scoredterminalpositions, eachposition wasgiven the scoreof the position
that would result from the bestmove, assumingthat the machinewould alwaystry
to maximizethe score, while the opponentwould alwaystry to minimize it. Samuel
calledthis thebacked-up scoreof the position. Whenthe minimaxprocedurereached
the searchtree' s root- the currentposition- it yielded the best move underthe assumption
that the opponentwould be using the sameevaluationcriterion, shifted
to its point of view. Someversionsof Samuel's programsusedsophisticatedsearch
control methodsanalogousto whatareknownas" alpha-beta" cutoffs(e.g., seePearl,
Samuelused two main learning methods, the simplestof which he called rote
learning. It consistedsimply of savinga descriptionof eachboardpositionencountered
during play togetherwith its backed-up valuedeterminedby the minimax procedure
. The result was that if a position that had alreadybeenencounteredwere to
occuragainasa terminalpositionof a searchtree, the depthof the searchwaseffectively
amplified sincethis position s storedvaluecachedthe resultsof one or more
searches conductedearlier. One initial problem was that the programwas not encouraged
to movealong the most direct path to a win. Samuelgaveit a " a senseof
direction by decreasinga position' s value a small amounteachtime it was backed
up a level (called a ply) during the minimax analysis. If the programis now faced
with a choiceof boardpositionswhosescoresdiffer only by the ply number, it will
winning and a high ply alternativeif losing (Samuel, 1959, p. 80). Samuelfound
this discounting-like techniqueessentialto successfullearning. Rote learning produced
slow but continuousimprovementthat was most effective for opening and
endgameplay. His program becamea better-than-averagenovice after learning
from manygamesagainstitself, a varietyof humanopponents
Rote learning and other aspectsof Samuel's work strongly suggestthe essential
ideaof temporal-differencelearning- that the valueof a stateshouldequalthe value
of likely following states. Samuelcameclosestto this idea in his secondlearning
method, his learningby generalizationprocedurefor modifying theparametersof the
valuefunction. Samuel's methodwasthe samein conceptasthat usedmuchlater by
Tesauroin TO-Gammon. He playedhis programmanygamesagainstanotherversion
of itself and performeda backupoperationafter eachmove. The idea of Samuel's
backup is suggestedby the diagramin Figure 11.3. Each open circle representsa
position wherethe programmovesnext, an on-moveposition, and eachsolid circle
representsa position where the opponentmovesnext. A backup was madeto the
Figure11.3 Thebackupdiagramfor Samuels checkers
valueof eachon-moveposition after a moveby eachside, resultingin a secondonmoveposition. The backupwastowardthe minimax valueof a searchlaunchedfrom
the secondon-moveposition. Thus, the overalleffect wasthat of a backupconsisting
of one full moveof real eventsand then a searchover possibleevents, as suggested
by Figure 11.3. Samuels actualalgorithm was significantlymore complexthan this
Samueldid not include explicit rewards. Instead, he fixed the weight of the most
importantfeature, thepieceadvantagefeature, which measuredthe numberof pieces
the program had relative to how many its opponenthad, giving higher weight to
kings, and including refinementsso that it was betterto tradepieceswhen winning
than when losing. Thus, the goal of Samuels program was to improve its piece
, which in checkersis highly correlatedwith winning.
However, Samuel's learningmethodmay havebeenmissing an essentialpart of
a soundtemporal-differencealgorithm. Temporal-differencelearningcanbe viewed
asa way of makinga valuefunction consistentwith itself, andthis we canclearly see
in Samuel's method. But also neededis a way of tying the valuefunction to the true
valueof the states. We haveenforcedthis via rewardsand by discountingor giving
a fixed valueto the terminal state. But Samuels methodincludedno rewardsandno
specialtreatmentof the terminalpositionsof games.As Samuelhimself pointedout,
his value function could havebecomeconsistentmerely by giving a constantvalue
to all positions. He hopedto discouragesuchsolutionsby giving his piece-advantage
term a large, nonmodifiableweight. But althoughthis may decreasethe likelihood
of finding uselessevaluationfunctions, it does not prohibit them. For example, a
constantfunction could still be attainedby setting the modifiable weights so as to
cancelthe effect of the nonmodifiableone.
SinceSamuel's learningprocedurewas not constrainedto find useful evaluation
functions, it shouldhavebeenpossiblefor it to becomeworse with experience
fact, Samuelreportedobservingthis during extensiveself-play training sessions
To get the program improving again, Samuelhad to interveneand set the weight
with the largestabsolutevalue back to zero. His interpretationwas that this drastic
interventionjarred the programout of local optima, but anotherpossibility is that it
jarred the programout of evaluationfunctionsthat were consistentbut had little to
Despitethesepotentialproblems, Samuels checkersplayer using the generaliza
tion learning method approached better-than-average
opponentscharacterizedit as tricky but beatable (Samuel, 1959). In contrastto
the rote-learning version, this version was able to develop a good middle game
but remainedweak in opening and endgameplay. This programalso included an
ability to searchthrough sets of featuresto find those that were most useful in
fonning the value function. A later version (Samuel, 1967) included refinements
in its searchprocedure, such as alpha-beta pruning, extensiveuse of a supervised
learning mode called book learning, and hierarchicallookup tablescalled signature
tables (Griffith , 1966) to representthe value function insteadof linear function
approximation. This version learnedto play much better than the 1959 program
, though still not at a masterlevel. Samuel's checkers
widely recognizedasa significantachievementin artificial intelligenceandmachine
Reinforcementlearninghasbeenappliedto a wide variety of physicalconttol tasks
(e.g., for a collection of robotics applications, seeConnell and Mahadevan
Onesuchtaskis the acrobat, a two-link , underactuated
gymnastswinging on a high bar ( Figure11.4). The first joint (correspondingto the
gymnasts handson the bar) cannotexerttorque, but the secondjoint (corresponding
to the gymnastbending at the waist) can. The systemhas four continuousstate
variables: two joint positionsand two joint velocities. The equationsof motion are
given in Figure 11.5. This systemhas been widely studied by conttol engineers
(e.g., Spong, 1994) andmachine-learningresearchers
<PI= - mpIlc282sin~ - 2mpIlc2~ 81sin~ + (mllcl + mpI)g cos(81- 1r/ 2) + 4>2
Figure 11.5 The equationsof motion of the simulatedacrobat. A time stepof 0.05 seconds
wasusedin the simulation, with actionschosenafter everyfour time steps. The torqueapplied
at the secondjoint is denotedby 'Ce {+ 1, - I , OJ. There were no consb
positions, but the angularvelocitieswere limited to 81 e (- 47f, 47f) and~ e (- 97f, 97f). The
constantswereml = m2 = 1 (massesof the links), ll = h = I (lengthsoflinks ), lcl = lc2= 0.5
(lengthsto centerof massof links), II = h = I (momentsof inertia of links), and g = 9.8
Oneobjectivefor controlling the acrobatis to swing the tip (the " feet" ) abovethe
first joint by an amountequalto one of the links in minimum time. In this task, the
torque applied at the secondjoint is limited to three choices: positive torque of a
fixed magnitude, negativetorqueof the samemagnitude, or no torque. A rewardof
goal is reached, which endsthe episode. No
optimal value, V (s), of any state, s, is the
minimum time to reachthe goal (an integernumberof steps) startingfrom s.
Sutton( 1996) addressedthe acrobatswing-up task in an on-line, modelfreecontext
. Although the acrobatwas simulated, the simulatorwasnot availablefor useby
the agent/controller in any way. The training and interactionwerejust as if a real,
physicalacrobathad beenused. Eachepisodebeganwith both links of the acrobat
hangingstraightdown and at rest. Torqueswere appliedby the reinforcementlearning
agentuntil the goal was reached, which alwayshappenedeventually. Then the
acrobatwasrestoredto its initial rest positionanda new episodewasbegun.
The learning algorithm used was Sarsa(A) with linear function approximation,
tile coding, and replacingtracesas in Figure 8.8. With a small, discreteaction set,
it is natural to use a separateset of tilings for each action. The next choice is of
the continuousvariableswith which to representthe state. A cleverdesignerwould
probably representthe state in terms of the angular position and velocity of the
centerof massand of the secondlink , which might makethe solution simpler and
. But sincethis wasjust a test problem, a more
naive, direct representationwas usedin termsof the positionsand velocitiesof the
links: 81, 61, ~ , and~ . The two angularvelocitiesarerestrictedto a limited rangeby
thephysicsof the acrobat(seeFigure 11.5) andthe two anglesarenaturallyrestricted
] . Thus, the statespacein this task is a boundedrectangularregion in four
This leavesthequestionof what tilings to use. Therearemanypossibilities, asdiscussed
in Chapter8. Oneis to usea completegrid, slicing the four-dimensionalspace
alongall dimensions,andthusinto manysmall four-dimensionaltiles. Alternatively,
onecould slice alongonly oneof the dimensions,makinghyperplanarstripes. In this
caseone hasto pick which dimensionto slice along. And of coursein all casesone
hasto pick the width of the slices, the numberof tilings of eachkind, and, if there
are multiple tilings, how to offset them. One could also slice along pairs or triplets
of dimensionsto get other tilings. For example, if one expectedthe velocitiesof the
two links to interact strongly in their effect on value, then one might make many
tilings that sliced along both of thesedimensions. If one thoughtthe region around
zero velocity was particularly critical, then the slicescould be more closely spaced
Sutton usedtilings that sliced in a variety of simple ways. Each of the four dimensions
was divided into six equal intervals. A seventhinterval was addedto the
angular velocities so that tilings could be offset by a random fraction of an interval
in all dimensions(seeChapter8, subsection"Tile Coding" ). Of the total of 48
tilings, 12 sliced along all four dimensionsas discussedabove, dividing the space
into 6 x 7 x 6 x 7 = 1764tiles each. Another 12 tilings sliced along threedimensions
(3 randomlyoffset tilings eachfor eachof the 4 setsof threedimensions), and
another12slicedalongtwo dimensions(2 tilings for eachof the 6 setsof two dimensions
). Finally, a setof 12tilings dependedeachon only onedimension(3 tilings for
eachof the 4 dimensions). This resultedin a total of approximately25, 000 tiles for
eachaction. This numberis small enoughthat hashingwasnot necessary
wereoffset by a randomfraction of an interval in all relevantdimensions.
The remainingparametersof the learning algorithm were a = 0.2/ 48, ).. = 0.9,
E" = 0, and Qo = O. The useof a greedypolicy (E" = 0) seemedpreferableon this task
becauselong sequencesof correct actionsare neededto do well. One exploratory
actioncould spoil a wholesequenceof goodactions. Explorationwasensuredinstead
by starting the action valuesoptimistically, at the low value of O. As discussedin
Section2.7 and Example 8.2, this makesthe agentcontinually disappointedwith
showslearningcurvesfor the acrobottask andthe learningalgorithm
describedabove. Note from the single-run curvethat singleepisodesweresometimes
, the acrobot was usually spinning repeatedly
at the secondjoint while the first joint changedonly slightly from vertical down.
Figure 11.6 Learningcurvesfor Sarsa()..} on the acrobottask.
Figure 11.7 A typical learnedbehaviorof the acrobat. Eachgroup is a seriesof consecutive
positions, the thicker line being the first. The arrow indicatesthe torqueappliedat the second
Although this often happened for many time steps, it always eventually ended as the
action values were driven lower. All runs ended with an efficient policy for solving
the problem , usually lasting about 75 steps. A typical final solution is shown in
Figure 11.7. First the acrobot pumps back and forth several times symmetrically ,
with the second link always down . Then , once enough energy has been added to
the system, the second link is swung upright and stabbed to the goal height .
Waiting for an elevator is a situation with which we are all familiar . We press a
button and then wait for an elevator to arrive traveling in the right direction . We may
have to wait a long time if there are too many passengersor not enough elevators.
Just how long we wait depends on the dispatching strategy the elevators use to
decide where to go . For example, if passengers on several floors have requested
pickups , which should be served first ? If there are no pickup requests, how should
the elevators distribute themselves to await the next request? Elevator dispatching is
a good example of a stochastic optimal control problem of economic importance that
is too large to solve by classical techniques such as dynamic programming .
Four elevators in a ten- story building .
Crites and Barto ( 1996; Crites, 1996) studiedthe application of reinforcement
learning techniquesto the four-elevator, ten-floor system shown in Figure 11.8.
Along the right-handsidearepickup requestsandan indicationof how long eachhas
beenwaiting. Eachelevatorhasa position, direction, andspeed,plus a setof buttons
to indicate where passengerswant to get off. Roughly quantizing the continuous
variables, CritesandBarto estimatedthat the systemhasover 1022states.This large
stateset rules out classicaldynamic programmingmethodssuchas value iteration.
Even if one statecould be backedup every microsecondit would still requireover
1000yearsto completejust one sweepthroughthe statespace.
In practice, modemelevatordispatchersare designedheuristically and evaluated
on simulatedbuildings. The simulatorsare quite sophisticatedand detailed. The
physicsof each elevatorcar is modeledin continuoustime with continuousstate
variables. Passengerarrivalsare modeledas discrete, stochasticevents, with arrival
rates varying frequently over the courseof a simulatedday. Not surprisingly, the
times of greatesttraffic and greatestchallengeto the dispatchingalgorithm are the
morning and eveningrush hours. Dispatchersare generallydesignedprimarily for
The performanceof elevatordispatchersis measuredin severaldifferent ways, all
with respectto an averagepassengerenteringthe system. The averagewaiting time
waits beforegettingon an elevator, andthe averagesystem
time is how long the passengerwaits before being droppedoff at the destination
floor. Anotherfrequentlyencounteredstatisticis the percentageof passengers
is the averagesquaredwaiting time. This objective is commonly used becauseit
tendsto keep the waiting times low while also encouragingfairnessin servingall
Crites and Barto applied a versionof one-step Q-learning augmentedin several
ways to take advantageof specialfeaturesof the problem. The most important of
theseconcernedthe formulation of the actions. First, eachelevatormade its own
decisionsindependentlyof the others. Second, a numberof constraintswere placed
on the decisions. An elevatorcarrying passengers
its passengerswantedto get off there, nor could it reversedirection until all of its
wantingto go in its currentdirectionhadreachedtheir floors. In addition,
a car wasnot allowedto stopat a floor unlesssomeonewantedto get on or off there,
and it could not stop to pick up passengers
constrainedalwaysto move up (otherwiseeveningrush hour ttaffic would tend to
pushall the elevatorsdown to the lobby). Theselast threeconstraintswereexplicitly
included to provide someprior knowledgeand make the problem easier. The net
result of all theseconstraintswas that each elevatorhad to make few and simple
decisions.The only decisionthat hadto be madewaswhetheror not to stopat a floor
that was being approachedand that had passengers
That eachelevatormadechoicesonly infrequentlypennitteda secondsimplification of the problem. As far as the learning agentwas concerned
discretejumps from one time at which it had to makea decisionto the next. When
a continuous-time decisionproblemis treatedas a discrete-time systemin this way
it is known asa semi-Markov decisionprocess.To a largeextent, suchprocess
be treatedjust like any other Markov decisionprocessby taking the rewardon each
discretetransition as the integral of the rewardover the correspondingcontinuoustime interval. The notion of return generalizesnaturally from a discountedsum of
future rewardsto a discountedintegral of future rewards:
where r , on the left is the usual immediaterewardin discretetime and r '+-r on the
right is the instantaneousrewardat continuoustime t + 1'. In the elevatorproblem
the continuous-time rewardis the negativeof the sum of the squaredwaiting times
. The parameterfJ > 0 plays a role similar to that of the
The basicidea of the extensionof Q-Ieamingto semi-Markov decisionproblems
can now be explained. Supposethe systemis in states and takesactiona at time I },
and then the next decisionis requiredat time 12in states . After this discrete-event
transition, the semi-Markov Q-Ieamingbackupfor a tabularaction-value function,
Note how e {J(t2 tl) actsas a variablediscountfactor that dependson the amountof
time betweenevents. This methodis dueto BradtkeandDuff ( 1995).
One complicationis that the rewardas defined- the negativesum of the squared
waiting times- is not somethingthat would normally be known while an actualelevator
wasrunning. This is becausein a real elevatorsystemone doesnot know how
many peopleare waiting at a floor, only how long it has beensincethe button requesting
a simulator, andCritesandBarto usedit to obtaintheir bestresults. They alsoexperimented
with anothertechniquethat usedonly information that would be known in
an on-line learningsituationwith a real setof elevators.In this caseonecanusehow
long sinceeachbuttonhasbeenpushedtogetherwith an estimateof the arrival rate
to computean expectedsummedsquaredwaiting time for eachfloor. Using this in
the rewardmeasureprovednearly as effectiveas using the actual summedsquared
For function approximation, a nonlinearneuralnetwork trainedby backpropaga
tion was usedto representthe action valuefunction.
with a wide variety of waysof representingstatesto the network. After muchexploration
, their bestresultswereobtainedusingnetworkswith 47 input units, 20 hidden
units, and two output units, one for eachaction. The way the statewas encodedby
the input units was found to be critical to the effectivenessof the learning. The 47
. 18 units: Two units encodedinformation about eachof the nine hall buttonsfor
. A real-valuedunit encodedthe elapsedtime if the buttonhad
beenpushed, anda binary unit wason if the buttonhad not beenpushed.
. 16units: A unit for eachpossiblelocationanddirectionfor the car whosedecision
wasrequired. Exactly one of theseunits wason at any given time.
. 10units: The locationof the otherelevatorssuperimposed
elevatorhad a " footprint that dependedon its direction and speed. For example, a
stoppedelevatorcausedactivationonly on the unit correspondingto its currentfloor,
but a moving elevatorcausedactivationon severalunits correspondingto the floors
, with the highestactivationson the closestfloors. No information
wasprovidedaboutwhich oneof the other carswasat a particularlocation.
. 1 unit: This unit was on if the elevatorwhosedecisionwas requiredwas at the
. 1 unit: This unit wason if the elevatorwhosedecisionwasrequiredwasat the floor
with the passengerwho hadbeenwaiting for the longestamountof time.
Two architectureswereused. In RLI , eachelevatorwasgivenits own action-value
function and its own neuralnetwork. In RL2, there was only one network and one
action-valuefunction, with the experiencesof all four elevatorscontributingto learning
in the onenetwork. In both cases,eachelevatormadeits decisionsindependently
of the other elevators, but shareda single rewardsignal with them. This introduced
additionalstochasticityasfar aseachelevatorwasconcernedbecauseits rewarddepended
in part on the actionsof the otherelevators,which it could not control. In the
architecturein which eachelevatorhadits own action-valuefunction, it waspossible
for different elevatorsto learn different specializedstrategies(althoughin fact they
tendedto learn the samestrategy). On the other hand, the architecturewith acommon action-valuefunction could learnfasterbecauseit learnedsimultaneouslyfrom
of all elevators.Trainingtime wasan issuehere, eventhoughthe system
in simulation. The reinforcementlearningmethodsweretrainedfor
aboutfour daysof computertime on a 100mips processor(correspondingto about
>hoursof simulatedtime). While this is a considerableamountof computation,
it is negligiblecomparedwith what would be requiredby any conventionaldynamic
The networkswere trainedby simulatinga great manyeveningrush hourswhile
making dispatchingdecisionsusing the developing, learnedaction value functions.
Crites and Barto usedthe Gibbs softrnaxprocedureto selectactionsas describedin
Section2.3, reducingthe temperature gradually over training. A temperatureof
zero was usedduring test runs on which the performanceof the learneddispatchers
shows the performanceof severaldispatchersduring a simulated
call down-peak traffic. The dispatchersinclude
in the industry, a variety of heuristicmethused
Figure 11.9 Comparisonof elevatordispatchers
ods, sophisticatedresearchalgorithms that repeatedlyrun complex optimization
algorithms on-line (Bao et al., 1994), and dispatcherslearnedby using the two
optimal policy for this problem is unknown, and the state
pin down becausedetailsof commercialdispatchingstrategiesareproprietary, these
learneddispatchersappearedto performvery well.
An important problem in the operationof a cellular telephonesystemis how to
efficiently usethe availablebandwidthto providegoodserviceto asmanycustomers
as possible. This problem is becomingcritical with the rapid growth in the useof
. Here we describea study due to Singh and Bertsekas( 1997) in
which they appliedreinforcementlearningto this problem.
Mobile telephone systemstake advantageof the fact that a communication
- can be used simultaneouslyby many callers if
thesecallers are spacedphysically far enoughapartthat their calls do not interfere
with eachanother.The minimum distanceat which thereis no interferenceis called
the channelreuseconstraint. In a cellular telephonesystem, the serviceareais divided
into a numberof regionscalledcells. In eachcell is a basestationthat handles
all the calls madewithin the cell. The total availablebandwidthis divided permanently
into a numberof channels. Channelsmust then be allocatedto cells and to
calls madewithin cells without violating the channelreuseconstraint. There are a
great many ways to do this, someof which are better than othersin terms of how
reliably they makechannelsavailableto new calls, or to calls that are handedoff"
from one cell to anotheras the caller crossesa cell boundary. If no channelis available
for a new or a handed-off call, the call is lost, or blocked. Singh and Bertsekas
consideredthe problemof allocatingchannelsso that the numberof blockedcalls is
A simpleexampleprovidessomeintuition aboutthe natureof the problem. Imagine
a situationwith threecells sharingtwo channels.The threecells are arrangedin
a line where no two adjacentcells can use the samechannelwithout violating the
channelreuseconsttaint. If the left cell is servinga call on channell while the right
cell is servinganothercall on channel2, as in the left diagrambelow, then any new
call arriving in the middle cell mustbe blocked.
Obviously, it would be better for both the left and the right cells to use channell
for their calls. Then a new call in the middle cell could be assignedchannel2, as in
the right diagram, without violating the channelreuseconstraint. Suchinteractions
and possibleoptimizationsare typical of the channelassignmentproblem. In larger
and morerealistic caseswith manycells, channels,and calls, and uncertaintyabout
when and where new calls will arrive or existing calls will haveto be handedoff ,
the problem of allocating channelsto minimize blocking can becomeextremely
The simplestapproachis to permanentlyassignchannelsto cells in sucha way
that the channelreuseconstraintcan neverbe violated even if all channelsof all
. This is calledafixed assignmentmethod. In a dynamic
, all channelsarepotentiallyavailableto all cells and
are assignedto cells dynamically as calls arrive. If this is done right, it can take
advantageof temporarychangesin the spatialand temporaldistribution of calls in
order to servemore users. For example, when calls are concentratedin a few cells,
thesecells canbe assignedmorechannelswithout increasingthe blocking ratein the
The channelassignmentproblem can be formulatedas a semi-Markov decision
processmuch as the elevatordispatchingproblem was in the previoussection. A
. The first is the configurastatein the semi-MDP formulation hastwo components
tion of the entire cellular systemthat gives for eachcell the usagestate(occupied
or unoccupied) of eachchannelfor that cell. A typical cellular systemwith 49 cells
and 70 channelshasa staggering7049configurations, ruling out the useof conventional
dynamicprogrammingmethods. The other statecomponentis an indicatorof
what kind of eventcauseda statetransition: arrival, departure,or handoff. This state
componentdetermineswhat kinds of actionsare possible. When a call arrives, the
possibleactionsare to assignit a free channelor to block it if no channelsareavailable
. When a call departs, that is, when a caller hangsup, the systemis allowedto
reassignthe channelsin usein that cell in an attemptto createa betterconfiguration.
At time t the immediatereward, r " is the numberof calls taking placeat that time,
wherefJ > 0 playsa role similar to that of the discount-rateparametery. Maximizing
the expectationof this return is the sameas minimizing the expected discounted)
numberof calls blockedover an infinite horizon.
This is anotherproblem greatly simplified if tteatedin terms of afterstates(Section
6.8). For eachstateand action, the immediateresult is a new configuration, an
afterstate.A valuefunction is learnedoverjust theseconfigurations.To selectamong
the possibleactions, the resultingconfigurationwas determinedand evaluated
action was then selectedthat would lead to the configurationof highestestimated
value. For example, when a new call arrivedat a cell, it could be assignedto any of
the free channels, if there were any; otherwise, it had to be blocked. The new configuration
that would result from eachassignmentwas easyto computebecauseit
was alwaysa simple deterministicconsequence
the configurationwith the highestestimatedvalue.
Linear function approximationwasusedfor the valuefunction: theestimatedvalue
of a configurationwas a weightedsumof features. Configurationswere represented
by two setsof features: an availability featurefor eachcell anda packingfeaturefor
eachcell- channelpair. For any configuration, the availability featurefor a cell gave
the numberof additionalcalls it could acceptwithout conflict if the rest of the cells
were frozen in the current configuration. For any given configuration, the packing
featurefor a cell- channelpair gavethe numberof timesthat channelwasbeingused
in that configurationwithin a four-cell radiusof that cell. All of thesefeatureswere
normalizedto lie between- I and 1. A semi-Markov versionof linear TD (O) was
Singh and Bertsekascomparedthreechannelallocationmethodsusing a simulation
of a 7 x 7 cellular arraywith 70 channels.The channelreuseconsttaintwasthat
calls had to be 3 cells apartto be allowedto usethe samechannel. Calls arrivedat
cellsrandomlyaccordingto Poissondistributionspossiblyhavingdifferentmeansfor
differentcells, andcall durationsweredeterminedrandomlyby anexponentialdistribution
with a meanof threeminutes. The methodscomparedwerea fixed assignment
method(FA), a dynamicallocationmethodcalled borrowingwith directionalchan"
nellocking (BDCL), and the reinforcementlearningmethod(RL). BDCL (Zhang
and Yum, 1989) was the bestdynamicchannelallocationmethodthey found in the
literature. It is a heuristicmethodthat assignschannelsto cells asin FA, but channels
canbe borrowedfrom neighboringcells whenneeded.It ordersthe channelsin each
cell and usesthis orderingto determinewhich channelsto borrow andhow calls are
dynamicallyreassignedchannelswithin a cell.
Figure 11.10 showsthe blocking probabilitiesof thesemethodsfor meanarrival
ratesof 150, 200, and 300 calls/hour as well as for a casein which different cells
haddifferent meanarrival rates. The reinforcementlearningmethodlearnedon-line.
The data shownare for its asymptoticperformance
The RL method blocked calls less frequently than did the other methodsfor all
arrival ratesand soon after starting to learn. Note that the differencesbetweenthe
methodsdecreasedas the call arrival rate increased
as the systemgets saturatedwith calls there are fewer opportunitiesfor a dynamic
allocation method to set up favorable usagepatterns. In practice, however, it is
the performanceof the unsaturatedsystemthat is most important. For marketing
, cellular telephonesystemsare built with enoughcapacitythat more than
Nie and Haykin ( 1996) also studiedthe applicationof reinforcementlearningto
dynamicchannelallocation. They formulatedthe problemsomewhatdifferently than
Singhand Bertsekasdid. Insteadof trying to minimize the probability of blocking a
call directly, their systemtried to minimize a more indirect measureof systemperformance
. Cost was assignedto patternsof channelusedependingon the distances
betweencalls using the samechannels.Patternsin which channelswere being used
by multiple calls that were closeto eachother were favoredover patternsin which
channel-sharingcalls were far apart. Nie and Haykin comparedtheir systemwith a
methodcalled MAXAVAIL (Sivarajan, McEliece, and Ketchum, 1990), considered
to be one of the bestdynamicchannelallocationmethods. For eachnew call, it selects
the channelthat maximizesthe total numberof channelsavailablein the entire
system. Nie and Haykin showed that the blocking probability achieved by their reinforcement
learning system was closely comparable to that of M ~
a variety of conditions in a 49 cell , 70- channel simulation . A key point , however, is
that the allocation policy produced by reinforcement learning can be implemented
computation that it is not feasible for large systems.
The studies we described in this section are so recent that the many questions they
raise have not yet been answered. We can see, though , that there can be different
ways to apply reinforcement learning to the same real world problem . In the near
new applications of reinforcement learning to problems arising in communication
Many jobs in industry andelsewhererequirecompletinga collection of taskswhile
satisfying temporal and resourceconsttaints. Temporalconsttaintssay that some
taskshaveto be finishedbeforeotherscan be started; resourceconsttaintssay that
two tasksrequiringthe sameresourcecannotbe donesimultaneously(e.g., the same
machinecannotdo two tasksat once). The objectiveis to createa schedulespecifying
when eachtask is to begin and what resourcesit will usethat satisfiesall the
consttaintswhile taking aslittle overalltime aspossible. This is thejob-shopscheduling
problem. In its generalfonn , it is NP-complete, meaningthat thereis probably
no efficient procedurefor exactly finding shortestschedulesfor arbitrary instances
of the problem. Job- shopschedulingis usually doneusing heuristicalgorithmsthat
takeadvantageof specialpropertiesof eachspecificinstance.
Zhang and Dietterich ( 1995, 1996; Zhang, 1996) were motivatedto apply reinforcemen
learning to job- shop schedulingbecausethe designof domain specific,
heuristicalgorithmscan be expensiveand time consuming. Their goal was to show
how reinforcementlearning can be used to learn how to quickly find constraintsatisfying schedulesof short duration in specific domains, thereby reducing the
amountof handengineeringrequired. They addressedthe NASA spaceshuttlepayload
processingproblem (SSPP), which requiresschedulingthe tasksrequiredfor
installation and testing of shuttle cargo bay payloads. An SSPPtypically requires
schedulingfor two to six shuttlemissions, eachrequiringbetween34 and 164tasks.
An exampleof a taskis MISSION-SEQUENCE-TEST, which hasa durationof 7200
: two quality controlofficers, two technicians
time unitsandrequiresthe following resources
, oneATE, oneSPCDS, andoneHITS. Someresourcesaredividedinto pools,
and if a task needsmore than oneresourceof a specifictype, the resourcesmustbelong
to the samepool, and the pool hasto be the right one. For example, if a task
needstwo quality control officers, they both haveto be in the pool of quality control
officers working on the sameshift at the right site. It is not too hard to find a
conflict-free schedulefor a job , one that meetsall the temporaland resourceconstraints
, but the objectiveis to find a conflict-free schedulewith the shortestpossible
total duration, which is muchmore difficult.
How can you do this using reinforcementlearning? Job-shop schedulingis usually
fonnulated as a searchin the spaceof schedules
combinatorial, optimizationproblem. A typical solutionmethodwould sequentially
in tennsof con, attemptingto improveeachover its predecessor
sttaint violations and duration(a hill climbing, or local search, method). You could
reinforcementlearningproblemof the type we discussed
in Chapter2 with a very large numberof possibleactions: all the possible
! But asidefrom the problemof having so many actions, any solutionobtained
this way would just be a single schedulefor a singlejob instance. In contrast,
what ZhangandDietterichwantedtheir learningsystemto end up with wasa policy
that could quickly find good schedulesfor any SSPP
For cluesabouthow to do this, they lookedto an existing optimizationapproach
, in fact, the one actually in useby NASA at the time of their research
iterativerepair methoddevelopedby Zweben, Daun, andDeale( 1994). The starting
point for the searchis a critical path schedule, a schedulethat meetsthe temporal
constraintsbut ignores the resourceconstraints. This schedulecan be constructed
efficiently by schedulingeachtaskprior to launchaslate asthe temporalconstraints
pennit, and eachtask after landing as early as theseconstraintspennit. Resource
pools are assignedrandomly. Two typesof operatorsare usedto modify schedules
POOLoperatorchangesthe pool assignedto one of the task s resources
of operatorappliesonly if it can reassigna pool so that the resourcerequirementis
satisfied. A MOVEoperatormovesa task to the first earlier or later time at which its
resourceneedscanbe satisfiedand usesthe critical path methodto rescheduleall of
At eachstepof the iterative repair search, one operatoris applied to the current
schedule,selectedaccordingto the following rules. The earliesttask with a resource
constraintviolation is found, and a REASSIGN
if possible. If more than one applies, that , severaldifferent pool reassignments
arepossible, oneis selectedat random. If no REASSIGN
MOVEoperatoris selectedat randombasedon a heuristicthat prefersshort-distance
movesof taskshaving few temporaldependentsand whoseresourcerequirements
are close to the task' s overallocation. After an operatoris applied, the numberof
constraintviolations of the resultingscheduleis detennined. A simulatedannealing
procedureis useddecidewhetherto acceptor rejectthis newschedule.If ~ V denotes
the numberof constraintviolations removedby the repair, then the new schedule
is acceptedwith probability exp( - ~ V/ T ), where T is the current computational
temperaturethat is graduallydecreasedthroughoutthe search. If accepted
schedulebecomesthecurrentschedulefor thenextiteration; otherwise, thealgorithm
attemptsto repairthe old scheduleagain, which will usuallyproducedifferentresults
dueto the randomdecisionsinvolved. Searchstopswhenall constraintsaresatisfied.
Shortschedulesareobtainedby runningthe algorithmseveraltimesandselectingthe
shortestof the resultingconflict-free schedules
typically numberingabout 20. The problem was treatedas episodic, eachepisode
startingwith the samecritical path schedulethat the iterativerepair algorithmwould
startwith andendingwhena schedulewasfound that did not violate any constraint.
The initial state- a critical pathschedule
to promotethe quick constructionof conflict-free schedulesof short durations. The
systemreceiveda small negativereward ( 0.001) on each step that resultedin a
schedulethat still violated a constraint. This encouragedthe agentto find conflictfree schedulesquickly, that is, with a small numberof repairsto so. Encouragingthe
systemto find shortschedulesis moredifficult becausewhat it meansfor a schedule
to be shortdependson the specificSSPPinstance. The shortestschedulefor a difficult
instance, one with a lot of tasksandconstraints,will be longer than the shortest
schedulefor a simpler instance. Zhang and Dietterich deviseda formula for a resource
dilation factor (RDF) , intendedto be an instance-independentmeasureof
a schedule's duration. To accountfor an instances intrinsic difficulty, the formula
makesuseof a measureof the resourceoverallocationof so. Sincelonger schedules
tendto producelargerRDFs, the negativeof the RDF of the final conflict-free schedule
wasusedas a rewardat the end of eachepisode. With this rewardfunction, if it
takesN repairsstartingfrom aschedulesto obtain a final conflict-free schedule,sf ,
the return from s is - RDF (sf ) - O.OOI(N - I ) .
This reward function was designedto try to make a systemlearn to satisfy the
two goals of finding conflict-free schedulesof short duration and finding conflictfree schedulesquickly. But the reinforcementlearning systemreally has only one
goal- maximizingexpectedreturn- sothe particularrewardvaluesdeterminehow a
learningsystemwill tendto tradeoff thesetwo goals. Settingthe immediaterewardto
the small valueof - 0.001 meansthat the learningsystemwill regardonerepair, one
stepin the schedulingprocess,asbeingworth 0.001 unitsof RDF. So, for example, if
from somescheduleit is possibleto producea conflict-free schedulewith onerepair
or with two, anoptimal policy will takethe extrarepaironly if it promisesa reduction
Zhang and Dietterich usedTO(A) to learn the value function. Function approximation
was by a multilayer neuralnetwork trainedby backpropagatingTO errors.
Actions wereselectedby anE-greedypolicy, with Edecreasingduring learning. Onestep lookaheadsearchwas usedto find the greedyaction. Their knowledgeof the
problemmadeit easyto predictthe schedulesthat would resultfrom eachrepairoperation
. Theyexperimentedwith a numberof modificationsto this basicprocedureto
. One was to usethe TO(A) algorithm backwardafter each
eligibility traceextendingto future ratherthanto paststates.Their
resultssuggestedthat this was moreaccurateandefficient than forward learning. In
updatingthe weightsof the network, they alsosometimesperformedmultiple weight
updateswhen the TO error was large. This is apparentlyequivalentto dynamically
varying the step-sizeparameterin an error-dependentway during learning.
They also hied an experiencereplay techniquedue to Lin ( 1992). At any point
in learning, the agent rememberedthe best episodeup to that point. After every
, it replayedthis rememberedepisode, learning from it as if it were
a new episode. At the start of training, they similarly allowed the systemto learn
from episodesgeneratedby a good scheduler
in learning. To make the lookaheadsearchfaster for large-scaleproblems, which
typically had a branchingfactor of about20, they useda variantthey called random
samplegreedysearchthat estimatedthe greedyaction by consideringonly random
samplesof actions, increasingthe samplesize until a presetconfidencewasreached
that the greedy action of the samplewas the true greedy action. Finally, having
discoveredthat learning could be slowedconsiderably by excessivelooping in the
schedulingprocess,theymadetheir systemexplicitly checkfor loopsandalteraction
selectionswhena loop wasdetected.Although all of thesetechniquescould improve
the efficiencyof learning, it is not clear how crucial all of them were for the success
Zhang and Dietterich experimentedwith two different network architectures
the first version of their system, each schedulewas representedusing a set of 20
handcraftedfeatures. To define thesefeatures, they studiedsmall schedulingproblems
to find featuresthat had someability to predict RDF. For example, experience
with small problemsshowedthat only four of the resourcepools tendedto causeallocation
problems. The meanand standarddeviationof eachof thesepools unused
portionsover the entire schedulewerecomputed, resultingin 8 real-valuedfeatures.
Two otherfeatureswerethe RDF of thecurrentscheduleandthe percentageof its duration
during which it violatedresourceconstraints.The networkhad 20 input units,
one for eachfeature, a hiddenlayer of 40 sigmoidalunits, and an output layer of 8
sigmoidalunits. Theoutputunitscodedthe valueof a scheduleusinga codein which,
roughly, the locationof the activity peakoverthe 8 unitsrepresented
the appropriateTD error, the network weightswere updatedusingerror backpropagation, with the multiple weight-updatetechniquementionedabove.
The secondversion of the system (Zhang and Dietterich, 1996) used a more
complicatedtime-delay neural network (TDNN ) borrowedfrom the field of speech
recognition(Lang, Waibel, and Hinton, 1990). This versiondivided eachschedule
into a sequenceof blocks (maximal time intervalsduring which tasksand resource
assignmentsdid not change) and representedeachblock by a set of featuressimilar
to thoseusedin the first program. It then scanneda set of " kernel" networksacross
the blocks to createa set of more abstractfeatures. Since different scheduleshad
different numbersof blocks, anotherlayer averagedtheseabstractfeaturesovereach
of acceptedschedulerepairs. Reprintedwith permissionfrom
third of the blocks. Then a final layer of 8 sigmoidal output units representedthe
schedule's valueusingthe samecodeasin the first versionof the system. In all, this
A set of 100 artificial schedulingproblems was constructedand divided into
subsetsusedfor training, determiningwhen to stop training (a validation set), and
final testing. During training they testedthe systemon the validationset after every
100episodesand stoppedtraining when performanceon the validation set stopped
changing, which generallytook about 10,000 episodes
different valuesof )" (0.2 and 0.7), with threedifferent training sets, and they saved
both the final setof weightsandthe setof weightsproducingthe bestperformanceon
the validationset. Countingeachsetof weightsasa different network, this produced
12 networks, eachof which correspondedto a different schedulingalgorithm.
Figure 11.11showshow the meanperformanceof the 12TDNN networks(labeled
0 I2TDN ) comparedwith the performancesof two versionsof Zweben, Daun, and
Deale's iterative repair algorithm, one using the numberof constraintviolations as
the function to be minimizedby simulatedannealing(IR-V ) andthe other usingthe
RDF measure(IR-RDF) . The figure also showsthe performanceof the first version
of their systemthat did not usea TDNN (0 I2N ). The meanRDF of the bestschedule
found by repeatedlyrunning an algorithm is plotted againstthe total number of
Figure 11.12 Comparisonof CPU time. Reprintedwith permissionfrom Zhang and Dietterich, 1996.
schedulerepairs (using a log scale). Theseresults show that the learning system
producedschedulingalgorithms that neededmany fewer repairs to find conftictfree schedulesof the samequality as thosefound by the iterativerepair algorithms.
Figure 11.12 comparesthe computertime requiredby eachschedulingalgorithmto
find schedulesof variousRDFs. Accordingto this measureof perfonnance,the best
trade-off betweencomputertime andschedulequality is producedby the non-TDNN
algorithm (GI2N ). The TDNN algorithm(GI2TDN ) suffereddueto the time it took
to apply the kernel-scanningprocess, but Zhang and Dietterich point out that there
Theseresultsdo not unequivocallyestablishthe utility of reinforcementlearning
for job- shop schedulingor for other difficult searchproblems. But they do suggest
that it is possibleto use reinforcementlearning methodsto learn how to improve
the efficiency of search. Zhang and Dietterich' s job- shop schedulingsystemis the
first successfulinstanceof which we are aware in which reinforcementlearning
was applied in plan-space, that is, in which statesare complete plans (job-shop
schedulesin this case), and actionsare plan modifications. This is a more abstract
applicationof reinforcementlearningthan we are usedto thinking about. Note that
in this applicationthe systemlearnednotjust to efficiently createonegoodschedule
a skill that would not be particularly useful; it learnedhow to quickly find good
schedules for a class of related scheduling problems . It is clear that Zhang and
Dietterich went through a lot of trial -and-error learning of their own in developing
this example. But remember that this was a groundbreaking exploration of a new
aspect of reinforcement learning . We expect that future applications of this kind and
complexity will become more routine as experience accumulates.
Agre, P. E. ( 1988). The Dynamic Structure of EverydayUfe . PhiD. d1esis
Instituteof Technology.AI -TR 1085, MIT Artificial IntelligenceLaboratory.
Agre, P. E., andChapman,D. ( 1990) . What areplansfor? Roboticsand AutonomousSystems
Albus, J. S. ( 1971). A theoryof cerebellar
Anderson, C. W. ( 1986). Learning and ProblemSolving with Multilayer ConnectionistSystems
. PhiD. thesis, University of Massachusetts
Anderson, C. W. ( 1987). Strategylearningwith multilayer connectionistrepresentations
of the Fourth International Workshopon MachineLearning, pp. 103- 114. Morgan
Anderson, J. A., Silverstein, J. W., Ritz, S. A., and Jones, R. S. ( 1977). Distinctive features,
categoricalperception, and probability learning: Someapplicationsof a neural model. Psychological
Andreae, J. H. ( 1963) . STELLA: A schemefor a learningmachine. In Proceedingsof the 2nd
, Basle, pp. 497- 502. Butterworths, London.
Andreae, J. H. ( 1969a). A learningmachinewith monologue. InternationalJournal of ManMachineStudies, 1:1- 20.
Andreae, J. H. ( 1969b). Learningmachines- a unified view. In A. R. MeethamandR. A. Hudson (eds.), Encyclopediaof Infonnation, Linguistics, and Control, pp. 261- 270. Pergamon
Andreae, J. H. ( 1977) . Thinkingwith the TeachableMachine. AcademicPress, London.
Baird, L. C. ( 1995). Residualalgorithms: Reinforcementlearningwith function approximation
. In Proceedingsof the TwelfthInternational Conferenceon MachineLearning, pp. 30- 37.
, C. G., Diaferis, T. E., Gandhi, A. D., and Looze, D. P. ( 1994) . Elevator
dispatchersfor down peak traffic. Technicalreport. ECE Department
Barnard , E. ( 1993) . Temporal -difference medlods and Markov models. IEEE Transactions on
Barto, A. G. ( 1985). Learningby statistical
Barto, A. G. ( 1986). Game-theoretic cooperativity in networks of self-interestedunits. In
J. S. Denker(ed.), Neuml Networksfor Computing, pp. 41- 46. AmericanInstituteof Physics,
Barto, A. G. ( 1990) . Connectionistlearningfor control: An overview. In T. Miller , R. S. Sutton,
and P. J. Werbos(eds.), Neural Networksfor Control, pp. 5- 58. MIT Press, Cambridge, MA .
Barto, A. G. ( 1991). Some learning tasks from a control perspective
D. L. Stein (eds.), 1990 Lecturesin ComplexSystems
Barto, A. G. ( 1992) . Reinforcementlearningand adaptivecritic methods. In D. A. White and
D. A. Sofge(eds.), Handbookof Intelligent Control: Neural, Fuzzy, andAdaptiveApproaches,
pp. 469- 491. VanNostrandReinhold, New York.
Barto, A. G. ( 1995a). Adaptivecritics and the basalganglia. In J. C. Hook, J. L. Davis, and
D. G. Beiser (eds.), Models of Information Processingin the Basal Ganglia, pp. 215- 232.
Barto, A. G. ( 1995b). Reinforcementlearning. In M. A . Arbib (ed.), Handbookof Brain
Theoryand Neural Networks, pp. 804- 809. MIT Press, Cambridge, MA .
Barto, A. G., andAnandan, P. ( 1985). Patternrecognizingstochasticlearningautomata.IEEE
Barto, A. G., and Anderson, C. W. ( 1985). Structura11earning
Programof the SeventhAnnual Conferenceof the CognitiveScienceSociety, pp. 43- 54.
Barto, A . G., Anderson, C. W., and Sutton, R. S. ( 1982). Synthesisof nonlinear control
surfacesby a layeredassociativesearchnetwork. Biological Cybernetics
Barto, A. G., Bradtke, S. J., and Singh, S. P. ( 1991). Real-time learning and control using
asynchronousdynamicprogramming. TechnicalReport91-57. Depamnentof Computerand
Information Science, University of Massachusetts
Barto, A. G., Bradtke, S. J., and Singh, S. P. ( 1995) . Learningto act using real-time dynamic
programming.Artificial Intelligence, 72:81- 138.
Barto, A. G., and Duff , M. ( 1994) . Monte Carlo matrix inversionand reinforcementlearning.
In J. D. Cohen, G. Tesauro, and J. Alspector (eds.), Advancesin Neural Information Pro-
Barto, A. G., andJordan, M . I. ( 1987). Gradientfollowing without back-propagationin layered
networks. In M. Caudill andC. Butler (eds.), Proceedingsof theIEEE First Annual Conference
Barto, A. G., and Sutton, R. S. ( 1981a). Goal seekingcomponentsfor adaptiveintelligence:
. TechnicalReportAFWAL- TR-81- 1070. Air ForceWright Aeronautical
/ Avionics Laboratory, Wright-PattersonAFR, 08 .
Barto, A. G., and Sutton, R. S. ( 1981b). Landmark learning: An illustration of associative
Barto, A. G., and Sutton, R. S. ( 1982). Simulation of anticipatory responsesin classical
conditioningby a neuron-like adaptiveelement. BehaviouralBrain Research
Barto, A. G., Sutton, R. S., and Anderson, C. W. ( 1983). Neuronlikeelementsthat can solve
difficult learning control problems. IEEE Transactionson Systems
13:835- 846. Reprintedin J. A. Andersonand E. Rosenfeld(eds.), Neurocomputing
, pp. 53S- S49. MIT Press, Cambridge, MA , 1988.
Barto, A. G., Sutton, R. S., and Brouwer, P. S. ( 1981). Associativesearchnetwork: A reinforcement
learningassociativememory. Biological Cybernetics
Bellman, R. E. ( 1956). A problemin the sequentialdesignof experiments
Bellman, R. E. ( 1957a). DynamicProgramming. PrincetonUniversityPress, Princeton.
Bellman, R. E., and Dreyfus, SE . ( 1959) . Functionalapproximationsanddynamicprogramming
. MathematicalTablesand OtherAids to Computation, 13:247- 251.
Bellman, R. E., Kalaba, R., and Kotkin, B. ( 1973). Polynomialapproximation- A new computationaltechniquein dynamicprogramming: Allocation processes. MathematicalComputation
Berry, D. A., and Fristedt, B. ( 1985). Bandit Problems. Chapmanand Hall, London.
, D. P. ( 1982). Distributeddynamicprogramming. IEEE Transactionson Automatic
, D. P. ( 1983). Distributedasynchronouscomputationof fixed points. Mathematical
, D. P. ( 1987). Dynamic Programming: Deterministic and StochasticModels.
. D. P. ( 1995) . Dynamic Programmingand Optimal Control. Athena Scientific.
. D. P.. andTsitsiklis. J. N. ( 1989) . ParallelandDistributedComputation:Numerical
Methods. Prentice-Hall. EnglewoodCliffs . NJ.
. D. P.. andTsitsiklis. J. N. ( 1996). NeuroDynamicProgramming.AthenaScientific.
Biennann. A. W.. Fairfield. J. R. C.. and BeresT . R. ( 1982). Signaturetable systemsand
Bishop. C. M. ( 1995) . Neural Networksfor PanernRecognition. Clarendon. Oxford.
Booker. L. B. ( 1982). Intelligent Behavioras an Adaptationto the TaskEnvironment. PhiD.
thesis. University of Michigan. Ann Arbor.
Boone. G. ( 1997) . Minimum-time control of the acrobot. in 1997InternationalConferenceon
Roboticsand Automation. pp. 3281- 3287. IEEE Roboticsand AutomationSociety.
Boutilier, C., Dearden,R., andGoldszmidt, M. ( 1995). Exploiting structurein policy construction
. 1nProceedingsof theFourteenthInternationalJoint ConferenceonArtificial Intelligence,
Boyan, J. A., and Moore, A. W. ( 1995). Generalizationin reinforcementlearning: Safely
approximatingthe valuefunction. In G. Tesauro,D. S. Touretzky, andT. Leen(eds.), Advances
Boyan, J. A., Moore, A. W., and Sutton, R. S. (eds.). ( 1995). Proceedingsof the Workshopon
. MachineLearning Conference1995. TechnicalReportCMUValueFunctionApproximation
CS-95-206. Schoolof ComputerScience,CarnegieMellon University, Pittsburgh, PA.
Bradtke, S. J. ( 1993). Reinforcementlearning applied to linear quadratic regulation. In
S. J. Hanson, J. D. Cowan, andC. L . Giles (eds.), Advancesin Neumllnformation Processing
, pp. 295- 302. Morgan Kaufmann, SanMateo,
Bradtke, s . J. ( 1994) . IncrementalDynamic Programmingfor On-Une Adaptive Optimal
Control. Ph.D. thesis, Universityof Massachusetts
Bradtke, S. J., andBarto, A. G. ( 1996). Linearleast--squares
S. J. Bradtke, B. E. Ydstie, andA. G. Barto ( 1994). Adaptivelinearquadraticcontrol usingpolicy
iteration. In Proceedingsof the American Control Conference
Bradtke, S. J., and Duff , MO . ( 1995). Reinforcementlearningmethodsfor continuous-time
Markov decision problems. In G. Tesauro, D. Touretzky, and T. Leen (eds.), Advancesin
Bridle, J. S. ( 1990) . Training stochasticmodelrecognitionalgorithmsasnetworkscan leadto
maximummutual informationestimatesof parameters
Broomhead, D. S., and LoweD . ( 1988). Multivariable functional interpolationand adaptive
, 13(3):26Bryson, A. E., Jr. ( 1996) . Optimal control- 1950to 1985. IEEE Control Systems
Bush, R. R., and Mosteller, F. ( 1955). StochasticModelsfor Learning. Wiley, New York.
Byrne, J. H., Gingrich, K. J., and BaxterD . A. ( 1990) . Computationalcapabilitiesof single
neurons: Relationshipto simple forms of associativeand nonassociativelearningin aplysia.
In R. D. Hawkins and G. H. Bower (eds.), ComputationalModels of Leaming, pp. 31- 63.
Campbell, D. T. ( 1960). Blind variation and selective survival as a general Sb
es. In M. C. Yovits and S. Cameron (eds.), Self-Organizing Systems
Carlsb'Om, J., and NordSb'Om, E. ( 1997). Control of self-similar ATM call b' affic by reinforcement
learning. In Proceedingsof the International Workshopon Applicationsof Neural
Chapman,D., andKaelbling, LP . ( 1991). Input generalizationin delayedreinforcementlearning
. In Proceedingsof the 1WelfthInternational
Chow, C.-S., andTsitsiklis, J. N. ( 1991). An optimal one-way multigrid algorid1mfordiscretetime stochasticcontrol. IEEE Transactionson AutomaticControl, 36:898- 914.
Chrisman, L. ( 1992). Reinforcementlearning with perceptualaliasing: The perceptualdistinctions
approach.In Proceedingsof the TenthNational Conferenceon Artificial Intelligence,
pp. 183- 188. AAA Il MITPress, Menlo Park, CA.
, J., and Korf, RE . ( 1986). A unified theory of heuristic evaluationfunctions
and its applicationto learning. In Proceedingsof the Fifth National Conferenceon Artificial
Intelligence, pp. 148- 152. Morgan Kaufmann, SanMateo, CA.
Cichosz, P. ( 1995). Truncatingtemporaldifferences:On the efficient implementationofffi ()' )
for reinforcementlearning. Journal of Artificial IntelligenceResearch
Clark, W. A., and Farley, B. G. ( 1955). Generalizationof pattern recognition in a selforganizing
system. In Proceedingsof the 1955WesternJoint ComputerConference
Clouse, J. ( 1996). On IntegratingApprenticeLearning and ReinforcementLearning TITLE2.
Ph.D. thesis, University of Massachusetts
, Amherst. Appearedas CMPSCI TechnicalReport
. TechnicalReportAI-TRfor anartificialcreature
Crites, R. H. ( 1996). Large-ScaleDynamicOptimizationUsingTeamsof Reinforcement
). WithoutMiracles: UniversalSelectionTheoryandtheSecond
Daniel,J. W. ( 1976). Splinesandefficiencyin dynamicprogramming
, P., andHinton, G. E. ( 1993). Feudalreinforcement
Dean,T., andLin, SH . ( 1995). Decomposition
: A unifiedview. In A. PrieditisandS. Russell(eds.), Proceedings
, S. E., andLaw, A. M. ( 1977). TheArt and Theoryof DynamicProgramming
, R. 0 ., andHart, P. E. ( 1973). PanernClassification
Duff, M. O. ( 1995). Q-leamingfor banditproblems
Estes, W. K. ( 1950). Towarda statisticaldteoryof learning. PsychololgicalReview, 57:94- 107.
Farley, B. G., and Clark, W. A. ( 1954). Simulation of self-organizing systemsby digital
computer. IRE Transactionson Information Theory, 4:76- 84.
Feldbaum, A. A. ( 1965). Optimal Control Systems
Friston, K. J., Tononi, G., Reeke, G. N., Spoms, 0 ., and Edelman, G. M . ( 1994). Valuedependentselection in the brain: Simulation in a synthetic neural model. Neuroscience
Fu, K. S. ( 1970). Learning control systems- Review and outlook. IEEE Transactionson
, M . ( 1956). On thought: The extrinsic theory. Psychological
GallantS . I. ( 1993). Neural Network Learning and Expert Systems
Gillrno , O. and Asplund, L. ( 1995). Reinforcementlearningby constructionof hypothetical
targets. In J. Alspector, R. Goodman, and T. X. Brown (eds.), Proceedingsof the International
Workshopon Applicationsof Neural Networksto Telecommunications
Gardner, M. ( 1973). Mathematicalgames. ScientificAmerican, 228( 1) :108- 115.
Gelperin, A., Hopfield, J. J., andTank, D. W. ( 1985). The logic of limax learning. In ASelver
Gittins, J. C., andJones, D. M . ( 1974). A dynamicallocationindexfor the sequentialdesignof
. In J. Gani, K. Sarkadi, and I. Vincze (eds.), Progressin Statistics, pp. 241- 266.
Goldberg, D. E. ( 1989). GeneticAlgorithms in Search
Goldstein, H. ( 1957) . ClassicalMechanics. Addison-Wesley, Reading, MA .
Goodwin, G. C., and Sin, K. S. ( 1984) . AdaptiveFiltering Predictionand Control. PrenticeHall, EnglewoodCliffs , NJ.
Gordon, G. J. ( 1995). Stablefunction approximationin dynamicprogramming.In A. Prieditis
andS. Russell(eds.), Proceedingsof the 1WelfthInternational Conferenceon MachineLearning
, pp. 261- 268. Morgan Kaufmann, SanFrancisco. An expandedversionwas publishedas
TechnicalReportCMU-CS-95- 103. CarnegieMellon University, Pittsburgh, PA, 1995.
Gordon, G. J. ( 1996). Stablefitted reinforcementlearning. In D. S. Touretzky, M. C. Mozer,
M. E. Hasselmo(eds.), Advancesin Neural Infonnation ProcessingSystems
, pp. 1052- 1058. MIT Press, Cambridge, MA .
Griffith , A. K. ( 1966). A new machinelearning techniqueapplied to the gameof checkers.
TechnicalReport Project MAC, Artificial IntelligenceMemo 94. Massachusetts
Griffith , A. K. ( 1974). A comparisonand evaluationof threemachinelearningproceduresas
appliedto the gameof checkers.Artificial Intelligence, 5:137 148.
Gullapalli, V. ( 1990) . A stochasticreinforcementalgorithmfor learningreal valuedfunctions.
Gurvits, L., Lin , L.-J., and Hanson, S. J. ( 1994). Incrementallearningof evaluationfunctions
for absorbingMarkov chains: New methodsandtheorems.Preprint.
Hampson, S. E. ( 1983) . A Neural Model of Adaptive Behavior. PhiD. thesis, University of
Hampson, S. E. ( 1989). ConnectionistProblemSolving: ComputationalAspectsof Biological
Hawkins, R. D., and Kandel, E. R. ( 1984) . Is therea cell-biological alphabetfor simpleforms
of learning? PsychologicalReview, 91:375- 391.
, R. J. ( 1969). Brownianmotionandpotentialdleory.Scientific
, J. ( 1997). LTSMcan solvehardtime lag problems
Holland,J. H. ( 1975). Adaptationin NarumlandArtificialSystems
. In R. RosenandF. M. Snell(eds.), Progress
andusesneuralsignalsdlatpredictreinforcement
. In J. C. Hook, J. L. Davis,andD. G. Beiser
in the BasalGanglia, pp. 249- 270. MIT Press
on DecisionandControl, pp. 1283of the28thConference
In Proceedingsof the TenthInternational Conferenc~ on Machine Leaminx. pp. 167- 173.
Kumar, P. R., and Varaiya, P. ( 1986) . StochasticSystems
AdaptiveControl. Prentice-Hall, EnglewoodCliffs , NJ.
Kumar, P. R. ( 1985). A surveyof someresultsin stochasticadaptivecontrol. SIAM Journal of
Kumar, V., and Kanal, L. N. ( 1988). The CDP: A unifying fonnulation for heuristic search,
dynamicprogramming, andbranch-and-bound. In L . N. Kanal andV. Kumar (eds.), Searchin
Artificial Intelligence, pp. 1- 37. Springer-Verlag, Berlin.
Kushner, H. J., andDupuis, P. ( 1992). NumericalMethodsfor StochasticControl Problemsin
Continuouslime . Springer-Verlag, New York.
Lai, T. L. ( 1987). Adaptive treatmentallocation and the multiarmed bandit problem. The
Lang, K. J., Waibel, A . H., andHinton, G. E. ( 1990) . A time- delayneuralnetworkarchitecture
for isolatedword recognition. Neural Networks, 3:33- 43.
Lin , C.-S., and Kim , H. ( 1991). CMAC-basedadaptivecritic self-learning control. IEEE
Transactionson Neural Networks, 2:530- 533.
Lin , L.-J. ( 1992). Self-improving reactiveagentsbasedon reinforcementlearning, planning
andteaching. MachineLearning, 8:293- 321.
Lin , L.-J., andMitchell, T. ( 1992). Reinforcementlearningwith hiddenstates. In Proceedings
of the SecondInternational Conferenceon Simulationof AdaptiveBehavior: FromAnimalsto
Animats, pp. 271- 280. Mit Press, Cambridge, MA .
Littman, M . L. ( 1994). Markov gamesasa frameworkfor multiagent reinforcementlearning.
In Proceedingsof the EleventhInternational Conferenceon MachineLearning, pp. 157- 163.
. ( 1995). Learningpolicies for partially
: Scalingup. In A. Prieditis and S. Russell(eds.), Proceedingsof the
n. velfthInternational Conferenceon MachineLearning, pp. 362- 370. MorganKaufmann, San
Littman, M. L., Dean, T. L., andKaelbling
, T. ( 1983). TheoryandPracticeof Recursive
Luce, D. ( 1959). IndividualChoiceBehavior
adviceinto agentsthatlearnfrom reinforceme
. In J. M. MendelandK. S. Fu (eds.), Adaptive
). Trial anderror. In S. A. BarnettandA. McLaren(eds.), Science
Michie, D. ( 1974). OnMachineIntelligence
E. Dale and D. Michie (eds.), Machine Intelligence2, pp. 137- 152. Oliver and Boyd, Edinburgh
Miller , S., andWilliams, R. J. ( 1992) . Learningto cannol a bioreactorusinga neuralnetDyna,
Q system. In Proceedingsof the SeventhYaleWorkshopon Adaptiveand Learning Systems
pp. 167- 172. Centerfor SystemsScience, DunhamLaboratory, Yale University, New Haven.
, S. M ., and Kim , A. ( 1994). Neuralnetworkconnol ofdynaInic balance
for a bipedwalking robot. In Proceedingsof the Eighth YaleWorkshopon Adaptiveand Learning
, pp. 156- 161. Centerfor SystemsScience, DunhamLaboratory, Yale University,
Minsky, M . L. ( 1954). Theoryof Neural-AnalogReinforcementSystemsand Its Application to
the Brain-Model Problem. PhiD. thesis, PrincetonUniversity.
Minsky, M . L . ( 1961). Stepstowardartificial intelligence. Proceedingsof theInstituteof Radio
Engineers, 49:8- 30. Reprintedin E. A. Feigenbaumand J. Feldman(eds.), Computersand
Thought, pp. 406- 450. McGraw-Hill , New York, 1963.
Minsky, M . L. ( 1967). Computation: Finite and Infinite Machines. Prentice-Hall, Englewood
Montague, P. R., Dayan, P., and Sejnowski, T. J. ( 1996) . A framework for mesencephalic
, 16: 1936dopaInine systemsbasedon predictiveHebbianlearning. Journal of Neuroscience
Moore, A. W. ( 1990). Efficient Memory-Based Learningfor Robot Control. PhiD. thesis,
Moore, A. W. ( 1994) . The parti-gamealgorithmfor variableresolutionreinforcementlearning
. In J. D. Cohen, G. Tesauroand J. Alspector(eds.), Advancesin
Moore, A. W., and Atkeson, C. G. ( 1993). Prioritized sweeping: Reinforcementlearningwith
lessdataand lessreal time. MachineLearning, 13:103- 130.
Moore, J. W., Desmond, J. E., Berthier, N. E., Blazis, E. J., Sutton, R. S., and Barto, A. G.
( 1986) . Simulationof the classicallyconditionednictitating membraneresponseby aneuronlike
adaptiveelement: I. Responsetopography, neuronalfiring , and interstimulusintervals.
, M. A. L. ( 1989). Learning Automata: An Introduction.
, K. S., and Wheeler, R. M . ( 1986). Decentralizedlearningin finite Markov chains.
IEEE Transactionson AutomaticControl, AC31(6):519- 526.
Nie, J., andHaykin, S. ( 1996). A dynamicchannelassignmentpolicy throughQ-learning. CRL
Report334. CommunicationsResearchLaboratory, McMasterUniversity, Hamilton , Ontario.
Page, C. V. ( 1977). Heuristicsfor signaturetable analysisas a patternrecognitiontechnique.
Parr, R., and RussellS . ( 1995) . Approximating optimal policies for partially observable
stochasticdomains. In Proceedingsof the FourteenthInternational Joint Conferenceon Ar tificiallntelligence , pp. 1088- 1094, Morgan Kaufmann.
Pavlov, P. I. ( 1927). ConditionedRejiexes
Pearl, J. ( 1984). Heuristics: Intelligent SearchStrategiesfor ComputerProblem Solving.
Peng, J. ( 1993). Efficient Dynamic Programming-BasedLearningfor Control. PhiD. thesis,
Peng, J. and Williams, R. J. ( 1993). Efficient learningand planning within the Dyna framework
Peng, J., and Williams, R. J. ( 1994). Incrementalmulti-stepQ-learning. In W. W. Cohenand
H. Hirsh (eds.), Proceedingsof the EleventhInternational Conferenceon MachineLearning,
pp. 226--232. Morgan Kaufmann, SanFrancisco.
Peng, J., and Williams, R. J. ( 1996). Incrementalmulti-stepQ-learning. Machine Learning,
, M. A. L. ( 1995). Local andglobal optimizationalgorithms
for generalizedlearningautomata.Neural Computation,7:950--973.
Poggio, T., and Girosi, F. ( 1989). A theory of networks for approximationand learning.
A.I. Memo 1140. Artificial IntelligenceLaboratory, Massachusetts
Poggio, T., andGirosi, F. ( 1990). Regularizationalgorithmsfor learningthat areequivalentto
multilayer networks. Science,247:978- 982.
Powell, M . J. D. ( 1987). Radial basisfunctions for multivariate interpolation: A review. In
J. C. Mason and M . G. Cox (eds.), Algorithmsfor Approximation, pp. 143--167. Clarendon
Puterman,M. L. ( 1994). Markov DecisionProblems. Wiley, New York.
Puterman,M . L., and Shin, M. C. ( 1978). Modified policy iteration algorithmsfor discounted
Markov decisionproblems. ManagementScience,24: 1127- 1137.
Reetz, D. ( 1977). Approximatesolutionsof a discountedMarkoviandecisionprocess.Bonner
Ring, M. B. ( 1994) . Continual Learning in ReinforcementEnvironments
Rivest, R. L ., and Schapire, RE . ( 1987). Diversity-basedinferenceof finite automata. In
-Eighth Annual Symposiumon Foundationsof ComputerScience,
ComputerSocietyPressof the IEEE, Washington
Robbins, H. ( 1952). Some aspectsof the sequentialdesign of experiments
AmericanMathematicalSociety, 58:527- 535.
Robertie, B. ( 1992). Carbonversussilicon: Matching wits with m -Gammon. Inside Backgammon
, F. ( 1962). Principles of Neurodynamics
Ross, S. ( 1983). Introduction to StochasticDynamic Programming. Academic Press, New
Rubinstein, R. Y. ( 1981). Simulationand the Monte Carlo Method. Wiley, New York.
Rumelhart, D. E., Hinton, G. E., andWilliams, R. J. ( 1986). Learninginternalrepresentations
by error propagation.In DE . RumelhartandJ. L. McClelland(eds.), Parallel DistributedPro-
. , G. A . ( 1995). ProblemSolving with ReinforcementLearning. PhiD. thesis, Cam-
, M. ( 1994). On-line Q-learningusingconnectionist
TechnicalReportCUEDIFI NFENGtr R 166. Engineering
RussellS., andNorvig, P. ( 1995). ArtificiallnteUigence
Rust, J. ( 1996). Numericaldynamicprogramming
). Somestudiesin machinelearningusingthegameof checkers
, 3:211- 229. Reprintedin E. A. Feigenbaum
, A. L. ( 1967). Somestudiesin machinelearningusingthe gameof checkers
Schwartz, A. ( 1993) . A reinforcementlearningmethodfor maximizingundiscountedrewards.
In Proceedingsof the TenthInternational Conferenceon Machine Learning, pp. 298- 305.
Schweitzer,P. J., andSeidmann,A. ( 1985) . Generalizedpolynomialapproximationsin Markovian decisionprocesses. Journal of MathematicalAnalysisand Applications, 110:568- 582.
Selfridge, O. J., Sutton, R. S., and Barto, A. G. ( 1985). Training and tracking in robotics. In
A. Joshi (ed.), Proceedingsof the Ninth International Joint Conferenceon Artificiallntelli gence, pp. 670- 672. Morgan Kaufmann, SanMateo, CA.
Shannon,C. E. ( 1950). Programminga computerfor playing chess. PhilosophicalMagazine,
Shewchuk, J., and Dean, T. ( 1990). Towardslearningtime-varying functionswith high input
dimensionality. In Proceedingsof the Fifth IEEE International Symposiumon Intelligent
Control, pp. 383- 388. IEEE ComputerSocietyPress, Los Alamitos, CA.
Singh, S. P. ( 1992a). Reinforcementlearning with a hierarchy of abstractmodels. In Proceedings
of the TenthNational Conferenceon Artificial Intelligence, pp. 202- 207. AAAl / MIT
Singh, S. P. ( 1992b). Scalingreinforcementlearningalgorithmsby learningvariabletemporal
resolutionmodels. In Proceedingsof the Ninth International MachineLearning Conference
pp. 406- 415. MorganKaufmann, SanMateo, CA.
Singh, S. P. ( 1993). Learningto SolveMarkovianDecisionProcesses. PhiD. thesis, University
, Amherst. AppearedasCMPSCITechnicalReport93-77.
, D. ( 1997) . Reinforcementlearningfor dynamicchannelallocation
in cellular telephonesystems. In Advancesin Neural Information ProcessingSystems
, pp. 974- 980. MIT Press, Cambridge, MA .
, T., and Jordan, M. I. ( 1994). Learning without state-estimationin
partially observableMarkovian decision problems. In W. W. Cohen and H. Hirsch (eds.),
Proceedingsof the EleventhInternational Conferenceon Machine Learning, pp. 284- 292.
Singh, S. P., Jaakkola, T., andJordan, M . I. ( 1995). Reinforcementleafing with soft stateaggregation
. In G. Tesauro, D. S. Touretzky, T. Leen (eds.), Advancesin Neural Information
resultsfor single-stepon-policy reinforcement-learningalgorithms.
Singh, S. P., andSutton, R. S. ( 1996). Reinforcementlearningwith replacingeligibility traces.
Sivarajan, K. N., McEliece, R. J., and Ketchum, J. W. ( 1990). Dynamicchannelassignmentin
cellular radio. In Proceedingsof the 40th VehicularTechnologyConference
Skinner, B. F. ( 1938) . TheBehaviorof Organisms.Appleton-Century, New York.
Sofge, D. A., and White, D. A. ( 1992). Applied learning: Optimal control for manufacturing.
In D. A. White and D. A. Sofge(eds.), Handbookof Intelligent Control: Neural, Fuzzy, and
AdaptiveApproaches, pp. 259- 281. VanNosb' andReinhold, New York.
Spong, M. W. ( 1994). Swing up control of the acrobat. In Proceedingsof the 1994 IEEE
Conferenceon Roboticsand Automation, pp. 2356- 2361. IEEE ComputerSocietyPress, Los
Staddon, J. E. R. ( 1983) . Adaptive Behavior and Learning. CambridgeUniversity Press,
Sutton, R. S. ( 1978a). Learning theory support for a single channel theory of the brain.
Sutton, R. S. ( 1978b). Single channel theory: A neuronal theory of learning. Brain Theory
Newslener, 4:72- 75. Center for SystemsNeuroscience
Sutton, R. S. ( 1978c). A unified theoryof expectationin classicaland instrumentalconditionlog. Bachelorsthesis. StanfordUniversity.
Sutton, R. S. ( 1984). TemporalCredit Assignmentin ReinforcementLearning. PhiD. thesis,
Sutton, R. S. ( 1988) . Learning to predict by the method of temporaldifferences. Machine
Sutton, R. S. ( 1990). Integratedarchitecturesfor learning, planning, andreactingbasedon approximating
dynamicprogramming. In Proceedingsof the SeventhInternational Conference
on MachineLearning, pp. 216- 224. Morgan Kaufmann, SanMateo, CA.
Sutton, R. S. ( 1991a). Dyna, an integratedarchitecturefor learning, planning, and reacting.
Sutton, R. S. ( 1991b). Planningby incrementaldynamicprogramming. In L. A. Birnbaumand
G. C. Collins (eds.), Proceedingsof the Eighth International Workshopon MachineLearning,
pp. 353- 357. MorganKaufmann, SanMateo, CA.
Sutton, R. S. ( 1995). m models: Modeling the world at a mixture of time scales.In A. Prieditis and S. Russell (eds.), Proceedingsof the 1WelfthInternational Conferenceon Machine
Learning, pp. 531- 539. Morgan Kaufmann, SanFrancisco.
Sutton, R. S. ( 1996). Generalizationin reinforcementlearning: Successfulexamplesusing
sparsecoarsecoding. In D. S. Touretzky, M. C. Mozer andME . Hasselmo(eds.), Advancesin
Sutton, R. S. (ed.) . ( 1992) . Specialissueof MachineLearning on reinforcementlearning, 8.
Also publishedasReinforcementLearning. Kluwer Academic, Boston, 1992.
Sutton. R. S.. and Barto. A. G. ( 1981a). Toward a modem theory of adaptivenetworks:
Expectationandprediction. PsychologicalReview. 88:135- 170.
Sutton. R. S.. and Barto. A . G. ( 1981b). An adaptivenetwork that constructsand usesan
internalmodelof its world. Cognitionand Brain Theory. 3:217- 246.
Sutton, R. S., andBarto, A. G. ( 1987). A temporal-difJerencemodelof classicalconditioning.
In Proceedingsof the Ninth Annual Conferenceof the CognitiveScienceSociety, pp. 355-378.
Sutton, R. S., and Barto, A. G. ( 1990). Time-derivativemodelsof Pavlovianreinforcement.In
M . Gabriel and J. Moore (eds.), Learning and ComputationalNeuroscience
Sutton, R. S., andPinette, B. ( 1985) . The learningof world modelsby connectionistnetworks.
In Proceedingsof the SeventhAnnual Conferenceof the CognitiveScienceSociety, pp. 54-64.
Sutton, R. S., and Singh, S. ( 1994). On bias and step size in temporal-differencelearning.
In Proceedingsof the Eighth YaleWorkshopon Adaptiveand Learning Systems
Centerfor SystemsScience,DunhamLaboratory, YaleUniversity, New Haven.
Sutton, R. S., and Whitehead, D. S. ( 1993) . Online learningwith randomrepresentations
Proceedingsof the TenthInternational MachineLearning Conference
Tadepalli, P., and Ok, D. ( 1994). H-learning: A reinforcementlearning methodto optimize
undiscountedaveragereward. TechnicalReport94- 30- 01. OregonStateUniversity, Computer
Tan, M. ( 1991). Learninga cost-sensitiveinternal representationfor reinforcementlearning.
In L. A. BirnbaumandG. C. Collins (eds.), Proceedingsof the Eighth International Workshop
on MachineLearning, pp. 358- 362. Morgan Kaufmann, SanMateo, CA.
Tan, M. ( 1993) . Multi -agentreinforcementlearning: Independentvs. cooperativeagents. In
Proceedingsof the TenthInternational Conferenceon MachineLearning, pp. 330- 337. Morgan
Tesauro,G. J. ( 1986) . Simpleneuralmodelsof classicalconditioning. Biological Cybernetics
Tesauro, G. J. ( 1992) . Practical issuesin temporaldifference learning. Machine Learning,
Tesauro,G. J. ( 1994). m -Gammon, a self-teachingbackgammonprogram. achievesmasterlevel play. Neural Computation, 6( 2):215- 219.
Tesauro,G. J. ( 1995) . Temporaldifferencelearningandm -Gammon. Communications
Tesauro, G. J., and Galperin, G. R. ( 1997). On-line policy improvementusing Monte-Carlo
search. In Advancesin Neurallnforma1ion ProcessingSystems
, pp. 1068- 1074. MIT Press, Cambridge, MA .
Tham, C. K. ( 1994). Modular On-Line FunctionApproximationfor Scalingup Reinforcement
Learning. PhD thesis, CambridgeUniversity.
, M. A . L. and Sastty, P. S. ( 1985). A new approachto the designof reinforcement
schemesfor learningautomata. IEEE Transactionson Systems
Thompson, W. R. ( 1933) . On the likelihood that one unknownprobability exceedsanotherin
view of the evidenceof two samples.Biometrika, 25:285--294.
Thompson, W. R. ( 1934) . On the theoryof apportionment
Thorndike, E. L. ( 1911) . Animal Intelligence. Hafner, Darien, CT.
Thorp, E. O. ( 1966). Beat the Dealer: A Winning Strategyfor the Game of 1Wenty
Tolman, E. C. ( 1932). PurposiveBehaviorin Animalsand Men. Century, New York.
Tsetlin, M. L. ( 1973). AutomatonTheory and Modeling of Biological Systems
Tsitsiklis, J. N. ( 1994). Asynchronousstochasticapproximationand Q-learning. Machine
Tsitsiklis, J. N. and Van Roy, B. ( 1996) . Feature-basedmethodsfor large scale dynamic
Tsitsiklis, J. N., and Van Roy, B. ( l997a). An analysisof temporal-differencelearning with
function approximation. IEEE Transactionson AutomaticControl, 42:674- 690.
Tsitsiklis, J. N., and Van Roy, B. ( 1997b). Averagecost temporal-differencelearning. IEEE
Transactionson AutomaticControl, 42:674- 690.
Thring, A. M . ( 1950). Computingmachineryand intelligence, Mind , 59:433- 460. Reprinted
in E. A. FeigenbaumandJ. Feldman(eds), Computersand Thought, pp. 11-35. McGraw-Hill ,
Ungar, L. H. ( 1990). A bioreactorbenchmarkfor adaptivenetwork-basedprocesscontrol. In
W. T. Miller , R. S. Sutton, andP. J. Werbos(eds.), Neural Networks/ or Control, pp. 387- 402.
Waltz, M . D., and Fu, K. S. ( 1965). A heuristic approachto reinforcementlearningcontrol
systems.IEEE Transactionson AutomaticControl, 10:390- 398.
Watkins, C. J. C. H. ( 1989) . Learningfrom DelayedRewards. PhiD. thesis, CambridgeUniversity
Watkins, C. J. C. H., andDayan, P. ( 1992) . Q-Iearning. MachineLearning, 8:279- 292.
Werbos, P. J. ( 1977) . Advancedforecastingmethodsfor global crisis warningand modelsof
Werbos, P. J. ( 1982) . Applications of advancesin nonlinear sensitivity analysis. In R. F.
Drenick and F. Kozin (eds.), SystemModeling and Optimization, pp. 762- 770. SpringerVerlag, Berlin.
Werbos, P. J. ( 1987). Building and understandingadaptivesystems: A statistical/numerical
approachto factory automationandbrain research
Werbos, P. J. ( 1988). Generalizationof back propagationwith applicationsto a recurrentgas
marketmodel. Neural Networks, 1:339- 356.
Werbos, P. J. ( 1989) . Neuralnetworksfor control and systemidentification. In Proceedingsof
the 28th Conferenceon Decisionand Control, pp. 260- 265. IEEE Control SystemsSociety.
Werbos, P. J. ( 1990). Consistencyof HDP appliedto a simplereinforcementlearningproblem.
Werbos, P. J. ( 1992). Approximatedynamic programmingfor real-time control and neural
modeling. In D. A. White and D. A. Sofge (eds.), Handbookof Intelligent Control: Neural,
Fuzzy, and AdaptiveApproaches, pp. 493- 525. VanNostrandReinhold, New York.
White, D. J. ( 1969). DynamicProgramming. Holden-Day, SanFrancisco.
White, D. J. ( 1985). Realapplicationsof Markov decisionprocesses. Interfaces, 15:73- 83.
White, D. J. ( 1988) . Furtherreal applicationsof Markov decisionprocesses. Interfaces, 18:5561.
White, D. J. ( 1993) . A surveyof applicationsof Markov decisionprocesses. Journal of the
OperationalResearchSociety, 44:1073- 1096.
Whitehead, S. D., and Ballard, D. H. ( 1991). Learningto perceiveand act by trial and error.
Whitt, W. ( 1978) . Approximationsof dynamic programsI. Mathematicsof OperationsResearch
Whittle, P. ( 1982) . Optimizationover lime , vol. 1. Wiley, New York.
Whittle, P. ( 1983). Optimizationover lime , vol. 2. Wiley, New York.
Widrow, B., Gupta, N. K., and Maitra, S. ( 1973) . Punish/reward: Learning with a critic in
adaptivethresholdsystems.IEEE Transactionson Systems
). Adaptiveswitchingcircuits.In 1960WESCON
RecordPartIV , pp. 96- 104. Instituteof RadioEngineers
ReportICS8605.Institutefor CognitiveScience
ReportNUCCS-87-3. Collegeof ComputerScience, NortheasternUniversity, Boston.
Williams, R. J. ( 1988) . On the useof backpropagation
Proceedingsof the IEEE International Conferenceon Neural Networks, pp. 1263- 1270. IEEE
SanDiego sectionand IEEE TAB Neural Network Committee.
Williams, R. J. ( 1992). Simple statisticalgradient-following algorithmsfor connectionistreinforcement
Williams, R. J., and Baird, L. C. ( 1990). A mathematicalanalysisof actor-critic architectures
for learningoptimal controls through incrementaldynamic programming. In Proceedingsof
the Sixth YaleWorkshopon Adaptiveand Learning Systems
Science, DunhamLaboratory, YaleUniversity, New Haven.
WilsonS . W. ( 1994). ZCS: A zerothorderclassifiersystem. EvolutionaryComputation,2: 118.
Witten, I. H. ( 1976). The apparentconflict betweenestimationand control- A surveyof the
two- annedproblem. Journal of the Franklin Institute, 301:161- 189.
Witten, I. H. ( 1977) . An adaptiveoptimal controller for discrete-time Markov environments
Witten, I. H., andCorbin, M . J. ( 1973) . Humanoperatorsandautomaticadaptivecontrollers: A
comparativestudyon a particularcontrol task. InternationalJournalof Man- MachineStudies,
, S., Utgoff, P. E., and Barto, A. G. ( 1990) . Explaining temporaldifferences
to createuseful conceptsfor evaluatingstates. In Proceedingsof the Eighth National
Conferenceon Artificial Intelligence, pp. 882- 888. AMI Press, Menlo Park, CA.
Young, P. ( 1984). RecursiveEstimationand lime -SeriesAnalysis. Springer-Verlag, Berlin.
Zhang, W., and Dietterich, T. G. ( 1996). High perfonnancejob-shopschedulingwith a timo.
C. Mozer, M . E. Hasselmo(eds.), Advancesin
Zweben, M ., Daun, B., and Deale, M . ( 1994) . Schedulingand reschedulingwith iterative
Fox (eds.), Intelligent Scheduling, pp. 241- 255. Morgan
return (cumulativediscountedreward) following t
probability of taking actiona in an n armedbandit task
actiontakenin states underdeterministicpolicy 11
probability of taking actiona in states understochasticpolicy 11
setof all states, including the terminal state
probability of transitionfrom states to states underactiona
expectedimmediliterewardon transitionfrom s to s underactiona
valueof taking actiona in states underpolicy 7r
probability of randomactionin E-greedypolicy
valueof taking actiona in states underthe optimal policy
Selectivebootstrapadaptation,20, 225- 226
State(s), 52- 55, 61- 65, 81, 83. Seealso State
State-actioneligibility traces, 179- 180, 188
Statevaluesor Statevaluefunctions, 68- 73,
Targetor Targetfunction,37, 134, 164- 165,
Valuefunctions. 8- 9. 68- 80. 82. 229. 286
