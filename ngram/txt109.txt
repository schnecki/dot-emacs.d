Applied Artificial Intelligence, 19:783–809
ndez, Ricardo Aler, and Daniel Borrajo & Departamento de
a tica, Universidad Carlos III of Madrid, Legane´s (Madrid), Spain
& The application of AI planning techniques to manufacturing systems is being widely deployed
for all the tasks involved in the process, from product design to production planning and control.
One of these problems is the automatic generation of control sequences for the entire manufacturing
system in such a way that final plans can be directly used as the sequential control programs which
drive the operation of manufacturing systems. HYBIS is a hierarchical and nonlinear planner whose
goal is to obtain partially ordered plans at such a level of detail that they can be used as sequential
control programs for manufacturing systems. Currently, those sequential control programs are being
generated by hand using modeling tools. This document describes a work aimed to improve the
efficiency of solving problems with HYBIS by using machine learning techniques. It implements a
deductive learning method that is able to automatically acquire control knowledge (heuristics) by
generating bounded explanations of the problem-solving episodes. The learning approach builds
on HAMLET, a system that learns control knowledge in the form of control rules.
A manufacturing system consists of a set of processes, machines, and factories where raw products are transformed into higher-value manufactured
products. The design of the sequential control program for a manufacturing system, i.e., the ordered sequence of actions with all of the actions of
devices needed to transform the raw products into the manufactured ones,
is a difficult task that is traditionally carried out by engineers by hand, using
modeling tools. Artificial intelligence (AI) techniques are proving to be very
useful in solving this task, allowing for an error-free, fast, and low-cost building process of these control programs (Fabian et al. 1997; Gil 1991; Luger
noz-Avila and Huellen 1995; Nau et al. 1995;
Park et al. 1993). In particular, it is becoming an area of growing interest
This work was partially supported by a grant from the Ministerio de Ciencia y Tecnologı́a through
projects TAP1999-0535-C02-02, TIC2001-4936-E, and TIC2002-04146-C05-05. The authors would also
like to thank Luis Castillo and Juan Fdez-Olivares for their help on using HYBIS.
Address correspondence to Susana Fernandez, Departamento de Informatica, Universidad Carlos
III of Madrid, 2891, Leganés (Madrid), Spain. E-mail: susana.fernandez@uc3m.es
for researchers from the field of AI planning systems (Aylett et al. 1997;
Castillo et al. 2000a; Klein et al. 1998; Viswanathan et al. 1998). The most
commonly used planners in industry are based on hierarchical planning
techniques (Erol et al. 1994; Giunchiglia 1999; Sacerdoti 1974), which
are useful to represent the hierarchical structure of devices and their operation in manufacturing systems and are closer to the way control engineers
represent a control program (modular and hierarchical). A hierarchical planner has the effect of increasing the level of abstraction at which a human user
can operate, since it can allow problems to be posed as high-level goals and
work out the detailed implications. This makes planning quicker and also less
prone to error, as low-level interactions between actions are dealt with automatically. A good example is HYBIS (Castillo et al. 2000b), a hybrid planner
which solves real-world problems from manufacturing systems. It mixes hierarchical task network (HTN) approaches and partial order planning (POP)
techniques (Weld 1994). It decomposes a problem into subproblems using
either a default method or a user-defined decomposition method. Then, at
each level of abstraction, it generates a plan at that level using a POP.
In this paper, we propose to improve the efficiency of solving problems
with HYBIS, by automatically acquiring knowledge to guide the planning
process. This knowledge is based on the experience of solving previous real
problems. HYBIS has two main steps. The first one decomposes an action
into another set of actions in a lower level of the hierarchy (this is the
HTN part). Although the user can program specific decomposition methods, by default, HYBIS poses the decomposition problem as a planning problem for a lower level in the hierarchy. In any case, decomposition can be
done in many different ways, and it is useful to learn control knowledge
at this point so that good methods are tried first. The second step calls a
partial order planner at every level in the HTN hierarchy. POP planners
include decision points, such as whether to insert a new operator in the
plan, or to use an already existing one, which operator to insert=reuse,
etc. Our approach starts by obtaining a trace of HYBIS after solving a planning problem. This trace is a search tree, whose nodes are tagged by the
algorithm as successful, a failure, abandoned, or unexplored. At successful
nodes, our method learns control rules by goal regression and generalization. If the planner is executed again with the learned control rules, only
the successful nodes would be explored, with the subsequent efficiency gain.
BACKGROUND ON PLANNING IN MANUFACTURING SYSTEMS
The aim of this section is twofold. First, the manufacturing system
domain is explained by adopting a representation of the manufacturing
resources based on agents. Agent-based representation is an approach commonly used, although there are others, mostly based on objects (Algeo et al.
1996; Becker and Pereira 2002; Braatz 2003; Guang-hong and Zu-shu 2003;
Kiritsis et al. 1998; Peschke 2003). Second, it compares our approach with
previous related work, not only within the AI planning field, but also within
the context of the wider scientific literature, particularly in operation
The definition of manufacturing systems and their main characteristics
are widely explained in (Castillo et al. 2000a). In this section, we will point
out the main ideas. A manufacturing system is the set of processes,
machines, and factories where raw products are transformed into highervalue manufactured products. These transformations are carried out by
the devices of the manufacturing system. In an agent approach, every
device can be represented as an agent whose operation is described by a
finite state automata. Thus, every agent has a set of possible states and a
set of actions. Each action describes a transformation as well as a change
of state in the agent. Additionally, an agent has a name, which must be
unique, a set of variables, which are used to represent the objects related
to the operation of the agent (like products, chemicals, interconnection
points between agents, or constants), and a set of codesignation constraints
defined on the set of variables, which defines the set of valid values for every
variable. Every action of every agent is defined by a unique name, a set of
effects, which is represented by means of an addition list, and a deletion list
of literals that represent the transformation made by the action, and a set of
requirements. These are divided into a list of previous requirements, which
must hold before the action, a list of simultaneous requirements, which
must hold during the execution of the action, and a list of later requirements, which must hold after the action.
The description of the problems that appear in a manufacturing system
consists of a set of transformations which must be made on raw products in
order to obtain the manufactured ones. A problem is defined by the following components:
. Domain: A knowledge-based model of the manufacturing system. It is
comprised of a set of agents, which represent the set of devices, their
operations, and their interconnections described by the model of action;
and a set of axioms, which describe facts that are always true.
. Initial state: A conjunction of literals which describe what is true at the
starting point of both the manufacturing system, and the raw products.
. Goal: A conjunction of literals that describe the transformations needed
in order to obtain manufactured products from raw ones. This is known
in manufacturing as a recipe (ANSI=ISA 1995). In AI planning, a goal is a
conjunction of literals that describe a desired partial state instead of a
partially ordered set of subgoals, which describes a desired behavior, as
The solution of a problem is an ordered sequence of actions of the
agents of the domain which achieves the goal, starting from the specified
initial state. This is called a control sequence (or a plan in planning) and
it is a difficult task that is usually carried out by engineers. Traditionally,
control engineers have been using different methodologies, standards, formal tools, and computer utilities to carry out this task. The ISA-SP88
(ANSI=ISA 1995) standard is one of such methodologies used to hierarchically design control programs for manufacturing systems. This standard
allows for a hierarchical specification of physical, process, and control models of a manufacturing system.
Manufacturing planning researchers typically want to solve a particular
manufacturing problem, and present their research results within the context of this problem, without discussing how the approach might generalize
to other planning domains. This can lead AI planning researchers to view
manufacturing planning as a domain full of ad-hoc, domain-specific programs rather than general principles and approaches (Nau et al. 1995).
But manufacturing systems are evolving into a new generation of systems driven by the demands of a constantly changing market and they must adapt to
these changes in a timely fashion. There is no other alternative, but to generalize to other similar planning domains. This is specially certain nowadays
where the tendency is to manufacture under demand, flexibly, and quickness, allowing to produce different products in the same plant even if there
are faults such as broken machines or any other type of failure.
There are close synergies in the research carried out on planning and
other scientific areas. In particular, the field of operations research has an
extensive area in manufacturing scheduling (Braccesi et al. 2004; Haskose
et al. 2004; Heinonen and Pettersson 2003; Kaparthi et al. 1993; Valyov
et al. 2004). AI planning and scheduling complement each other, and overlap to some extent, but tackle different problems. AI planning is the application of artificial intelligence technologies to the problem of generating a
correct and efficient sequence of activities, chosen from a knowledge base
of generic actions, which, when executed, will move a system from some
initial state to some desirable end-state. This sequence is typically a partial
order one; only essential ordering relations between the actions are considered, so that actions allow a pseudo-parallelism and can be executed
in any order for achieving the desired goals. On the other hand, scheduling
is applied to the related problem of efficiently, or sometimes optimally, allocating time and other resources to an already-known sequence of actions
(plan). Scheduling can therefore be seen as selecting among the various
action sequences implicit in a partial-order plan in order to find the one
that meets efficiency or optimality conditions with respect to time constraints, and filling in all the resource’s details to the point at which each
In planning, several approaches have been successfully used in order to
guide the search process by adding control knowledge to the planning procedure, either by learning this control knowledge (Aler et al. 2002; Borrajo
and Veloso 1997; Estlin and Mooney 1997; Huang et al. 2000; Minton 1988;
Veloso 1994), or by adding it directly by a human (Bacchus and Kabanza
2000). Perhaps, the most basic scheme for learning control knowledge has
been deductive learning techniques that generate control rules from a single
or a set of problem-solving episodes and a correct underlying domain theory.
This is the case of pure EBL techniques (Kambhampati 2000; Minton 1988),
and techniques built on top of it (Aler et al. 2002). These rules can be used in
future situations to prune the search space. Others use case-based reasoning
for production scheduling (Bezirgan 1992; Cunningham and Smyth 1997;
Szelke and Markus 1997), but they focus on scheduling rather than on the
planning aspect. As far as we know, our approach is the only one which learns
control knowledge for HTN-POP planners and it can be applied to solve realworld problems from manufacturing systems.
Machine learning has also been extended to hierarchical planners. The
system HICAP (Hierarchical Interactive Case-Based Architecture for Planning)
noz-Avila et al. 1999) integrates the SHOP hierarchical planner (Nau
et al. 1999) together with a case-based reasoning (CBR) system named
NACODAE (Breslow and Aha 1997). HICAP has been used to assist with
the authoring of plans for noncombatant evacuation operations. KNOMIC
(Knowledge Mimic) (van Lent and Laird 1999) is a machine learning system that extracts hierarchical performance knowledge by observation to
develop automated agents that intelligently perform complex real-world
tasks. The knowledge representation learned by KNOMIC is a specialized
form of Soar’s production rule format (Laird et al. 1987). The rules
implement a hierarchy of operators with higher-level operators having multiple suboperators representing steps in the completion of the high-level
operator. These rules are then used by an agent to perform the same task.
CAMEL (Ilghami et al. 2002) uses an incremental algorithm to learn
expansions methods in an HTN planner under supervision of an expert.
In Garland et al. (2001), a technique called programming by demonstration is used to build a system in which a domain expert performs a task
by executing actions and then reviews and annotates a log of the actions.
This information is then used to learn hierarchical task models.
GIPO-II is a GUI able to create and maintain hierarchical domain specifications, and verify them using a structural property checker, and plan
using the forward hybrid task-reduction planner HYHTN (McCluskey et al.
2003). It is based on the previously released GIPO (Simpson et al. 2001),
the GUI and tools environment for building classical planning domain
models, providing help for those involved in knowledge acquisition and
the subsequent task of domain modeling. It integrates the algorithm
opmaker to induce operator schema from example sequences (McCluskey
et al. 2002). Although all these systems are applied to hierarchical planners
that solve problems of the real world, they differ from our approach in the
learning method. While we used deductive learning, they apply inductive
learning. Also, they focus on learning the domain model, not the heuristics, as it is in our case. One advantage of deductive learning approaches
is that it is enough to provide a few solved problems, as opposed to inductive methods that require a large number of examples.
A recent application of machine learning and rule-based techniques on
planning has been done to build an adaptive planning system, called HAP,
that can automatically fine-tune its parameters based on the values of specific measurable characteristics of each problem (Vrakas et al. 2003). Adaptation is guided by a rule-based system, whose knowledge has been acquired
through machine learning techniques. It uses the DM-II program (Liu et al.
1999) that performs classification based on association rules (Liu et al.
1998) in order to discover useful and interpretable rules from the data.
Neither can learn heuristics, nor can they use deductive learning.
An extensive survey of research work related to machine learning applied to automated planning can be found in (Zimmerman and
Other works related to our learning approach are those that have
been devoted to hyperheuristics. The term hyperheuristic was first used
in early 2000 to describe heuristics (or metaheuristics) that choose heuristics (Cowling et al. 2000). The hyperheuristics manage the choices of
which lower-level heuristic method should be applied at any given time,
depending on the characteristics of the region of the solution space currently under exploration. Hyperheuristics could be classified, depending
on the learning mechanism employed, such as genetic algorithms (Burke
et al. 2001; Han et al.), reinforcement learning (Nareyek 2001), and
case-based learning (Burke et al. 2002), etc. These systems work with
the idea that there are known heuristics, and they use inductive learning
methods that require many training examples. Our learning method is
able to acquire heuristics in the form of control rules without considering
the heuristics that might (or might not) use the planner. It implements a
deductive learning algorithm that is able to generate the control rules
HYBIS is a hierarchical and nonlinear planner with an automata-based
representation of operators, which is able to obtain control sequences for
manufacturing processes (Castillo et al. 2000b). The aim of HYBIS is to provide a tool for helping humans define such programs. Also, it allows to easily adapt to changes in the manufacturing plant requirements.
A domain model in HYBIS represents an industrial plant as a device hierarchy at different levels of granularity, which accepts ISA-SP88 descriptions.
A planning domain is represented as a hierarchy of agents where the root
(a dummy agent) represents the whole plant, leaf nodes are primitive agents
corresponding to the field devices of the plant, and intermediate nodes are
aggregate agents. The structure and behavior of the aggregate agents represent a composition of a set of agents at lower levels of abstraction. Properties and behavior of a given aggregate agent are related with those of its
components by means of the interface of the aggregate agent. The user
can also program specific decomposition methods defining the aggregate
action. The expansion slot of an aggregate activity is used to specify different ways to carry out that activity. These alternative ways are described as
a set of different methods where every method is represented by a set of
ordered literals representing a problem to be solved by the agents of the
next lower abstraction level. All the aggregate actions have a default
method that is obtained from the interface of the agent.
A planning problem consists of an initial state, which represents a
conjunction of literals which describe both the manufacturing system
and the raw products, and a goal, which is a conjunction of literals that
describe the transformations to be carried out by the aggregate agents
of the highest abstraction level, to obtain the manufactured products
The MIXTURE domain, which is inspired by a real factory, can be used
as an example of domain definition. Figure 1 shows a high-level diagram of
the plant. It is composed of 26 varied interconnected agents (pumps,
valves, mixers, heaters, belts, and a bottler), 46 different actions, and 2
abstractions levels. The goal in this domain consists of adding an ingredient
(flour), initially contained in ADDITIVE-1, to the milk, initially contained
in MILK-TANK, and then proceed to bottle the mixture. The milk and
an enzyme are transformed into ferment in the REACTOR. The result is
mixed with the flour to produce the final mixture. The design of a low-level
control program for such a system is difficult, even for a control engineer,
due to the number of agents and the total number of available actions.
FIGURE 1 A real-world manufacturing system, MIXTURE, that produces an elaborated mixture from
The planning process is a generative and regressive planning algorithm at
different levels of detail. Each plan at a given level of abstraction is refined into
lower-level plans, until no aggregate activities exist on the lowest abstraction
level of a hierarchical plan. At each level, the plans are generated by a POP
system, MACHINE (Castillo et al. 2001). The input to the entire HYBIS planner
is a domain description (hierarchy of agents) and a problem to be solved
(recipe at the highest abstraction level, or procedure level recipe in SP88).
A hierarchical plan H-Plan, initially only with the highest abstraction level, is
partially built, containing the set of literals that represent the problem stated
by that recipe. The inputs to the hybrid algorithm are the hierarchical
Domain, the initial abstraction Level (the highest one is 1), an initialized task
Agenda, and the initial hierarchical H-Plan. Then, it proceeds as follows:
. First, by means of a generative POP process, as in MACHINE, it obtains a
sequence of control activities to be carried out by the highest level agents.
. Second, if the sequence obtained is only composed of primitive activities,
then the problem is solved. Otherwise, the sequence is hierarchically
refined; that is, the algorithm expands every aggregate activity, according
to its agent interface and its default method or any other method specifically defined, obtaining a new lower-level problem.
. Third, the algorithm recursively proceeds to solve the new problem by
Therefore, the final plan obtained by this algorithm is a hierarchy of control sequences at different granularity levels. In a POP approach, planning is
conceived as a search through plan space where nodes denote plans (Weld
1994). A plan is represented as a tuple hA; O; Li in which A is a set of actions,
O is a set of ordering constraints over A, and L is a set of causal links. A causal
link (Tate 1976) is a data structure with three fields: Two contain pointers to
plan actions (the link’s producer, Ap , and its consumer, Ac ); the third field
is a proposition, Q , which is both an effect of Ap and a precondition of Ac .
Such a causal link is written as Ap !Q Ac and store a plan’s links in the set
L. Causal links are used to detect when a newly introduced action interferes
with a past decision. It is called a threat. There are some ways to protect against
threats but the most common are demotion and promotion. POP also
requires another data structure called agenda. This is composed of a set of
tasks, each of which describes a pending problem in the plan, or a flaw. The
different pending problems which may be found in agenda are: pending subgoals that represent unmet preconditions; threats from actions to causal links;
interferences between actions; and order inconsistencies caused by a loop in
the order structure, which must be a strict order. Flaws in the agenda are
ordered following a LIFO strategy. Unsolved subgoals are solved by an existing
action in the partial plan or by the addition of a new action. Threats are solved
by promotion or demotion of operators. Order inconsistencies can not be
solved. The search process to solve the tasks in the agenda is the depth first
engine over the set of choices to solve every task and the well known heuristic
evaluation function that accounts for the number of steps and the number of
open conditions (Gerevini and Schubert 1998; Pollack et al. 1997).
The start point is a null plan with two dummy actions start and end which
encode the planning problem P ¼ hD; I ; Gi in a domain D. The initial state I
is encoded as the addition list of the action start; the goal G is encoded as an
unsatisfied preconditions of action end. The agenda initially contains only
these pending subgoals specified in the goal G and the causal links set is
initially empty. Some changes were necessary to integrate the HTN part with
the POP approach in order to inherit all the data structures through the different levels of abstraction. When an action must be refined in a lower abstraction level, this is considered as a new flaw to be inserted in the agenda and it will
be solved by any of its decomposition methods. The heuristic function that
guides the search also needs to be adapted for integration with the HTN part.
The reader is referred to Castillo et al. (2001b) for more details on the
planning algorithm and to Weld (1994) for partial order planning.
The planner defined in the previous section is complete, but not necessarily efficient even with the domain-independent heuristics it uses. In
some situations, the heuristics do not guide the planner towards the best
search path. Learning from experience can help to identify the particular
situations for which the domain-independent heuristics need to be overridden. In this section, we propose a machine learning mechanism to improve
HYBIS efficiency based on previous experience. It generates explanations for
the local decisions made during the search process. These explanations
become control rules that are used in future situations to prune the search
space. In order to learn control knowledge for this HTN-POP planner, we
1. The planner runs on a planning problem. Then, the planning search
tree is labeled so that the successful decision nodes are identified.
2. At successful decision points, control rules are created in such a way that
were the planner to be run again on this problem, only the right
3. Constants in the control rules are generalized, so that they can be
applied to other problems involving other objects with different names.
Now, we will present each step in more detail.
As many other planners, HYBIS generates a search tree for every planning problem. There are four kinds of nodes in the search tree:
. Success: If the node belongs to a solution path, it represents one of the
actions of the control sequence that constitutes the solution of the problem.
. Failure: If it is a failed node or all its successor nodes have failed, the failure takes place when a flaw cannot be solved, i.e. there is no action to
solve a pending subgoal, there is no way of solving a threat, or the order
. Abandoned: The planner started to expand this node but the heuristics of
HYBIS preferred other nodes and it was abandoned before it failed.
. Unknown: If the planner did not expand the node when a task is removed
from the agenda, it generates as many nodes as there are ways to solve it.
Depending on the values of the heuristic function, only one node every
time is chosen to be expanded. It can happen that there are nodes whose
heuristic value is worse than the values of the remaining the nodes, so
All nodes are initially labeled as unknown. If a node fails during the
planning process, its label changes to failure. Once the planner finds a
solution, all the nodes that belong to the solution path are labeled as success
and a bottom-up recursive algorithm assigns the failure or abandoned label to
Once the search tree has been labeled, two kinds of decision points
(i.e., nodes) are considered as candidates for learning control rules:
. Failure-Success: These are nodes that have at least two children: one a
. Abandoned-Success: The same as the first, but, instead of a failure node,
Figure 2 shows an example of a labeled search tree and the decision
points where the rules are generated. For example, N6 represents a flaw
with three possible ways to solve it, which generates three nodes N7, N8,
and N9. N8 and N9 fail during the planning process and N7 belongs to
the solution path. So, it is a decision point in which one child is a successful
node (N7) and another is a failure one (N8), so a control rule is generated
(Rule-1). Another example is N21; there are three possible ways to solve the
corresponding task: N22 was never chosen to expanded (the value of the
FIGURE 2 An example of a search tree and some of its decision points. The learning algorithm would
learn from nodes N6, N19, N21, N24, and N27.
heuristic function for this node is worse than the value of other nodes) and
it remains with the initially unknown label; N23 is expanded but none of its
children fail nor become part of the solution path, so the labeled algorithm
assigns it the abandoned label; and, finally, N24 is a successful node. Therefore, this is the other type of decision point from which we learn, one child
is an abandoned node (N23), and another is a success one (N24), so Rule-3 is
At the learnable decision nodes, control rules are generated so that the
planner will select the successful node on later problem solving episodes.
More generally, control knowledge can either select a node, reject it, or prefer one over another (Veloso et al. 1995). We have focused on the most
In hybrid HTN-POP planners, there are also different types of nodes
1. HTN points: How to downward refine (i.e., which expansion method
(a) Whether to use an already existing operator or a new one to achieve
(b) In both cases, the operator to be selected.
(c) Whether to promote or demote an operator to solve a threat.
In this work, we have studied the operator selection and the downward
refine problems. Particularly, we learn the following kinds of control rules:
. SELECT OPERATOR-PLAN: To select an operator already present in the
. SELECT OPERATOR-NEW: To select a hitherto unused operator to
. SELECT EXPANSION: To select the expansion method to refine an
The kind of rule to be learned depends on what the planner did. All
control rules have a typical rule format (for instance, IF conditions THEN
consequences), but the condition part and the consequences are different
in each type of rule.2 One of the most important decisions to be made in
learning systems refers to the language to be used to represent target
concepts. In our case, the condition part is made of a conjunction of metapredicates that check for local features of the search process:
. HTN-LEVEL to know the HTN-LEVEL in which the planner is currently
. CURRENT-GOAL to identify which goal the planner is currently trying to
. SOME-CANDIDATE-GOALS to identify what other goals need to be
. OPERATOR-NOT-IN-PLAN so that a SELECT OPERATOR-NEW rule is
activated only if the operator to insert was not already present.
Similarly, SELECT OPERATOR-PLAN rules include the OPERATORIN-PLAN metapredicate to make sure the action to be reused is already
in the plan. These metapredicates also check that the arguments of the
operator verify the constraints of the agent (this information is in the
domain description). All the operators have at least one argument that
represents an agent. The rest of arguments, if they exist, must be
included in the constraints of that agent.
. TRUE-IN-STATE to test that a literal is true in the planning initial state. Actually, in order to make the control rules more general and reduce the number
of TRUE-IN-STATE meta-predicates, a goal regression is carried out as in
most EBL techniques (Dejong and Mooney 1986). Only those literals in
the initial state that are required, directly or indirectly, by the preconditions
of the operator involved in the rule are included. The regression of the preconditions is done by using the causal-link structure of the partial ordered
component of the HYBIS planner. As we said before, a causal link is represented as Ap !Q Ac , where Ap is a plan action that produces a literal Q , which
is a precondition of another plan action Ac . The algorithm to obtain the
regressed state R of an action Ai from the causal link structure L is:
Aq ¼ action which adds q according with L (ðAq !q Ai Þ 2 L)
. For example, if L ¼ fA0 !Q 1 A1 ; A0 !Q 2 A4 ; A2 !Q 3 A3 ; A1 !Q 4 A3 ; A4
!Q 5 A2 g is the set of causal links and A3 the action that was selected in
the success node we are learning from, the goal regression would
proceed as follows: Q 3 and Q 4 are the preconditions of A3 as the causal
links A2 !Q 3 A3 ; A1 !Q 4 A3 show. A2 and A1 are different from A0 so their
preconditions would be computed, Q 1 from causal link A0 !Q 1 A1 and
Q 5 from causal link A4 !Q 5 A2 , Q 1 belong to the initial state but Q 5 does
not so the precondition of A4 would be computed from causal link
A0 !Q 2 A4 . Therefore, the initial literals required would be ðQ 2 ; Q 1 Þ.
. OPERATOR-TO-EXPAND to identify which action the planner is trying
to expand in order to do a downward refine. This is only for the rules
of SELECT EXPANSION. This metapredicate also checks that the arguments of the operator verify the constraints of the agent, in the same
. COMPONENTS-OF to identify the agents of a lower level that compound
An expansion method is a set of literals representing subgoals to be
achieved for the agents of the next level of abstraction. Sometimes these
subgoals are nothing but states to be reached for the component agents
of the aggregate agent whose action is to be refined. In that case, a COMPONENTS-OF metapredicate is necessary. This led us to have two kinds
of expansion rules; one when the expansion method is a set of literals of
the type (STATE < AGENT > state) and another type for the rest of the
In summary, there are four different types of control rules: two related
to the POP points and two concerned with the HTN decision points, with a
1. POP nodes (o decisions) to use a new operator to achieve a goal:
(SOME-CANDIDATE-GOALS (list-of-subgoals))
(OPERATOR-NOT-IN-PLAN (operator arguments))
(THEN SELECT OPERATOR-NEW (operator arguments)))
Figure 3 shows an example of a rule that chooses to use a new action not in
the partial plan, (ONLINE ADDITIVE-TRANSPORT1). When the planner
is at level 1, it is working on goal (CONTAINS FLOUR RTANK)}, there is
at least one of the pending subgoals that appears as an argument of the
metapredicate SOME-CANDIDATE-GOALS, and in the initial state the
literals (CONTAINS FLOUR ADDITIVE1) and (STATE ADDITIVETRANSPORT1 OFF) are true.
2. POP nodes to use an operator already existing in the partial plan to
(SOME-CANDIDATE-GOALS (List-of-subgoals))
(THEN SELECT OPERATOR-PLAN (operator arguments)))
Figure 4 shows an example of a rule that chooses to reuse the action START
already in the partial plan when the planner is at level 1. It is working on
goal (STATE REACTOR READY), there is at least one of the pending
subgoals that appears as argument of the metapredicate SOME-CANDIDATE-GOALS, and, in the initial state, the literal (STATE REACTOR
3. HTN nodes to select an expansion method that is a set of literals representing the state of several lower-level agents:
(OPERATOR-TO-EXPAND (operator arguments))
(COMPONENTS-OF (agent-agg (agent1 . . . agentr)))))
(THEN SELECT EXPANSION ((STATE agent1 ESTADO1)
Figure 5 shows an example of a rule that chooses to use the expansion
method ((STATE HEAT1 OFF) (STATES ST1 OFF) (STATES SV OFF).
When the planner decides to work on the downward refinement of the
action (OFF REACTOR ?RESULT171), the compounded agents of REACTOR are (HEAT1 ST1 SV), and, in the initial state, the literals that appear
as arguments of the metapredicate TRUE-IN-STATE are true. This rule
would force the HTN component of the planner to refine the (OFF REACTOR ?RESULT171) into ((STATE HEAT1 OFF) (STATE ST1 OFF) (STATE
SV OFF)) at the next level of abstraction.
4. HTN nodes to select an expansion method different from the previous
(OPERATOR-TO-EXPAND (operator arguments))
Figure 6 shows an example of a rule that chooses to use the expansion
method (CONTAINS FERMENT REACTOR) when the planner decides to
work on the downward refinement of the action (REACT REACTOR FERMENT), and, in the initial state, the literal (STATE REACTOR READY) is true.
To avoid rules that depend on the names of objects or agents of the
particular planning problem used for learning, constants are generalized
into variables that belong to the same type as the constants. Every variable
can only match with a certain kind of object a type that is coded as a prefix
in the variable name (what appears before the mark %%). For example, if
during the planning process a constant REACTOR appears, it would be
generalized into < REACTOR-MIXTURE %%REACTOR > because
REACTOR is defined in the domain as an agent of type REACTORMIXTURE. In fact, only constants that represent agents of the domain
are typed. The remaining planning constants (like products, chemicals or
interconnection points between agents) are generalized without the type
part. Typing preserves semantics and makes the matching process more
Actually, not all constants are parameterized as explained. In some
cases, it makes no sense to generalize them. For instance, let us consider
the literal (STATE REACTOR READY). REACTOR is a good candidate
for parameterization, but READY is not, because, in that case, the meaning
that a reactor object is ready would be lost. Currently, we do not generalize
the second argument of STATE predicates, because they usually have this
The SELECT EXPANSION rules cannot be completely generalized
either. In the metapredicate OPERATOR-TO-EXPAND, only the first argument of the operator, i.e., the agent, is generalized because the rest of the
arguments influence the selection of the correct expansion method. For
example, there are two possible ways to expand the operator REACT,
depending on if it is (REACT REACTOR FERMENT) or (REACT REACTOR MIXTURE), so we only generalize REACTOR. In other words, the
control rule might be generalized for any REACTOR but not for every product to be generated.
This generalization scheme is very planner dependent, but it is domain
independent. The example 1 rule of Figure 3 would be generalized as:
FIGURE 7 Control rule generalization example.
The learned control rules are used in future planning episodes in the
following way. The planning algorithm cycle starts by removing a task from
the agenda and then finds the possible ways to solve it, generating a list of
choices that will become new nodes of the search tree. Depending on the
type of task, the choices can be: existing actions in the partial plan or new
actions (if the task is to achieve a subgoal); demotion or promotion actions
(if the task is to solve a threat); or expansion methods (if the task is an
action downward refine). Then all the control rules are matched against
the current metastate of the search, generating another list of choices by
means of the consequences (RHS) of the matched rules. The intersection
of both lists will constitute the possible ways to solve the task and the new
nodes to add to the search tree. In case that the intersection is empty, the
first list is used (no control rule matched or they selected an incorrect
choice). A rule matches when all its preconditions (LHS) are fulfilled.
Actually, a control rule can be fulfilled in many different ways. The matching algorithm generates all of them. Each possible matching is represented
by a list of bindings (i.e., lists of pairs variable=value). The list of bindings
are incrementally generated. This is a straightforward implementation of an
unification procedure. For example, the metapredicate (TRUE-IN-STATE
(STATE < VALVE-FWD%%SV> OFF)), solving a problem whose initial state
are the literals: ((STATE SV OFF), (STATE SV1 OFF), (STATE HEAT1 OFF)),
would generate two pairs of bindings: (< VALVE-FWD%%SV> .SV) and
(VALVE-FWD%%SV> .SV1), assuming that SV and SV1 are of type VALVEFWD. If the precondition has another metapredicate, for example, (TRUEIN-STATE (STATE < HEATER-3PROD%%HEAT1> OFF)), a new binding
(< HEATER-3PROD%%HEAT1> .HEAT1) would be added (if HEAT1 is of
type HEATER-3PROD) and the list of bindings would become: (((< VALVEFWD%%SV> .SV), (< HEATER-3PROD> .HEAT1)) ((VALVE-FWD%%
SV> .SV1), (< HEATER-3PROD> .HEAT1))), which represents two possible
The experimentation has been divided in two phases. In the first one,
we have used our approach in several domains to test its validity. Basically,
we wanted to check whether the rules were correct and save resources in
the planning process. If the preconditions of a rule are too general, the
rule can be triggered at a wrong point and the planner may make the
wrong decision. Because the rule discards the alternatives not selected,
the correct one might be pruned from the search tree, and the planner will
never find the solution. This is especially relevant in manufacturing
domains where there are several agents belonging to the same type with
the same named actions. Thus, it is very important to determine whether
the learning process is producing correct rules. If the preconditions are
too specific, they might work for just a very small set of problems.
In the second phase, we have checked that the control rules generalize
and improve efficiency to unseen planning problems in a similar domain,
i.e., industrial plants that have a different number of agents of the same
type as in the original plant. We have also studied the utility of the generated rules.
The first experiments have been carried out in different domains with a
particular problem in each one. The domains are composed of interconnected devices (agents), tanks, pumps, valves, mixers, heaters, belts, bottler
etc., similar to the ones explained earlier (MIXTURE problem). There is
one defined problem for each domain consisting of a high-level goal that
specifies a process to be performed. For example, the ITOPS problem
(Viswanathan et al. 1998) consists of five raw products, r1 , r2 , r3 , r4 , and r5 ,
initially in five different tanks. The goal is to obtain a final product, i4 , by
allowing a reaction to occur among them, according to the following
scheme: mix r1 , r2 , and r3 to obtain i1 ; filter i1 to obtain i3 ; and heat r4 , r5 ,
and i3 to obtain i4 . The other problems are of the same type. A complete
description can be found in Fdez-Olivares (2001).3 From the viewpoint of
planning, it is enough to consider that the domains have different characteristics such as the number of hierarchical levels, number of agents, and number of total actions. And the problems differ in the number of literals in the
initial state and goals. The characteristics of the domains and the problems
TABLE 2 Results of the Execution of HYBIS With and Without Rules
are shown in Table 1. It displays the number of agents (Agents), the number
of levels (Levels), the total number of actions (Actions), the number of literals in the initial states (Inits), and the number of goals (Goals).
Table 2 shows the results of running the planner with and without rules
in a problem per domain. We obtained all the control rules, HTN and POP,
for all the levels by running the planner with one problem and one
domain. Then we ran the planning process again with the same problem
twice, once using the rules learned before, and the other without the rules,
and we compared the results. It displays the number of nodes generated for
the planner process, the time (in seconds) until it finds the solution, the
savings (%) in time to solve due to the use of rules, the total matching time
of the rules (in seconds), and the number of used rules.
It can be observed that the control rules are correct, because they do
not restrain the planner from finding the solution. Also, nodes and time
decrease when the rules are used. Finally, the planner found the same solutions with and without rules in all the domains, so there was no variation in
Generalization of Rules to Unseen Problems
In order to test the generalization power of the algorithm, we have performed experiments in the MIXTURE domain shown in Figure 1. It represents a real manufacturing plant of a dairy product industry that has
collaborated in the accomplishment of this project.4 We considered that
this domain is sufficiently complex and significant to verify the total validity
of our approach. The goal is to test whether the control rules are also useful
if the industrial plant changes (by adding new agents, for instance). We
generated all the control rules by running the planner with the problem
described earlier, obtaining 43 rules. Afterward, we modified the domain
by adding more agents to check the generality of the rules. The new
. MIXTURE_2B: A second bottler is added to solve the same problem, but
the first bottler is not ready to work (in the initial state, one of the preconditions of the operator who activates the bottler is missing). The planner
does not detect its nonoperation until much later in the search tree, so it
expands a nonuseful part of the search tree, therefore needing more time
to find the solution. If the second bottler would be ready to work, the
planner would find the same solution and we wouldn’t test the generalization.
. MIXTURE_3B: Two more bottlers are added to solve the same problem,
. MIXTURE_4B: Three more bottlers are added to solve the same problem
. MIXTURE_2B2R: A second bottler and a second reactor are added. The
problem consists of bottling mixtures obtained in each reactor.
. MIXTURE_3B2R: Same as the previous, but with one more bottler.
We also generated all the control rules by running the planner with the
MIXTURE_2B problem obtaining another 43 rules. We ran the planning
process with each problem three times: first, using the rules learned from
the original problem (rules_1), second, using the MIXTURE_2B rules
No control knowledge was extracted from the other problems due to
memory space limitations. The learning algorithm explores the search tree,
generated during problem resolution, after solving the problem. It then
finds the right decision points in the solution path of the tree to generate
the control rules. Therefore, the training problems must be of small size to
guarantee that the planner will find a solution and that it will be able to
keep in memory the search tree. The planner HYBIS offers the posibility
to solve a problem by either keeping the history of the resolution process
(the search tree) or without keeping it. In order to solve the last and more
complex problems, the history option was disabled to avoid failures of allocating memory because of a maximum address space limitation. Also,
according to our previous experience on learning for other planners
and domains, rules learned from complex problems will be less general.
TABLE 3 Results of the Execution of Rules to Unseen Problems
TABLE 4 Total Matching Time of Rules in the Different Domains
A control rule is generated by extracting the metastate, and performing a
goal regression for finding which literals from the initial state were needed.
So, the simpler the training problem is, the simpler the metastate will be
(fewer literals in the regressed state, and, therefore, fewer conditions in
the control rule). Then, the control rules obtained will be simpler and
more general. We plan to use inductive techniques to correct the control-rules, as it has been done for other planners (Aler et al. 2002; Borrajo
Table 3 shows these results with the two sets of rules. It displays the
number of nodes generated for the planning process, the time (in seconds)
until it finds the solution, and the savings (%) in time to find a solution
It can be observed that nodes and time decrease when the rules are
used. The average saved time due to the use of rules is 36%. We have also
measured the total matching time of the rules and it appears to be not too
high, as Table 4 shows. Column TIME represents the matching time in seconds, and column %TOTAL represents the % with respect the total search
time. So, it seems there is no big utility problem in this case.
The results using the first rules are better than the results using the
other ones in all the domains. This shows the convenience of training with
simple problems and then applying the learned knowledge to more
Manufacturing systems are evolving into a new generation of systems driven by the demands of a constantly changing market and they must adapt to
their changes in a timely fashion. AI planning techniques have been very
useful to satisfy these needs, as have special HTN planners where the hierarchical semantics of this kind of planning gives us the ability to model planning problems in domains that are naturally hierarchical, such as industry
environments. HYBIS is a hybrid hierarchical planner that provides a default
method to step from one level to another. This plan refinement requires to
solve a new planning problem, which is performed by a partial order planner (POP). However, although using a hierarchy limits the computational
complexity, the process is still inefficient. Moreover, in a hybrid planner like
HYBIS, efficiency can be gained both at HTN and POP decision points. Given
that machine learning techniques have been successfully used in older planners to improve the search process by means of previous experience, we
have extended them for learning planning heuristics for HTN-POP
The concept of domain in HYBIS differs from other planners. For HYBIS,
a domain is an industrial plant, designed specifically to solve a single problem; that is to say, to generate a manufactured product from initial raw
materials. However, at some times the plant might undergo a decision
modification so that new (but similar) problems can be solved. An example
of modification is adding a new bottler, having a broken machine, etc. In
that case, a new plan for the plant would have to be found and time would
be wasted due to inefficiencies of the planner. Although the modification
might be minimal, the planning process must begin again from scratch.
If learned control rules in the first execution are obtained and these are
correct, this second planning episode will be solved much more efficiently.
In this paper, we discuss some of the issues on machine learning
applied to these types of planners. We have extended some machine learning ideas to deal with hybrid HTN-POP planners. In particular, we have
focused in a decision point where the planner has to decide whether to
apply an operator already in the plan, or not, and in any case, which operator to apply. We have also studied hierarchical refinement. Each task can
be decomposed into several subtasks using predefined methods. Each possible decomposition represents a new branch in the search space of the
problem and control rules can be learned to prune the unsuccessful
branches. We have measured the effectiveness of the rules in terms of time
and nodes expanded. It has been found that the rules generated are correct and useful. They improve the efficiency of the planner by saving
resources (time and memory) during the search process. We have verified
the convenience of acquiring the control knowledge from small problems
to use it for solving more complex ones later. In the case of having an unsolved
problem (for the planner), a strategy for solving it would consist of defining a
relaxed problem with which to train the system, for instance, one in which we
define fewer agents or machines in the plant. Then, we allow the learning system to acquire some control rules. Finally, we try to solve the original problem
using the learned control rules in the simpler problem.
In the future, we plan to extend the learning scheme so that control
rules can be inductively corrected by specializing, generalizing, or modifying them. Here, we want to follow previous approaches that have been
successful for other planners (in particular, HAMLET [Borrajo and Veloso
1997] and EVOCK [Aler et al. 2002]. Also, HYBIS has been extended to be able
to generate conditional plans, which offers new learning opportunities.
Aler, R., D. Borrajo, and P. Isasi. 2002. Using genetic programming to learn and improve control knowledge. Artificial Intelligence 141(1–2):29–56.
Algeo, M. E. A., J. E. Fowler, and K. K. Jurrens. 1996. Computerized representations of manufacturing
resources: validation and standardization efforts. In: Proceedings of the Japan–USA Symposium on Flexible Automation, volume 1, eds. K. Stelson and F. Oba, pages 699–702. New York, NY, USA. ASME.
ANSI=ISA, 1995. Batch Control Part I, Models & Terminology (S88.01).
Aylett, R., G. Petley, P. Chung, J. Soutter, and A. Rushton. 1997. Planning and chemical plan operation
procedure synthesis: a case study. In Proceedings in Fourth European Conference on Planning, pages 41–53.
Bacchus, F. and F. Kabanza. 2000. Using temporal logics to express search control knowledge for planning. Artificial Intelligence 116:123–191.
Becker, L. and C. Pereira. 2002. SIMOO-RT an object-oriented framework for the development of realtime industrial automation systems. IEEE Transactions on Robotics and Automation 18(4):421–30.
Bezirgan, A. 1992. An application of case-based expert system technology to dynamic job-shop scheduling. In Proceedings of Expert Systems 92, The Twelfth Annual Technical Conference of the British Computer
Society, ed. R. M. A. Bramer, pages 225–235.
Borrajo, D. and M. M. Veloso. 1997. Lazy incremental learning of control knowledge for efficiently
obtaining quality plans. AI Review Journal. Special Issue on Lazy Learning 11(1–5):371–405.
Braatz, A. 2003. Development of a UML-based function block-model for object-oriented control-design.
Automatisierungstechnische Praxis 45(6):38–44.
Braccesi, L., M. Monsignori, and P. Nesi. 2004. Monitoring and optimizing industrial production processes. In Proceedings of the Ninth IEEE International Conference on Engineering of Complex Computer
Breslow, L. and D. W. Aha. 1997. NaCoDAE: Navy conversational decision aids environment. Technical
Report AIC-97-018, Washington, D.C.: Naval Research Laboratory, Navy Center for Applied
Burke, E. K., P. Cowling, J. D. Landa Silva, and S. Petrovic. 2001. Combining hybrid metaheuristics and
populations for the multiobjective optimisation of space allocation problems. In Proceedings of the
Genetic and Evolutionary Computation Conference, (GECCO’2001), eds. L. Spector, E. D. Goodman,
A. Wu, W. B. Langdon, H.–M. Voigt, M. Gen, S. Sen, M. Dorigo, S. Pezeshk, M. H. Garzon, and
E. Burke, pages 1252–1259, San Francisco, California, USA. Morgan Kaufmann Publishers.
Burke, E. K., S. Petrovic, and R. Qu. 2002. Case-based heuristic selection for examination timetabling. In
4th Asia-Pacific Conference on Simulated Evolution and Learning (SEAL’02). Nayang Technology
Carbonell, J. G., J. Blythe, O. Etzioni, Y. Gil, R. Joseph, D. Kahn, C. Knoblock, S. Minton, A. Pérez, S.
Reilly, M. M. Veloso, and X. Wang. 1992. PRODIGY4.0: The manual and tutorial. Technical Report
CMU-CS-92-150, Department of Computer Science, Carnegie Mellon University.
Castillo, L., J. Fdez-Olivares, and A. Gonzalez. 2000a. Automatic generation of control sequences for
manufacturing systems based on nonlinear planning techniques. Artificial Intelligence in Engineering
Castillo, L., J. Fdez-Olivares, and A. Gonzalez. 2000b. A hybrid hierarchical=operator-based planning
approach for the design of control programs. In ECAI Workshop on Planning and configuration:
New Results in Planning, Scheduling and Design.
Castillo, L., J. Fernandez-Olivares, and A. Gonzalez. 2001. Mixing expressiveness and efficiency in a
manufacturing planner. Journal of Experimental and Theoretical Artificial Intelligence 13:141–162.
Cowling, P., G. Kedall, and E. Soubeiga. 2000. A hyperheuristic approach to scheduling a sales summit.
In Selected Papers of the Third International Conference PATAT 2000, Lecture Notes in Computer
Science, pages 176–190, Konstanz, Germany. Springer.
Cunningham P. and B. Smyth. 1997. Case based reasoning in scheduling: Reusing solution components.
International Journal of Production Research 35:2947–2961.
DeJong G. and R. J. Mooney. 1986. Explanation-based learning: An alternative view. Machine Learning
Erol, K., J. Hendler, and D. Nau. 1994. UMCP: A sound and complete procedure for hierarchical tasknetwork planning. In Artificial Intelligence Planning Systems, 249–254.
Estlin, T. A. and R. J. Mooney. 1997. Learning to improve both efficiency and quality of planning. In
Proceedings of the 15th International Joint Conference on Artificial Intelligence (IJCAI-97), ed. M. Pollack,
Fabian M., B. Lennartson, P. Gullander, S. Andreasson, and A. Adlemo. 1997. Integrating process planning and control for flexible production systems. In Proceedings of ECC’97, Brussels, Belgium, July
a tico de programas de control industrial. Ph.D.
thesis, Universidad de Granada. E.T.S de Ingenierı́a Informatica. Departamento de Ciencias de la
Garland, A., K. Ryall, and C. Rich. 2001. Learning hierarchical task models by defining and refining
examples. In First International Conference on Knowledge Capture.
Gerevini A. and L. Schubert. 1998. Inferring state constraints for domain-independent planning. In Proceedings of the Fifteenth National Conference on Artificial Intelligence and the Eighth Innovative Applications
of Artificial Intelligence Conference, pages 905–912, Menlo Park, California, USA. AAAI Press.
Gil, Y. 1991. A specification of manufacturing processes for planning. Technical Report CMU-CS-91-179,
School of Computer Science, Carnegie Mellon University, Pittsburgh, Pennsylvania, USA, August 1991.
Giunchiglia, F. 1999. Using abstrips abstractions: Where do we stand? Artificial Intelligence Review 13:201–
Guang-hong, H. and L. Zu-shu. 2003. Intelligent control system based on industrial PC for middling and
small coal boiler. Control Engineering China 10(4):339–363.
Han, L., G. Kendall, and P. Cowling. 2002. An adaptive length chromosome hyperheuristic genetic
algorithm for a trainer scheduling problem. Proceedings of the 4th Asia-Pacific Conference on Simulated
Haskose, A., B. Kingsman, and D. Worthington. 2004. Performance analysis of make-to-order manufacturing systems under different workload control regimes. International Journal of Production Economics 90(2):169–86.
Heinonen, J. and F. Pettersson. 2003. Scheduling a specific type of batch process with evolutionary computation. In Proceedings of the Congress on Evolutionary Computation, volume 2, pages 966–970,
Huang, Y., B. Selman, and H. Kautz. 2000. Learning declarative control rules for constraint-based planning. In Proceedings of the Seventeenth International Conference on Machine Learning (ICML’00), ed.
P. Langley, Stanford, California, USA, June–July 2000.
noz-Avila, and D. Aha. 2002. Camel: Learning method preconditions for
HTN planning. In Proceedings of the Sixth International Conference on Artificial Intelligence Planning Systems (AIPS-02) eds. M. Ghallab, J. Hertzberg, and P. Traverso, pages 131–141, Toulouse, France,
Kambhampati, S. 2000. Planning graph as a (dynamic) CSP: Exploiting EBL, DDB and other CSP search
techniques in graphplan. Journal of Artificial Intelligence Research 12:1–34.
Kaparthi, S., N. Suresh, and R. Cervaney. 1993. An improved neural network leader algorithm for partmachine grouping in group technology. European Journal of Operational Research 69(3):342–356.
Kiritsis, D., P. Xirouchakis, and C. Gunther. 1998. Petri net representation for the process specification
language. 1. manufacture process planning. In First International Workshop on Intelligent Manufacturing Systems (IMS-Europe 1998), pages 595–608, Ecole Polytech, Federale de Lausanne, Lausanne,
Klein, I., P. Jonsson, and C. Backstrom. 1998. Efficient planning for a miniature assembly line. Artificial
Laird, J. E., A. Newell, and P. S. Rosenbloom. 1987. Soar: An architecture for general intelligence. Artificial Intelligence 33(1):1–64.
Liu, B., W. Hsu, and Y. Ma. 1998. Integrating classification and association rule mining. In Knowledge
Liu, B., W. Hsu, Y. Ma, and S. Chen. 1999. Discovering interesting knowledge using DM-II. In Proceedings
of the ACM SIGKDD International Conference on Knowledge Discovery and Data Mining (KDD-99), San
Luger, G., ed. 1998. AAAI Special Interest Group in Manufacturing Workshop: Artificial Intelligence and Manufacturing, State of the Art and Sate of the Practice. AAAI Press.
Maropoulos, P. 1995. Review of research in tooling technology, process modelling and process planning.
Part II: Process planning. Int. Journal of Computer Integrated Manufacturing Systems 8(1):13–20.
McCluskey, T., D. Liu, and R. Simpson. 2003. GIPO II: HTN Planning in a Tool-Supported Knowledge
Engineering Environment. In Proceedings of the Thirteenth International Conference on Automated Planning and Scheduling (ICAPS-03), eds. N. M. Enrico Giunchiglia and D. Nau, pages 92–101, Trento,
McCluskey, T., N. Richardson, and R. Simpson. 2002. An interactive method for inducing operator
descriptions. In Proceedings of the Sixth International Conference on Artificial Intelligence Planning Systems
(AIPS-02), eds. M. Ghallab, J. Hertzberg, and P. Traverso, Toulouse, France, April 23–17, 2002.
Minton, S. 1988. Learning Effective Search Control Knowledge: An Explanation-Based Approach. Ph.D. thesis,
Computer Science Department, Carnegie Mellon University. Available as technical report CMUCS-88-133.
noz-Avila, H., D. W. Aha, L. A. Breslow, R. Weber, and D. Nau. 1999. HICAP: An interactive casebased planning architecture and its application to noncombatant evacuation operations. In Proceedings of the Ninth Conference on Innovative Applications of Artificial Intelligence, pages 879–885, 1999.
noz-Avila, H. and J. Huellen. 1995. Retrieving cases in structured domains by using goal dependencies. In Proceedings of the 1st International Conference on Case-Based Reasoning Research and Development,
volume 1010 of LNAI, eds. M. Veloso and A. Aamodt, pages 241–252, Berlin, Germany 1995.
Nareyek, A. 2001. Choosing search heuristics by non-stationary reinforcement learning.
noz-Avila. 1999. SHOP: simple hierarchical ordered planner. In
Proceedings of the IJCAI-99, pages 968–973, Stockholm, Sweden, August 1999.
Nau, D., S. Gupta, and W. C. Regli. 1995. AI planning versus manufacturing-operation planning: A case
study. In Proceedings of IJCAI-95, pages 1670–1676, 1995.
Park, S., M. Gervasio, M. Shaw, and G. D. Jong. 1993. Explanation-based learning for intelligent process
planning. IEEE Transactions on Systems Man and Cybernetics 23(6):1597–1616.
Peschke, J. 2003. Real-time Java for industrial controls in flexible manufacturing systems. In Proceedings of
IEEE International Conference on Industrial Informatics (IEEE Cat. No.03EX768), pages 325–331.
Pollack, M. E., D. Joslin, and M. Paolucci. 1997. Flaw selection strategies for partial-order planning.
Journal of Artificial Intelligence Research 6:223–262.
Sacerdoti, E. D. 1974. Planning in a hierarchy of abstraction spaces. Artificial Intelligence. 5:115–135.
Simpson, R., T. McCluskey, W. Zhao, R. Aylett, and C. Doniat. 2001. GIPO: An integrated graphical tool
to support knowledge engineering in AI Planning. In Proceedings of the 6th European Conference on
Szelke, E. and G. Markus. 1997. A learning reactive scheduler using CBR. Computer Industry 33:31–36.
Tate, A. 1976. Project planning using a hierarchic non-linear planner. Research Report 25, Department
of Artificial Intelligence, University of Edinburgh, Edinburgh, Scotland, 1976.
Valyov, M., C. Potts, and V. Strusevich. 2004. Batching decisions for assembly production systems. European Journal of Operational Research 157(3):620–642.
van Lent, M. and J. Laird. 1999. Learning hierarchical performance knowledge by observation. In
Proceedings of the 16th International Conference on Machine Learning, pages 229–238, San Francisco,
Veloso, M. M. 1994. Planning and Learning by Analogical Reasoning. Springer Verlag.
Veloso, M. M., J. Carbonell, A. Pérez, D. Borrajo, E. Fink, and J. Blythe. 1995. Integrating planning and
learning: The PRODIGY architecture. Journal of Experimental and Theoretical AI 7:81–120.
Viswanathan, S., C. Johnsson, R. Srinivasan, V. Venkatasubramanian, and K.-E. Arzen. 1998. Procedure
synthesis for batch processes: Part I: knowledge representation and planning framework. Computers
Vrakas, D., G. Tsoumakas, N. Bassiliades, and I. Vlahavas. 2003. Learning rules for adaptive planning. In
Proceedings of the Thirteenth International Conference on Automated Planning and Scheduling (ICAPS–03),
eds. N. M. Enrico Giunchiglia and D. Nau, pages 82–91, Trento, Italy, June 2003. AAAI Press.
Weld, D. S. 1994. An introduction to least commitment planning. AI Magazine 15(4):27–61.
Zimmerman, T. and S. Kambhampati. 2003. Learning-assisted automated planning: Looking back,
taking stock, going forward. AI Magazine 24(2):73–96.
1. Selecting a node means also rejecting any other alternative, according to the semantics of our control
2. This is based on the meta-predicates defined by the PRODIGY planner (Carbonell et al. 1992).
3. The problems and domains are available by request for the purpose of scientific comparison.
4. The company is PULEVA and the project TIC2001-4936-E from the Ministerio de Ciencia y Tecnologia in
